{"doc_id": 0, "doc": {"video_name": "needle_32.mp4", "duration": 467.98, "question": "What does the hand coming out of the computer do?\n(A) Delivers a product\n(B) Shakes the woman's hand\n(C) Takes the woman's credit card\n(D) Points at something on the screen\n", "candidates": ["Delivers a product", "Shakes the woman's hand", "Takes the woman's credit card", "Points at something on the screen"], "answer": "A", "task_type": "needle", "question_id": "Q0"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9d675c091c2f81fa81f69d58737fda4152a8fda711953240ad5cc8fb09eacee2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What does the hand coming out of the computer do?\n(A) Delivers a product\n(B) Shakes the woman's hand\n(C) Takes the woman's credit card\n(D) Points at something on the screen\n", "task_type": "needle", "pred_answer": "D", "answer": "A"}, "input": "What does the hand coming out of the computer do?\n(A) Delivers a product\n(B) Shakes the woman's hand\n(C) Takes the woman's credit card\n(D) Points at something on the screen\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 4, "doc": {"video_name": "needle_119.mp4", "duration": 766.14, "question": "Where is the basketball court located in the video?\n(A) In a school\n(B) In a park\n(C) In a gym\n(D) On a cruise ship\n", "candidates": ["In a school", "In a park", "In a gym", "On a cruise ship"], "answer": "D", "task_type": "needle", "question_id": "Q4"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fc6b9d659b8f53a9a085b0d7e673c7cc68113b628f2c99b219611e233ea3f4bd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where is the basketball court located in the video?\n(A) In a school\n(B) In a park\n(C) In a gym\n(D) On a cruise ship\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "Where is the basketball court located in the video?\n(A) In a school\n(B) In a park\n(C) In a gym\n(D) On a cruise ship\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 8, "doc": {"video_name": "needle_104.mp4", "duration": 635.6, "question": "What is the obstacle in the middle of the road as the car with a broken windshield rides through the mountain country village?\n(A) A flock of sheep\n(B) A fallen tree\n(C) A group of people\n(D) A cow\n", "candidates": ["A flock of sheep", "A fallen tree", "A group of people", "A cow"], "answer": "D", "task_type": "needle", "question_id": "Q8"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fe20f4ec829f532fc3a10f860db50b9824df2c3a50b86bf6122e61a6ed4d16d7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the obstacle in the middle of the road as the car with a broken windshield rides through the mountain country village?\n(A) A flock of sheep\n(B) A fallen tree\n(C) A group of people\n(D) A cow\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "What is the obstacle in the middle of the road as the car with a broken windshield rides through the mountain country village?\n(A) A flock of sheep\n(B) A fallen tree\n(C) A group of people\n(D) A cow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 12, "doc": {"video_name": "needle_20.mp4", "duration": 458.58, "question": "From whose perspective is the scene of the car with a broken windshield riding through the mountain country village viewed?\n(A) Pedestrian's view\n(B) Driver's view\n(C) Bird's eye view\n(D) Backseat passenger's view\n", "candidates": ["Pedestrian's view", "Driver's view", "Bird's eye view", "Backseat passenger's view"], "answer": "D", "task_type": "needle", "question_id": "Q12"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "86759f2c024afef0139c9a0b4e7f4bdd4941c0c8287236bbd036bc93ef1700b1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "From whose perspective is the scene of the car with a broken windshield riding through the mountain country village viewed?\n(A) Pedestrian's view\n(B) Driver's view\n(C) Bird's eye view\n(D) Backseat passenger's view\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "From whose perspective is the scene of the car with a broken windshield riding through the mountain country village viewed?\n(A) Pedestrian's view\n(B) Driver's view\n(C) Bird's eye view\n(D) Backseat passenger's view\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 16, "doc": {"video_name": "needle_20.mp4", "duration": 458.58, "question": "What is the condition of the car's windshield as it rides through the mountain country village?\n(A) The windshield is foggy\n(B) The windshield is covered in snow\n(C) The windshield is broken\n(D) The windshield is clean\n", "candidates": ["The windshield is foggy", "The windshield is covered in snow", "The windshield is broken", "The windshield is clean"], "answer": "C", "task_type": "needle", "question_id": "Q16"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9e2222e86d05928302c75172d1e833ee2af8663909e03789adbe5430bbb8de0d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the condition of the car's windshield as it rides through the mountain country village?\n(A) The windshield is foggy\n(B) The windshield is covered in snow\n(C) The windshield is broken\n(D) The windshield is clean\n", "task_type": "needle", "pred_answer": "D", "answer": "C"}, "input": "What is the condition of the car's windshield as it rides through the mountain country village?\n(A) The windshield is foggy\n(B) The windshield is covered in snow\n(C) The windshield is broken\n(D) The windshield is clean\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 20, "doc": {"video_name": "needle_27.mp4", "duration": 469.20000000000005, "question": "What is attached to the ears of the black cows eating hay at the farm outdoors on a summer sunny day?\n(A) Blue tags\n(B) Red tags\n(C) Yellow tags\n(D) Nothing\n", "candidates": ["Blue tags", "Red tags", "Yellow tags", "Nothing"], "answer": "C", "task_type": "needle", "question_id": "Q20"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "35acbe1a2c1dea5e47a126e9bcdd5669a2da291962f0bbb6479fc9d588caea8c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is attached to the ears of the black cows eating hay at the farm outdoors on a summer sunny day?\n(A) Blue tags\n(B) Red tags\n(C) Yellow tags\n(D) Nothing\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is attached to the ears of the black cows eating hay at the farm outdoors on a summer sunny day?\n(A) Blue tags\n(B) Red tags\n(C) Yellow tags\n(D) Nothing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 24, "doc": {"video_name": "needle_18.mp4", "duration": 494.0, "question": "Where are the people tourists walking?\n(A) In a city\n(B) In a forest\n(C) On the embankment in a fishing village\n(D) On a beach\n", "candidates": ["In a city", "In a forest", "On the embankment in a fishing village", "On a beach"], "answer": "C", "task_type": "needle", "question_id": "Q24"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a47dee4d4c11996895b3845f7f0af7301dd23a3aaccd2487b51e9383d8d69d61", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where are the people tourists walking?\n(A) In a city\n(B) In a forest\n(C) On the embankment in a fishing village\n(D) On a beach\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "Where are the people tourists walking?\n(A) In a city\n(B) In a forest\n(C) On the embankment in a fishing village\n(D) On a beach\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 28, "doc": {"video_name": "needle_4.mp4", "duration": 510.0, "question": "What is the man in the black silhouette wearing on the lake shore?\n(A) A hat\n(B) A hood\n(C) A suit\n(D) A swimsuit\n", "candidates": ["A hat", "A hood", "A suit", "A swimsuit"], "answer": "B", "task_type": "needle", "question_id": "Q28"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bdd5277743d4de07a23b2e6406f375472a30a8f991ef4cb1ef6f5075fd701e4a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the man in the black silhouette wearing on the lake shore?\n(A) A hat\n(B) A hood\n(C) A suit\n(D) A swimsuit\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the man in the black silhouette wearing on the lake shore?\n(A) A hat\n(B) A hood\n(C) A suit\n(D) A swimsuit\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 32, "doc": {"video_name": "needle_4.mp4", "duration": 510.0, "question": "What is the man in the black silhouette holding on the lake shore?\n(A) A fishing rod\n(B) A book\n(C) A controller\n(D) A beach ball\n", "candidates": ["A fishing rod", "A book", "A controller", "A beach ball"], "answer": "C", "task_type": "needle", "question_id": "Q32"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5fe78297322d028383f8a957487c1e44dbd8bb6c81ec7ae06e5b98dc13e584d9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the man in the black silhouette holding on the lake shore?\n(A) A fishing rod\n(B) A book\n(C) A controller\n(D) A beach ball\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the man in the black silhouette holding on the lake shore?\n(A) A fishing rod\n(B) A book\n(C) A controller\n(D) A beach ball\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 36, "doc": {"video_name": "needle_3.mp4", "duration": 496.21999999999997, "question": "What is the gender of the patient the young attractive hispanic medical doctor is discussing health issues with?\n(A) Not specified\n(B) Female\n(C) Male\n(D) Both male and female\n", "candidates": ["Not specified", "Female", "Male", "Both male and female"], "answer": "B", "task_type": "needle", "question_id": "Q36"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7f9aefada6d5af4740b260336515c50901d4b7c67174987e9a95bc0f4afe9a78", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the gender of the patient the young attractive hispanic medical doctor is discussing health issues with?\n(A) Not specified\n(B) Female\n(C) Male\n(D) Both male and female\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the gender of the patient the young attractive hispanic medical doctor is discussing health issues with?\n(A) Not specified\n(B) Female\n(C) Male\n(D) Both male and female\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 40, "doc": {"video_name": "needle_138.mp4", "duration": 762.02, "question": "What are the two young women doing on the bench in the street?\n(A) Sleeping\n(B) Running\n(C) Eating\n(D) Communicating\n", "candidates": ["Sleeping", "Running", "Eating", "Communicating"], "answer": "D", "task_type": "needle", "question_id": "Q40"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D. Communicating"]], "filtered_resps": ["D. Communicating"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "751eeeaf47c2ba35d01fcb4108ff35fe88716ecb8df9f6a4ed124080f2ed1c31", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What are the two young women doing on the bench in the street?\n(A) Sleeping\n(B) Running\n(C) Eating\n(D) Communicating\n", "task_type": "needle", "pred_answer": "D. Communicating", "answer": "D"}, "input": "What are the two young women doing on the bench in the street?\n(A) Sleeping\n(B) Running\n(C) Eating\n(D) Communicating\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 44, "doc": {"video_name": "needle_50.mp4", "duration": 458.64, "question": "What nationality are the kids having fun in the paddy field?\n(A) Indian\n(B) Chinese\n(C) American\n(D) Malays\n", "candidates": ["Indian", "Chinese", "American", "Malays"], "answer": "D", "task_type": "needle", "question_id": "Q44"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1d2ce02ad99f7d4c76cf0e13045cd4ad252310faf281586a70b547d73bee9e2d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What nationality are the kids having fun in the paddy field?\n(A) Indian\n(B) Chinese\n(C) American\n(D) Malays\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What nationality are the kids having fun in the paddy field?\n(A) Indian\n(B) Chinese\n(C) American\n(D) Malays\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 48, "doc": {"video_name": "needle_139.mp4", "duration": 239.06, "question": "What is the habitat of the blue fin trevally shown in the video?\n(A) Open ocean\n(B) Coral reef\n(C) Mangrove forest\n(D) Freshwater river\n", "candidates": ["Open ocean", "Coral reef", "Mangrove forest", "Freshwater river"], "answer": "B", "task_type": "needle", "question_id": "Q48"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "18fd540c31cbac7dfbd210dfb053f2533b3b735a8528d543e03f093c1e24b0ee", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the habitat of the blue fin trevally shown in the video?\n(A) Open ocean\n(B) Coral reef\n(C) Mangrove forest\n(D) Freshwater river\n", "task_type": "needle", "pred_answer": "A", "answer": "B"}, "input": "What is the habitat of the blue fin trevally shown in the video?\n(A) Open ocean\n(B) Coral reef\n(C) Mangrove forest\n(D) Freshwater river\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 52, "doc": {"video_name": "needle_48.mp4", "duration": 490.01, "question": "What is the ethnicity of the children playing game on mobile phone in the video?\n(A) Asian\n(B) African\n(C) Hispanic\n(D) Caucasian\n", "candidates": ["Asian", "African", "Hispanic", "Caucasian"], "answer": "A", "task_type": "needle", "question_id": "Q52"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "585abdb1a6c8a1ba72ea6f57491cc39f88e6e8c38d676a8af9bc870bc668db40", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the ethnicity of the children playing game on mobile phone in the video?\n(A) Asian\n(B) African\n(C) Hispanic\n(D) Caucasian\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the ethnicity of the children playing game on mobile phone in the video?\n(A) Asian\n(B) African\n(C) Hispanic\n(D) Caucasian\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 56, "doc": {"video_name": "needle_119.mp4", "duration": 766.14, "question": "What is happening to the net on the basketball field in the video?\n(A) The sea wind is pumping it\n(B) It is being replaced\n(C) It is being painted\n(D) It is being cut\n", "candidates": ["The sea wind is pumping it", "It is being replaced", "It is being painted", "It is being cut"], "answer": "A", "task_type": "needle", "question_id": "Q56"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c8db4faa8877f3758f56d6089ead8e02427a2c4501447b46c80ecde9d13c52df", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is happening to the net on the basketball field in the video?\n(A) The sea wind is pumping it\n(B) It is being replaced\n(C) It is being painted\n(D) It is being cut\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is happening to the net on the basketball field in the video?\n(A) The sea wind is pumping it\n(B) It is being replaced\n(C) It is being painted\n(D) It is being cut\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 60, "doc": {"video_name": "needle_98.mp4", "duration": 461.7, "question": "What is the young girl in the tracksuit doing on a rug in the park?\n(A) Reading a book\n(B) Having a picnic\n(C) Doing yoga\n(D) Playing soccer\n", "candidates": ["Reading a book", "Having a picnic", "Doing yoga", "Playing soccer"], "answer": "C", "task_type": "needle", "question_id": "Q60"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6c5a7b91a0b22c5ac23bb0577c9f533eb6f44fb1338a21475a4d3333ef15b72a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the young girl in the tracksuit doing on a rug in the park?\n(A) Reading a book\n(B) Having a picnic\n(C) Doing yoga\n(D) Playing soccer\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the young girl in the tracksuit doing on a rug in the park?\n(A) Reading a book\n(B) Having a picnic\n(C) Doing yoga\n(D) Playing soccer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 64, "doc": {"video_name": "needle_44.mp4", "duration": 470.52, "question": "What is being transferred to the beaker in the laboratory?\n(A) Solid substance\n(B) Gas\n(C) Nothing\n(D) Liquid tester\n", "candidates": ["Solid substance", "Gas", "Nothing", "Liquid tester"], "answer": "D", "task_type": "needle", "question_id": "Q64"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "de86d66518161f20b50d922e212c9586d1efada1ee42c19f27fe9c9a97eccb74", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is being transferred to the beaker in the laboratory?\n(A) Solid substance\n(B) Gas\n(C) Nothing\n(D) Liquid tester\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "What is being transferred to the beaker in the laboratory?\n(A) Solid substance\n(B) Gas\n(C) Nothing\n(D) Liquid tester\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 68, "doc": {"video_name": "needle_116.mp4", "duration": 343.07, "question": "What is the purpose of the SUV being on the side of the desert highway?\n(A) For green screen or chroma key\n(B) For a car race\n(C) For a road trip\n(D) For a car show\n", "candidates": ["For green screen or chroma key", "For a car race", "For a road trip", "For a car show"], "answer": "A", "task_type": "needle", "question_id": "Q68"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d12996c5cc2ac287b7ee29262c018f1cc77bb3dd44a8ff200a074b38cd9978aa", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the purpose of the SUV being on the side of the desert highway?\n(A) For green screen or chroma key\n(B) For a car race\n(C) For a road trip\n(D) For a car show\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the purpose of the SUV being on the side of the desert highway?\n(A) For green screen or chroma key\n(B) For a car race\n(C) For a road trip\n(D) For a car show\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 72, "doc": {"video_name": "needle_113.mp4", "duration": 5604.42, "question": "What are the volunteers searching for during the simulation drill?\n(A) Survivors of a natural disaster\n(B) Rocket attack casualties\n(C) Hidden treasure\n(D) Lost items\n", "candidates": ["Survivors of a natural disaster", "Rocket attack casualties", "Hidden treasure", "Lost items"], "answer": "B", "task_type": "needle", "question_id": "Q72"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "78c96489476b715ec4fdd099bb18008c06848d5e77adc568664ad2eb9dccd120", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What are the volunteers searching for during the simulation drill?\n(A) Survivors of a natural disaster\n(B) Rocket attack casualties\n(C) Hidden treasure\n(D) Lost items\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What are the volunteers searching for during the simulation drill?\n(A) Survivors of a natural disaster\n(B) Rocket attack casualties\n(C) Hidden treasure\n(D) Lost items\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 76, "doc": {"video_name": "needle_59.mp4", "duration": 460.02000000000004, "question": "What is the gender of the patient receiving the microblasting routine in the video?\n(A) Not specified\n(B) Both male and female\n(C) Female\n(D) Male\n", "candidates": ["Not specified", "Both male and female", "Female", "Male"], "answer": "C", "task_type": "needle", "question_id": "Q76"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1002b80f58572758dee6446307c309332bf4c9a5a8f0bccf94fff19ef4f7e03a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the gender of the patient receiving the microblasting routine in the video?\n(A) Not specified\n(B) Both male and female\n(C) Female\n(D) Male\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the gender of the patient receiving the microblasting routine in the video?\n(A) Not specified\n(B) Both male and female\n(C) Female\n(D) Male\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 80, "doc": {"video_name": "needle_84.mp4", "duration": 505.83000000000004, "question": "What is the profession of the person who begins work with the drawings in the video?\n(A) Artist\n(B) Engineer\n(C) Doctor\n(D) Teacher\n", "candidates": ["Artist", "Engineer", "Doctor", "Teacher"], "answer": "B", "task_type": "needle", "question_id": "Q80"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7584cd610da5cd391467571d0715d345c1565f0b1833d261edc639a2bcfc8ce3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the profession of the person who begins work with the drawings in the video?\n(A) Artist\n(B) Engineer\n(C) Doctor\n(D) Teacher\n", "task_type": "needle", "pred_answer": "A", "answer": "B"}, "input": "What is the profession of the person who begins work with the drawings in the video?\n(A) Artist\n(B) Engineer\n(C) Doctor\n(D) Teacher\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 84, "doc": {"video_name": "needle_93.mp4", "duration": 458.01, "question": "What is the woman doing during the summer sunset?\n(A) Taking a moment to enjoy life\n(B) Reading a book\n(C) Having a picnic\n(D) Swimming in the sea\n", "candidates": ["Taking a moment to enjoy life", "Reading a book", "Having a picnic", "Swimming in the sea"], "answer": "A", "task_type": "needle", "question_id": "Q84"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0e224fc023661d0594e0121eb9a412ceaa55c564b27f151796c6e42cda650d33", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the woman doing during the summer sunset?\n(A) Taking a moment to enjoy life\n(B) Reading a book\n(C) Having a picnic\n(D) Swimming in the sea\n", "task_type": "needle", "pred_answer": "A.", "answer": "A"}, "input": "What is the woman doing during the summer sunset?\n(A) Taking a moment to enjoy life\n(B) Reading a book\n(C) Having a picnic\n(D) Swimming in the sea\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 88, "doc": {"video_name": "needle_40.mp4", "duration": 477.20000000000005, "question": "What is the American toad doing at the mouth of the den in the video?\n(A) Jumping\n(B) Sleeping\n(C) Eating\n(D) Breathing and waiting\n", "candidates": ["Jumping", "Sleeping", "Eating", "Breathing and waiting"], "answer": "D", "task_type": "needle", "question_id": "Q88"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "45627f9d81882fe2e6992ef93ca9cf7974c739b8761f98b669e2f06f4d4697e2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the American toad doing at the mouth of the den in the video?\n(A) Jumping\n(B) Sleeping\n(C) Eating\n(D) Breathing and waiting\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the American toad doing at the mouth of the den in the video?\n(A) Jumping\n(B) Sleeping\n(C) Eating\n(D) Breathing and waiting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 92, "doc": {"video_name": "needle_41.mp4", "duration": 477.07, "question": "What is the state of movement of the American toad at the mouth of the den in the video?\n(A) Very little movement\n(B) No movement at all\n(C) Moderate movement\n(D) Constant movement\n", "candidates": ["Very little movement", "No movement at all", "Moderate movement", "Constant movement"], "answer": "A", "task_type": "needle", "question_id": "Q92"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4ac852a20072d70d10d7c057e18bce9b27d5ec289f902f0f625e34f6d0699ecc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the state of movement of the American toad at the mouth of the den in the video?\n(A) Very little movement\n(B) No movement at all\n(C) Moderate movement\n(D) Constant movement\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the state of movement of the American toad at the mouth of the den in the video?\n(A) Very little movement\n(B) No movement at all\n(C) Moderate movement\n(D) Constant movement\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 96, "doc": {"video_name": "needle_106.mp4", "duration": 744.44, "question": "Who are decorating the Christmas tree outdoors in the video?\n(A) A young father and his son\n(B) A young mother and her daughter\n(C) A young mother and her son\n(D) An elderly couple\n", "candidates": ["A young father and his son", "A young mother and her daughter", "A young mother and her son", "An elderly couple"], "answer": "C", "task_type": "needle", "question_id": "Q96"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5921f944fa1514c1af377f3645367525755cb735df3fc7c5f8665ac0b3fe048e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who are decorating the Christmas tree outdoors in the video?\n(A) A young father and his son\n(B) A young mother and her daughter\n(C) A young mother and her son\n(D) An elderly couple\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "Who are decorating the Christmas tree outdoors in the video?\n(A) A young father and his son\n(B) A young mother and her daughter\n(C) A young mother and her son\n(D) An elderly couple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 100, "doc": {"video_name": "needle_109.mp4", "duration": 6005.21, "question": "What does the engineer begin to work with in the video?\n(A) Drawings\n(B) Computer Codes\n(C) Blueprints\n(D) 3D Models\n", "candidates": ["Drawings", "Computer Codes", "Blueprints", "3D Models"], "answer": "A", "task_type": "needle", "question_id": "Q100"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "30af20973e761bf4e780a568487ce279b1c73a9993d0448a3df7ed30a8fa7318", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What does the engineer begin to work with in the video?\n(A) Drawings\n(B) Computer Codes\n(C) Blueprints\n(D) 3D Models\n", "task_type": "needle", "pred_answer": "C", "answer": "A"}, "input": "What does the engineer begin to work with in the video?\n(A) Drawings\n(B) Computer Codes\n(C) Blueprints\n(D) 3D Models\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 104, "doc": {"video_name": "needle_56.mp4", "duration": 462.1, "question": "What is the woman doing in the garment factory?\n(A) She is designing clothes\n(B) She is managing the factory\n(C) She is working on the production line\n(D) She is selling clothes\n", "candidates": ["She is designing clothes", "She is managing the factory", "She is working on the production line", "She is selling clothes"], "answer": "C", "task_type": "needle", "question_id": "Q104"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d7e6b802726713a331074756106638044302fa7c1a8f889d7dd050180633575f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the woman doing in the garment factory?\n(A) She is designing clothes\n(B) She is managing the factory\n(C) She is working on the production line\n(D) She is selling clothes\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "What is the woman doing in the garment factory?\n(A) She is designing clothes\n(B) She is managing the factory\n(C) She is working on the production line\n(D) She is selling clothes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 108, "doc": {"video_name": "needle_34.mp4", "duration": 455.36, "question": "What tool is being used by the dentist and his assistant while treating the patient's teeth in the video segment?\n(A) Dental drill\n(B) Dental scaler\n(C) Dental mirror\n(D) Face bow\n", "candidates": ["Dental drill", "Dental scaler", "Dental mirror", "Face bow"], "answer": "D", "task_type": "needle", "question_id": "Q108"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b8709f6e172f94f9a6a14e3163edaa461624ba04091c8d2869f5faaf9bec525", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What tool is being used by the dentist and his assistant while treating the patient's teeth in the video segment?\n(A) Dental drill\n(B) Dental scaler\n(C) Dental mirror\n(D) Face bow\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What tool is being used by the dentist and his assistant while treating the patient's teeth in the video segment?\n(A) Dental drill\n(B) Dental scaler\n(C) Dental mirror\n(D) Face bow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 112, "doc": {"video_name": "needle_143.mp4", "duration": 624.8699999999999, "question": "What unusual event happens while the woman is using the computer?\n(A) The computer crashes\n(B) A hand comes out of the computer\n(C) The screen goes blank\n(D) The computer starts talking\n", "candidates": ["The computer crashes", "A hand comes out of the computer", "The screen goes blank", "The computer starts talking"], "answer": "B", "task_type": "needle", "question_id": "Q112"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2f208a6b07d95dce51f3a7845938d298e3832c9223c9a9831790894eea9c506a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What unusual event happens while the woman is using the computer?\n(A) The computer crashes\n(B) A hand comes out of the computer\n(C) The screen goes blank\n(D) The computer starts talking\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What unusual event happens while the woman is using the computer?\n(A) The computer crashes\n(B) A hand comes out of the computer\n(C) The screen goes blank\n(D) The computer starts talking\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 116, "doc": {"video_name": "needle_104.mp4", "duration": 635.6, "question": "What is the weather like as the car with a broken windshield rides through the mountain country village?\n(A) Autumn rainy day\n(B) Winter snowy day\n(C) Spring cloudy day\n(D) Summer sunny day\n", "candidates": ["Autumn rainy day", "Winter snowy day", "Spring cloudy day", "Summer sunny day"], "answer": "D", "task_type": "needle", "question_id": "Q116"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "80cb03611b3ef92f3a91d03ab137ba2915cb088f2bfc95af10d96085a954f6cc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the weather like as the car with a broken windshield rides through the mountain country village?\n(A) Autumn rainy day\n(B) Winter snowy day\n(C) Spring cloudy day\n(D) Summer sunny day\n", "task_type": "needle", "pred_answer": "A", "answer": "D"}, "input": "What is the weather like as the car with a broken windshield rides through the mountain country village?\n(A) Autumn rainy day\n(B) Winter snowy day\n(C) Spring cloudy day\n(D) Summer sunny day\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 120, "doc": {"video_name": "needle_128.mp4", "duration": 8332.900000000001, "question": "From where is the female making the video call?\n(A) From her home living room\n(B) From her office\n(C) From her car\n(D) From a coffee shop\n", "candidates": ["From her home living room", "From her office", "From her car", "From a coffee shop"], "answer": "A", "task_type": "needle", "question_id": "Q120"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "807a9ad5bc5a7ab741469784577285bca8717dc8750be786758950e512770998", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "From where is the female making the video call?\n(A) From her home living room\n(B) From her office\n(C) From her car\n(D) From a coffee shop\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "From where is the female making the video call?\n(A) From her home living room\n(B) From her office\n(C) From her car\n(D) From a coffee shop\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 124, "doc": {"video_name": "needle_76.mp4", "duration": 1522.71, "question": "Where is the chef preparing the dinner?\n(A) In a hotel\n(B) By the ocean shore on an island\n(C) In a kitchen\n(D) In a restaurant\n", "candidates": ["In a hotel", "By the ocean shore on an island", "In a kitchen", "In a restaurant"], "answer": "B", "task_type": "needle", "question_id": "Q124"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d189ee1af5215a9848b76ed39b46b9d9eb6217e61557b9a4b16cf96b1bb5c139", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where is the chef preparing the dinner?\n(A) In a hotel\n(B) By the ocean shore on an island\n(C) In a kitchen\n(D) In a restaurant\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Where is the chef preparing the dinner?\n(A) In a hotel\n(B) By the ocean shore on an island\n(C) In a kitchen\n(D) In a restaurant\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 128, "doc": {"video_name": "needle_102.mp4", "duration": 6028.639999999999, "question": "What is the state of movement of the American toad at the mouth of the den in the video?\n(A) Very little movement\n(B) No movement at all\n(C) Constant movement\n(D) Moderate movement\n", "candidates": ["Very little movement", "No movement at all", "Constant movement", "Moderate movement"], "answer": "A", "task_type": "needle", "question_id": "Q128"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "779ca4c66e09e89b7ff4e08588bc90d4dcd461461617abb252d4634315b6610f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the state of movement of the American toad at the mouth of the den in the video?\n(A) Very little movement\n(B) No movement at all\n(C) Constant movement\n(D) Moderate movement\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the state of movement of the American toad at the mouth of the den in the video?\n(A) Very little movement\n(B) No movement at all\n(C) Constant movement\n(D) Moderate movement\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 132, "doc": {"video_name": "needle_80.mp4", "duration": 463.02, "question": "What happens to the egg when it falls on the glass floor in the video?\n(A) It cracks but doesn't break\n(B) It breaks and makes a mess\n(C) Nothing happens to it\n(D) It bounces back\n", "candidates": ["It cracks but doesn't break", "It breaks and makes a mess", "Nothing happens to it", "It bounces back"], "answer": "B", "task_type": "needle", "question_id": "Q132"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f107bad2c3408d99aaa7e4e673d7bcb1544100ca7b5bb2fd8dca1d0c20bd3423", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What happens to the egg when it falls on the glass floor in the video?\n(A) It cracks but doesn't break\n(B) It breaks and makes a mess\n(C) Nothing happens to it\n(D) It bounces back\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What happens to the egg when it falls on the glass floor in the video?\n(A) It cracks but doesn't break\n(B) It breaks and makes a mess\n(C) Nothing happens to it\n(D) It bounces back\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 136, "doc": {"video_name": "needle_29.mp4", "duration": 461.57, "question": "What is the activity involving the little girl and her grandmother?\n(A) They are painting\n(B) They are cooking\n(C) They are reading a children's book\n(D) They are gardening\n", "candidates": ["They are painting", "They are cooking", "They are reading a children's book", "They are gardening"], "answer": "C", "task_type": "needle", "question_id": "Q136"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1d7145505bce22284cc3e9cf26ad6049c8104e26fd0b9cddcf6e7bd220e6af70", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the activity involving the little girl and her grandmother?\n(A) They are painting\n(B) They are cooking\n(C) They are reading a children's book\n(D) They are gardening\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the activity involving the little girl and her grandmother?\n(A) They are painting\n(B) They are cooking\n(C) They are reading a children's book\n(D) They are gardening\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 140, "doc": {"video_name": "needle_144.mp4", "duration": 6380.89, "question": "What color is the pumpkin that the chef is using to prepare the soup?\n(A) Orange\n(B) White\n(C) Yellow\n(D) Green\n", "candidates": ["Orange", "White", "Yellow", "Green"], "answer": "A", "task_type": "needle", "question_id": "Q140"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4dde7a153f76b26c1fb00641fcfaa0df736393fabb0bcb4b4c11d5b6bd0a1c0c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the pumpkin that the chef is using to prepare the soup?\n(A) Orange\n(B) White\n(C) Yellow\n(D) Green\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What color is the pumpkin that the chef is using to prepare the soup?\n(A) Orange\n(B) White\n(C) Yellow\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 144, "doc": {"video_name": "needle_14.mp4", "duration": 468.41999999999996, "question": "Who are decorating the Christmas tree outdoors in the video?\n(A) An elderly couple\n(B) A young mother and her son\n(C) A young father and his son\n(D) A young mother and her daughter\n", "candidates": ["An elderly couple", "A young mother and her son", "A young father and his son", "A young mother and her daughter"], "answer": "B", "task_type": "needle", "question_id": "Q144"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D."]], "filtered_resps": ["D."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07262458cd35133f34f6aeba1c7e29211838dda5f0d5ceb91b3d0aa6a32fd94a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who are decorating the Christmas tree outdoors in the video?\n(A) An elderly couple\n(B) A young mother and her son\n(C) A young father and his son\n(D) A young mother and her daughter\n", "task_type": "needle", "pred_answer": "D.", "answer": "B"}, "input": "Who are decorating the Christmas tree outdoors in the video?\n(A) An elderly couple\n(B) A young mother and her son\n(C) A young father and his son\n(D) A young mother and her daughter\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 148, "doc": {"video_name": "needle_54.mp4", "duration": 469.72, "question": "Where is the middle aged female executive working on the corporate strategy?\n(A) In a cafe\n(B) In a park\n(C) In office\n(D) At home\n", "candidates": ["In a cafe", "In a park", "In office", "At home"], "answer": "C", "task_type": "needle", "question_id": "Q148"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b10b68e1c50942b11a833edc175b733406bbaca7ea53111b2de11fafe284ace7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the middle aged female executive working on the corporate strategy?\n(A) In a cafe\n(B) In a park\n(C) In office\n(D) At home\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "Where is the middle aged female executive working on the corporate strategy?\n(A) In a cafe\n(B) In a park\n(C) In office\n(D) At home\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 152, "doc": {"video_name": "needle_13.mp4", "duration": 493.15, "question": "What are the volunteers doing in the rubble during the simulation drill?\n(A) Hiding from an enemy\n(B) Creating a barricade\n(C) Building a structure\n(D) Searching for rocket attack casualties\n", "candidates": ["Hiding from an enemy", "Creating a barricade", "Building a structure", "Searching for rocket attack casualties"], "answer": "D", "task_type": "needle", "question_id": "Q152"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e993eca7c3b82f645e7b2d3965e5ef9cf2b4ec03ac7b4bdae2c4357d599570dd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What are the volunteers doing in the rubble during the simulation drill?\n(A) Hiding from an enemy\n(B) Creating a barricade\n(C) Building a structure\n(D) Searching for rocket attack casualties\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What are the volunteers doing in the rubble during the simulation drill?\n(A) Hiding from an enemy\n(B) Creating a barricade\n(C) Building a structure\n(D) Searching for rocket attack casualties\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 156, "doc": {"video_name": "needle_12.mp4", "duration": 463.15, "question": "What is the context of the simulation drill where volunteers are digging through the rubble?\n(A) An earthquake\n(B) A building demolition\n(C) A construction site accident\n(D) A rocket attack\n", "candidates": ["An earthquake", "A building demolition", "A construction site accident", "A rocket attack"], "answer": "D", "task_type": "needle", "question_id": "Q156"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "65a11ba8ee4f1cded1cb7e7fefca56d66e41b895640ef6492daa41cbbb7a5798", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the context of the simulation drill where volunteers are digging through the rubble?\n(A) An earthquake\n(B) A building demolition\n(C) A construction site accident\n(D) A rocket attack\n", "task_type": "needle", "pred_answer": "A", "answer": "D"}, "input": "What is the context of the simulation drill where volunteers are digging through the rubble?\n(A) An earthquake\n(B) A building demolition\n(C) A construction site accident\n(D) A rocket attack\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 160, "doc": {"video_name": "needle_51.mp4", "duration": 1504.7, "question": "What animals are tied beside the paddy field where the two Malays kids are having fun?\n(A) Dogs\n(B) Horses\n(C) Cows\n(D) Sheep\n", "candidates": ["Dogs", "Horses", "Cows", "Sheep"], "answer": "C", "task_type": "needle", "question_id": "Q160"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "feb8a1788fc751dc05a93f240e9a6f8b93d989d35c039c078583835110a0bf68", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What animals are tied beside the paddy field where the two Malays kids are having fun?\n(A) Dogs\n(B) Horses\n(C) Cows\n(D) Sheep\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What animals are tied beside the paddy field where the two Malays kids are having fun?\n(A) Dogs\n(B) Horses\n(C) Cows\n(D) Sheep\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 164, "doc": {"video_name": "needle_95.mp4", "duration": 464.54, "question": "What is the female potter doing with the brush?\n(A) Cleaning the brush\n(B) Drawing on the table\n(C) Stirring paint in a cup\n(D) Painting a picture\n", "candidates": ["Cleaning the brush", "Drawing on the table", "Stirring paint in a cup", "Painting a picture"], "answer": "C", "task_type": "needle", "question_id": "Q164"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e4c13753455091e522c131ef5d5674202a16586b6518170b9b23b00fed62fbad", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the female potter doing with the brush?\n(A) Cleaning the brush\n(B) Drawing on the table\n(C) Stirring paint in a cup\n(D) Painting a picture\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the female potter doing with the brush?\n(A) Cleaning the brush\n(B) Drawing on the table\n(C) Stirring paint in a cup\n(D) Painting a picture\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 168, "doc": {"video_name": "needle_69.mp4", "duration": 461.04, "question": "What is the little Asian girl wearing while creating sand piles on the beach at sunset?\n(A) A dress\n(B) A swimsuit\n(C) Shorts and a t-shirt\n(D) A bikini\n", "candidates": ["A dress", "A swimsuit", "Shorts and a t-shirt", "A bikini"], "answer": "D", "task_type": "needle", "question_id": "Q168"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fe5265969ed4d606ba135789d8fe55c384b46544e1e4806cb5bd5a277b54dff7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the little Asian girl wearing while creating sand piles on the beach at sunset?\n(A) A dress\n(B) A swimsuit\n(C) Shorts and a t-shirt\n(D) A bikini\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "What is the little Asian girl wearing while creating sand piles on the beach at sunset?\n(A) A dress\n(B) A swimsuit\n(C) Shorts and a t-shirt\n(D) A bikini\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 172, "doc": {"video_name": "needle_119.mp4", "duration": 766.14, "question": "What is the backdrop of the basketball in the video?\n(A) A forest\n(B) A city skyline\n(C) A bright multi-colored cloudy sky\n(D) A clear blue sky\n", "candidates": ["A forest", "A city skyline", "A bright multi-colored cloudy sky", "A clear blue sky"], "answer": "C", "task_type": "needle", "question_id": "Q172"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d3e95927ca733140bde61a30ac51d0611d48790ada804856d5550d2e1cc0efb0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the backdrop of the basketball in the video?\n(A) A forest\n(B) A city skyline\n(C) A bright multi-colored cloudy sky\n(D) A clear blue sky\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the backdrop of the basketball in the video?\n(A) A forest\n(B) A city skyline\n(C) A bright multi-colored cloudy sky\n(D) A clear blue sky\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 176, "doc": {"video_name": "needle_23.mp4", "duration": 487.29, "question": "What is the chef preparing with the fresh orange pumpkin?\n(A) Pumpkin pie\n(B) Creamy pumpkin soup\n(C) Roasted pumpkin\n(D) Pumpkin salad\n", "candidates": ["Pumpkin pie", "Creamy pumpkin soup", "Roasted pumpkin", "Pumpkin salad"], "answer": "B", "task_type": "needle", "question_id": "Q176"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "971f64848ff45c62d0dbaa1234da64850f3acb1d7e1f828839dec8715e9b38a2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the chef preparing with the fresh orange pumpkin?\n(A) Pumpkin pie\n(B) Creamy pumpkin soup\n(C) Roasted pumpkin\n(D) Pumpkin salad\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the chef preparing with the fresh orange pumpkin?\n(A) Pumpkin pie\n(B) Creamy pumpkin soup\n(C) Roasted pumpkin\n(D) Pumpkin salad\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 180, "doc": {"video_name": "needle_84.mp4", "duration": 505.83000000000004, "question": "What does the engineer begin to work with in the video?\n(A) 3D Models\n(B) Computer Codes\n(C) Drawings\n(D) Blueprints\n", "candidates": ["3D Models", "Computer Codes", "Drawings", "Blueprints"], "answer": "C", "task_type": "needle", "question_id": "Q180"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "947e1776a045eadb363f8a4419280eefcd008a01a8435eac300cbc7aeb354188", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What does the engineer begin to work with in the video?\n(A) 3D Models\n(B) Computer Codes\n(C) Drawings\n(D) Blueprints\n", "task_type": "needle", "pred_answer": "A", "answer": "C"}, "input": "What does the engineer begin to work with in the video?\n(A) 3D Models\n(B) Computer Codes\n(C) Drawings\n(D) Blueprints\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 184, "doc": {"video_name": "needle_139.mp4", "duration": 239.06, "question": "What species of fish is hunting small reef fish and anthias along a coral reef in the video?\n(A) Clown fish\n(B) Barracuda\n(C) Great white shark\n(D) Blue fin trevally\n", "candidates": ["Clown fish", "Barracuda", "Great white shark", "Blue fin trevally"], "answer": "D", "task_type": "needle", "question_id": "Q184"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5665bdbb8b0f5325c5855d6789552ec81e71b0bcd73d8c32f9594f2c6ed1e5e8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What species of fish is hunting small reef fish and anthias along a coral reef in the video?\n(A) Clown fish\n(B) Barracuda\n(C) Great white shark\n(D) Blue fin trevally\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "What species of fish is hunting small reef fish and anthias along a coral reef in the video?\n(A) Clown fish\n(B) Barracuda\n(C) Great white shark\n(D) Blue fin trevally\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 188, "doc": {"video_name": "needle_21.mp4", "duration": 846.12, "question": "What is the weather like as the car with a broken windshield rides through the mountain country village?\n(A) Spring cloudy day\n(B) Summer sunny day\n(C) Winter snowy day\n(D) Autumn rainy day\n", "candidates": ["Spring cloudy day", "Summer sunny day", "Winter snowy day", "Autumn rainy day"], "answer": "B", "task_type": "needle", "question_id": "Q188"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "65712d89aad384f2da256777d677b6f0c559feaaa96aa076f9afc723d29ba0e5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the weather like as the car with a broken windshield rides through the mountain country village?\n(A) Spring cloudy day\n(B) Summer sunny day\n(C) Winter snowy day\n(D) Autumn rainy day\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the weather like as the car with a broken windshield rides through the mountain country village?\n(A) Spring cloudy day\n(B) Summer sunny day\n(C) Winter snowy day\n(D) Autumn rainy day\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 192, "doc": {"video_name": "needle_47.mp4", "duration": 461.21000000000004, "question": "Where are the students celebrating their graduation?\n(A) In the classroom\n(B) In the park\n(C) At home\n(D) In the school auditorium\n", "candidates": ["In the classroom", "In the park", "At home", "In the school auditorium"], "answer": "B", "task_type": "needle", "question_id": "Q192"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d96c769b18582e7359649697d4632cf693cd8ee5e91a08b1ec5bdcbaaf5c2d62", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where are the students celebrating their graduation?\n(A) In the classroom\n(B) In the park\n(C) At home\n(D) In the school auditorium\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Where are the students celebrating their graduation?\n(A) In the classroom\n(B) In the park\n(C) At home\n(D) In the school auditorium\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 196, "doc": {"video_name": "needle_91.mp4", "duration": 461.59999999999997, "question": "What color are the leaves of the plant shown in the video segment?\n(A) Green\n(B) Red\n(C) Yellow\n(D) Blue\n", "candidates": ["Green", "Red", "Yellow", "Blue"], "answer": "A", "task_type": "needle", "question_id": "Q196"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3bfe35a2995724f9572a207cc755a8ca1d3158780f482a25ed1b2bce427d125f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color are the leaves of the plant shown in the video segment?\n(A) Green\n(B) Red\n(C) Yellow\n(D) Blue\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What color are the leaves of the plant shown in the video segment?\n(A) Green\n(B) Red\n(C) Yellow\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 200, "doc": {"video_name": "needle_84.mp4", "duration": 505.83000000000004, "question": "What is the background when the engineer begins work with the drawings in the video?\n(A) Mountains\n(B) Forest\n(C) Windmills\n(D) Cityscape\n", "candidates": ["Mountains", "Forest", "Windmills", "Cityscape"], "answer": "C", "task_type": "needle", "question_id": "Q200"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8fc0f9176e0725e1beac882055df4ac82b4160c1bcd868f759d5d422c9f6fa39", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the background when the engineer begins work with the drawings in the video?\n(A) Mountains\n(B) Forest\n(C) Windmills\n(D) Cityscape\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "What is the background when the engineer begins work with the drawings in the video?\n(A) Mountains\n(B) Forest\n(C) Windmills\n(D) Cityscape\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 204, "doc": {"video_name": "needle_130.mp4", "duration": 395.09999999999997, "question": "What type of dog is the owner in sports boots stroking?\n(A) Jack Russell\n(B) Bulldog\n(C) Golden Retriever\n(D) German Shepherd\n", "candidates": ["Jack Russell", "Bulldog", "Golden Retriever", "German Shepherd"], "answer": "A", "task_type": "needle", "question_id": "Q204"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d9bf36efba796dd2bd209a1036446fd0853810f80424cc72ad3e0f5587ccca8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of dog is the owner in sports boots stroking?\n(A) Jack Russell\n(B) Bulldog\n(C) Golden Retriever\n(D) German Shepherd\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What type of dog is the owner in sports boots stroking?\n(A) Jack Russell\n(B) Bulldog\n(C) Golden Retriever\n(D) German Shepherd\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 208, "doc": {"video_name": "needle_68.mp4", "duration": 1367.32, "question": "What is the little Asian girl wearing while creating sand piles on the beach at sunset?\n(A) A bikini\n(B) A swimsuit\n(C) A dress\n(D) Shorts and a t-shirt\n", "candidates": ["A bikini", "A swimsuit", "A dress", "Shorts and a t-shirt"], "answer": "A", "task_type": "needle", "question_id": "Q208"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "55c98bf01d7802254b20d07124cae61215ea10b51ac02128131add58e3d8ca2f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the little Asian girl wearing while creating sand piles on the beach at sunset?\n(A) A bikini\n(B) A swimsuit\n(C) A dress\n(D) Shorts and a t-shirt\n", "task_type": "needle", "pred_answer": "D", "answer": "A"}, "input": "What is the little Asian girl wearing while creating sand piles on the beach at sunset?\n(A) A bikini\n(B) A swimsuit\n(C) A dress\n(D) Shorts and a t-shirt\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 212, "doc": {"video_name": "needle_11.mp4", "duration": 454.52, "question": "What is the profession of the person moving levers of a multitrack mixing console in the video?\n(A) Sound engineer\n(B) Cameraman\n(C) Musician\n(D) Director\n", "candidates": ["Sound engineer", "Cameraman", "Musician", "Director"], "answer": "A", "task_type": "needle", "question_id": "Q212"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3cafd6ec6d564b5e84f2c4135805d1db532600b198db520b14e393dca32ec7e2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the profession of the person moving levers of a multitrack mixing console in the video?\n(A) Sound engineer\n(B) Cameraman\n(C) Musician\n(D) Director\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the profession of the person moving levers of a multitrack mixing console in the video?\n(A) Sound engineer\n(B) Cameraman\n(C) Musician\n(D) Director\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 216, "doc": {"video_name": "needle_10.mp4", "duration": 454.52, "question": "Where is the sound engineer moving levers of a multitrack mixing console?\n(A) In the control room\n(B) In the recording studio\n(C) On the stage\n(D) In the editing suite\n", "candidates": ["In the control room", "In the recording studio", "On the stage", "In the editing suite"], "answer": "A", "task_type": "needle", "question_id": "Q216"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f2343c8e3e5192cb53a193eb91fc7a5a6c09a8def6457e1955cc214ab04211ab", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where is the sound engineer moving levers of a multitrack mixing console?\n(A) In the control room\n(B) In the recording studio\n(C) On the stage\n(D) In the editing suite\n", "task_type": "needle", "pred_answer": "B", "answer": "A"}, "input": "Where is the sound engineer moving levers of a multitrack mixing console?\n(A) In the control room\n(B) In the recording studio\n(C) On the stage\n(D) In the editing suite\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 220, "doc": {"video_name": "needle_9.mp4", "duration": 461.25, "question": "What device is the female using for the video call?\n(A) Smartphone\n(B) Laptop\n(C) Desktop computer\n(D) Tablet\n", "candidates": ["Smartphone", "Laptop", "Desktop computer", "Tablet"], "answer": "A", "task_type": "needle", "question_id": "Q220"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1460944bb57d68c811200a63a5f0d38c83b663c0c42809dab4a2e06f03cdcafc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What device is the female using for the video call?\n(A) Smartphone\n(B) Laptop\n(C) Desktop computer\n(D) Tablet\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What device is the female using for the video call?\n(A) Smartphone\n(B) Laptop\n(C) Desktop computer\n(D) Tablet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 224, "doc": {"video_name": "needle_58.mp4", "duration": 459.98, "question": "Where does the woman performing the microblasting routine work?\n(A) Cosmetology Clinic\n(B) Beauty Salon\n(C) Spa\n(D) Hospital\n", "candidates": ["Cosmetology Clinic", "Beauty Salon", "Spa", "Hospital"], "answer": "A", "task_type": "needle", "question_id": "Q224"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ca5afe706da1761710f98efa147969c8bd83f9ca04b90439914c0cbc66f6ff79", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where does the woman performing the microblasting routine work?\n(A) Cosmetology Clinic\n(B) Beauty Salon\n(C) Spa\n(D) Hospital\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "Where does the woman performing the microblasting routine work?\n(A) Cosmetology Clinic\n(B) Beauty Salon\n(C) Spa\n(D) Hospital\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 228, "doc": {"video_name": "needle_39.mp4", "duration": 583.8199999999999, "question": "Where are the silhouette fishermen performing their popular activity during the monsoon season?\n(A) In a forest\n(B) On the beach\n(C) In a city\n(D) On a boat\n", "candidates": ["In a forest", "On the beach", "In a city", "On a boat"], "answer": "B", "task_type": "needle", "question_id": "Q228"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bfab613be5b4c4102ff316393a0c0471a87c6e14f2dee62fca17e9f9807e4ecb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where are the silhouette fishermen performing their popular activity during the monsoon season?\n(A) In a forest\n(B) On the beach\n(C) In a city\n(D) On a boat\n", "task_type": "needle", "pred_answer": "D", "answer": "B"}, "input": "Where are the silhouette fishermen performing their popular activity during the monsoon season?\n(A) In a forest\n(B) On the beach\n(C) In a city\n(D) On a boat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 232, "doc": {"video_name": "needle_63.mp4", "duration": 467.04999999999995, "question": "Who is hanging a golden christmas toy on the christmas tree in preparation for the new year?\n(A) An old woman\n(B) A young handsome man\n(C) A young beautiful woman\n(D) A child\n", "candidates": ["An old woman", "A young handsome man", "A young beautiful woman", "A child"], "answer": "C", "task_type": "needle", "question_id": "Q232"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3e9be16a200abd5525132ee6fe7051c6f41b293f44801f3f4a2f202afdb3172b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who is hanging a golden christmas toy on the christmas tree in preparation for the new year?\n(A) An old woman\n(B) A young handsome man\n(C) A young beautiful woman\n(D) A child\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "Who is hanging a golden christmas toy on the christmas tree in preparation for the new year?\n(A) An old woman\n(B) A young handsome man\n(C) A young beautiful woman\n(D) A child\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 236, "doc": {"video_name": "needle_58.mp4", "duration": 459.98, "question": "What is the gender of the patient receiving the microblasting routine in the video?\n(A) Male\n(B) Not specified\n(C) Both male and female\n(D) Female\n", "candidates": ["Male", "Not specified", "Both male and female", "Female"], "answer": "D", "task_type": "needle", "question_id": "Q236"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "226b95427ba256f92d12a0f17bffe05565cf371cfead6e92285c11fd1dd5c50b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the gender of the patient receiving the microblasting routine in the video?\n(A) Male\n(B) Not specified\n(C) Both male and female\n(D) Female\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "What is the gender of the patient receiving the microblasting routine in the video?\n(A) Male\n(B) Not specified\n(C) Both male and female\n(D) Female\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 240, "doc": {"video_name": "needle_100.mp4", "duration": 751.67, "question": "What is the chef preparing in the dinner by the ocean shore on an island?\n(A) Lobster\n(B) Fish\n(C) Chicken\n(D) Pork\n", "candidates": ["Lobster", "Fish", "Chicken", "Pork"], "answer": "A", "task_type": "needle", "question_id": "Q240"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9826a4e1a60052cf98d668354df65ec9e1d94b259e881ec4cf3cef0f95e73922", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the chef preparing in the dinner by the ocean shore on an island?\n(A) Lobster\n(B) Fish\n(C) Chicken\n(D) Pork\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the chef preparing in the dinner by the ocean shore on an island?\n(A) Lobster\n(B) Fish\n(C) Chicken\n(D) Pork\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 244, "doc": {"video_name": "needle_147.mp4", "duration": 622.0, "question": "What is the appearance of the senior businessman by the sea shore?\n(A) Posh looking with glasses\n(B) Sporty with a tracksuit\n(C) Casual with a hat\n(D) Formal with a suit\n", "candidates": ["Posh looking with glasses", "Sporty with a tracksuit", "Casual with a hat", "Formal with a suit"], "answer": "A", "task_type": "needle", "question_id": "Q244"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bf7a3239372945b78c5675d5c7459dffada8345d8a1b8769e26e7f94cc55545e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the appearance of the senior businessman by the sea shore?\n(A) Posh looking with glasses\n(B) Sporty with a tracksuit\n(C) Casual with a hat\n(D) Formal with a suit\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the appearance of the senior businessman by the sea shore?\n(A) Posh looking with glasses\n(B) Sporty with a tracksuit\n(C) Casual with a hat\n(D) Formal with a suit\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 248, "doc": {"video_name": "needle_29.mp4", "duration": 461.57, "question": "Who is the little girl reading a book with?\n(A) Her brother\n(B) Her grandmother\n(C) Her friend\n(D) Her teacher\n", "candidates": ["Her brother", "Her grandmother", "Her friend", "Her teacher"], "answer": "B", "task_type": "needle", "question_id": "Q248"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "65dbd30c042dfc9948be9108708ae61c14dc014be7a87ec69ab9868d080f75cb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who is the little girl reading a book with?\n(A) Her brother\n(B) Her grandmother\n(C) Her friend\n(D) Her teacher\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Who is the little girl reading a book with?\n(A) Her brother\n(B) Her grandmother\n(C) Her friend\n(D) Her teacher\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 252, "doc": {"video_name": "needle_36.mp4", "duration": 478.7, "question": "What is happening with the ocean waves in the video?\n(A) The waves are receding\n(B) The waves are crashing on a wild rocky beach\n(C) The waves are calm\n(D) The waves are crashing on a sandy beach\n", "candidates": ["The waves are receding", "The waves are crashing on a wild rocky beach", "The waves are calm", "The waves are crashing on a sandy beach"], "answer": "B", "task_type": "needle", "question_id": "Q252"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D."]], "filtered_resps": ["D."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4220a3ec1a5eeab648eab72effe86ccafa47a12ae831b7c4fabfb77a3bf2a2c4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is happening with the ocean waves in the video?\n(A) The waves are receding\n(B) The waves are crashing on a wild rocky beach\n(C) The waves are calm\n(D) The waves are crashing on a sandy beach\n", "task_type": "needle", "pred_answer": "D.", "answer": "B"}, "input": "What is happening with the ocean waves in the video?\n(A) The waves are receding\n(B) The waves are crashing on a wild rocky beach\n(C) The waves are calm\n(D) The waves are crashing on a sandy beach\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 256, "doc": {"video_name": "needle_99.mp4", "duration": 866.3299999999999, "question": "What is the weather like when the young girl in a tracksuit is doing yoga in the park?\n(A) Snowy\n(B) Cloudy\n(C) Rainy\n(D) Sunny\n", "candidates": ["Snowy", "Cloudy", "Rainy", "Sunny"], "answer": "D", "task_type": "needle", "question_id": "Q256"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "57302e7cbdd1c48667bb06f7ca9fc40063ed07d9df5d0853dd372d369f360f72", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the weather like when the young girl in a tracksuit is doing yoga in the park?\n(A) Snowy\n(B) Cloudy\n(C) Rainy\n(D) Sunny\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the weather like when the young girl in a tracksuit is doing yoga in the park?\n(A) Snowy\n(B) Cloudy\n(C) Rainy\n(D) Sunny\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 260, "doc": {"video_name": "needle_75.mp4", "duration": 505.0, "question": "What is the mood of the two young women sitting on the bench in the street?\n(A) Sad\n(B) Angry\n(C) Indifferent\n(D) Positive\n", "candidates": ["Sad", "Angry", "Indifferent", "Positive"], "answer": "D", "task_type": "needle", "question_id": "Q260"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c7a01e82178448d5bd5bfc54715eda41bbb0adad4535fcb02827bbdb37d6feab", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the mood of the two young women sitting on the bench in the street?\n(A) Sad\n(B) Angry\n(C) Indifferent\n(D) Positive\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the mood of the two young women sitting on the bench in the street?\n(A) Sad\n(B) Angry\n(C) Indifferent\n(D) Positive\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 264, "doc": {"video_name": "needle_33.mp4", "duration": 467.89000000000004, "question": "What does the hand coming out of the computer do?\n(A) Takes the woman's credit card\n(B) Shakes the woman's hand\n(C) Points at something on the screen\n(D) Delivers a product\n", "candidates": ["Takes the woman's credit card", "Shakes the woman's hand", "Points at something on the screen", "Delivers a product"], "answer": "D", "task_type": "needle", "question_id": "Q264"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ab1610ba087627ea788aee6ab055ba99f705524936436e4bbb0226e24164f7a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What does the hand coming out of the computer do?\n(A) Takes the woman's credit card\n(B) Shakes the woman's hand\n(C) Points at something on the screen\n(D) Delivers a product\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What does the hand coming out of the computer do?\n(A) Takes the woman's credit card\n(B) Shakes the woman's hand\n(C) Points at something on the screen\n(D) Delivers a product\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 268, "doc": {"video_name": "needle_69.mp4", "duration": 461.04, "question": "What time of day is the little Asian girl in a bikini creating sand piles on the beach?\n(A) Morning\n(B) Sunset\n(C) Afternoon\n(D) Midnight\n", "candidates": ["Morning", "Sunset", "Afternoon", "Midnight"], "answer": "B", "task_type": "needle", "question_id": "Q268"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6f6e6be8f21264bd962038291a68b01196cd4f600d3e0e0aa8ff07e79274e378", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What time of day is the little Asian girl in a bikini creating sand piles on the beach?\n(A) Morning\n(B) Sunset\n(C) Afternoon\n(D) Midnight\n", "task_type": "needle", "pred_answer": "C", "answer": "B"}, "input": "What time of day is the little Asian girl in a bikini creating sand piles on the beach?\n(A) Morning\n(B) Sunset\n(C) Afternoon\n(D) Midnight\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 272, "doc": {"video_name": "needle_8.mp4", "duration": 461.12, "question": "Who is the female chatting with in the video call from her home living room?\n(A) Her boss\n(B) Her mother\n(C) Her beautiful friend\n(D) Her brother\n", "candidates": ["Her boss", "Her mother", "Her beautiful friend", "Her brother"], "answer": "C", "task_type": "needle", "question_id": "Q272"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8689054de104cce10961d2fb3bfa0f02b20f4ec931d2eaee3f188cd005f5877b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who is the female chatting with in the video call from her home living room?\n(A) Her boss\n(B) Her mother\n(C) Her beautiful friend\n(D) Her brother\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "Who is the female chatting with in the video call from her home living room?\n(A) Her boss\n(B) Her mother\n(C) Her beautiful friend\n(D) Her brother\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 276, "doc": {"video_name": "needle_114.mp4", "duration": 773.72, "question": "What is the young girl in the tracksuit doing on a rug in the park?\n(A) Doing yoga\n(B) Reading a book\n(C) Having a picnic\n(D) Playing soccer\n", "candidates": ["Doing yoga", "Reading a book", "Having a picnic", "Playing soccer"], "answer": "A", "task_type": "needle", "question_id": "Q276"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "56d9299c7d24c6f1eb91b8b054b3634893c5eb334d57d8867652e5abd95956a8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the young girl in the tracksuit doing on a rug in the park?\n(A) Doing yoga\n(B) Reading a book\n(C) Having a picnic\n(D) Playing soccer\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the young girl in the tracksuit doing on a rug in the park?\n(A) Doing yoga\n(B) Reading a book\n(C) Having a picnic\n(D) Playing soccer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 280, "doc": {"video_name": "needle_137.mp4", "duration": 424.09999999999997, "question": "What is the woman doing in the flower shop?\n(A) Cleaning the shop\n(B) Making a bouquet of fresh flowers\n(C) Arranging books\n(D) Watering the plants\n", "candidates": ["Cleaning the shop", "Making a bouquet of fresh flowers", "Arranging books", "Watering the plants"], "answer": "B", "task_type": "needle", "question_id": "Q280"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B."]], "filtered_resps": ["B."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a9155aaf0a43815258fca9261db84227b4627234de8ff616510f97ca426343c2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the woman doing in the flower shop?\n(A) Cleaning the shop\n(B) Making a bouquet of fresh flowers\n(C) Arranging books\n(D) Watering the plants\n", "task_type": "needle", "pred_answer": "B.", "answer": "B"}, "input": "What is the woman doing in the flower shop?\n(A) Cleaning the shop\n(B) Making a bouquet of fresh flowers\n(C) Arranging books\n(D) Watering the plants\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 284, "doc": {"video_name": "needle_143.mp4", "duration": 624.8699999999999, "question": "What is the woman's appearance?\n(A) She is a young girl\n(B) She is an elderly woman\n(C) She is a middle-aged woman\n(D) She is a beautiful woman\n", "candidates": ["She is a young girl", "She is an elderly woman", "She is a middle-aged woman", "She is a beautiful woman"], "answer": "D", "task_type": "needle", "question_id": "Q284"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2c53df84f2ebf3987854d851f2f9fbd3436f3128c08e606136fd3b7fd4a1fa56", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the woman's appearance?\n(A) She is a young girl\n(B) She is an elderly woman\n(C) She is a middle-aged woman\n(D) She is a beautiful woman\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the woman's appearance?\n(A) She is a young girl\n(B) She is an elderly woman\n(C) She is a middle-aged woman\n(D) She is a beautiful woman\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 288, "doc": {"video_name": "needle_139.mp4", "duration": 239.06, "question": "Where is the group of blue fin trevally hunting in the video?\n(A) Great Barrier Reef, Australia\n(B) Caribbean Sea, Bahamas\n(C) Yap, Micronesia\n(D) Red Sea, Egypt\n", "candidates": ["Great Barrier Reef, Australia", "Caribbean Sea, Bahamas", "Yap, Micronesia", "Red Sea, Egypt"], "answer": "C", "task_type": "needle", "question_id": "Q288"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "993a912e760b02028fce813c43cc221cd9fffc62e9bc705674624975fe009ef2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the group of blue fin trevally hunting in the video?\n(A) Great Barrier Reef, Australia\n(B) Caribbean Sea, Bahamas\n(C) Yap, Micronesia\n(D) Red Sea, Egypt\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "Where is the group of blue fin trevally hunting in the video?\n(A) Great Barrier Reef, Australia\n(B) Caribbean Sea, Bahamas\n(C) Yap, Micronesia\n(D) Red Sea, Egypt\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 292, "doc": {"video_name": "needle_100.mp4", "duration": 751.67, "question": "What is the chef doing with the lobster in the dinner preparation?\n(A) Cutting\n(B) Grilling\n(C) Baking\n(D) Boiling\n", "candidates": ["Cutting", "Grilling", "Baking", "Boiling"], "answer": "A", "task_type": "needle", "question_id": "Q292"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d60b116dcff8d80e00d244eabcd0efc8c4b56a064d8012191f8209565d29d12a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the chef doing with the lobster in the dinner preparation?\n(A) Cutting\n(B) Grilling\n(C) Baking\n(D) Boiling\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the chef doing with the lobster in the dinner preparation?\n(A) Cutting\n(B) Grilling\n(C) Baking\n(D) Boiling\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 296, "doc": {"video_name": "needle_48.mp4", "duration": 490.01, "question": "What is the child doing at home in the video?\n(A) Eating\n(B) Sleeping\n(C) Playing game on mobile phone\n(D) Playing with toys\n", "candidates": ["Eating", "Sleeping", "Playing game on mobile phone", "Playing with toys"], "answer": "C", "task_type": "needle", "question_id": "Q296"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "259aa62cc5edde5cfe63008e5e2603ae72f5ce28521deec5f7f8eeab2c19fd2d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the child doing at home in the video?\n(A) Eating\n(B) Sleeping\n(C) Playing game on mobile phone\n(D) Playing with toys\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the child doing at home in the video?\n(A) Eating\n(B) Sleeping\n(C) Playing game on mobile phone\n(D) Playing with toys\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 300, "doc": {"video_name": "needle_136.mp4", "duration": 7979.45, "question": "What is the middle aged female executive doing on the white board?\n(A) Writing a letter\n(B) Drawing a portrait\n(C) Drawing a project plan scheme\n(D) Erasing notes\n", "candidates": ["Writing a letter", "Drawing a portrait", "Drawing a project plan scheme", "Erasing notes"], "answer": "C", "task_type": "needle", "question_id": "Q300"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5f796040b51e28261a2ddb24bd5d0800f7aee6d485e1e56f3c39dcfaed3aa601", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the middle aged female executive doing on the white board?\n(A) Writing a letter\n(B) Drawing a portrait\n(C) Drawing a project plan scheme\n(D) Erasing notes\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the middle aged female executive doing on the white board?\n(A) Writing a letter\n(B) Drawing a portrait\n(C) Drawing a project plan scheme\n(D) Erasing notes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 304, "doc": {"video_name": "needle_85.mp4", "duration": 475.83000000000004, "question": "What is the profession of the person who begins work with the drawings in the video?\n(A) Engineer\n(B) Artist\n(C) Doctor\n(D) Teacher\n", "candidates": ["Engineer", "Artist", "Doctor", "Teacher"], "answer": "A", "task_type": "needle", "question_id": "Q304"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ee17c22fde6838973cefbafd83b991523567eb4d2b82d60396f97a768cc1f0d8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the profession of the person who begins work with the drawings in the video?\n(A) Engineer\n(B) Artist\n(C) Doctor\n(D) Teacher\n", "task_type": "needle", "pred_answer": "B", "answer": "A"}, "input": "What is the profession of the person who begins work with the drawings in the video?\n(A) Engineer\n(B) Artist\n(C) Doctor\n(D) Teacher\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 308, "doc": {"video_name": "needle_62.mp4", "duration": 497.05, "question": "Who is hanging a golden christmas toy on the christmas tree in preparation for the new year?\n(A) A young handsome man\n(B) A child\n(C) A young beautiful woman\n(D) An old woman\n", "candidates": ["A young handsome man", "A child", "A young beautiful woman", "An old woman"], "answer": "C", "task_type": "needle", "question_id": "Q308"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e29676860f11504e6fa8795184ce1360b36876a1f29e2ff1d6bbc551daf012ce", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who is hanging a golden christmas toy on the christmas tree in preparation for the new year?\n(A) A young handsome man\n(B) A child\n(C) A young beautiful woman\n(D) An old woman\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "Who is hanging a golden christmas toy on the christmas tree in preparation for the new year?\n(A) A young handsome man\n(B) A child\n(C) A young beautiful woman\n(D) An old woman\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 312, "doc": {"video_name": "needle_132.mp4", "duration": 558.0699999999999, "question": "Who is hanging a golden christmas toy on the christmas tree in preparation for the new year?\n(A) A young beautiful woman\n(B) A child\n(C) An old woman\n(D) A young handsome man\n", "candidates": ["A young beautiful woman", "A child", "An old woman", "A young handsome man"], "answer": "A", "task_type": "needle", "question_id": "Q312"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d3d986efa155aa1329d705e487b81cfba4f28b24d7b5177c65bed641bbc6e8cd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Who is hanging a golden christmas toy on the christmas tree in preparation for the new year?\n(A) A young beautiful woman\n(B) A child\n(C) An old woman\n(D) A young handsome man\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "Who is hanging a golden christmas toy on the christmas tree in preparation for the new year?\n(A) A young beautiful woman\n(B) A child\n(C) An old woman\n(D) A young handsome man\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 316, "doc": {"video_name": "needle_70.mp4", "duration": 464.76, "question": "What are the tourists doing at the Plaza de Espana in Seville?\n(A) Taking a nap\n(B) Having a picnic\n(C) Admiring the fountain\n(D) Swimming in the fountain\n", "candidates": ["Taking a nap", "Having a picnic", "Admiring the fountain", "Swimming in the fountain"], "answer": "C", "task_type": "needle", "question_id": "Q316"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5487f923ec643354d3bb87418ac3a61805a2d015e22f8ff4f2cc34369ea81a87", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the tourists doing at the Plaza de Espana in Seville?\n(A) Taking a nap\n(B) Having a picnic\n(C) Admiring the fountain\n(D) Swimming in the fountain\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What are the tourists doing at the Plaza de Espana in Seville?\n(A) Taking a nap\n(B) Having a picnic\n(C) Admiring the fountain\n(D) Swimming in the fountain\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 320, "doc": {"video_name": "needle_40.mp4", "duration": 477.20000000000005, "question": "What animal is sitting very still at the mouth of the den in the video?\n(A) European toad\n(B) American toad\n(C) European frog\n(D) American frog\n", "candidates": ["European toad", "American toad", "European frog", "American frog"], "answer": "B", "task_type": "needle", "question_id": "Q320"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1a02f040477537250816db9f23742493ff6fe664b7e674d9619229d8355ee96e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What animal is sitting very still at the mouth of the den in the video?\n(A) European toad\n(B) American toad\n(C) European frog\n(D) American frog\n", "task_type": "needle", "pred_answer": "A", "answer": "B"}, "input": "What animal is sitting very still at the mouth of the den in the video?\n(A) European toad\n(B) American toad\n(C) European frog\n(D) American frog\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 324, "doc": {"video_name": "needle_45.mp4", "duration": 500.52, "question": "What type of protective gear are the scientists wearing in the laboratory?\n(A) Gloves and glasses\n(B) Safety boots\n(C) None\n(D) Helmets\n", "candidates": ["Gloves and glasses", "Safety boots", "None", "Helmets"], "answer": "A", "task_type": "needle", "question_id": "Q324"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e72beb720ee9a0d2e28fe28d92985e4b3f2963b3b825a40ca90976112fd31fc3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of protective gear are the scientists wearing in the laboratory?\n(A) Gloves and glasses\n(B) Safety boots\n(C) None\n(D) Helmets\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What type of protective gear are the scientists wearing in the laboratory?\n(A) Gloves and glasses\n(B) Safety boots\n(C) None\n(D) Helmets\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 328, "doc": {"video_name": "needle_81.mp4", "duration": 463.1, "question": "What is the floor made of where the egg falls and breaks?\n(A) Wood\n(B) Marble\n(C) Concrete\n(D) Glass\n", "candidates": ["Wood", "Marble", "Concrete", "Glass"], "answer": "D", "task_type": "needle", "question_id": "Q328"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7697a85bcad96cf736cf93834d001884a40b727b157fa052b42952ac524a39b0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the floor made of where the egg falls and breaks?\n(A) Wood\n(B) Marble\n(C) Concrete\n(D) Glass\n", "task_type": "needle", "pred_answer": "C", "answer": "D"}, "input": "What is the floor made of where the egg falls and breaks?\n(A) Wood\n(B) Marble\n(C) Concrete\n(D) Glass\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 332, "doc": {"video_name": "needle_5.mp4", "duration": 480.0, "question": "What is the man in the black silhouette wearing on the lake shore?\n(A) A suit\n(B) A swimsuit\n(C) A hat\n(D) A hood\n", "candidates": ["A suit", "A swimsuit", "A hat", "A hood"], "answer": "D", "task_type": "needle", "question_id": "Q332"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e22a210ce7348f1fd3c30745bfef301b18d281706445846b577462cfb2bdca6f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the man in the black silhouette wearing on the lake shore?\n(A) A suit\n(B) A swimsuit\n(C) A hat\n(D) A hood\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the man in the black silhouette wearing on the lake shore?\n(A) A suit\n(B) A swimsuit\n(C) A hat\n(D) A hood\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 336, "doc": {"video_name": "needle_5.mp4", "duration": 480.0, "question": "What is the man in the black silhouette holding on the lake shore?\n(A) A beach ball\n(B) A book\n(C) A controller\n(D) A fishing rod\n", "candidates": ["A beach ball", "A book", "A controller", "A fishing rod"], "answer": "C", "task_type": "needle", "question_id": "Q336"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "844271d916a6eb713c339d44db69c0baee557fd7b3124bd0f702472376ceb8c1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the man in the black silhouette holding on the lake shore?\n(A) A beach ball\n(B) A book\n(C) A controller\n(D) A fishing rod\n", "task_type": "needle", "pred_answer": "D", "answer": "C"}, "input": "What is the man in the black silhouette holding on the lake shore?\n(A) A beach ball\n(B) A book\n(C) A controller\n(D) A fishing rod\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 340, "doc": {"video_name": "needle_4.mp4", "duration": 510.0, "question": "What is the man in the black silhouette catching on the lake shore?\n(A) A drone quadcopter\n(B) A bird\n(C) A ball\n(D) A frisbee\n", "candidates": ["A drone quadcopter", "A bird", "A ball", "A frisbee"], "answer": "A", "task_type": "needle", "question_id": "Q340"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "30a2ec36de4c09c084a479bd09f4b683df54d470087700fa3db8fda89c5c512c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the man in the black silhouette catching on the lake shore?\n(A) A drone quadcopter\n(B) A bird\n(C) A ball\n(D) A frisbee\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the man in the black silhouette catching on the lake shore?\n(A) A drone quadcopter\n(B) A bird\n(C) A ball\n(D) A frisbee\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 344, "doc": {"video_name": "needle_35.mp4", "duration": 455.36, "question": "Who are using the face bow in the video segment?\n(A) The dentist and his assistant\n(B) The assistant alone\n(C) The patient\n(D) The dentist alone\n", "candidates": ["The dentist and his assistant", "The assistant alone", "The patient", "The dentist alone"], "answer": "A", "task_type": "needle", "question_id": "Q344"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f1d96e29d7677ff61b73b7858865d72c75c6d14825fa9131436035c18e15b2d9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Who are using the face bow in the video segment?\n(A) The dentist and his assistant\n(B) The assistant alone\n(C) The patient\n(D) The dentist alone\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "Who are using the face bow in the video segment?\n(A) The dentist and his assistant\n(B) The assistant alone\n(C) The patient\n(D) The dentist alone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 348, "doc": {"video_name": "needle_79.mp4", "duration": 465.05000000000007, "question": "What is painted in the colors of the Poland flag in the video?\n(A) The doctor's coat\n(B) The doctor's medical mask\n(C) The doctor's stethoscope\n(D) The doctor's glasses\n", "candidates": ["The doctor's coat", "The doctor's medical mask", "The doctor's stethoscope", "The doctor's glasses"], "answer": "D", "task_type": "needle", "question_id": "Q348"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1a6eae5d173bfde393f26af72f19a9e06c69be1040463d78028860c22d6bf63e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is painted in the colors of the Poland flag in the video?\n(A) The doctor's coat\n(B) The doctor's medical mask\n(C) The doctor's stethoscope\n(D) The doctor's glasses\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "What is painted in the colors of the Poland flag in the video?\n(A) The doctor's coat\n(B) The doctor's medical mask\n(C) The doctor's stethoscope\n(D) The doctor's glasses\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 352, "doc": {"video_name": "needle_82.mp4", "duration": 494.38, "question": "What is the background color when the man is laughing and covering his mouth?\n(A) Black\n(B) White\n(C) Green\n(D) Blue\n", "candidates": ["Black", "White", "Green", "Blue"], "answer": "B", "task_type": "needle", "question_id": "Q352"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f3a5ef334ada88e48c56a6f75383dfcf05ab9df51b6ed2d4c9301cc88d251753", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the background color when the man is laughing and covering his mouth?\n(A) Black\n(B) White\n(C) Green\n(D) Blue\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the background color when the man is laughing and covering his mouth?\n(A) Black\n(B) White\n(C) Green\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 356, "doc": {"video_name": "surveil_24.mp4", "duration": 227.4, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Fighting\n(B) Vandalism\n(C) Robbery\n(D) Assault\n", "candidates": ["Fighting", "Vandalism", "Robbery", "Assault"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q356"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "372f1b062c98b4e72ac9f925737adf4b05298b307de905eeedb6db4aaa9d6467", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Fighting\n(B) Vandalism\n(C) Robbery\n(D) Assault\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Fighting\n(B) Vandalism\n(C) Robbery\n(D) Assault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 360, "doc": {"video_name": "surveil_98.mp4", "duration": 331.59, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Stealing\n(C) Fighting\n(D) Assault\n", "candidates": ["Burglary", "Stealing", "Fighting", "Assault"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q360"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4ab69baf1e2718e05676e5bc251e588d216a22f388af093f2bbdf847f6b37a36", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Stealing\n(C) Fighting\n(D) Assault\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Stealing\n(C) Fighting\n(D) Assault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 364, "doc": {"video_name": "surveil_103.mp4", "duration": 3600.03, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Stealing\n(C) Vandalism\n(D) Burglary\n", "candidates": ["Normal", "Stealing", "Vandalism", "Burglary"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q364"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c136d559a3cd225bce12dac67402faec1d09cbd4c5dad84facae011a4cd1ffd7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Stealing\n(C) Vandalism\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Stealing\n(C) Vandalism\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 368, "doc": {"video_name": "surveil_3.mp4", "duration": 559.28, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Arrest\n(B) Assault\n(C) Fighting\n(D) Burglary\n", "candidates": ["Arrest", "Assault", "Fighting", "Burglary"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q368"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c170cfa2c14e8555bd3e3a3fd460671b8f2da8e03a1f179feb20b93a5883c556", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Arrest\n(B) Assault\n(C) Fighting\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Arrest\n(B) Assault\n(C) Fighting\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 372, "doc": {"video_name": "surveil_33.mp4", "duration": 430.79, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Vandalism\n(C) RoadAccidents\n(D) Burglary\n", "candidates": ["Normal", "Vandalism", "RoadAccidents", "Burglary"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q372"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ef9b379bf74b635f4968aaf9e836e72e2a308aa168a6e22c5784a158147bb3c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Vandalism\n(C) RoadAccidents\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "D"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Vandalism\n(C) RoadAccidents\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 376, "doc": {"video_name": "surveil_50.mp4", "duration": 212.74, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Normal\n(B) Arson\n(C) Robbery\n(D) Shooting\n", "candidates": ["Normal", "Arson", "Robbery", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q376"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ee9c37d98c5b45685a761c68b728a6180421ef8886d0dec4f690a3345f5337dd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Normal\n(B) Arson\n(C) Robbery\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Normal\n(B) Arson\n(C) Robbery\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 380, "doc": {"video_name": "surveil_180.mp4", "duration": 196.27, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Assault\n(C) Burglary\n(D) Vandalism\n", "candidates": ["Arrest", "Assault", "Burglary", "Vandalism"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q380"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc08194d36b81ae92c53f15be14050cd0251766f3395ad995ba4f253210631da", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Assault\n(C) Burglary\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Assault\n(C) Burglary\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 384, "doc": {"video_name": "surveil_171.mp4", "duration": 602.03, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Robbery\n(C) Explosion\n(D) RoadAccidents\n", "candidates": ["Stealing", "Robbery", "Explosion", "RoadAccidents"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q384"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "76db3df2953360aa63a46b6a2a981107180c85dc66a10a07db42e4bed82b74ad", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Robbery\n(C) Explosion\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Robbery\n(C) Explosion\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 388, "doc": {"video_name": "surveil_2.mp4", "duration": 225.89, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Arrest\n(C) Arson\n(D) Stealing\n", "candidates": ["Abuse", "Arrest", "Arson", "Stealing"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q388"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dcf5f8bc2dd86d98298d8991c61baa3e26ceae5eabe31d7a7d7346b5995e0448", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Arrest\n(C) Arson\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Arrest\n(C) Arson\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 392, "doc": {"video_name": "surveil_87.mp4", "duration": 300.03, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Normal\n(C) Shoplifting\n(D) Burglary\n", "candidates": ["RoadAccidents", "Normal", "Shoplifting", "Burglary"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q392"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4f982b27726699b067b9bdf4bc24166b0bedea9850421cbdd3c0431b4b00d62c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Normal\n(C) Shoplifting\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Normal\n(C) Shoplifting\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 396, "doc": {"video_name": "surveil_155.mp4", "duration": 266.68, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Abuse\n(C) RoadAccidents\n(D) Vandalism\n", "candidates": ["Arson", "Abuse", "RoadAccidents", "Vandalism"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q396"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c698484b5bed64274752c8f85e5e62ee1a303669bdf9273169b9722ef5996b1d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Abuse\n(C) RoadAccidents\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Abuse\n(C) RoadAccidents\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 400, "doc": {"video_name": "surveil_86.mp4", "duration": 252.27, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) RoadAccidents\n(C) Shooting\n(D) Shooting\n", "candidates": ["Stealing", "RoadAccidents", "Shooting", "Shooting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q400"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "83ba52751324a58b8871abdc1797c625ce082f1a8773f7bc05241e0076ef9b35", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) RoadAccidents\n(C) Shooting\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) RoadAccidents\n(C) Shooting\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 404, "doc": {"video_name": "surveil_176.mp4", "duration": 32550.13, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Shooting\n(C) Normal\n(D) Fighting\n", "candidates": ["Stealing", "Shooting", "Normal", "Fighting"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q404"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "835d1cca61945977e459444c4ca10de38e5e7ba5ade7cc05c98b101d2034c096", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Shooting\n(C) Normal\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Shooting\n(C) Normal\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 408, "doc": {"video_name": "surveil_123.mp4", "duration": 600.36, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Arrest\n(C) RoadAccidents\n(D) Normal\n", "candidates": ["Arson", "Arrest", "RoadAccidents", "Normal"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q408"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1a0d4e0163989ae251abffb0154298e5d4ab80d59be5722138a2880342ccc078", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Arrest\n(C) RoadAccidents\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Arrest\n(C) RoadAccidents\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 412, "doc": {"video_name": "surveil_104.mp4", "duration": 220.13, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Burglary\n(B) Stealing\n(C) Arrest\n(D) Abuse\n", "candidates": ["Burglary", "Stealing", "Arrest", "Abuse"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q412"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "57b9415df8aa50b553992c48b143bb71892b454333bd2df6c272fd093f775b67", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Burglary\n(B) Stealing\n(C) Arrest\n(D) Abuse\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "B"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Burglary\n(B) Stealing\n(C) Arrest\n(D) Abuse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 416, "doc": {"video_name": "surveil_62.mp4", "duration": 231.67000000000002, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Assault\n(B) Vandalism\n(C) Shooting\n(D) Robbery\n", "candidates": ["Assault", "Vandalism", "Shooting", "Robbery"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q416"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8335ad083d82ee9befab7381e747af7124a5fc834a9a4d59ca7af65e09700534", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Assault\n(B) Vandalism\n(C) Shooting\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Assault\n(B) Vandalism\n(C) Shooting\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 420, "doc": {"video_name": "surveil_169.mp4", "duration": 196.63, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Vandalism\n(C) Burglary\n(D) Shoplifting\n", "candidates": ["Arson", "Vandalism", "Burglary", "Shoplifting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q420"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b82aaa7222f23e53b9dd3e90e6934c11f1e99894586aaa50bc3b75f5404b76da", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Vandalism\n(C) Burglary\n(D) Shoplifting\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Vandalism\n(C) Burglary\n(D) Shoplifting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 424, "doc": {"video_name": "surveil_167.mp4", "duration": 375.89, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Normal\n(C) Robbery\n(D) Shoplifting\n", "candidates": ["RoadAccidents", "Normal", "Robbery", "Shoplifting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q424"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "32fa7ac478774ed61ac02b5bc81ecb36c00bf002bb8c21a66d51e154e7d88d35", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Normal\n(C) Robbery\n(D) Shoplifting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Normal\n(C) Robbery\n(D) Shoplifting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 428, "doc": {"video_name": "surveil_189.mp4", "duration": 220.32, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shoplifting\n(B) Arrest\n(C) Stealing\n(D) Robbery\n", "candidates": ["Shoplifting", "Arrest", "Stealing", "Robbery"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q428"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8e7c5ed3dbcf7003068bebb921b49461d07097d38047bef738d475677d8667f9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shoplifting\n(B) Arrest\n(C) Stealing\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shoplifting\n(B) Arrest\n(C) Stealing\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 432, "doc": {"video_name": "surveil_47.mp4", "duration": 1420.03, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Vandalism\n(B) Normal\n(C) Shoplifting\n(D) Shooting\n", "candidates": ["Vandalism", "Normal", "Shoplifting", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q432"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6bd6ef2aadfcc6ad694b100fb917eeff2d85263344cb43c569dae2e09aff2ab3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Vandalism\n(B) Normal\n(C) Shoplifting\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Vandalism\n(B) Normal\n(C) Shoplifting\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 436, "doc": {"video_name": "surveil_88.mp4", "duration": 248.34, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) RoadAccidents\n(C) Fighting\n(D) Explosion\n", "candidates": ["Shoplifting", "RoadAccidents", "Fighting", "Explosion"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q436"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c1782e946b7ef08e7822337853ead7e187d0d672f882f026443c78c33cb4eb1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) RoadAccidents\n(C) Fighting\n(D) Explosion\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) RoadAccidents\n(C) Fighting\n(D) Explosion\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 440, "doc": {"video_name": "surveil_28.mp4", "duration": 915.2, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Normal\n(B) Burglary\n(C) Abuse\n(D) Vandalism\n", "candidates": ["Normal", "Burglary", "Abuse", "Vandalism"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q440"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2cca53b66b26e0f0a2578d9124245bebabc3b0beece627c19689cda0bf179eed", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Normal\n(B) Burglary\n(C) Abuse\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Normal\n(B) Burglary\n(C) Abuse\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 444, "doc": {"video_name": "surveil_49.mp4", "duration": 484.49, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Robbery\n(B) RoadAccidents\n(C) Normal\n(D) Stealing\n", "candidates": ["Robbery", "RoadAccidents", "Normal", "Stealing"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q444"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8c9a3f1b94891a6c363182c15af3aed999eb3f654087e1d0b3db152f763922a7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Robbery\n(B) RoadAccidents\n(C) Normal\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Robbery\n(B) RoadAccidents\n(C) Normal\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 448, "doc": {"video_name": "surveil_97.mp4", "duration": 269.87, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Assault\n(B) Shooting\n(C) Fighting\n(D) RoadAccidents\n", "candidates": ["Assault", "Shooting", "Fighting", "RoadAccidents"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q448"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7fb8d4590eda7f9d3ea0f8392f4355ecc948a0838da866efb755f7435995bc0f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Assault\n(B) Shooting\n(C) Fighting\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Assault\n(B) Shooting\n(C) Fighting\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 452, "doc": {"video_name": "surveil_112.mp4", "duration": 317.3, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Fighting\n(C) Assault\n(D) Arrest\n", "candidates": ["Shoplifting", "Fighting", "Assault", "Arrest"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q452"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8a280b86639c6046182723269d0a966576db36d99fcd53199839c3e36c1d536d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Fighting\n(C) Assault\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Fighting\n(C) Assault\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 456, "doc": {"video_name": "surveil_184.mp4", "duration": 218.17000000000002, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Shoplifting\n(C) Fighting\n(D) Explosion\n", "candidates": ["Arrest", "Shoplifting", "Fighting", "Explosion"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q456"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "914a6086f7dd2275a47dd81d3235f7e636bd13b94fa5c569b1c1d457e5176014", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Shoplifting\n(C) Fighting\n(D) Explosion\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Shoplifting\n(C) Fighting\n(D) Explosion\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 460, "doc": {"video_name": "surveil_151.mp4", "duration": 228.67000000000002, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Shoplifting\n(C) Fighting\n(D) Normal\n", "candidates": ["Stealing", "Shoplifting", "Fighting", "Normal"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q460"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ac2ada2db1d44aa62294680796e94f6e5f5cb5fd20542c63327748abf54bbe77", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Shoplifting\n(C) Fighting\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "A"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Shoplifting\n(C) Fighting\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 464, "doc": {"video_name": "surveil_95.mp4", "duration": 301.13, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Fighting\n(B) Vandalism\n(C) Shooting\n(D) Normal\n", "candidates": ["Fighting", "Vandalism", "Shooting", "Normal"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q464"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cb48d4bbf1ba91f5a79df400db8f3db02e385567e75dd99895078e6e96f00adb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Fighting\n(B) Vandalism\n(C) Shooting\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Fighting\n(B) Vandalism\n(C) Shooting\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 468, "doc": {"video_name": "surveil_168.mp4", "duration": 315.14, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Explosion\n(C) Robbery\n(D) Stealing\n", "candidates": ["Arson", "Explosion", "Robbery", "Stealing"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q468"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ec0781b5786c0ed6fa519ec0f2dd56dfcbd981ab92a29ff80de26624d4eddcd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Explosion\n(C) Robbery\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Explosion\n(C) Robbery\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 472, "doc": {"video_name": "surveil_10.mp4", "duration": 359.97, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Arrest\n(C) Assault\n(D) Stealing\n", "candidates": ["RoadAccidents", "Arrest", "Assault", "Stealing"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q472"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e0e1a9c6e27863fd99d6370ead2d260c02b2e73769847a0bb851b71444926522", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Arrest\n(C) Assault\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Arrest\n(C) Assault\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 476, "doc": {"video_name": "surveil_39.mp4", "duration": 196.05, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Abuse\n(B) RoadAccidents\n(C) Burglary\n(D) Arson\n", "candidates": ["Abuse", "RoadAccidents", "Burglary", "Arson"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q476"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68427866d9c4a741c408385eebe110d940e762e9424c6619be7fdb9144e74b5a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Abuse\n(B) RoadAccidents\n(C) Burglary\n(D) Arson\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Abuse\n(B) RoadAccidents\n(C) Burglary\n(D) Arson\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 480, "doc": {"video_name": "surveil_110.mp4", "duration": 527.86, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arrest\n(B) Normal\n(C) Burglary\n(D) Shoplifting\n", "candidates": ["Arrest", "Normal", "Burglary", "Shoplifting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q480"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fb7ea64b3f878d4db4bd417648c968b4739a904106264ac826e6ded2d5ff8e1c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arrest\n(B) Normal\n(C) Burglary\n(D) Shoplifting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arrest\n(B) Normal\n(C) Burglary\n(D) Shoplifting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 484, "doc": {"video_name": "surveil_102.mp4", "duration": 415.0, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Arrest\n(C) Vandalism\n(D) Fighting\n", "candidates": ["Shooting", "Arrest", "Vandalism", "Fighting"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q484"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1d01fdae937a1921494eac28daebd1bd4ce88b19c1f2f6d506696d2f9ea987ad", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Arrest\n(C) Vandalism\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Arrest\n(C) Vandalism\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 488, "doc": {"video_name": "surveil_99.mp4", "duration": 334.18, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Arson\n(C) Burglary\n(D) Normal\n", "candidates": ["Stealing", "Arson", "Burglary", "Normal"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q488"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0183b4fec4bbc02314ecb33450f69699be3b6f9fd8a2cdb35e0fafe89e766dc9", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Arson\n(C) Burglary\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Arson\n(C) Burglary\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 492, "doc": {"video_name": "surveil_56.mp4", "duration": 237.76, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Normal\n(C) Arson\n(D) Stealing\n", "candidates": ["Burglary", "Normal", "Arson", "Stealing"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q492"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "943fffe967da305c6999f771bd42d33d5526bb47da89df88937ec83995d892c7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Normal\n(C) Arson\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Normal\n(C) Arson\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 496, "doc": {"video_name": "surveil_7.mp4", "duration": 262.83, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Abuse\n(C) Vandalism\n(D) Arrest\n", "candidates": ["Robbery", "Abuse", "Vandalism", "Arrest"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q496"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1126278f1f44c9008c6eef8e773553921f247c9db0c535735255b8ac2aaa0e1e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Abuse\n(C) Vandalism\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "D"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Abuse\n(C) Vandalism\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 500, "doc": {"video_name": "surveil_42.mp4", "duration": 240.64, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Shooting\n(C) Explosion\n(D) Vandalism\n", "candidates": ["Shoplifting", "Shooting", "Explosion", "Vandalism"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q500"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f16888cc0640d4b141559d8d74d349857a31706402bc3bda49637f223baf9eb", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Shooting\n(C) Explosion\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Shooting\n(C) Explosion\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 504, "doc": {"video_name": "surveil_55.mp4", "duration": 220.6, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Burglary\n(C) Assault\n(D) Vandalism\n", "candidates": ["Shooting", "Burglary", "Assault", "Vandalism"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q504"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "06378f12d0190123ff588a9c8c86ab822f62f26aee6d52a0c0ef98d847552d03", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Burglary\n(C) Assault\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Burglary\n(C) Assault\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 508, "doc": {"video_name": "surveil_114.mp4", "duration": 220.27, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Shooting\n(C) Explosion\n(D) Fighting\n", "candidates": ["Abuse", "Shooting", "Explosion", "Fighting"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q508"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "46bd7df2bfed21470c1524d6a34c0b0b31e8c9307d97b0f4b6cc14730720e61b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Shooting\n(C) Explosion\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Shooting\n(C) Explosion\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 512, "doc": {"video_name": "surveil_83.mp4", "duration": 511.91, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Stealing\n(C) Shooting\n(D) Fighting\n", "candidates": ["Normal", "Stealing", "Shooting", "Fighting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q512"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6af5f152939ce17e7bca5b50934943dc3a53bffa5a711abcb108781483b82767", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Stealing\n(C) Shooting\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Stealing\n(C) Shooting\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 516, "doc": {"video_name": "surveil_170.mp4", "duration": 184.83, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Abuse\n(C) Arrest\n(D) Normal\n", "candidates": ["Arson", "Abuse", "Arrest", "Normal"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q516"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85d17fbea6de22ae333527ed0e77f7bb931ac664df0a0fee13a8ab693b07718c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Abuse\n(C) Arrest\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Abuse\n(C) Arrest\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 520, "doc": {"video_name": "surveil_74.mp4", "duration": 524.46, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Robbery\n(C) Stealing\n(D) Abuse\n", "candidates": ["Shooting", "Robbery", "Stealing", "Abuse"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q520"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a89bbc78be78fa37fd4312b9b1ea4de06a0350041f6aa4c3a6e53cabaf69a44", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Robbery\n(C) Stealing\n(D) Abuse\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Robbery\n(C) Stealing\n(D) Abuse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 524, "doc": {"video_name": "surveil_196.mp4", "duration": 312.55, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Assault\n(C) Burglary\n(D) Normal\n", "candidates": ["RoadAccidents", "Assault", "Burglary", "Normal"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q524"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ae07a6c86509a549e1712f668790b183b1178387fb9ae7fb484d27445cd997c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Assault\n(C) Burglary\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Assault\n(C) Burglary\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 528, "doc": {"video_name": "surveil_163.mp4", "duration": 509.27, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) RoadAccidents\n(B) Robbery\n(C) Arrest\n(D) Shooting\n", "candidates": ["RoadAccidents", "Robbery", "Arrest", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q528"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "84334bd9bcd421c8e32196194d81f8424c515009bd1e8e7c1487dda1c6f42d73", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) RoadAccidents\n(B) Robbery\n(C) Arrest\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) RoadAccidents\n(B) Robbery\n(C) Arrest\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 532, "doc": {"video_name": "surveil_94.mp4", "duration": 451.44, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shoplifting\n(B) Arson\n(C) Stealing\n(D) Assault\n", "candidates": ["Shoplifting", "Arson", "Stealing", "Assault"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q532"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c7b6deaaf8dd9697d8bc5c19cc68eda86b2b4fb1f192cd07f8032a1561e673b3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shoplifting\n(B) Arson\n(C) Stealing\n(D) Assault\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shoplifting\n(B) Arson\n(C) Stealing\n(D) Assault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 536, "doc": {"video_name": "surveil_31.mp4", "duration": 220.96, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Shooting\n(C) Normal\n(D) RoadAccidents\n", "candidates": ["Stealing", "Shooting", "Normal", "RoadAccidents"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q536"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7e7f7881962096a9fc87c9f56ac54ad8204b569d75fef2943f46bb7865c5d290", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Shooting\n(C) Normal\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Shooting\n(C) Normal\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 540, "doc": {"video_name": "surveil_61.mp4", "duration": 396.5, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arrest\n(B) RoadAccidents\n(C) Abuse\n(D) Shooting\n", "candidates": ["Arrest", "RoadAccidents", "Abuse", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q540"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87d5c56fe75dee31d522e446f57610df9ddcf273224c12c3adc17ca3b6c73bf5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arrest\n(B) RoadAccidents\n(C) Abuse\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arrest\n(B) RoadAccidents\n(C) Abuse\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 544, "doc": {"video_name": "surveil_113.mp4", "duration": 202.62, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Vandalism\n(B) Arson\n(C) Shooting\n(D) Stealing\n", "candidates": ["Vandalism", "Arson", "Shooting", "Stealing"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q544"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c7bd14885e785a8d1a6a0a195efc1fed94a0babe978b7bede326081050c68d34", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Vandalism\n(B) Arson\n(C) Shooting\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Vandalism\n(B) Arson\n(C) Shooting\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 548, "doc": {"video_name": "surveil_89.mp4", "duration": 187.37, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) RoadAccidents\n(B) Fighting\n(C) Shooting\n(D) Arson\n", "candidates": ["RoadAccidents", "Fighting", "Shooting", "Arson"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q548"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fb1883197d1ebe8662b21478d2b5632c235725ba8457675719366e2641eec7b1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) RoadAccidents\n(B) Fighting\n(C) Shooting\n(D) Arson\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) RoadAccidents\n(B) Fighting\n(C) Shooting\n(D) Arson\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 552, "doc": {"video_name": "surveil_158.mp4", "duration": 213.69, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Shoplifting\n(C) Explosion\n(D) Normal\n", "candidates": ["Shooting", "Shoplifting", "Explosion", "Normal"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q552"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5142cf913c16c30bf3d958b5ad150d7cb26bb8f48783775c6260f5bd0a31ee4d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Shoplifting\n(C) Explosion\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Shoplifting\n(C) Explosion\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 556, "doc": {"video_name": "count_151.mp4", "duration": 872.9900000000001, "question": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 3\n(B) 2\n(C) 1\n(D) 6\n", "candidates": ["3", "2", "1", "6"], "answer": "B", "task_type": "count", "question_id": "Q556"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b54308ce412594e84116fca5bd84ca1eb41b67cc9e3274405894a525ffd2a73b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 3\n(B) 2\n(C) 1\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "B"}, "input": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 3\n(B) 2\n(C) 1\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 560, "doc": {"video_name": "count_140.mp4", "duration": 509.12999999999994, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'javelin throw' action\n(A) 4\n(B) 2\n(C) 3\n(D) 1\n", "candidates": ["4", "2", "3", "1"], "answer": "C", "task_type": "count", "question_id": "Q560"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eb3da8960ff30656ed67cbc8741b3fb90b2be26972ae5dd048ded50af7c706fb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'javelin throw' action\n(A) 4\n(B) 2\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'javelin throw' action\n(A) 4\n(B) 2\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 564, "doc": {"video_name": "count_76.mp4", "duration": 480.14, "question": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 3\n(B) 4\n(C) 0\n(D) 2\n", "candidates": ["3", "4", "0", "2"], "answer": "A", "task_type": "count", "question_id": "Q564"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0020c5f5620707f8bd9866c5c90cd061f51fe800189218e3d86282a1b26d0e00", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 3\n(B) 4\n(C) 0\n(D) 2\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 3\n(B) 4\n(C) 0\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 568, "doc": {"video_name": "count_72.mp4", "duration": 470.02, "question": "In this video, how many instances are there of the 'cleaning toilet' action scene in total?\n(A) 2\n(B) 5\n(C) 3\n(D) 0\n", "candidates": ["2", "5", "3", "0"], "answer": "A", "task_type": "count", "question_id": "Q568"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8247d76482c7178e827628fbd199936147251d33e1f5065f17b4679561caff01", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'cleaning toilet' action scene in total?\n(A) 2\n(B) 5\n(C) 3\n(D) 0\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "In this video, how many instances are there of the 'cleaning toilet' action scene in total?\n(A) 2\n(B) 5\n(C) 3\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 572, "doc": {"video_name": "count_19.mp4", "duration": 470.02, "question": "In this video, how many times does the scene of the 'tossing coin' action appear in total?\n(A) 5\n(B) 3\n(C) 2\n(D) 6\n", "candidates": ["5", "3", "2", "6"], "answer": "C", "task_type": "count", "question_id": "Q572"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "52ea0d6c9edb035602028d9dce2ef014b0c913481d379ddd34aabd575cf61b43", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'tossing coin' action appear in total?\n(A) 5\n(B) 3\n(C) 2\n(D) 6\n", "task_type": "count", "pred_answer": "B", "answer": "C"}, "input": "In this video, how many times does the scene of the 'tossing coin' action appear in total?\n(A) 5\n(B) 3\n(C) 2\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 576, "doc": {"video_name": "count_188.mp4", "duration": 480.03, "question": "In this video, how many times does the scene of the 'paragliding' action appear in total?\n(A) 6\n(B) 3\n(C) 2\n(D) 5\n", "candidates": ["6", "3", "2", "5"], "answer": "B", "task_type": "count", "question_id": "Q576"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "88ec2271edae116aea631b8668ac6972982846a784d4a11464a7facb8a4330c2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'paragliding' action appear in total?\n(A) 6\n(B) 3\n(C) 2\n(D) 5\n", "task_type": "count", "pred_answer": "A", "answer": "B"}, "input": "In this video, how many times does the scene of the 'paragliding' action appear in total?\n(A) 6\n(B) 3\n(C) 2\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 580, "doc": {"video_name": "count_12.mp4", "duration": 460.01, "question": "In this video, how many times does the scene of the 'tossing coin' action appear in total?\n(A) 3\n(B) 4\n(C) 6\n(D) 1\n", "candidates": ["3", "4", "6", "1"], "answer": "D", "task_type": "count", "question_id": "Q580"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bbbc849c33a3102d9b3d04e4ebc43e5a007ef08a11599a33006ec7789324439a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'tossing coin' action appear in total?\n(A) 3\n(B) 4\n(C) 6\n(D) 1\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "In this video, how many times does the scene of the 'tossing coin' action appear in total?\n(A) 3\n(B) 4\n(C) 6\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 584, "doc": {"video_name": "count_10.mp4", "duration": 480.04999999999995, "question": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 2\n", "candidates": ["1", "6", "3", "2"], "answer": "C", "task_type": "count", "question_id": "Q584"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "277d691e3faac0a04965ef57f8f0b849feffd1b8fdb8fa94573c2e1ec208d5f7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 2\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 588, "doc": {"video_name": "count_169.mp4", "duration": 480.15, "question": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 0\n(B) 4\n(C) 3\n(D) 6\n", "candidates": ["0", "4", "3", "6"], "answer": "C", "task_type": "count", "question_id": "Q588"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b98bcab36830a664a655ba6d220b46eb3ff23128bc26b7a75cd44b4540496cba", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 0\n(B) 4\n(C) 3\n(D) 6\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 0\n(B) 4\n(C) 3\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 592, "doc": {"video_name": "count_183.mp4", "duration": 460.01, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 0\n(B) 1\n(C) 3\n(D) 4\n", "candidates": ["0", "1", "3", "4"], "answer": "B", "task_type": "count", "question_id": "Q592"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6ce60f85c3b8016e9b567847b9f911210b71fd26b57cffc1a1565ea2dae29799", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 0\n(B) 1\n(C) 3\n(D) 4\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 0\n(B) 1\n(C) 3\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 596, "doc": {"video_name": "count_69.mp4", "duration": 458.28999999999996, "question": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 1\n(B) 6\n(C) 0\n(D) 5\n", "candidates": ["1", "6", "0", "5"], "answer": "A", "task_type": "count", "question_id": "Q596"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1a4474df2692d52138dc6e7802a993fd6e5270dc9f8b2f1821d5dac7c4a6500a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 1\n(B) 6\n(C) 0\n(D) 5\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 1\n(B) 6\n(C) 0\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 600, "doc": {"video_name": "count_66.mp4", "duration": 521.34, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'pole vault' action\n(A) 0\n(B) 6\n(C) 4\n(D) 2\n", "candidates": ["0", "6", "4", "2"], "answer": "D", "task_type": "count", "question_id": "Q600"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "86d9d5f3d8ac58e59d7aa113dc4b2dba3821d3257da076a528829400ee532306", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'pole vault' action\n(A) 0\n(B) 6\n(C) 4\n(D) 2\n", "task_type": "count", "pred_answer": "D", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'pole vault' action\n(A) 0\n(B) 6\n(C) 4\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 604, "doc": {"video_name": "count_157.mp4", "duration": 470.01, "question": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 3\n(B) 2\n(C) 0\n(D) 6\n", "candidates": ["3", "2", "0", "6"], "answer": "B", "task_type": "count", "question_id": "Q604"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "166068a8adf99cdc41e09c0490931be96decc8a90eb9d00f638ad24a7fbbf81e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 3\n(B) 2\n(C) 0\n(D) 6\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 3\n(B) 2\n(C) 0\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 608, "doc": {"video_name": "count_60.mp4", "duration": 498.28999999999996, "question": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 2\n(B) 5\n(C) 4\n(D) 6\n", "candidates": ["2", "5", "4", "6"], "answer": "A", "task_type": "count", "question_id": "Q608"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "45205e2658af9d4d1b7b9b4c01563679ed489d6ea03abcb2e5454a2b814d0010", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 2\n(B) 5\n(C) 4\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 2\n(B) 5\n(C) 4\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 612, "doc": {"video_name": "count_209.mp4", "duration": 512.04, "question": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 6\n(B) 2\n(C) 4\n(D) 3\n", "candidates": ["6", "2", "4", "3"], "answer": "D", "task_type": "count", "question_id": "Q612"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cbefd3e6d3cd26b4e76220b23e6cc90a80b07e1a225bd01f5c8c87f50cc38651", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 6\n(B) 2\n(C) 4\n(D) 3\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 6\n(B) 2\n(C) 4\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 616, "doc": {"video_name": "count_216.mp4", "duration": 7486.57, "question": "In this video, how many instances are there of the 'shredding paper' action scene in total?\n(A) 4\n(B) 2\n(C) 3\n(D) 1\n", "candidates": ["4", "2", "3", "1"], "answer": "C", "task_type": "count", "question_id": "Q616"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc87693180e003fc163c61b003d801fa157ee14fe22dd4cbde9418f1c657b53a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'shredding paper' action scene in total?\n(A) 4\n(B) 2\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many instances are there of the 'shredding paper' action scene in total?\n(A) 4\n(B) 2\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 620, "doc": {"video_name": "count_99.mp4", "duration": 480.09999999999997, "question": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 2\n(B) 0\n(C) 5\n(D) 3\n", "candidates": ["2", "0", "5", "3"], "answer": "D", "task_type": "count", "question_id": "Q620"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "983a6e5edbb9d1d6dc689e444a5f1fdd4d51a7dcf5dcf919260eed2fb6b385e1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 2\n(B) 0\n(C) 5\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 2\n(B) 0\n(C) 5\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 624, "doc": {"video_name": "count_95.mp4", "duration": 500.03999999999996, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'playing harp' action\n(A) 0\n(B) 5\n(C) 3\n(D) 4\n", "candidates": ["0", "5", "3", "4"], "answer": "B", "task_type": "count", "question_id": "Q624"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f45a77aa44df6a6d77595a4083b2d565f02fc3e562591d4fe7023779fc6ddb08", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'playing harp' action\n(A) 0\n(B) 5\n(C) 3\n(D) 4\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'playing harp' action\n(A) 0\n(B) 5\n(C) 3\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 628, "doc": {"video_name": "count_91.mp4", "duration": 530.04, "question": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 6\n(B) 5\n(C) 0\n(D) 2\n", "candidates": ["6", "5", "0", "2"], "answer": "B", "task_type": "count", "question_id": "Q628"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "81989df1a5fc55af6d753b1cd4ec2e43a0bc0930d85a1477996247120885c6e4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 6\n(B) 5\n(C) 0\n(D) 2\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 6\n(B) 5\n(C) 0\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 632, "doc": {"video_name": "count_116.mp4", "duration": 470.02, "question": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 3\n(B) 2\n(C) 0\n(D) 5\n", "candidates": ["3", "2", "0", "5"], "answer": "B", "task_type": "count", "question_id": "Q632"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf3e88c64dc48da3ab9ad5c2a4b2b5511891d68098523096f940bfd630010526", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 3\n(B) 2\n(C) 0\n(D) 5\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 3\n(B) 2\n(C) 0\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 636, "doc": {"video_name": "count_57.mp4", "duration": 489.34000000000003, "question": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 3\n(B) 5\n(C) 4\n(D) 6\n", "candidates": ["3", "5", "4", "6"], "answer": "C", "task_type": "count", "question_id": "Q636"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4f4d5a39766e61aff62cda35af4a3137f255713c4b6ae5126776566b8d15cfe7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 3\n(B) 5\n(C) 4\n(D) 6\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 3\n(B) 5\n(C) 4\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 640, "doc": {"video_name": "count_54.mp4", "duration": 510.01, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'milking cow' action\n(A) 3\n(B) 1\n(C) 2\n(D) 5\n", "candidates": ["3", "1", "2", "5"], "answer": "A", "task_type": "count", "question_id": "Q640"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ba520a6974a58c74b31a7cba30f9f21916cecc65604f107eecd616b796d66c06", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'milking cow' action\n(A) 3\n(B) 1\n(C) 2\n(D) 5\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'milking cow' action\n(A) 3\n(B) 1\n(C) 2\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 644, "doc": {"video_name": "count_51.mp4", "duration": 499.3, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'milking cow' action\n(A) 2\n(B) 4\n(C) 0\n(D) 5\n", "candidates": ["2", "4", "0", "5"], "answer": "D", "task_type": "count", "question_id": "Q644"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "737a7ced6de074102da624bad22a3ccb30faec88a5998b3043d807992a77028d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'milking cow' action\n(A) 2\n(B) 4\n(C) 0\n(D) 5\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'milking cow' action\n(A) 2\n(B) 4\n(C) 0\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 648, "doc": {"video_name": "count_122.mp4", "duration": 476.95, "question": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 4\n(B) 3\n(C) 6\n(D) 5\n", "candidates": ["4", "3", "6", "5"], "answer": "B", "task_type": "count", "question_id": "Q648"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ed5073c5210eab37f7b57ee53b28b87978c92ca40282f526821d878a1cd23554", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 4\n(B) 3\n(C) 6\n(D) 5\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 4\n(B) 3\n(C) 6\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 652, "doc": {"video_name": "count_185.mp4", "duration": 894.3299999999999, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 4\n(B) 0\n(C) 5\n(D) 6\n", "candidates": ["4", "0", "5", "6"], "answer": "C", "task_type": "count", "question_id": "Q652"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ac8391a77d4fd74910f3cd96a2b38a134f8abab2345f0af759251f0e6ed3b701", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 4\n(B) 0\n(C) 5\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 4\n(B) 0\n(C) 5\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 656, "doc": {"video_name": "count_192.mp4", "duration": 490.0299999999999, "question": "In this video, how many times does the scene of the 'baking cookies' action appear in total?\n(A) 6\n(B) 5\n(C) 4\n(D) 1\n", "candidates": ["6", "5", "4", "1"], "answer": "C", "task_type": "count", "question_id": "Q656"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d858527d58009e5d35651c01e2361f265f2a3cd857547cb6d970dec6b7bfba04", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'baking cookies' action appear in total?\n(A) 6\n(B) 5\n(C) 4\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many times does the scene of the 'baking cookies' action appear in total?\n(A) 6\n(B) 5\n(C) 4\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 660, "doc": {"video_name": "count_85.mp4", "duration": 490.08, "question": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 5\n(B) 4\n(C) 6\n(D) 0\n", "candidates": ["5", "4", "6", "0"], "answer": "B", "task_type": "count", "question_id": "Q660"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d7b3256e9f7384a69248b1ebec2c11c80f72c751f8a3de61299c6a26a785de5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 5\n(B) 4\n(C) 6\n(D) 0\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 5\n(B) 4\n(C) 6\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 664, "doc": {"video_name": "count_82.mp4", "duration": 480.03, "question": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 5\n(B) 0\n(C) 2\n(D) 3\n", "candidates": ["5", "0", "2", "3"], "answer": "D", "task_type": "count", "question_id": "Q664"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f91f0ca926cd39644933724cad0ad3b6c6af7134372fd4cb5beab68a5a8dde4f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 5\n(B) 0\n(C) 2\n(D) 3\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 5\n(B) 0\n(C) 2\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 668, "doc": {"video_name": "count_184.mp4", "duration": 470.02, "question": "In this video, how many instances are there of the 'paragliding' action scene in total?\n(A) 4\n(B) 5\n(C) 2\n(D) 6\n", "candidates": ["4", "5", "2", "6"], "answer": "C", "task_type": "count", "question_id": "Q668"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "596f6d4bb4d0eed5330ce1834a1f8a702e63fd2d0c9356f184045166ad176c91", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'paragliding' action scene in total?\n(A) 4\n(B) 5\n(C) 2\n(D) 6\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many instances are there of the 'paragliding' action scene in total?\n(A) 4\n(B) 5\n(C) 2\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 672, "doc": {"video_name": "count_148.mp4", "duration": 460.01, "question": "In this video, how many instances are there of the 'javelin throw' action scene in total?\n(A) 0\n(B) 2\n(C) 5\n(D) 1\n", "candidates": ["0", "2", "5", "1"], "answer": "D", "task_type": "count", "question_id": "Q672"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d8ef69d04689463330f4733c061b4d6eba5bddd561bc5f30294ee8e9ac91a193", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'javelin throw' action scene in total?\n(A) 0\n(B) 2\n(C) 5\n(D) 1\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "In this video, how many instances are there of the 'javelin throw' action scene in total?\n(A) 0\n(B) 2\n(C) 5\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 676, "doc": {"video_name": "count_49.mp4", "duration": 486.76, "question": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 2\n(B) 1\n(C) 3\n(D) 5\n", "candidates": ["2", "1", "3", "5"], "answer": "B", "task_type": "count", "question_id": "Q676"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eddf0578785917fd25d21132db6db19b93fb827e992112206e022f42c43f34a9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 2\n(B) 1\n(C) 3\n(D) 5\n", "task_type": "count", "pred_answer": "A", "answer": "B"}, "input": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 2\n(B) 1\n(C) 3\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 680, "doc": {"video_name": "count_155.mp4", "duration": 1546.17, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'shredding paper' action\n(A) 2\n(B) 6\n(C) 0\n(D) 5\n", "candidates": ["2", "6", "0", "5"], "answer": "D", "task_type": "count", "question_id": "Q680"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "27487ccc9d784e61937ac321bd11725f5802b4bc6336dfcc1d2aafa37eaa56bd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'shredding paper' action\n(A) 2\n(B) 6\n(C) 0\n(D) 5\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'shredding paper' action\n(A) 2\n(B) 6\n(C) 0\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 684, "doc": {"video_name": "count_180.mp4", "duration": 460.01000000000005, "question": "In this video, how many times does the scene of the 'paragliding' action appear in total?\n(A) 1\n(B) 5\n(C) 3\n(D) 2\n", "candidates": ["1", "5", "3", "2"], "answer": "A", "task_type": "count", "question_id": "Q684"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dbe0e05194b17ecf1c4ecc1ef88f1794dae78eb237b6c4b324ab754b3d1c8afa", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'paragliding' action appear in total?\n(A) 1\n(B) 5\n(C) 3\n(D) 2\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "In this video, how many times does the scene of the 'paragliding' action appear in total?\n(A) 1\n(B) 5\n(C) 3\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 688, "doc": {"video_name": "count_162.mp4", "duration": 470.02, "question": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 2\n(B) 6\n(C) 3\n(D) 1\n", "candidates": ["2", "6", "3", "1"], "answer": "A", "task_type": "count", "question_id": "Q688"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1e55d58479f7723c029b8f512f8d31fad4d1d892bb4713ec05566f3b59f770a7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 2\n(B) 6\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 2\n(B) 6\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 692, "doc": {"video_name": "count_129.mp4", "duration": 480.04999999999995, "question": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 4\n(B) 1\n(C) 3\n(D) 5\n", "candidates": ["4", "1", "3", "5"], "answer": "C", "task_type": "count", "question_id": "Q692"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e3f0360f62310b45c5b4082d1eb53fd7d77de53acd98a79158e0b37b8850ac98", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 4\n(B) 1\n(C) 3\n(D) 5\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 4\n(B) 1\n(C) 3\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 696, "doc": {"video_name": "count_161.mp4", "duration": 470.02, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 0\n(B) 2\n(C) 1\n(D) 3\n", "candidates": ["0", "2", "1", "3"], "answer": "B", "task_type": "count", "question_id": "Q696"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "36a6e0d6598f648f3b90a4484ae84e23982b033559f7d4409c8292e48f794700", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 0\n(B) 2\n(C) 1\n(D) 3\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 0\n(B) 2\n(C) 1\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 700, "doc": {"video_name": "count_125.mp4", "duration": 500.03, "question": "In this video, how many times does the scene of the 'playing trombone' action appear in total?\n(A) 2\n(B) 0\n(C) 5\n(D) 1\n", "candidates": ["2", "0", "5", "1"], "answer": "C", "task_type": "count", "question_id": "Q700"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2004ccd415f26182d4a0150d4aedec2317b336688a372f0da6b1d6bafbc5df0a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'playing trombone' action appear in total?\n(A) 2\n(B) 0\n(C) 5\n(D) 1\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "In this video, how many times does the scene of the 'playing trombone' action appear in total?\n(A) 2\n(B) 0\n(C) 5\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 704, "doc": {"video_name": "count_220.mp4", "duration": 793.05, "question": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 2\n(B) 4\n(C) 6\n(D) 3\n", "candidates": ["2", "4", "6", "3"], "answer": "D", "task_type": "count", "question_id": "Q704"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "01994865a270cd7e5a9be2b0d9388d655fb7e8d8e5279e24b846098cbd4b0914", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 2\n(B) 4\n(C) 6\n(D) 3\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 2\n(B) 4\n(C) 6\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 708, "doc": {"video_name": "count_106.mp4", "duration": 490.03999999999996, "question": "In this video, how many instances are there of the 'carving pumpkin' action scene in total?\n(A) 5\n(B) 6\n(C) 0\n(D) 4\n", "candidates": ["5", "6", "0", "4"], "answer": "D", "task_type": "count", "question_id": "Q708"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bf54e66d6fbcdb0b8621a264e72459e0bfc2e2ba7904073da8ef8e6678fa92dd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'carving pumpkin' action scene in total?\n(A) 5\n(B) 6\n(C) 0\n(D) 4\n", "task_type": "count", "pred_answer": "D", "answer": "D"}, "input": "In this video, how many instances are there of the 'carving pumpkin' action scene in total?\n(A) 5\n(B) 6\n(C) 0\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 712, "doc": {"video_name": "count_35.mp4", "duration": 497.57, "question": "In this video, how many times does the scene of the 'water sliding' action appear in total?\n(A) 4\n(B) 5\n(C) 2\n(D) 6\n", "candidates": ["4", "5", "2", "6"], "answer": "C", "task_type": "count", "question_id": "Q712"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2deb45abe1c124b4d9b8473379bfb421812f1640896e70a299fb3b8bfba3cd51", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'water sliding' action appear in total?\n(A) 4\n(B) 5\n(C) 2\n(D) 6\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many times does the scene of the 'water sliding' action appear in total?\n(A) 4\n(B) 5\n(C) 2\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 716, "doc": {"video_name": "count_32.mp4", "duration": 513.6100000000001, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 5\n(B) 0\n(C) 6\n(D) 3\n", "candidates": ["5", "0", "6", "3"], "answer": "A", "task_type": "count", "question_id": "Q716"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d940d501e10f9ad32fa685c252d80983ae5caee228ea995009dabacaeb9bd6c1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 5\n(B) 0\n(C) 6\n(D) 3\n", "task_type": "count", "pred_answer": "D", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 5\n(B) 0\n(C) 6\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 720, "doc": {"video_name": "count_197.mp4", "duration": 530.04, "question": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 4\n(B) 0\n(C) 3\n(D) 5\n", "candidates": ["4", "0", "3", "5"], "answer": "D", "task_type": "count", "question_id": "Q720"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "daaabf5ebfeb518744eb506b4c1a9b875ba76363e8dbc01380dfd6935b3453f6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 4\n(B) 0\n(C) 3\n(D) 5\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 4\n(B) 0\n(C) 3\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 724, "doc": {"video_name": "count_4.mp4", "duration": 1374.6299999999999, "question": "In this video, how many instances are there of the 'abseiling' action scene in total?\n(A) 2\n(B) 0\n(C) 5\n(D) 3\n", "candidates": ["2", "0", "5", "3"], "answer": "A", "task_type": "count", "question_id": "Q724"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "937b5375d5d4329cd266f930a2fe7dfe8d9f054fde83125b18e7b95ed9f39cdd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'abseiling' action scene in total?\n(A) 2\n(B) 0\n(C) 5\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many instances are there of the 'abseiling' action scene in total?\n(A) 2\n(B) 0\n(C) 5\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 728, "doc": {"video_name": "count_193.mp4", "duration": 460.12, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 3\n(B) 4\n(C) 1\n(D) 0\n", "candidates": ["3", "4", "1", "0"], "answer": "C", "task_type": "count", "question_id": "Q728"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "068fcdca3fc47a8a6fad31ab57244b9966387a752d194d0b5fe84393dc27ec52", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 3\n(B) 4\n(C) 1\n(D) 0\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 3\n(B) 4\n(C) 1\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 732, "doc": {"video_name": "count_182.mp4", "duration": 480.06000000000006, "question": "In this video, how many instances are there of the 'paragliding' action scene in total?\n(A) 3\n(B) 1\n(C) 5\n(D) 4\n", "candidates": ["3", "1", "5", "4"], "answer": "A", "task_type": "count", "question_id": "Q732"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f5c4afe9de81f029ff05852f6120aee27e6af8df3791a69e17be6f7b1154267b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'paragliding' action scene in total?\n(A) 3\n(B) 1\n(C) 5\n(D) 4\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many instances are there of the 'paragliding' action scene in total?\n(A) 3\n(B) 1\n(C) 5\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 736, "doc": {"video_name": "count_237.mp4", "duration": 772.03, "question": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 1\n(B) 5\n(C) 6\n(D) 2\n", "candidates": ["1", "5", "6", "2"], "answer": "A", "task_type": "count", "question_id": "Q736"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d751eebb385d76da14dd24ece77c1266844980dc1d7b1f55f27ca3a4baf2ad68", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 1\n(B) 5\n(C) 6\n(D) 2\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 1\n(B) 5\n(C) 6\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 740, "doc": {"video_name": "count_28.mp4", "duration": 492.86, "question": "In this video, how many instances are there of the 'clean and jerk' action scene in total?\n(A) 3\n(B) 1\n(C) 6\n(D) 2\n", "candidates": ["3", "1", "6", "2"], "answer": "D", "task_type": "count", "question_id": "Q740"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "57004ba2fc414f10781b29f4cd2833a63cb5ec25efc9a0432a5e92e8655c86d1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'clean and jerk' action scene in total?\n(A) 3\n(B) 1\n(C) 6\n(D) 2\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many instances are there of the 'clean and jerk' action scene in total?\n(A) 3\n(B) 1\n(C) 6\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 744, "doc": {"video_name": "count_138.mp4", "duration": 500.18, "question": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 4\n(B) 5\n(C) 2\n(D) 6\n", "candidates": ["4", "5", "2", "6"], "answer": "B", "task_type": "count", "question_id": "Q744"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "53b8f6a8799b3e3183eaccb63679daa59dd0f7ee3ba11d047cb7790760ff8416", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 4\n(B) 5\n(C) 2\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "B"}, "input": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 4\n(B) 5\n(C) 2\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 748, "doc": {"video_name": "count_226.mp4", "duration": 7044.0599999999995, "question": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 2\n(B) 4\n(C) 5\n(D) 1\n", "candidates": ["2", "4", "5", "1"], "answer": "D", "task_type": "count", "question_id": "Q748"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3a747f05f776792edb21cbde79b008c46ec44b1b3b12e6825572eb6f7555fe14", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 2\n(B) 4\n(C) 5\n(D) 1\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 2\n(B) 4\n(C) 5\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 752, "doc": {"video_name": "count_20.mp4", "duration": 1365.4, "question": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 1\n(B) 2\n(C) 5\n(D) 0\n", "candidates": ["1", "2", "5", "0"], "answer": "A", "task_type": "count", "question_id": "Q752"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fc0660f55f922d23186ef8e0fcc33c251254acb015a390f8c2eef5c65351a777", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 1\n(B) 2\n(C) 5\n(D) 0\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 1\n(B) 2\n(C) 5\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 756, "doc": {"video_name": "count_152.mp4", "duration": 480.06, "question": "In this video, how many instances are there of the 'shredding paper' action scene in total?\n(A) 3\n(B) 5\n(C) 2\n(D) 0\n", "candidates": ["3", "5", "2", "0"], "answer": "A", "task_type": "count", "question_id": "Q756"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c30ae8748bbc276f7f422472b7dc65445110a99d6f59323faeddc66609034996", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'shredding paper' action scene in total?\n(A) 3\n(B) 5\n(C) 2\n(D) 0\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many instances are there of the 'shredding paper' action scene in total?\n(A) 3\n(B) 5\n(C) 2\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 760, "doc": {"video_name": "count_232.mp4", "duration": 6407.330000000001, "question": "In this video, how many times does the scene of the 'paragliding' action appear in total?\n(A) 3\n(B) 4\n(C) 1\n(D) 6\n", "candidates": ["3", "4", "1", "6"], "answer": "B", "task_type": "count", "question_id": "Q760"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "83ff6ba57e874ac71623bee93d2d4363e402de98a01325bd67f0d9c735ad010c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'paragliding' action appear in total?\n(A) 3\n(B) 4\n(C) 1\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "B"}, "input": "In this video, how many times does the scene of the 'paragliding' action appear in total?\n(A) 3\n(B) 4\n(C) 1\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 764, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "How many oranges did I pick from the plastic bag?\n(A) 3\n(B) 10\n(C) 5\n(D) 1\n", "candidates": ["3", "10", "5", "1"], "answer": "A", "task_type": "ego", "question_id": "Q764"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1048392683b965f6f255eac041d3526763c0d154cd040f32a18ba2593271f043", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many oranges did I pick from the plastic bag?\n(A) 3\n(B) 10\n(C) 5\n(D) 1\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "How many oranges did I pick from the plastic bag?\n(A) 3\n(B) 10\n(C) 5\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 768, "doc": {"video_name": "ego_52.mp4", "duration": 480.0, "question": "How many were the Pepsi cans on the table ?\n(A) There were six Pepsi cans on the table.\n(B) There were four Pepsi cans on the table.\n(C) There were two Pepsi cans on the table.\n(D) There were eight Pepsi cans on the table.\n", "candidates": ["There were six Pepsi cans on the table.", "There were four Pepsi cans on the table.", "There were two Pepsi cans on the table.", "There were eight Pepsi cans on the table."], "answer": "B", "task_type": "ego", "question_id": "Q768"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0cf5141ccd972b40b0f92574bd0c9aa849d85eb2ac34925c30ed8ad8b3683fa8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many were the Pepsi cans on the table ?\n(A) There were six Pepsi cans on the table.\n(B) There were four Pepsi cans on the table.\n(C) There were two Pepsi cans on the table.\n(D) There were eight Pepsi cans on the table.\n", "task_type": "ego", "pred_answer": "C", "answer": "B"}, "input": "How many were the Pepsi cans on the table ?\n(A) There were six Pepsi cans on the table.\n(B) There were four Pepsi cans on the table.\n(C) There were two Pepsi cans on the table.\n(D) There were eight Pepsi cans on the table.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 772, "doc": {"video_name": "ego_22.mp4", "duration": 480.0, "question": "What colour was the blouse I held in my hands\n(A) green\n(B) red\n(C) yellow\n(D) blue\n", "candidates": ["green", "red", "yellow", "blue"], "answer": "B", "task_type": "ego", "question_id": "Q772"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5a74d1a6d9bc55acee598393cb6948fad71ef1ef736b25ffcfa4f88d99578c66", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What colour was the blouse I held in my hands\n(A) green\n(B) red\n(C) yellow\n(D) blue\n", "task_type": "ego", "pred_answer": "A", "answer": "B"}, "input": "What colour was the blouse I held in my hands\n(A) green\n(B) red\n(C) yellow\n(D) blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 776, "doc": {"video_name": "ego_48.mp4", "duration": 480.0, "question": "What colour was the towel I folded last?\n(A) Blue\n(B) Black\n(C) Red\n(D) White\n", "candidates": ["Blue", "Black", "Red", "White"], "answer": "D", "task_type": "ego", "question_id": "Q776"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7e37ecb710207e35dbbe72be73d3ca803d2ce0714cb6b5e8a924274806c2bca6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What colour was the towel I folded last?\n(A) Blue\n(B) Black\n(C) Red\n(D) White\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What colour was the towel I folded last?\n(A) Blue\n(B) Black\n(C) Red\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 780, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "Where was the dark blue towel before I first picked it up?\n(A) folded on the bedroom dresser\n(B) hanging on the bathroom door\n(C) on the kitchen sink\n(D) in the laundry basket\n", "candidates": ["folded on the bedroom dresser", "hanging on the bathroom door", "on the kitchen sink", "in the laundry basket"], "answer": "C", "task_type": "ego", "question_id": "Q780"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "340d96a87d5b66974214701dd60f39a1d988a6b197c8240191b119fc5f49282c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the dark blue towel before I first picked it up?\n(A) folded on the bedroom dresser\n(B) hanging on the bathroom door\n(C) on the kitchen sink\n(D) in the laundry basket\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was the dark blue towel before I first picked it up?\n(A) folded on the bedroom dresser\n(B) hanging on the bathroom door\n(C) on the kitchen sink\n(D) in the laundry basket\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 784, "doc": {"video_name": "ego_20.mp4", "duration": 480.0, "question": "Where did I last put the drill machine?\n(A) On the kitchen counter\n(B) In the garage\n(C) On the upper wooden block\n(D) In the toolbox\n", "candidates": ["On the kitchen counter", "In the garage", "On the upper wooden block", "In the toolbox"], "answer": "C", "task_type": "ego", "question_id": "Q784"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ef0c5abf9dbbe6f450f39a5cbb89d5818c773307e1a63122c16abb59410c847d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I last put the drill machine?\n(A) On the kitchen counter\n(B) In the garage\n(C) On the upper wooden block\n(D) In the toolbox\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where did I last put the drill machine?\n(A) On the kitchen counter\n(B) In the garage\n(C) On the upper wooden block\n(D) In the toolbox\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 788, "doc": {"video_name": "ego_20.mp4", "duration": 480.0, "question": "Where was the bunch of nail before I took it?\n(A) On the floor\n(B) In my pocket\n(C) In the garbage can\n(D) In the toolbox\n", "candidates": ["On the floor", "In my pocket", "In the garbage can", "In the toolbox"], "answer": "B", "task_type": "ego", "question_id": "Q788"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c5e4e37eb59208f4d467a616053b17d7e1548b525ab285148b3caf7943a9cd8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the bunch of nail before I took it?\n(A) On the floor\n(B) In my pocket\n(C) In the garbage can\n(D) In the toolbox\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the bunch of nail before I took it?\n(A) On the floor\n(B) In my pocket\n(C) In the garbage can\n(D) In the toolbox\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 792, "doc": {"video_name": "ego_4.mp4", "duration": 480.0, "question": "What did I put in the basket?\n(A) Apples and Oranges\n(B) Cream and Tablets\n(C) Pens and Paper\n(D) Milk and Bread\n", "candidates": ["Apples and Oranges", "Cream and Tablets", "Pens and Paper", "Milk and Bread"], "answer": "B", "task_type": "ego", "question_id": "Q792"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "10dad549d0242f6a1043d709ef26bb6cfe839613bcf3e7ce02e2b8f23607b558", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I put in the basket?\n(A) Apples and Oranges\n(B) Cream and Tablets\n(C) Pens and Paper\n(D) Milk and Bread\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What did I put in the basket?\n(A) Apples and Oranges\n(B) Cream and Tablets\n(C) Pens and Paper\n(D) Milk and Bread\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 796, "doc": {"video_name": "ego_3.mp4", "duration": 418.0333332874998, "question": "Where did I last put the orange top hanger?\n(A) In the laundry basket\n(B) On the bedroom floor\n(C) In the back of the closet\n(D) On the front hanger rack\n", "candidates": ["In the laundry basket", "On the bedroom floor", "In the back of the closet", "On the front hanger rack"], "answer": "D", "task_type": "ego", "question_id": "Q796"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156504514b727af7776984d70d01b8d1090c1241729d93dc844fc6731a10e7a3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where did I last put the orange top hanger?\n(A) In the laundry basket\n(B) On the bedroom floor\n(C) In the back of the closet\n(D) On the front hanger rack\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where did I last put the orange top hanger?\n(A) In the laundry basket\n(B) On the bedroom floor\n(C) In the back of the closet\n(D) On the front hanger rack\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 800, "doc": {"video_name": "ego_81.mp4", "duration": 1200.0333333333333, "question": "How many eggs did I pick?\n(A) 3\n(B) 2\n(C) 4\n(D) 1\n", "candidates": ["3", "2", "4", "1"], "answer": "A", "task_type": "ego", "question_id": "Q800"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e382d84612aa23fbb688731102e1fb74b8068f04f7a57cb16416096f093b4c8f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many eggs did I pick?\n(A) 3\n(B) 2\n(C) 4\n(D) 1\n", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "How many eggs did I pick?\n(A) 3\n(B) 2\n(C) 4\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 804, "doc": {"video_name": "ego_3.mp4", "duration": 418.0333332874998, "question": "How many purses were kept on the display table?\n(A) Three\n(B) Four\n(C) Two.\n(D) One\n", "candidates": ["Three", "Four", "Two.", "One"], "answer": "C", "task_type": "ego", "question_id": "Q804"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1abfe8189932db87d7354fe6c6c663af9346c2d42c34e7f86c02465613cf1f0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many purses were kept on the display table?\n(A) Three\n(B) Four\n(C) Two.\n(D) One\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "How many purses were kept on the display table?\n(A) Three\n(B) Four\n(C) Two.\n(D) One\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 808, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "How many bowls did I pick from the plate rack?\n(A) 3\n(B) 1\n(C) 2\n(D) 4\n", "candidates": ["3", "1", "2", "4"], "answer": "B", "task_type": "ego", "question_id": "Q808"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "da343e86c18d647b71e599769835ad12419660e2e69acce3f9ca7cb696e7ff39", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many bowls did I pick from the plate rack?\n(A) 3\n(B) 1\n(C) 2\n(D) 4\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "How many bowls did I pick from the plate rack?\n(A) 3\n(B) 1\n(C) 2\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 812, "doc": {"video_name": "ego_80.mp4", "duration": 480.03333333333336, "question": "Where was the cap?\n(A) in the drawer\n(B) on the table\n(C) on the shelve \n(D) in the closet\n", "candidates": ["in the drawer", "on the table", "on the shelve ", "in the closet"], "answer": "C", "task_type": "ego", "question_id": "Q812"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b9f5b4056148fe8f52a3f215c92f1cdf89bd542e2d28a94034e413f675da8cf7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the cap?\n(A) in the drawer\n(B) on the table\n(C) on the shelve \n(D) in the closet\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "Where was the cap?\n(A) in the drawer\n(B) on the table\n(C) on the shelve \n(D) in the closet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 816, "doc": {"video_name": "ego_29.mp4", "duration": 428.0666666666666, "question": "Where was the handle of the first radio before I picked it up?\n(A) on the table\n(B) in the drawer\n(C) in my hand\n(D) on the floor\n", "candidates": ["on the table", "in the drawer", "in my hand", "on the floor"], "answer": "A", "task_type": "ego", "question_id": "Q816"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5eb53ad28712f882a4e7a02b573b1226d3dadaa32cf19d464cfe3a1df599934a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the handle of the first radio before I picked it up?\n(A) on the table\n(B) in the drawer\n(C) in my hand\n(D) on the floor\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "Where was the handle of the first radio before I picked it up?\n(A) on the table\n(B) in the drawer\n(C) in my hand\n(D) on the floor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 820, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "What bowl did I take from the cupboard?\n(A) a green bowl\n(B) a blue bowl\n(C) a white bowl\n(D) a red bowl\n", "candidates": ["a green bowl", "a blue bowl", "a white bowl", "a red bowl"], "answer": "C", "task_type": "ego", "question_id": "Q820"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "238ed782e5de02efb63e3853b90414dac0d8753e89dd886d0b03a7180c47ef38", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What bowl did I take from the cupboard?\n(A) a green bowl\n(B) a blue bowl\n(C) a white bowl\n(D) a red bowl\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "What bowl did I take from the cupboard?\n(A) a green bowl\n(B) a blue bowl\n(C) a white bowl\n(D) a red bowl\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 824, "doc": {"video_name": "ego_58.mp4", "duration": 414.5333333333333, "question": "Where was the wastebin?\n(A) In the bathroom\n(B) In the kitchen\n(C) In the living room\n(D) In the bedroom\n", "candidates": ["In the bathroom", "In the kitchen", "In the living room", "In the bedroom"], "answer": "B", "task_type": "ego", "question_id": "Q824"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "20dc6e9322309690998ba111d69b8d040eb5b60d8330207e4d1ac5b4cb503217", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the wastebin?\n(A) In the bathroom\n(B) In the kitchen\n(C) In the living room\n(D) In the bedroom\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the wastebin?\n(A) In the bathroom\n(B) In the kitchen\n(C) In the living room\n(D) In the bedroom\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 828, "doc": {"video_name": "ego_82.mp4", "duration": 480.0, "question": "Where is the bowl before I put a spoon into it ?\n(A) The bowl was in the refrigerator before I put a spoon into it.\n(B) The bowl was on the shelf before I put a spoon into it.\n(C) The bowl was on the table before I put a spoon into it.\n(D) The bowl was in the basin before I put a spoon into it.\n", "candidates": ["The bowl was in the refrigerator before I put a spoon into it.", "The bowl was on the shelf before I put a spoon into it.", "The bowl was on the table before I put a spoon into it.", "The bowl was in the basin before I put a spoon into it."], "answer": "D", "task_type": "ego", "question_id": "Q828"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "92726c28921012db82ca8732e3acd7ae14e91e0316f6eba52ce549c338e0c7b7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where is the bowl before I put a spoon into it ?\n(A) The bowl was in the refrigerator before I put a spoon into it.\n(B) The bowl was on the shelf before I put a spoon into it.\n(C) The bowl was on the table before I put a spoon into it.\n(D) The bowl was in the basin before I put a spoon into it.\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "Where is the bowl before I put a spoon into it ?\n(A) The bowl was in the refrigerator before I put a spoon into it.\n(B) The bowl was on the shelf before I put a spoon into it.\n(C) The bowl was on the table before I put a spoon into it.\n(D) The bowl was in the basin before I put a spoon into it.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 832, "doc": {"video_name": "ego_68.mp4", "duration": 480.03333333333336, "question": "What did I put in my pocket ?\n(A) Chewing gum\n(B) ATM card.\n(C) Driver's license\n(D) Lip balm\n", "candidates": ["Chewing gum", "ATM card.", "Driver's license", "Lip balm"], "answer": "B", "task_type": "ego", "question_id": "Q832"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "529128f2bce6d9a723f8c3e6137a8bf6a16c3fd339ae1fe92160db14b8c16045", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I put in my pocket ?\n(A) Chewing gum\n(B) ATM card.\n(C) Driver's license\n(D) Lip balm\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What did I put in my pocket ?\n(A) Chewing gum\n(B) ATM card.\n(C) Driver's license\n(D) Lip balm\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 836, "doc": {"video_name": "ego_47.mp4", "duration": 1200.0333333333333, "question": "What word was written on the hammer? \n(A) pliers\n(B) husky\n(C) wrench\n(D) screwdriver\n", "candidates": ["pliers", "husky", "wrench", "screwdriver"], "answer": "B", "task_type": "ego", "question_id": "Q836"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "11293aa08440e9491a33c8504b1a96b277aa19d86cfa9635253cc6547aaf71cb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What word was written on the hammer? \n(A) pliers\n(B) husky\n(C) wrench\n(D) screwdriver\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What word was written on the hammer? \n(A) pliers\n(B) husky\n(C) wrench\n(D) screwdriver\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 840, "doc": {"video_name": "ego_19.mp4", "duration": 480.0, "question": "Where was the measurement tape?\n(A) on the table\n(B) in the car\n(C) in the toolbox\n(D) in my pocket\n", "candidates": ["on the table", "in the car", "in the toolbox", "in my pocket"], "answer": "D", "task_type": "ego", "question_id": "Q840"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b44e807898642c1ffa43bc465de8f45c158c4f2196efcd644b1ab83c642e6e4a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the measurement tape?\n(A) on the table\n(B) in the car\n(C) in the toolbox\n(D) in my pocket\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the measurement tape?\n(A) on the table\n(B) in the car\n(C) in the toolbox\n(D) in my pocket\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 844, "doc": {"video_name": "ego_19.mp4", "duration": 480.0, "question": "Did I cut the wood?\n(A) maybe\n(B) no\n(C) I don't know\n(D) yes\n", "candidates": ["maybe", "no", "I don't know", "yes"], "answer": "D", "task_type": "ego", "question_id": "Q844"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "505e949c993ec6e0550b1e22724ce6a54cf903ab01a7e86db4c2877098b33614", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Did I cut the wood?\n(A) maybe\n(B) no\n(C) I don't know\n(D) yes\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Did I cut the wood?\n(A) maybe\n(B) no\n(C) I don't know\n(D) yes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 848, "doc": {"video_name": "ego_39.mp4", "duration": 480.0333333333333, "question": "What did I remove from the box?\n(A) metal\n(B) paper\n(C) cloth\n(D) plastic\n", "candidates": ["metal", "paper", "cloth", "plastic"], "answer": "B", "task_type": "ego", "question_id": "Q848"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "da93bce9c4b6e845aa8a65267090ef5ce48ba0ebabe001c2fb98e9f314be1ace", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I remove from the box?\n(A) metal\n(B) paper\n(C) cloth\n(D) plastic\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What did I remove from the box?\n(A) metal\n(B) paper\n(C) cloth\n(D) plastic\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 852, "doc": {"video_name": "ego_18.mp4", "duration": 460.7333333333331, "question": "What did I pick from the garden ?\n(A) Wooden bench\n(B) Brass stairs\n(C) Glass vase\n(D) Red flowers\n", "candidates": ["Wooden bench", "Brass stairs", "Glass vase", "Red flowers"], "answer": "B", "task_type": "ego", "question_id": "Q852"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3fb5cd5c8d614bd0b3c374a9067de6a3412eabc4b41e29c98b9da56e55da1b3f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I pick from the garden ?\n(A) Wooden bench\n(B) Brass stairs\n(C) Glass vase\n(D) Red flowers\n", "task_type": "ego", "pred_answer": "A", "answer": "B"}, "input": "What did I pick from the garden ?\n(A) Wooden bench\n(B) Brass stairs\n(C) Glass vase\n(D) Red flowers\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 856, "doc": {"video_name": "ego_41.mp4", "duration": 480.0333333333333, "question": "Did I open the car hood?\n(A) I don't know\n(B) I can't remember\n(C) Yes \n(D) No\n", "candidates": ["I don't know", "I can't remember", "Yes ", "No"], "answer": "C", "task_type": "ego", "question_id": "Q856"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d28ceeabb0b704596a7a53c35e369eb49274ded31e8ce2515027b2635812134f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Did I open the car hood?\n(A) I don't know\n(B) I can't remember\n(C) Yes \n(D) No\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Did I open the car hood?\n(A) I don't know\n(B) I can't remember\n(C) Yes \n(D) No\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 860, "doc": {"video_name": "ego_74.mp4", "duration": 480.0, "question": "In what location did I last see the cat?\n(A) in the backyard\n(B) in the kitchen\n(C) in the living room\n(D) in the storage room\n", "candidates": ["in the backyard", "in the kitchen", "in the living room", "in the storage room"], "answer": "D", "task_type": "ego", "question_id": "Q860"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ecaba94f7965bcb3c4e7940d5077f0030afad0bb6e95b99c2e2eaddd1048e095", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what location did I last see the cat?\n(A) in the backyard\n(B) in the kitchen\n(C) in the living room\n(D) in the storage room\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "In what location did I last see the cat?\n(A) in the backyard\n(B) in the kitchen\n(C) in the living room\n(D) in the storage room\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 864, "doc": {"video_name": "ego_57.mp4", "duration": 480.0, "question": "Did I open the fridge?\n(A) no\n(B) I don't know\n(C) maybe\n(D) yes\n", "candidates": ["no", "I don't know", "maybe", "yes"], "answer": "D", "task_type": "ego", "question_id": "Q864"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e8ce5f1c8fa11993cd05f6d70272ad8444f700e729db26e94ba13195651d9f1f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Did I open the fridge?\n(A) no\n(B) I don't know\n(C) maybe\n(D) yes\n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "Did I open the fridge?\n(A) no\n(B) I don't know\n(C) maybe\n(D) yes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 868, "doc": {"video_name": "ego_52.mp4", "duration": 480.0, "question": "Where was the Tostitos queso Cheese jar?\n(A) On the shelf\n(B) In the car\n(C) In the Fridge\n(D) In the pantry\n", "candidates": ["On the shelf", "In the car", "In the Fridge", "In the pantry"], "answer": "C", "task_type": "ego", "question_id": "Q868"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bbefe0bdeea7f8e3f6687f0766ead762e39118eb640638a75f91808647455f10", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the Tostitos queso Cheese jar?\n(A) On the shelf\n(B) In the car\n(C) In the Fridge\n(D) In the pantry\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was the Tostitos queso Cheese jar?\n(A) On the shelf\n(B) In the car\n(C) In the Fridge\n(D) In the pantry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 872, "doc": {"video_name": "ego_40.mp4", "duration": 480.0666666666667, "question": "Where was the Unauthorized vehicle poster ?\n(A) In a parking lot\n(B) In a garage\n(C) On the street\n(D) In a driveway\n", "candidates": ["In a parking lot", "In a garage", "On the street", "In a driveway"], "answer": "C", "task_type": "ego", "question_id": "Q872"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4a6c08421d9f68e1117515939b4a9e9f6445f82d519f08d48ebc23a9683c6f87", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the Unauthorized vehicle poster ?\n(A) In a parking lot\n(B) In a garage\n(C) On the street\n(D) In a driveway\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was the Unauthorized vehicle poster ?\n(A) In a parking lot\n(B) In a garage\n(C) On the street\n(D) In a driveway\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 876, "doc": {"video_name": "ego_73.mp4", "duration": 480.0, "question": "What did I put in the trash can?\n(A) pen\n(B) banana\n(C) wrapper\n(D) sock\n", "candidates": ["pen", "banana", "wrapper", "sock"], "answer": "C", "task_type": "ego", "question_id": "Q876"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "da26460fff1b94d50ba70e80286d5235ecc9e3790c0f758b3d7c3dbeb14ff10a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did I put in the trash can?\n(A) pen\n(B) banana\n(C) wrapper\n(D) sock\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What did I put in the trash can?\n(A) pen\n(B) banana\n(C) wrapper\n(D) sock\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 880, "doc": {"video_name": "ego_26.mp4", "duration": 375.3666666666667, "question": "Where was the drill before I picked it up?\n(A) In the garage\n(B) In the toolbox\n(C) \n(D) On the shelf\n", "candidates": ["In the garage", "In the toolbox", "", "On the shelf"], "answer": "C", "task_type": "ego", "question_id": "Q880"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C)"]], "filtered_resps": ["C)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6c367f5659e98ffa000869419923a31b3c1f23fb1caed69cb10b9e00d8a68168", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the drill before I picked it up?\n(A) In the garage\n(B) In the toolbox\n(C) \n(D) On the shelf\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was the drill before I picked it up?\n(A) In the garage\n(B) In the toolbox\n(C) \n(D) On the shelf\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 884, "doc": {"video_name": "ego_25.mp4", "duration": 480.0, "question": "What color is the bag on the second rider?\n(A) blue\n(B) yellow\n(C) green\n(D) red\n", "candidates": ["blue", "yellow", "green", "red"], "answer": "A", "task_type": "ego", "question_id": "Q884"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "da8dc496b8370d806fcc2b5bdec2d3c8f6e8009900948e5cce64ba01ed189d5b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the bag on the second rider?\n(A) blue\n(B) yellow\n(C) green\n(D) red\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What color is the bag on the second rider?\n(A) blue\n(B) yellow\n(C) green\n(D) red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 888, "doc": {"video_name": "ego_29.mp4", "duration": 428.0666666666666, "question": "Where was the battery cover of the first radio before I picked it up?\n(A) in my pocket\n(B) on the table\n(C) underneath the chair\n(D) in the drawer\n", "candidates": ["in my pocket", "on the table", "underneath the chair", "in the drawer"], "answer": "B", "task_type": "ego", "question_id": "Q888"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "be5089f65e2118329e72a5c6511d3d465f662b38bc069ac8c3569b5f3203cf23", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the battery cover of the first radio before I picked it up?\n(A) in my pocket\n(B) on the table\n(C) underneath the chair\n(D) in the drawer\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the battery cover of the first radio before I picked it up?\n(A) in my pocket\n(B) on the table\n(C) underneath the chair\n(D) in the drawer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 892, "doc": {"video_name": "ego_52.mp4", "duration": 480.0, "question": "What instrument did I play ?\n(A) The piano instrument I was playing\n(B) The guitar instrument I was playing\n(C) The violin instrument I was playing\n(D) The drums instrument I was playing\n", "candidates": ["The piano instrument I was playing", "The guitar instrument I was playing", "The violin instrument I was playing", "The drums instrument I was playing"], "answer": "A", "task_type": "ego", "question_id": "Q892"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c362f6215e032565a561563db11bd35e5a8b59b85343f1af5e078dc91f716a1b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What instrument did I play ?\n(A) The piano instrument I was playing\n(B) The guitar instrument I was playing\n(C) The violin instrument I was playing\n(D) The drums instrument I was playing\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What instrument did I play ?\n(A) The piano instrument I was playing\n(B) The guitar instrument I was playing\n(C) The violin instrument I was playing\n(D) The drums instrument I was playing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 896, "doc": {"video_name": "ego_50.mp4", "duration": 480.0333333333333, "question": "What food did I put in the plastic bag?\n(A) banana\n(B) apple\n(C) chicken\n(D) bread\n", "candidates": ["banana", "apple", "chicken", "bread"], "answer": "D", "task_type": "ego", "question_id": "Q896"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4a4b6f39c73e50c05e2a080b37d9da3c29a16a81280bd8aea85627e1d1da5e8b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What food did I put in the plastic bag?\n(A) banana\n(B) apple\n(C) chicken\n(D) bread\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What food did I put in the plastic bag?\n(A) banana\n(B) apple\n(C) chicken\n(D) bread\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 900, "doc": {"video_name": "ego_17.mp4", "duration": 480.0333333333333, "question": "What color was the measuring tape I removed from my pocket?\n(A) green\n(B) blue\n(C) red\n(D) Yellow\n", "candidates": ["green", "blue", "red", "Yellow"], "answer": "D", "task_type": "ego", "question_id": "Q900"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1923ef9400bc0baa2f152ce552490a65c14c0dba3726f5decefb8f60763eb18e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color was the measuring tape I removed from my pocket?\n(A) green\n(B) blue\n(C) red\n(D) Yellow\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What color was the measuring tape I removed from my pocket?\n(A) green\n(B) blue\n(C) red\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 904, "doc": {"video_name": "ego_16.mp4", "duration": 480.0, "question": "In what room did I see the black suitcase?\n(A) In the kitchen on the counter\n(B) In the bathroom on the sink\n(C) In the living room on the couch\n(D) In the bedroom on the bed\n", "candidates": ["In the kitchen on the counter", "In the bathroom on the sink", "In the living room on the couch", "In the bedroom on the bed"], "answer": "D", "task_type": "ego", "question_id": "Q904"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7fe71468ef4a96220a4795a79e2cff3ae199f48184e8109699e680b25c5f57f9", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what room did I see the black suitcase?\n(A) In the kitchen on the counter\n(B) In the bathroom on the sink\n(C) In the living room on the couch\n(D) In the bedroom on the bed\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "In what room did I see the black suitcase?\n(A) In the kitchen on the counter\n(B) In the bathroom on the sink\n(C) In the living room on the couch\n(D) In the bedroom on the bed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 908, "doc": {"video_name": "ego_16.mp4", "duration": 480.0, "question": "Where was the blue chair?\n(A) On the balcony\n(B) In the living room\n(C) In the bedroom\n(D) Beside the dining table\n", "candidates": ["On the balcony", "In the living room", "In the bedroom", "Beside the dining table"], "answer": "D", "task_type": "ego", "question_id": "Q908"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a899b70e572a0424f800b798f39a589c2de828ccfb68f4393ba2ee01d59be851", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the blue chair?\n(A) On the balcony\n(B) In the living room\n(C) In the bedroom\n(D) Beside the dining table\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "Where was the blue chair?\n(A) On the balcony\n(B) In the living room\n(C) In the bedroom\n(D) Beside the dining table\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 912, "doc": {"video_name": "ego_15.mp4", "duration": 480.0, "question": "What did I put in fridge ?\n(A) Tiffin box.\n(B) Water bottle\n(C) Spoon\n(D) Shoes\n", "candidates": ["Tiffin box.", "Water bottle", "Spoon", "Shoes"], "answer": "A", "task_type": "ego", "question_id": "Q912"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f97f450afb3ced6b508c73dae58b2c5a9562db97ccaf65c5b504b2931993e958", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I put in fridge ?\n(A) Tiffin box.\n(B) Water bottle\n(C) Spoon\n(D) Shoes\n", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "What did I put in fridge ?\n(A) Tiffin box.\n(B) Water bottle\n(C) Spoon\n(D) Shoes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 916, "doc": {"video_name": "ego_49.mp4", "duration": 480.0666666666667, "question": "What equipment did I take?\n(A) Hammer\n(B) Screwdriver\n(C) Mechanical Wire parts.\n(D) Paintbrush\n", "candidates": ["Hammer", "Screwdriver", "Mechanical Wire parts.", "Paintbrush"], "answer": "C", "task_type": "ego", "question_id": "Q916"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98e240097ebf017efe673b783a53238b9923d83533aacd81b552d7d151c231d8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What equipment did I take?\n(A) Hammer\n(B) Screwdriver\n(C) Mechanical Wire parts.\n(D) Paintbrush\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "What equipment did I take?\n(A) Hammer\n(B) Screwdriver\n(C) Mechanical Wire parts.\n(D) Paintbrush\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 920, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "How many spoons of oats did I put in each bowl?\n(A) 5\n(B) 3\n(C) 4\n(D) 2\n", "candidates": ["5", "3", "4", "2"], "answer": "C", "task_type": "ego", "question_id": "Q920"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "542fddd77f2102b9c840030796040fb155844dfd4aaad7e44f1e8f2d5036c6f5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many spoons of oats did I put in each bowl?\n(A) 5\n(B) 3\n(C) 4\n(D) 2\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "How many spoons of oats did I put in each bowl?\n(A) 5\n(B) 3\n(C) 4\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 924, "doc": {"video_name": "ego_65.mp4", "duration": 480.0, "question": "What word was written on the wood cutting machine that I used?\n(A) Milwaukee\n(B) Makita\n(C) Bosch\n(D) Dewalt.\n", "candidates": ["Milwaukee", "Makita", "Bosch", "Dewalt."], "answer": "D", "task_type": "ego", "question_id": "Q924"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "264703a4f9944661f571aedec80a6363c7e2121a48f2b10bd29d31632703884d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What word was written on the wood cutting machine that I used?\n(A) Milwaukee\n(B) Makita\n(C) Bosch\n(D) Dewalt.\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "What word was written on the wood cutting machine that I used?\n(A) Milwaukee\n(B) Makita\n(C) Bosch\n(D) Dewalt.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 928, "doc": {"video_name": "ego_46.mp4", "duration": 480.0, "question": "Where was the screw packet?\n(A) Under the bed\n(B) On the dustbin\n(C) In the refrigerator\n(D) In the mailbox\n", "candidates": ["Under the bed", "On the dustbin", "In the refrigerator", "In the mailbox"], "answer": "B", "task_type": "ego", "question_id": "Q928"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "13e7fe496705170df02fc2de9e4098a3682527747372593d4b4e3967bd7e39fa", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the screw packet?\n(A) Under the bed\n(B) On the dustbin\n(C) In the refrigerator\n(D) In the mailbox\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the screw packet?\n(A) Under the bed\n(B) On the dustbin\n(C) In the refrigerator\n(D) In the mailbox\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 932, "doc": {"video_name": "ego_0.mp4", "duration": 480.0, "question": "What color t-shirt was the man wearing holding the air-blower machine?\n(A) White\n(B) Red\n(C) Black\n(D) Grey\n", "candidates": ["White", "Red", "Black", "Grey"], "answer": "D", "task_type": "ego", "question_id": "Q932"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2618c357e3dca9522bda45d85ec3a897b8b9b1b37b62260e1f663097d6db6e45", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color t-shirt was the man wearing holding the air-blower machine?\n(A) White\n(B) Red\n(C) Black\n(D) Grey\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What color t-shirt was the man wearing holding the air-blower machine?\n(A) White\n(B) Red\n(C) Black\n(D) Grey\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 936, "doc": {"video_name": "ego_23.mp4", "duration": 479.9999999999998, "question": "Where did I keep the Jenga puzzle?\n(A) on the table\n(B) in the closet\n(C) in the kitchen\n(D) under the bed\n", "candidates": ["on the table", "in the closet", "in the kitchen", "under the bed"], "answer": "A", "task_type": "ego", "question_id": "Q936"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "850558632f4288e8d9af98498ebdf52e3d00ebe3b61130570c58fd0abf348f1f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I keep the Jenga puzzle?\n(A) on the table\n(B) in the closet\n(C) in the kitchen\n(D) under the bed\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I keep the Jenga puzzle?\n(A) on the table\n(B) in the closet\n(C) in the kitchen\n(D) under the bed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 940, "doc": {"video_name": "ego_43.mp4", "duration": 480.0, "question": "Where is the \"stop\" signboard ?\n(A) The \"stop\" signboard was at the four-way road intersection.\n(B) On the sidewalk\n(C) Inside a building\n(D) In a park\n", "candidates": ["The \"stop\" signboard was at the four-way road intersection.", "On the sidewalk", "Inside a building", "In a park"], "answer": "A", "task_type": "ego", "question_id": "Q940"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4a140cc77d42763c68bfaadc5f5fa5d2f5dd24ecec5e38c32752936df58d7ad0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where is the \"stop\" signboard ?\n(A) The \"stop\" signboard was at the four-way road intersection.\n(B) On the sidewalk\n(C) Inside a building\n(D) In a park\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where is the \"stop\" signboard ?\n(A) The \"stop\" signboard was at the four-way road intersection.\n(B) On the sidewalk\n(C) Inside a building\n(D) In a park\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 944, "doc": {"video_name": "ego_23.mp4", "duration": 479.9999999999998, "question": "Where did I put the Connect4 game box?\n(A) behind the couch\n(B) in the kitchen cabinet\n(C) under the television stand\n(D) on top of the bookshelf\n", "candidates": ["behind the couch", "in the kitchen cabinet", "under the television stand", "on top of the bookshelf"], "answer": "C", "task_type": "ego", "question_id": "Q944"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2abbd442fdd0428ea06200cf09cfe6788dbcb8b2884fdf18d5242635bc1f1f39", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I put the Connect4 game box?\n(A) behind the couch\n(B) in the kitchen cabinet\n(C) under the television stand\n(D) on top of the bookshelf\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "Where did I put the Connect4 game box?\n(A) behind the couch\n(B) in the kitchen cabinet\n(C) under the television stand\n(D) on top of the bookshelf\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 948, "doc": {"video_name": "ego_22.mp4", "duration": 480.0, "question": "How many green mugs were on the fridge top after I first opened the fridge\n(A) 0\n(B) 2\n(C) 3\n(D) 1\n", "candidates": ["0", "2", "3", "1"], "answer": "B", "task_type": "ego", "question_id": "Q948"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5ef3c1de4facc4dd7acd10e44ab71ca902dda59f12759c3741dfab66bb48119", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many green mugs were on the fridge top after I first opened the fridge\n(A) 0\n(B) 2\n(C) 3\n(D) 1\n", "task_type": "ego", "pred_answer": "A", "answer": "B"}, "input": "How many green mugs were on the fridge top after I first opened the fridge\n(A) 0\n(B) 2\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 952, "doc": {"video_name": "ego_82.mp4", "duration": 480.0, "question": "Where did I put the spoon?\n(A)  In sink \n(B) In the refrigerator\n(C) In the dishwasher\n(D) On the stove\n", "candidates": [" In sink ", "In the refrigerator", "In the dishwasher", "On the stove"], "answer": "A", "task_type": "ego", "question_id": "Q952"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7e5d72bf5f7eea57f9a023fbd167953a7b08c51159ca4cdd02708566a4f88ec4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I put the spoon?\n(A)  In sink \n(B) In the refrigerator\n(C) In the dishwasher\n(D) On the stove\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I put the spoon?\n(A)  In sink \n(B) In the refrigerator\n(C) In the dishwasher\n(D) On the stove\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 956, "doc": {"video_name": "ego_55.mp4", "duration": 480.0, "question": "Where was the machine before I took it out?\n(A) in the garage\n(B) inside the storage shed\n(C) in the basement\n(D) on the roof\n", "candidates": ["in the garage", "inside the storage shed", "in the basement", "on the roof"], "answer": "B", "task_type": "ego", "question_id": "Q956"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fb160da8fb2cb958d49829f402bdaed534971e6ed1b9f9f9f9c64fe6684d0658", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the machine before I took it out?\n(A) in the garage\n(B) inside the storage shed\n(C) in the basement\n(D) on the roof\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the machine before I took it out?\n(A) in the garage\n(B) inside the storage shed\n(C) in the basement\n(D) on the roof\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 960, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Where was the leash before I picked it up\n(A) on the kitchen counter\n(B) on the front door handle\n(C) under the couch\n(D) in the backyard\n", "candidates": ["on the kitchen counter", "on the front door handle", "under the couch", "in the backyard"], "answer": "B", "task_type": "ego", "question_id": "Q960"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b41dc4588fc026323b9aee23a99e77a6a4f4e7ffc9e19b0b1bace08045710d94", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the leash before I picked it up\n(A) on the kitchen counter\n(B) on the front door handle\n(C) under the couch\n(D) in the backyard\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the leash before I picked it up\n(A) on the kitchen counter\n(B) on the front door handle\n(C) under the couch\n(D) in the backyard\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 964, "doc": {"video_name": "ego_58.mp4", "duration": 414.5333333333333, "question": "Where did I put the bag of rice?\n(A) In the bedroom\n(B) In the refrigerator\n(C) On the kitchen slab/ In front of the microwave\n(D) Under the sink\n", "candidates": ["In the bedroom", "In the refrigerator", "On the kitchen slab/ In front of the microwave", "Under the sink"], "answer": "C", "task_type": "ego", "question_id": "Q964"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "51a7a7779eb177f1f00f732d836e6b00082147e5f780c2a4cda38a78a75f524d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I put the bag of rice?\n(A) In the bedroom\n(B) In the refrigerator\n(C) On the kitchen slab/ In front of the microwave\n(D) Under the sink\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where did I put the bag of rice?\n(A) In the bedroom\n(B) In the refrigerator\n(C) On the kitchen slab/ In front of the microwave\n(D) Under the sink\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 968, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "What did I put in the cabinet?\n(A) blue book\n(B) red ball\n(C) green vase\n(D) white box\n", "candidates": ["blue book", "red ball", "green vase", "white box"], "answer": "D", "task_type": "ego", "question_id": "Q968"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ba10160eaccd7a8a5ccfff4626ac40ab2809aca58e8f57a55c04002807d86120", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I put in the cabinet?\n(A) blue book\n(B) red ball\n(C) green vase\n(D) white box\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What did I put in the cabinet?\n(A) blue book\n(B) red ball\n(C) green vase\n(D) white box\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 972, "doc": {"video_name": "ego_14.mp4", "duration": 480.0333333333334, "question": "In what room did I see red shelf ?\n(A) Bathroom\n(B) Store room\n(C) Kitchen\n(D) Living room\n", "candidates": ["Bathroom", "Store room", "Kitchen", "Living room"], "answer": "B", "task_type": "ego", "question_id": "Q972"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "396f20b12ed9b8be36162b04623877d29cd4e53ca23ca9b524c98c82f60db575", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In what room did I see red shelf ?\n(A) Bathroom\n(B) Store room\n(C) Kitchen\n(D) Living room\n", "task_type": "ego", "pred_answer": "D", "answer": "B"}, "input": "In what room did I see red shelf ?\n(A) Bathroom\n(B) Store room\n(C) Kitchen\n(D) Living room\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 976, "doc": {"video_name": "ego_13.mp4", "duration": 480.0, "question": "Where is the pack of Jenga game?\n(A) on the bookshelf\n(B) in the television cabinet\n(C) under the bed\n(D) in the kitchen drawer\n", "candidates": ["on the bookshelf", "in the television cabinet", "under the bed", "in the kitchen drawer"], "answer": "B", "task_type": "ego", "question_id": "Q976"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7f125f656b9fc20e84f688a688c2ef5d9d1c9c13521f84f4a43240d36f952d5a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where is the pack of Jenga game?\n(A) on the bookshelf\n(B) in the television cabinet\n(C) under the bed\n(D) in the kitchen drawer\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where is the pack of Jenga game?\n(A) on the bookshelf\n(B) in the television cabinet\n(C) under the bed\n(D) in the kitchen drawer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 980, "doc": {"video_name": "ego_13.mp4", "duration": 480.0, "question": "What color is the towel on the rack?\n(A) pink\n(B) blue\n(C) white with ash stripes\n(D) red\n", "candidates": ["pink", "blue", "white with ash stripes", "red"], "answer": "C", "task_type": "ego", "question_id": "Q980"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4f6fb9733f941f6496c4b9ad478eba563b896d18ecd22924736ec4a9a8c48b09", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the towel on the rack?\n(A) pink\n(B) blue\n(C) white with ash stripes\n(D) red\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What color is the towel on the rack?\n(A) pink\n(B) blue\n(C) white with ash stripes\n(D) red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 984, "doc": {"video_name": "ego_13.mp4", "duration": 480.0, "question": "How many picture frames were on the wall?\n(A) 5\n(B) 6\n(C) 4\n(D) 3\n", "candidates": ["5", "6", "4", "3"], "answer": "C", "task_type": "ego", "question_id": "Q984"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b28aae90d456ee27193d5b0ef53e07189f6f7b9b7ca003b04bd54f828b090f95", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many picture frames were on the wall?\n(A) 5\n(B) 6\n(C) 4\n(D) 3\n", "task_type": "ego", "pred_answer": "D", "answer": "C"}, "input": "How many picture frames were on the wall?\n(A) 5\n(B) 6\n(C) 4\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 988, "doc": {"video_name": "ego_74.mp4", "duration": 480.0, "question": "Where was the pack of cling wrap before I picked it up?\n(A) in the bathroom cabinet\n(B) under the bed\n(C) on the living room table\n(D) in the kitchen drawer\n", "candidates": ["in the bathroom cabinet", "under the bed", "on the living room table", "in the kitchen drawer"], "answer": "D", "task_type": "ego", "question_id": "Q988"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c8c22a52e44f6c7d07a34dd4eacf1cd452136da82fa8b9f694abaed1449f0577", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the pack of cling wrap before I picked it up?\n(A) in the bathroom cabinet\n(B) under the bed\n(C) on the living room table\n(D) in the kitchen drawer\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the pack of cling wrap before I picked it up?\n(A) in the bathroom cabinet\n(B) under the bed\n(C) on the living room table\n(D) in the kitchen drawer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 992, "doc": {"video_name": "ego_77.mp4", "duration": 468.23333328749993, "question": "Where is the ATM machine?\n(A) next to the coffee shop\n(B) across the street\n(C) neat the book stand\n(D) in the parking lot\n", "candidates": ["next to the coffee shop", "across the street", "neat the book stand", "in the parking lot"], "answer": "C", "task_type": "ego", "question_id": "Q992"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cda5d88b3bb93baed07ecc6a21613120a15963c4aeb24efa81fc46d1ee49776c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the ATM machine?\n(A) next to the coffee shop\n(B) across the street\n(C) neat the book stand\n(D) in the parking lot\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where is the ATM machine?\n(A) next to the coffee shop\n(B) across the street\n(C) neat the book stand\n(D) in the parking lot\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 996, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "What colour was the helmet on the bicylce\n(A) red\n(B) blue\n(C) white\n(D) black\n", "candidates": ["red", "blue", "white", "black"], "answer": "B", "task_type": "ego", "question_id": "Q996"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "50169ebda12b02c1011e8ff7db577f4903a5c35e5c4cbafa55dc56a16ded8b3e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What colour was the helmet on the bicylce\n(A) red\n(B) blue\n(C) white\n(D) black\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What colour was the helmet on the bicylce\n(A) red\n(B) blue\n(C) white\n(D) black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1000, "doc": {"video_name": "ego_72.mp4", "duration": 479.9999999999998, "question": "Where was a fruit peeler?\n(A) in a drawer.\n(B) in the fridge\n(C) under the sink\n(D) on the counter\n", "candidates": ["in a drawer.", "in the fridge", "under the sink", "on the counter"], "answer": "A", "task_type": "ego", "question_id": "Q1000"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8cbfecee00d6a1df77525dadcd38a6cbb35f30b6ea3dd7d07179a48235b3e884", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was a fruit peeler?\n(A) in a drawer.\n(B) in the fridge\n(C) under the sink\n(D) on the counter\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "Where was a fruit peeler?\n(A) in a drawer.\n(B) in the fridge\n(C) under the sink\n(D) on the counter\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1004, "doc": {"video_name": "ego_50.mp4", "duration": 480.0333333333333, "question": "What did I put in the white dustbin?\n(A) Glass waste\n(B) Paper waste\n(C) Food waste\n(D) Plastic waste\n", "candidates": ["Glass waste", "Paper waste", "Food waste", "Plastic waste"], "answer": "C", "task_type": "ego", "question_id": "Q1004"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fd87f5ce995f34ab2ffcd19546f038b4084d2d594e33f1fa97afcb15960c4f16", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did I put in the white dustbin?\n(A) Glass waste\n(B) Paper waste\n(C) Food waste\n(D) Plastic waste\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "What did I put in the white dustbin?\n(A) Glass waste\n(B) Paper waste\n(C) Food waste\n(D) Plastic waste\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1008, "doc": {"video_name": "ego_51.mp4", "duration": 480.0333333333334, "question": "Where was the hand wash ?\n(A) The hand wash was in the kitchen\n(B) The hand wash was in the living room\n(C) The hand wash was in the bathroom\n(D) The hand wash was in the bedroom\n", "candidates": ["The hand wash was in the kitchen", "The hand wash was in the living room", "The hand wash was in the bathroom", "The hand wash was in the bedroom"], "answer": "C", "task_type": "ego", "question_id": "Q1008"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f94902c9f4c36b56ed4fc9212402025376b7b6570e6575f7900fcb23b6a9eef9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the hand wash ?\n(A) The hand wash was in the kitchen\n(B) The hand wash was in the living room\n(C) The hand wash was in the bathroom\n(D) The hand wash was in the bedroom\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "Where was the hand wash ?\n(A) The hand wash was in the kitchen\n(B) The hand wash was in the living room\n(C) The hand wash was in the bathroom\n(D) The hand wash was in the bedroom\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1012, "doc": {"video_name": "ego_52.mp4", "duration": 480.0, "question": "How many were the yoga mats?\n(A) There were five yoga mats.\n(B) There were six yoga mats.\n(C) There were two yoga mats.\n(D) There were three yoga mats.\n", "candidates": ["There were five yoga mats.", "There were six yoga mats.", "There were two yoga mats.", "There were three yoga mats."], "answer": "D", "task_type": "ego", "question_id": "Q1012"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a48253f9c268ea69738f7b39dd26545f19afc253e5a1a4050f857924c471a8dd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many were the yoga mats?\n(A) There were five yoga mats.\n(B) There were six yoga mats.\n(C) There were two yoga mats.\n(D) There were three yoga mats.\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "How many were the yoga mats?\n(A) There were five yoga mats.\n(B) There were six yoga mats.\n(C) There were two yoga mats.\n(D) There were three yoga mats.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1016, "doc": {"video_name": "ego_2.mp4", "duration": 480.0000000000001, "question": "What colour is the jacket I touched at the cloth rail? \n(A) black\n(B) red\n(C) blue\n(D) white\n", "candidates": ["black", "red", "blue", "white"], "answer": "A", "task_type": "ego", "question_id": "Q1016"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4d08dff30960ce4b09d1e8204eb70575a838447eedc325df69efffa04783f058", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What colour is the jacket I touched at the cloth rail? \n(A) black\n(B) red\n(C) blue\n(D) white\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What colour is the jacket I touched at the cloth rail? \n(A) black\n(B) red\n(C) blue\n(D) white\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1020, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "How many extra spoons of oats did I put in the second bowl?\n(A) 1\n(B) 3\n(C) 4\n(D) 2\n", "candidates": ["1", "3", "4", "2"], "answer": "D", "task_type": "ego", "question_id": "Q1020"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3620a36936afb79acc93a4fc5df02f2d815c5c7ebe8b9df47c6deadab666f74e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many extra spoons of oats did I put in the second bowl?\n(A) 1\n(B) 3\n(C) 4\n(D) 2\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "How many extra spoons of oats did I put in the second bowl?\n(A) 1\n(B) 3\n(C) 4\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1024, "doc": {"video_name": "ego_12.mp4", "duration": 480.0, "question": "What did i put in the trash bin?\n(A) paper towel\n(B) plastic bottle\n(C) empty can\n(D) banana peel\n", "candidates": ["paper towel", "plastic bottle", "empty can", "banana peel"], "answer": "C", "task_type": "ego", "question_id": "Q1024"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1231d6d018d7b0fbae8090d99ae5dae997923336c29c71ab7faed864a81dfe1f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did i put in the trash bin?\n(A) paper towel\n(B) plastic bottle\n(C) empty can\n(D) banana peel\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What did i put in the trash bin?\n(A) paper towel\n(B) plastic bottle\n(C) empty can\n(D) banana peel\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1028, "doc": {"video_name": "ego_11.mp4", "duration": 480.0, "question": "What colors are the pillows on the white sofas?\n(A) red\n(B) orange\n(C) brown\n(D) purple\n", "candidates": ["red", "orange", "brown", "purple"], "answer": "C", "task_type": "ego", "question_id": "Q1028"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3cafcb4773a628bf485576ed4bc7d62c8d022cf964e17949385f4442d7e55292", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What colors are the pillows on the white sofas?\n(A) red\n(B) orange\n(C) brown\n(D) purple\n", "task_type": "ego", "pred_answer": "D", "answer": "C"}, "input": "What colors are the pillows on the white sofas?\n(A) red\n(B) orange\n(C) brown\n(D) purple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1032, "doc": {"video_name": "ego_30.mp4", "duration": 480.0, "question": "Where I did put marker pen?\n(A) In the car.\n(B) In the fridge.\n(C) Under the bed.\n(D) On the table.\n", "candidates": ["In the car.", "In the fridge.", "Under the bed.", "On the table."], "answer": "D", "task_type": "ego", "question_id": "Q1032"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1e1da30d2289a899a03c40dbe0943b2f70cf6c50b39fbefa6ac2cd8c20a328d1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where I did put marker pen?\n(A) In the car.\n(B) In the fridge.\n(C) Under the bed.\n(D) On the table.\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where I did put marker pen?\n(A) In the car.\n(B) In the fridge.\n(C) Under the bed.\n(D) On the table.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1036, "doc": {"video_name": "ego_50.mp4", "duration": 480.0333333333333, "question": "Where was the baking glove before I hung it on the hook?\n(A) in the dishwasher\n(B) on the kitchen counter\n(C) in the oven\n(D) in the refrigerator\n", "candidates": ["in the dishwasher", "on the kitchen counter", "in the oven", "in the refrigerator"], "answer": "B", "task_type": "ego", "question_id": "Q1036"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a6a26db559e4207babd0aa3d4c39fe9430977f875aaa60b983d8bcd774bcccf2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the baking glove before I hung it on the hook?\n(A) in the dishwasher\n(B) on the kitchen counter\n(C) in the oven\n(D) in the refrigerator\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the baking glove before I hung it on the hook?\n(A) in the dishwasher\n(B) on the kitchen counter\n(C) in the oven\n(D) in the refrigerator\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1040, "doc": {"video_name": "ego_48.mp4", "duration": 480.0, "question": "What did I put in the dustbin?\n(A) Old newspaper\n(B) Used Tissu\n(C) Empty water bottle\n(D) Banana peel\n", "candidates": ["Old newspaper", "Used Tissu", "Empty water bottle", "Banana peel"], "answer": "B", "task_type": "ego", "question_id": "Q1040"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d31df52ef7348e89bcd80c98d6335667927d4b84fb3a899fef5ba8eb66974514", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I put in the dustbin?\n(A) Old newspaper\n(B) Used Tissu\n(C) Empty water bottle\n(D) Banana peel\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What did I put in the dustbin?\n(A) Old newspaper\n(B) Used Tissu\n(C) Empty water bottle\n(D) Banana peel\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1044, "doc": {"video_name": "ego_2.mp4", "duration": 480.0000000000001, "question": "How many belts are on the display table? \n(A) 1\n(B) 2\n(C) 3\n(D) 4\n", "candidates": ["1", "2", "3", "4"], "answer": "D", "task_type": "ego", "question_id": "Q1044"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "470e7e1438eeb4d38fa7e1260296bb5ba2551d935d8b48c55b02284aa08e892a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many belts are on the display table? \n(A) 1\n(B) 2\n(C) 3\n(D) 4\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "How many belts are on the display table? \n(A) 1\n(B) 2\n(C) 3\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1048, "doc": {"video_name": "ego_46.mp4", "duration": 480.0, "question": "Where was the yellow spring knot?\n(A) In the bedroom\n(B) On the floor\n(C) In the kitchen\n(D) On the dustbin\n", "candidates": ["In the bedroom", "On the floor", "In the kitchen", "On the dustbin"], "answer": "D", "task_type": "ego", "question_id": "Q1048"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b0ba3645c929562f6f3fbe7f65e85e3bcfdd1f5d2cf330e872a4f9fa1efc621e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the yellow spring knot?\n(A) In the bedroom\n(B) On the floor\n(C) In the kitchen\n(D) On the dustbin\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "Where was the yellow spring knot?\n(A) In the bedroom\n(B) On the floor\n(C) In the kitchen\n(D) On the dustbin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1052, "doc": {"video_name": "ego_41.mp4", "duration": 480.0333333333333, "question": "What did I throw in the dustbin?\n(A) Waste material\n(B) Plastic bottle\n(C) Food leftovers\n(D) Paper clip\n", "candidates": ["Waste material", "Plastic bottle", "Food leftovers", "Paper clip"], "answer": "A", "task_type": "ego", "question_id": "Q1052"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ec105cb9299d2ca0a089d04ac8d2f460044a7cee75418c962d0f016686e9556a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I throw in the dustbin?\n(A) Waste material\n(B) Plastic bottle\n(C) Food leftovers\n(D) Paper clip\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What did I throw in the dustbin?\n(A) Waste material\n(B) Plastic bottle\n(C) Food leftovers\n(D) Paper clip\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1056, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "What brand was the pack of tea I took from the cupboard?\n(A) Celestial Seasonings chamomile\n(B) Twinings infuso strawberry and mango\n(C) Bigelow green tea\n(D) Lipton black tea\n", "candidates": ["Celestial Seasonings chamomile", "Twinings infuso strawberry and mango", "Bigelow green tea", "Lipton black tea"], "answer": "B", "task_type": "ego", "question_id": "Q1056"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "159955df8ac4b76f60c3331771ec5aeb945c3f69b75b7684ada6085d15edfa84", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What brand was the pack of tea I took from the cupboard?\n(A) Celestial Seasonings chamomile\n(B) Twinings infuso strawberry and mango\n(C) Bigelow green tea\n(D) Lipton black tea\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What brand was the pack of tea I took from the cupboard?\n(A) Celestial Seasonings chamomile\n(B) Twinings infuso strawberry and mango\n(C) Bigelow green tea\n(D) Lipton black tea\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1060, "doc": {"video_name": "ego_58.mp4", "duration": 414.5333333333333, "question": "Where was the tray of sausages before I carried it?\n(A) On the dining table\n(B) On the gas cooker\n(C) In the oven\n(D) In the refrigerator\n", "candidates": ["On the dining table", "On the gas cooker", "In the oven", "In the refrigerator"], "answer": "B", "task_type": "ego", "question_id": "Q1060"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c13898559a8f5d0d392da7142286b5335cbcaae47139192fd485bd5cf5f08df8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the tray of sausages before I carried it?\n(A) On the dining table\n(B) On the gas cooker\n(C) In the oven\n(D) In the refrigerator\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the tray of sausages before I carried it?\n(A) On the dining table\n(B) On the gas cooker\n(C) In the oven\n(D) In the refrigerator\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1064, "doc": {"video_name": "ego_45.mp4", "duration": 428.83333328749984, "question": "Where was the swapping machine ?\n(A) exit\n(B) restroom\n(C) entrance\n(D) cash counter \n", "candidates": ["exit", "restroom", "entrance", "cash counter "], "answer": "D", "task_type": "ego", "question_id": "Q1064"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "52f1150341a3faa1b8cbeaa28eb5bcb20a2381cfaa026f2e65a2319baef15bc7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the swapping machine ?\n(A) exit\n(B) restroom\n(C) entrance\n(D) cash counter \n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the swapping machine ?\n(A) exit\n(B) restroom\n(C) entrance\n(D) cash counter \n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1068, "doc": {"video_name": "ego_41.mp4", "duration": 480.0333333333333, "question": "Did I leave the desktop on?\n(A) Maybe\n(B) Yes\n(C) I don't know\n(D) No\n", "candidates": ["Maybe", "Yes", "I don't know", "No"], "answer": "B", "task_type": "ego", "question_id": "Q1068"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "225152b5fac9611dba883c2323d34d358fd1646e32025c8f1c4216eb09d90dc1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did I leave the desktop on?\n(A) Maybe\n(B) Yes\n(C) I don't know\n(D) No\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Did I leave the desktop on?\n(A) Maybe\n(B) Yes\n(C) I don't know\n(D) No\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1072, "doc": {"video_name": "ego_43.mp4", "duration": 480.0, "question": "Did I leave the TV on ?\n(A) Maybe\n(B) I don't know\n(C) Yes\n(D) NO\n", "candidates": ["Maybe", "I don't know", "Yes", "NO"], "answer": "D", "task_type": "ego", "question_id": "Q1072"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f95ab3b5538ca1e9443e2b2c04ba24a37e57ad2440315e76a7fb6b07e53042df", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Did I leave the TV on ?\n(A) Maybe\n(B) I don't know\n(C) Yes\n(D) NO\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Did I leave the TV on ?\n(A) Maybe\n(B) I don't know\n(C) Yes\n(D) NO\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1076, "doc": {"video_name": "ego_79.mp4", "duration": 480.0, "question": "Where did I put the blue game ?\n(A) On the table.\n(B) Under the bed\n(C) In the closet\n(D) In the car\n", "candidates": ["On the table.", "Under the bed", "In the closet", "In the car"], "answer": "A", "task_type": "ego", "question_id": "Q1076"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2765140639a46496eee4df7d66a2abf1eb006250de3071fd265f3dc341f9e929", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I put the blue game ?\n(A) On the table.\n(B) Under the bed\n(C) In the closet\n(D) In the car\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I put the blue game ?\n(A) On the table.\n(B) Under the bed\n(C) In the closet\n(D) In the car\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1080, "doc": {"video_name": "ego_9.mp4", "duration": 480.03333333333336, "question": "Where was the fish thread tape?\n(A) in the bathroom cabinet\n(B) in the kitchen drawer\n(C) in the car trunk\n(D) in tool box\n", "candidates": ["in the bathroom cabinet", "in the kitchen drawer", "in the car trunk", "in tool box"], "answer": "D", "task_type": "ego", "question_id": "Q1080"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "674d5550393844ab75f059e12a8325473a140c1f4ba05d76860fd968054e1f87", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the fish thread tape?\n(A) in the bathroom cabinet\n(B) in the kitchen drawer\n(C) in the car trunk\n(D) in tool box\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the fish thread tape?\n(A) in the bathroom cabinet\n(B) in the kitchen drawer\n(C) in the car trunk\n(D) in tool box\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1084, "doc": {"video_name": "ego_73.mp4", "duration": 480.0, "question": "What did I pour in the bottle?\n(A) water\n(B) juice\n(C) milk\n(D) soda\n", "candidates": ["water", "juice", "milk", "soda"], "answer": "A", "task_type": "ego", "question_id": "Q1084"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8b092bd988a6567270a8c51ae84aa52f12467b0e099b0d797ec25ad1f8e2984a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I pour in the bottle?\n(A) water\n(B) juice\n(C) milk\n(D) soda\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What did I pour in the bottle?\n(A) water\n(B) juice\n(C) milk\n(D) soda\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1088, "doc": {"video_name": "ego_80.mp4", "duration": 480.03333333333336, "question": "In what place did I see the Tostitos bottle?\n(A) in the table shelve \n(B) in the bathroom\n(C) in the refrigerator\n(D) on the floor\n", "candidates": ["in the table shelve ", "in the bathroom", "in the refrigerator", "on the floor"], "answer": "A", "task_type": "ego", "question_id": "Q1088"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "97e8b3d4d1d6af91deb851fa8e28b7b342b643f1ce09c7940b582ce8aff0b1f6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what place did I see the Tostitos bottle?\n(A) in the table shelve \n(B) in the bathroom\n(C) in the refrigerator\n(D) on the floor\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "In what place did I see the Tostitos bottle?\n(A) in the table shelve \n(B) in the bathroom\n(C) in the refrigerator\n(D) on the floor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1092, "doc": {"video_name": "ego_7.mp4", "duration": 480.03333333333336, "question": "How many handkerchiefs were in the boxes I picked?\n(A) None\n(B) Two\n(C) Ten\n(D) Five\n", "candidates": ["None", "Two", "Ten", "Five"], "answer": "C", "task_type": "ego", "question_id": "Q1092"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "04166c761848ca6dfd063b0d804c571314921fc04f85dd01ecd480fa7df07a1b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many handkerchiefs were in the boxes I picked?\n(A) None\n(B) Two\n(C) Ten\n(D) Five\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "How many handkerchiefs were in the boxes I picked?\n(A) None\n(B) Two\n(C) Ten\n(D) Five\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1096, "doc": {"video_name": "ego_74.mp4", "duration": 480.0, "question": "How many picture frames were on the wall in the living room?\n(A) 3\n(B) 1\n(C) 2\n(D) 4\n", "candidates": ["3", "1", "2", "4"], "answer": "C", "task_type": "ego", "question_id": "Q1096"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8918039008cb5d6c66e2c77d66ebadd988b54cd7d3fc395c2063d76eeb4f68ff", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many picture frames were on the wall in the living room?\n(A) 3\n(B) 1\n(C) 2\n(D) 4\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "How many picture frames were on the wall in the living room?\n(A) 3\n(B) 1\n(C) 2\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1100, "doc": {"video_name": "ego_57.mp4", "duration": 480.0, "question": "What did I put inside the dustbin ?\n(A) compost\n(B) laundry\n(C) recyclables\n(D) garbage \n", "candidates": ["compost", "laundry", "recyclables", "garbage "], "answer": "D", "task_type": "ego", "question_id": "Q1100"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5ef0a11118221292752cbb77f2f44243e3ae871bab1b65acf20a1633ad2b96c7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I put inside the dustbin ?\n(A) compost\n(B) laundry\n(C) recyclables\n(D) garbage \n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What did I put inside the dustbin ?\n(A) compost\n(B) laundry\n(C) recyclables\n(D) garbage \n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1104, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Where was the dog after I opened the front door\n(A) in the kitchen\n(B) in the backyard\n(C) in the living room\n(D) in the lobby\n", "candidates": ["in the kitchen", "in the backyard", "in the living room", "in the lobby"], "answer": "D", "task_type": "ego", "question_id": "Q1104"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "df77c2ca418080f457acc53dfb171232e1d7f18afa1fbcc7301b13e636094958", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the dog after I opened the front door\n(A) in the kitchen\n(B) in the backyard\n(C) in the living room\n(D) in the lobby\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "Where was the dog after I opened the front door\n(A) in the kitchen\n(B) in the backyard\n(C) in the living room\n(D) in the lobby\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1108, "doc": {"video_name": "ego_32.mp4", "duration": 480.03333333333376, "question": "Did I leave the drawer open?\n(A) Maybe\n(B) Yes\n(C) I don't know\n(D) No\n", "candidates": ["Maybe", "Yes", "I don't know", "No"], "answer": "D", "task_type": "ego", "question_id": "Q1108"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d124f57f1e6faca79ba7ad4ca2ad0de1befb58fb4c9d376f484d00f440c1e004", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Did I leave the drawer open?\n(A) Maybe\n(B) Yes\n(C) I don't know\n(D) No\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "Did I leave the drawer open?\n(A) Maybe\n(B) Yes\n(C) I don't know\n(D) No\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1112, "doc": {"video_name": "ego_57.mp4", "duration": 480.0, "question": "Where was the red cloth ?\n(A) in the closet\n(B) under the bed\n(C) on the couch\n(D) on the table\n", "candidates": ["in the closet", "under the bed", "on the couch", "on the table"], "answer": "C", "task_type": "ego", "question_id": "Q1112"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a1aa57b2173395cdf369ca2e40be1c174f120c54357e4bae41ce0587744a7768", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the red cloth ?\n(A) in the closet\n(B) under the bed\n(C) on the couch\n(D) on the table\n", "task_type": "ego", "pred_answer": "D", "answer": "C"}, "input": "Where was the red cloth ?\n(A) in the closet\n(B) under the bed\n(C) on the couch\n(D) on the table\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1116, "doc": {"video_name": "movie101_58.mp4", "duration": 605.0, "question": "At the end of the video, what happens to the van?\n(A) Rolls down the cliff and catches fire\n(B) Breaks down\n(C) Collides with another car\n(D) Gets driven away\n", "candidates": ["Rolls down the cliff and catches fire", "Breaks down", "Collides with another car", "Gets driven away"], "answer": "A", "task_type": "plotQA", "question_id": "Q1116"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43c3e701221fd2c30f77662218000e7cab9b2feb9d2d5b89c98e7b23973dd0d0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "At the end of the video, what happens to the van?\n(A) Rolls down the cliff and catches fire\n(B) Breaks down\n(C) Collides with another car\n(D) Gets driven away\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "At the end of the video, what happens to the van?\n(A) Rolls down the cliff and catches fire\n(B) Breaks down\n(C) Collides with another car\n(D) Gets driven away\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1120, "doc": {"video_name": "movie101_112.mp4", "duration": 322.0, "question": "What happened after the person with the yellow stripe arrived at the camp?\n(A) He went to eat\n(B) He went hunting\n(C) He went to war\n(D) He started a fight with the person holding the pipe\n", "candidates": ["He went to eat", "He went hunting", "He went to war", "He started a fight with the person holding the pipe"], "answer": "D", "task_type": "plotQA", "question_id": "Q1120"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c34a1f882c421d18c9789196b5df94ad392fd7c95141b4d7f28130abfabedaed", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What happened after the person with the yellow stripe arrived at the camp?\n(A) He went to eat\n(B) He went hunting\n(C) He went to war\n(D) He started a fight with the person holding the pipe\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What happened after the person with the yellow stripe arrived at the camp?\n(A) He went to eat\n(B) He went hunting\n(C) He went to war\n(D) He started a fight with the person holding the pipe\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1124, "doc": {"video_name": "movie101_86.mp4", "duration": 611.0, "question": "What weather does the event occur in?\n(A) Snowy day\n(B) Rainy day\n(C) Overcast day\n(D) Sunny day\n", "candidates": ["Snowy day", "Rainy day", "Overcast day", "Sunny day"], "answer": "A", "task_type": "plotQA", "question_id": "Q1124"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fff11d83510896990d55aab41d0d8bfb385747ff3ffef87c0fd252ac62110a7d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What weather does the event occur in?\n(A) Snowy day\n(B) Rainy day\n(C) Overcast day\n(D) Sunny day\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What weather does the event occur in?\n(A) Snowy day\n(B) Rainy day\n(C) Overcast day\n(D) Sunny day\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1128, "doc": {"video_name": "tomjerry_4.mp4", "duration": 311.0, "question": "What does the cartoon cat use to hit the cartoon mouse?\n(A) A vase\n(B) Hammer\n(C) Stick\n(D) Stone\n", "candidates": ["A vase", "Hammer", "Stick", "Stone"], "answer": "A", "task_type": "plotQA", "question_id": "Q1128"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "edbc67fa8e3c9f39f153532315658be1757fc8a49b1a63aa9eec906eefac2ae4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What does the cartoon cat use to hit the cartoon mouse?\n(A) A vase\n(B) Hammer\n(C) Stick\n(D) Stone\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What does the cartoon cat use to hit the cartoon mouse?\n(A) A vase\n(B) Hammer\n(C) Stick\n(D) Stone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1132, "doc": {"video_name": "movie101_2.mp4", "duration": 259.0, "question": "What is the outcome for the man in black clothes?\n(A) He is caught by a net laid by a helicopter\n(B) He is shot dead by people in military uniform\n(C) He dies by jumping off the cliff\n(D) He is captured by people in military uniform\n", "candidates": ["He is caught by a net laid by a helicopter", "He is shot dead by people in military uniform", "He dies by jumping off the cliff", "He is captured by people in military uniform"], "answer": "A", "task_type": "plotQA", "question_id": "Q1132"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4441007308d050df38128975667ed02eb1b72539fb1673246a06c85cea383acc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the outcome for the man in black clothes?\n(A) He is caught by a net laid by a helicopter\n(B) He is shot dead by people in military uniform\n(C) He dies by jumping off the cliff\n(D) He is captured by people in military uniform\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What is the outcome for the man in black clothes?\n(A) He is caught by a net laid by a helicopter\n(B) He is shot dead by people in military uniform\n(C) He dies by jumping off the cliff\n(D) He is captured by people in military uniform\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1136, "doc": {"video_name": "movie101_5.mp4", "duration": 795.0, "question": "Is there a scene of archery in the video?\n(A) \n(B) There is an archery scene\n(C) There is no archery scene\n(D) \n", "candidates": ["", "There is an archery scene", "There is no archery scene", ""], "answer": "B", "task_type": "plotQA", "question_id": "Q1136"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B) There is an archery scene"]], "filtered_resps": ["B) There is an archery scene"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aacfd909e15fb5c59bb2a52fa8861229d8eceae4602cbbf5426382e6403c47a6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there a scene of archery in the video?\n(A) \n(B) There is an archery scene\n(C) There is no archery scene\n(D) \n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Is there a scene of archery in the video?\n(A) \n(B) There is an archery scene\n(C) There is no archery scene\n(D) \n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1140, "doc": {"video_name": "movie101_32.mp4", "duration": 247.0, "question": "What color is the car being repaired in the movie?\n(A) Black\n(B) Yellow\n(C) Blue\n(D) White\n", "candidates": ["Black", "Yellow", "Blue", "White"], "answer": "D", "task_type": "plotQA", "question_id": "Q1140"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "787ace0f4f487c6d9aa96f4ad39eb2eb45a07ca0955ad90c1b73313b8b852a4f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the car being repaired in the movie?\n(A) Black\n(B) Yellow\n(C) Blue\n(D) White\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "What color is the car being repaired in the movie?\n(A) Black\n(B) Yellow\n(C) Blue\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1144, "doc": {"video_name": "en_tv_25.mp4", "duration": 478.0, "question": "What color is the top worn by the woman making a phone call in the video?\n(A) Green\n(B) White\n(C) Blue\n(D) Yellow\n", "candidates": ["Green", "White", "Blue", "Yellow"], "answer": "A", "task_type": "plotQA", "question_id": "Q1144"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "818f420faeee33536e5f118d82b01290bdcada28f52185fdf1259180d7089641", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the top worn by the woman making a phone call in the video?\n(A) Green\n(B) White\n(C) Blue\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What color is the top worn by the woman making a phone call in the video?\n(A) Green\n(B) White\n(C) Blue\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1148, "doc": {"video_name": "en_tv_25.mp4", "duration": 478.0, "question": "What color is the table lamp in the background of the scene where two people are chatting?\n(A) White\n(B) Green\n(C) Yellow\n(D) Black\n", "candidates": ["White", "Green", "Yellow", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1148"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c6876d93c266feeafe219e5eab640f21b9442d14a44f80a4b5904cf66496c3b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the table lamp in the background of the scene where two people are chatting?\n(A) White\n(B) Green\n(C) Yellow\n(D) Black\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What color is the table lamp in the background of the scene where two people are chatting?\n(A) White\n(B) Green\n(C) Yellow\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1152, "doc": {"video_name": "en_tv_22.mp4", "duration": 441.0, "question": "What color is the vest worn by the character in the amusement park scene?\n(A) Yellow\n(B) Black\n(C) Green\n(D) White\n", "candidates": ["Yellow", "Black", "Green", "White"], "answer": "B", "task_type": "plotQA", "question_id": "Q1152"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B."]], "filtered_resps": ["B."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a85dde000dba730ffe8d988c12574acf030692680ca648a2e457aacd02e08a9b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the vest worn by the character in the amusement park scene?\n(A) Yellow\n(B) Black\n(C) Green\n(D) White\n", "task_type": "plotQA", "pred_answer": "B.", "answer": "B"}, "input": "What color is the vest worn by the character in the amusement park scene?\n(A) Yellow\n(B) Black\n(C) Green\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1156, "doc": {"video_name": "en_tv_16.mp4", "duration": 552.0, "question": "What is the man's expression when he is chatting with the black man in the video?\n(A) Smiling\n(B) Grimacing\n(C) Bitter smile\n(D) Shocked\n", "candidates": ["Smiling", "Grimacing", "Bitter smile", "Shocked"], "answer": "B", "task_type": "plotQA", "question_id": "Q1156"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "38d4e75484a07686d7709442df0434728d0e9f362a187aa869b0c01b6ba7f2ba", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the man's expression when he is chatting with the black man in the video?\n(A) Smiling\n(B) Grimacing\n(C) Bitter smile\n(D) Shocked\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What is the man's expression when he is chatting with the black man in the video?\n(A) Smiling\n(B) Grimacing\n(C) Bitter smile\n(D) Shocked\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1160, "doc": {"video_name": "movie101_80.mp4", "duration": 467.0, "question": "What is the weather like in the video?\n(A) Sunny\n(B) Rainy\n(C) Snowy\n(D) Cloudy\n", "candidates": ["Sunny", "Rainy", "Snowy", "Cloudy"], "answer": "C", "task_type": "plotQA", "question_id": "Q1160"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43e37d7d1a519981db7f12a811bd16262cc6e01828829caeffc40686a02fe019", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the weather like in the video?\n(A) Sunny\n(B) Rainy\n(C) Snowy\n(D) Cloudy\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What is the weather like in the video?\n(A) Sunny\n(B) Rainy\n(C) Snowy\n(D) Cloudy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1164, "doc": {"video_name": "haimian_8.mp4", "duration": 445.0, "question": "Why does the cartoon sponge have many bumps?\n(A) Because it was stung by a cartoon jellyfish\n(B) Because the cartoon jellyfish was hit\n(C) Because of a cartoon jellyfish allergy\n(D) Because the cartoon jellyfish was bitten\n", "candidates": ["Because it was stung by a cartoon jellyfish", "Because the cartoon jellyfish was hit", "Because of a cartoon jellyfish allergy", "Because the cartoon jellyfish was bitten"], "answer": "A", "task_type": "plotQA", "question_id": "Q1164"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07676dd64784af1ac3a211d9ef4d4260323e03ddca2ea405b37b7e534f875251", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why does the cartoon sponge have many bumps?\n(A) Because it was stung by a cartoon jellyfish\n(B) Because the cartoon jellyfish was hit\n(C) Because of a cartoon jellyfish allergy\n(D) Because the cartoon jellyfish was bitten\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "Why does the cartoon sponge have many bumps?\n(A) Because it was stung by a cartoon jellyfish\n(B) Because the cartoon jellyfish was hit\n(C) Because of a cartoon jellyfish allergy\n(D) Because the cartoon jellyfish was bitten\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1168, "doc": {"video_name": "en_tv_18.mp4", "duration": 568.0, "question": "What color is the hair of the woman giving a speech on the stage?\n(A) Green\n(B) Blue\n(C) Orange\n(D) White\n", "candidates": ["Green", "Blue", "Orange", "White"], "answer": "D", "task_type": "plotQA", "question_id": "Q1168"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "932ade4eb8adb41fd1faf98e6d026b05c5ca76de8a4305bc4c5bcd39efaeb4ea", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the hair of the woman giving a speech on the stage?\n(A) Green\n(B) Blue\n(C) Orange\n(D) White\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What color is the hair of the woman giving a speech on the stage?\n(A) Green\n(B) Blue\n(C) Orange\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1172, "doc": {"video_name": "movie101_73.mp4", "duration": 421.0, "question": "What color is the clothing of the first man to appear in the video?\n(A) White\n(B) Yellow\n(C) Green\n(D) Black\n", "candidates": ["White", "Yellow", "Green", "Black"], "answer": "B", "task_type": "plotQA", "question_id": "Q1172"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "523f4c19a4e157dbb98bee75a5057875e4b09742eb0713aae1becb1d26adc33e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the clothing of the first man to appear in the video?\n(A) White\n(B) Yellow\n(C) Green\n(D) Black\n", "task_type": "plotQA", "pred_answer": "D", "answer": "B"}, "input": "What color is the clothing of the first man to appear in the video?\n(A) White\n(B) Yellow\n(C) Green\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1176, "doc": {"video_name": "movie101_70.mp4", "duration": 361.0, "question": "What color is the man's clothes at the beginning of the video?\n(A) White\n(B) Green\n(C) Yellow\n(D) Black\n", "candidates": ["White", "Green", "Yellow", "Black"], "answer": "D", "task_type": "plotQA", "question_id": "Q1176"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aece10d9e924e5b6b097c29a0c30b9497d663f53e02ed4482245cd9c557e6402", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the man's clothes at the beginning of the video?\n(A) White\n(B) Green\n(C) Yellow\n(D) Black\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the man's clothes at the beginning of the video?\n(A) White\n(B) Green\n(C) Yellow\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1180, "doc": {"video_name": "movie101_25.mp4", "duration": 737.0, "question": "What color is the dog being held by the man in the movie?\n(A) Blue\n(B) Yellow\n(C) White\n(D) Red\n", "candidates": ["Blue", "Yellow", "White", "Red"], "answer": "C", "task_type": "plotQA", "question_id": "Q1180"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d9ddb0727b3a383da66e491a5c0e6502bf3cf8d8e2d0d34ecbfd96d57d535fce", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the dog being held by the man in the movie?\n(A) Blue\n(B) Yellow\n(C) White\n(D) Red\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the dog being held by the man in the movie?\n(A) Blue\n(B) Yellow\n(C) White\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1184, "doc": {"video_name": "tomjerry_4.mp4", "duration": 311.0, "question": "What are the cartoon cat and mouse eating together on the wall?\n(A) Chicken leg\n(B) Burger\n(C) Apple\n(D) Orange\n", "candidates": ["Chicken leg", "Burger", "Apple", "Orange"], "answer": "A", "task_type": "plotQA", "question_id": "Q1184"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf8ef2ab8f564d0316fe471607fe8b56f720e53c9dc62f24cd0dc9858a548215", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What are the cartoon cat and mouse eating together on the wall?\n(A) Chicken leg\n(B) Burger\n(C) Apple\n(D) Orange\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What are the cartoon cat and mouse eating together on the wall?\n(A) Chicken leg\n(B) Burger\n(C) Apple\n(D) Orange\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1188, "doc": {"video_name": "movie101_104.mp4", "duration": 514.0, "question": "Why does the girl with glasses run downstairs in a hurry?\n(A) Chasing someone\n(B) Being chased\n(C) To help another girl out of a situation\n(D) To get something\n", "candidates": ["Chasing someone", "Being chased", "To help another girl out of a situation", "To get something"], "answer": "C", "task_type": "plotQA", "question_id": "Q1188"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eb00654ab108a14adaa4ecea1cdfadcc6bebb87a6ee33ba518526c20336c05ae", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why does the girl with glasses run downstairs in a hurry?\n(A) Chasing someone\n(B) Being chased\n(C) To help another girl out of a situation\n(D) To get something\n", "task_type": "plotQA", "pred_answer": "D", "answer": "C"}, "input": "Why does the girl with glasses run downstairs in a hurry?\n(A) Chasing someone\n(B) Being chased\n(C) To help another girl out of a situation\n(D) To get something\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1192, "doc": {"video_name": "movie101_83.mp4", "duration": 640.0, "question": "What color is the woman's clothes at the beginning of the video?\n(A) Red\n(B) Green\n(C) Black\n(D) White\n", "candidates": ["Red", "Green", "Black", "White"], "answer": "A", "task_type": "plotQA", "question_id": "Q1192"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9a57a2daa4a999655a3f8e298dd875b5748382d5cdaff1636113e31b5802490f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the woman's clothes at the beginning of the video?\n(A) Red\n(B) Green\n(C) Black\n(D) White\n", "task_type": "plotQA", "pred_answer": "C.", "answer": "A"}, "input": "What color is the woman's clothes at the beginning of the video?\n(A) Red\n(B) Green\n(C) Black\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1196, "doc": {"video_name": "en_tv_36.mp4", "duration": 574.0, "question": "What color is the skirt worn by the old woman in the video?\n(A) Black\n(B) Green\n(C) Red\n(D) White\n", "candidates": ["Black", "Green", "Red", "White"], "answer": "C", "task_type": "plotQA", "question_id": "Q1196"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "42c5fbaa7bea966b1f4052c871058a7d563d25bd32b51a1383c2fcddaff435d9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the skirt worn by the old woman in the video?\n(A) Black\n(B) Green\n(C) Red\n(D) White\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What color is the skirt worn by the old woman in the video?\n(A) Black\n(B) Green\n(C) Red\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1200, "doc": {"video_name": "movie101_18.mp4", "duration": 318.0, "question": "What color are the glasses the little boy is wearing in the video?\n(A) Green\n(B) Black\n(C) Red\n(D) Blue\n", "candidates": ["Green", "Black", "Red", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1200"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8a71f9d14c6b2b02e765c89fb089133deda4235c5d95f1dd82f502edd23ec7fc", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color are the glasses the little boy is wearing in the video?\n(A) Green\n(B) Black\n(C) Red\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color are the glasses the little boy is wearing in the video?\n(A) Green\n(B) Black\n(C) Red\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1204, "doc": {"video_name": "en_tv_27.mp4", "duration": 486.0, "question": "What color is the old woman's clothes in the video?\n(A) Purple\n(B) Black\n(C) Yellow\n(D) Red\n", "candidates": ["Purple", "Black", "Yellow", "Red"], "answer": "A", "task_type": "plotQA", "question_id": "Q1204"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a2adf14d889c345ffddb611e5add1d795bdd3e8bb9efd5622c7d20f4cb0647df", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the old woman's clothes in the video?\n(A) Purple\n(B) Black\n(C) Yellow\n(D) Red\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the old woman's clothes in the video?\n(A) Purple\n(B) Black\n(C) Yellow\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1208, "doc": {"video_name": "8.mp4", "duration": 507.29, "question": "What color is the beard of the captain on the ship in the video?\n(A) Brown\n(B) Red\n(C) Black\n(D) Has a long white beard\n", "candidates": ["Brown", "Red", "Black", "Has a long white beard"], "answer": "D", "task_type": "plotQA", "question_id": "Q1208"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a7aca4aec257f545861deebb832330e8435319728dc0631d9f4d75d93465ba70", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the beard of the captain on the ship in the video?\n(A) Brown\n(B) Red\n(C) Black\n(D) Has a long white beard\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What color is the beard of the captain on the ship in the video?\n(A) Brown\n(B) Red\n(C) Black\n(D) Has a long white beard\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1212, "doc": {"video_name": "movie101_17.mp4", "duration": 613.0, "question": "What color is the clothes of the man who enters the door at the beginning of the video?\n(A) White\n(B) Red\n(C) Green\n(D) Blue\n", "candidates": ["White", "Red", "Green", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1212"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b479676ec469cd758dfd7c87096dd73701b2a5233bed55c2bc85a703ea2637d3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the clothes of the man who enters the door at the beginning of the video?\n(A) White\n(B) Red\n(C) Green\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What color is the clothes of the man who enters the door at the beginning of the video?\n(A) White\n(B) Red\n(C) Green\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1216, "doc": {"video_name": "tomjerry_6.mp4", "duration": 313.0, "question": "What broke the cartoon cat's teeth?\n(A) Knife\n(B) Stone\n(C) Stick\n(D) The golf ball\n", "candidates": ["Knife", "Stone", "Stick", "The golf ball"], "answer": "D", "task_type": "plotQA", "question_id": "Q1216"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5eed91d62e7cc925ac5d1295cd702dc5185dacf846d41c42745caa4c54d3c783", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What broke the cartoon cat's teeth?\n(A) Knife\n(B) Stone\n(C) Stick\n(D) The golf ball\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What broke the cartoon cat's teeth?\n(A) Knife\n(B) Stone\n(C) Stick\n(D) The golf ball\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1220, "doc": {"video_name": "movie101_18.mp4", "duration": 318.0, "question": "What color is the woman's dress in the video?\n(A) White\n(B) Blue\n(C) Red\n(D) Green\n", "candidates": ["White", "Blue", "Red", "Green"], "answer": "A", "task_type": "plotQA", "question_id": "Q1220"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7407418f9cf570cb612bd58f60fd2866d81896a3366189d7ef349642f54d9112", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the woman's dress in the video?\n(A) White\n(B) Blue\n(C) Red\n(D) Green\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the woman's dress in the video?\n(A) White\n(B) Blue\n(C) Red\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1224, "doc": {"video_name": "movie101_35.mp4", "duration": 481.0, "question": "What color is the dress of the woman holding the bowl in the video?\n(A) Blue\n(B) Red\n(C) Purple\n(D) Green\n", "candidates": ["Blue", "Red", "Purple", "Green"], "answer": "B", "task_type": "plotQA", "question_id": "Q1224"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6f872f1b6cd640fd64940a9af8ebc4c3b235520a38c9db56811740154ec3bdad", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the dress of the woman holding the bowl in the video?\n(A) Blue\n(B) Red\n(C) Purple\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What color is the dress of the woman holding the bowl in the video?\n(A) Blue\n(B) Red\n(C) Purple\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1228, "doc": {"video_name": "en_tv_21.mp4", "duration": 538.0, "question": "What color is the fence on the road in the movie?\n(A) Blue\n(B) Orange\n(C) Yellow\n(D) Green\n", "candidates": ["Blue", "Orange", "Yellow", "Green"], "answer": "C", "task_type": "plotQA", "question_id": "Q1228"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b9a2051eb58d825538d9591412c4cb28b25f5c33fda90de75f15e66a510eeb2f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the fence on the road in the movie?\n(A) Blue\n(B) Orange\n(C) Yellow\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "What color is the fence on the road in the movie?\n(A) Blue\n(B) Orange\n(C) Yellow\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1232, "doc": {"video_name": "en_tv_12.mp4", "duration": 587.0, "question": "What unexpected situation occurred at the rock climbing site?\n(A) A sudden storm came\n(B) The facilities of the rock climbing site had a technical fault\n(C) The child fell off the rock\n(D) The child started climbing alone\n", "candidates": ["A sudden storm came", "The facilities of the rock climbing site had a technical fault", "The child fell off the rock", "The child started climbing alone"], "answer": "D", "task_type": "plotQA", "question_id": "Q1232"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "49c64e98c9b76b80efd40147ad61ae2cde20a8f4c9558ddd8f6a1a3233092209", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What unexpected situation occurred at the rock climbing site?\n(A) A sudden storm came\n(B) The facilities of the rock climbing site had a technical fault\n(C) The child fell off the rock\n(D) The child started climbing alone\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What unexpected situation occurred at the rock climbing site?\n(A) A sudden storm came\n(B) The facilities of the rock climbing site had a technical fault\n(C) The child fell off the rock\n(D) The child started climbing alone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1236, "doc": {"video_name": "movie101_67.mp4", "duration": 530.0, "question": "What falls when the woman is eavesdropping?\n(A) Backpack\n(B) Mobile phone\n(C) Recorder\n(D) Earphones\n", "candidates": ["Backpack", "Mobile phone", "Recorder", "Earphones"], "answer": "C", "task_type": "plotQA", "question_id": "Q1236"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b4a453c942be4f0a379e104a2bcc17e01112db6e92651456cdc3f6d50fb340d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What falls when the woman is eavesdropping?\n(A) Backpack\n(B) Mobile phone\n(C) Recorder\n(D) Earphones\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What falls when the woman is eavesdropping?\n(A) Backpack\n(B) Mobile phone\n(C) Recorder\n(D) Earphones\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1240, "doc": {"video_name": "movie101_81.mp4", "duration": 252.0, "question": "Where is the wedding in the video held?\n(A) Forest\n(B) Beach\n(C) Desert\n(D) Church\n", "candidates": ["Forest", "Beach", "Desert", "Church"], "answer": "D", "task_type": "plotQA", "question_id": "Q1240"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d2433dd186ce86e23dfcd7753d3c6f1278f548cbf8330f2a567763d8174d65cf", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where is the wedding in the video held?\n(A) Forest\n(B) Beach\n(C) Desert\n(D) Church\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Where is the wedding in the video held?\n(A) Forest\n(B) Beach\n(C) Desert\n(D) Church\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1244, "doc": {"video_name": "movie101_26.mp4", "duration": 763.0, "question": "What color is the safety helmet the worker in the video is wearing?\n(A) Blue\n(B) Yellow\n(C) Green\n(D) Red\n", "candidates": ["Blue", "Yellow", "Green", "Red"], "answer": "B", "task_type": "plotQA", "question_id": "Q1244"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1f8acea09325898cb993cb12a30069b6f6a7d1d468fbcd9f04de4492eb8bb68e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the safety helmet the worker in the video is wearing?\n(A) Blue\n(B) Yellow\n(C) Green\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the safety helmet the worker in the video is wearing?\n(A) Blue\n(B) Yellow\n(C) Green\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1248, "doc": {"video_name": "tomjerry_5.mp4", "duration": 303.0, "question": "How did the cartoon cat enter the yard?\n(A) Riding a car\n(B) Holding a hammer\n(C) Wearing a cartoon dog's headgear\n(D) Wearing a blanket\n", "candidates": ["Riding a car", "Holding a hammer", "Wearing a cartoon dog's headgear", "Wearing a blanket"], "answer": "C", "task_type": "plotQA", "question_id": "Q1248"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b69eeff3b87719f1c6e8d4c906b33840f4c8ce7ef6802a635edc7fa4df7172d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How did the cartoon cat enter the yard?\n(A) Riding a car\n(B) Holding a hammer\n(C) Wearing a cartoon dog's headgear\n(D) Wearing a blanket\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "How did the cartoon cat enter the yard?\n(A) Riding a car\n(B) Holding a hammer\n(C) Wearing a cartoon dog's headgear\n(D) Wearing a blanket\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1252, "doc": {"video_name": "xiaoliyu_2.mp4", "duration": 364.0, "question": "Why did the frog faint?\n(A) It was poisoned by a conch\n(B) It was knocked out by a conch\n(C) It was angered by a conch\n(D) It was scared by a conch\n", "candidates": ["It was poisoned by a conch", "It was knocked out by a conch", "It was angered by a conch", "It was scared by a conch"], "answer": "B", "task_type": "plotQA", "question_id": "Q1252"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "17ac23dafa9c1173d16091b679f9c29df35d19f5eaa0f6f9fe5378195f091360", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why did the frog faint?\n(A) It was poisoned by a conch\n(B) It was knocked out by a conch\n(C) It was angered by a conch\n(D) It was scared by a conch\n", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "Why did the frog faint?\n(A) It was poisoned by a conch\n(B) It was knocked out by a conch\n(C) It was angered by a conch\n(D) It was scared by a conch\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1256, "doc": {"video_name": "en_tv_21.mp4", "duration": 538.0, "question": "What color is the bag that the man is carrying in the rooftop conversation scene?\n(A) Blue\n(B) Orange\n(C) Black\n(D) Green\n", "candidates": ["Blue", "Orange", "Black", "Green"], "answer": "C", "task_type": "plotQA", "question_id": "Q1256"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8558487129029b945e2231116f12cd03518f10e427fc332f674600d912ee10b2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the bag that the man is carrying in the rooftop conversation scene?\n(A) Blue\n(B) Orange\n(C) Black\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "What color is the bag that the man is carrying in the rooftop conversation scene?\n(A) Blue\n(B) Orange\n(C) Black\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1260, "doc": {"video_name": "en_tv_8.mp4", "duration": 460.0, "question": "What does the man with a scar on his face do after drawing blood?\n(A) Talks with someone\n(B) Eats bread\n(C) Takes a pill from a yellow bottle\n(D) Drinks milk\n", "candidates": ["Talks with someone", "Eats bread", "Takes a pill from a yellow bottle", "Drinks milk"], "answer": "C", "task_type": "plotQA", "question_id": "Q1260"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "24172948dac631fd9fa0723b73265e50fb39186f6c1b4e7a2f9a7a8665e64f11", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What does the man with a scar on his face do after drawing blood?\n(A) Talks with someone\n(B) Eats bread\n(C) Takes a pill from a yellow bottle\n(D) Drinks milk\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What does the man with a scar on his face do after drawing blood?\n(A) Talks with someone\n(B) Eats bread\n(C) Takes a pill from a yellow bottle\n(D) Drinks milk\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1264, "doc": {"video_name": "movie101_78.mp4", "duration": 607.0, "question": "What animal is the woman and man holding in their hands while standing in the night?\n(A) Pig\n(B) Cat\n(C) Rabbit\n(D) Dog\n", "candidates": ["Pig", "Cat", "Rabbit", "Dog"], "answer": "D", "task_type": "plotQA", "question_id": "Q1264"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "10ddb25b85bd48b05b4e5027a23da4766fc627898755be78944b02d1eb090678", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What animal is the woman and man holding in their hands while standing in the night?\n(A) Pig\n(B) Cat\n(C) Rabbit\n(D) Dog\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "What animal is the woman and man holding in their hands while standing in the night?\n(A) Pig\n(B) Cat\n(C) Rabbit\n(D) Dog\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1268, "doc": {"video_name": "movie101_110.mp4", "duration": 423.0, "question": "In the red corridor, what scene is shown in the video?\n(A) Eating\n(B) Conversation\n(C) Playing\n(D) Fighting\n", "candidates": ["Eating", "Conversation", "Playing", "Fighting"], "answer": "B", "task_type": "plotQA", "question_id": "Q1268"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b7a2d0f4bbc4a2d1b8c96ac0ddc845ecc593327d78030806c3276304147d1d1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the red corridor, what scene is shown in the video?\n(A) Eating\n(B) Conversation\n(C) Playing\n(D) Fighting\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "In the red corridor, what scene is shown in the video?\n(A) Eating\n(B) Conversation\n(C) Playing\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1272, "doc": {"video_name": "movie101_63.mp4", "duration": 748.0, "question": "What animal is lifted up at the end of the video?\n(A) Snake\n(B) Fish\n(C) Chicken\n(D) Duck\n", "candidates": ["Snake", "Fish", "Chicken", "Duck"], "answer": "C", "task_type": "plotQA", "question_id": "Q1272"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f1d565b5ca08791f54f2ffefca030641c5c8f006d26ae88895d679ad9b3855b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What animal is lifted up at the end of the video?\n(A) Snake\n(B) Fish\n(C) Chicken\n(D) Duck\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What animal is lifted up at the end of the video?\n(A) Snake\n(B) Fish\n(C) Chicken\n(D) Duck\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1276, "doc": {"video_name": "movie101_106.mp4", "duration": 746.0, "question": "Why is the woman scared at the beginning of the video?\n(A) Because she is afraid of riding a horse\n(B) Because she is afraid of getting her clothes dirty\n(C) Because she might fall\n(D) Because she can't ride a horse\n", "candidates": ["Because she is afraid of riding a horse", "Because she is afraid of getting her clothes dirty", "Because she might fall", "Because she can't ride a horse"], "answer": "A", "task_type": "plotQA", "question_id": "Q1276"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "33dd86716276bf04aa1877859717e7638fd4a1690d208376cccf2d4b7c3b4f43", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why is the woman scared at the beginning of the video?\n(A) Because she is afraid of riding a horse\n(B) Because she is afraid of getting her clothes dirty\n(C) Because she might fall\n(D) Because she can't ride a horse\n", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "Why is the woman scared at the beginning of the video?\n(A) Because she is afraid of riding a horse\n(B) Because she is afraid of getting her clothes dirty\n(C) Because she might fall\n(D) Because she can't ride a horse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1280, "doc": {"video_name": "movie101_20.mp4", "duration": 476.0, "question": "What type of animal appears in a group in the video?\n(A) Ducks\n(B) Sheep\n(C) Chickens\n(D) Cows\n", "candidates": ["Ducks", "Sheep", "Chickens", "Cows"], "answer": "B", "task_type": "plotQA", "question_id": "Q1280"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "353f428a3f5216b515c84f7d25ee54b745c34f62cfd7d47c694f2477e4eb5a56", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of animal appears in a group in the video?\n(A) Ducks\n(B) Sheep\n(C) Chickens\n(D) Cows\n", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "What type of animal appears in a group in the video?\n(A) Ducks\n(B) Sheep\n(C) Chickens\n(D) Cows\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1284, "doc": {"video_name": "tomjerry_8.mp4", "duration": 308.0, "question": "Why did the cartoon cat's body become flat like a pancake?\n(A) It hit the door\n(B) It was crushed by a rock\n(C) It hit the wall\n(D) It was flattened by a stick\n", "candidates": ["It hit the door", "It was crushed by a rock", "It hit the wall", "It was flattened by a stick"], "answer": "C", "task_type": "plotQA", "question_id": "Q1284"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "400bc15659ad3e27477069abdd53604c8d4e82ba965e8c04c18267f0070701f2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why did the cartoon cat's body become flat like a pancake?\n(A) It hit the door\n(B) It was crushed by a rock\n(C) It hit the wall\n(D) It was flattened by a stick\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "Why did the cartoon cat's body become flat like a pancake?\n(A) It hit the door\n(B) It was crushed by a rock\n(C) It hit the wall\n(D) It was flattened by a stick\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1288, "doc": {"video_name": "movie101_30.mp4", "duration": 726.0, "question": "What color is the hoodie the boy in the video is wearing?\n(A) White\n(B) Purple\n(C) Blue\n(D) Green\n", "candidates": ["White", "Purple", "Blue", "Green"], "answer": "B", "task_type": "plotQA", "question_id": "Q1288"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6f851fc56dc0894049c162fee2c5e998bec1bf3ec9e89eff8cb077b05ff55111", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the hoodie the boy in the video is wearing?\n(A) White\n(B) Purple\n(C) Blue\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the hoodie the boy in the video is wearing?\n(A) White\n(B) Purple\n(C) Blue\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1292, "doc": {"video_name": "movie101_72.mp4", "duration": 289.0, "question": "Where does the man throw the woman?\n(A) On the boat\n(B) On the ground\n(C) On the tree\n(D) In the river\n", "candidates": ["On the boat", "On the ground", "On the tree", "In the river"], "answer": "A", "task_type": "plotQA", "question_id": "Q1292"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9bab823f211677d5a3b2f7974e0d4c9f2d732f150de54aa2d3f5cb994209d9e7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where does the man throw the woman?\n(A) On the boat\n(B) On the ground\n(C) On the tree\n(D) In the river\n", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "Where does the man throw the woman?\n(A) On the boat\n(B) On the ground\n(C) On the tree\n(D) In the river\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1296, "doc": {"video_name": "haimian_9.mp4", "duration": 444.0, "question": "Where did the cartoon sponge and the cartoon starfish hide while talking?\n(A) In the mailbox\n(B) In the house\n(C) In the wooden box\n(D) In the car\n", "candidates": ["In the mailbox", "In the house", "In the wooden box", "In the car"], "answer": "A", "task_type": "plotQA", "question_id": "Q1296"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "06cd8914e2a0b316f27d6e8af2b86903cd6a679dcb4d8db83d6e53225de4395f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did the cartoon sponge and the cartoon starfish hide while talking?\n(A) In the mailbox\n(B) In the house\n(C) In the wooden box\n(D) In the car\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "Where did the cartoon sponge and the cartoon starfish hide while talking?\n(A) In the mailbox\n(B) In the house\n(C) In the wooden box\n(D) In the car\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1300, "doc": {"video_name": "xiaoliyu_3.mp4", "duration": 405.0, "question": "What did the cartoon dragon do when the lava flowed into the sea?\n(A) Flew into the volcano with the sea water wrapped around it\n(B) Drink water\n(C) Eat snacks\n(D) Play ball\n", "candidates": ["Flew into the volcano with the sea water wrapped around it", "Drink water", "Eat snacks", "Play ball"], "answer": "A", "task_type": "plotQA", "question_id": "Q1300"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "169ad8d762fad23416b7548c97993664b89f4ef409c7ed12cd2928e24d2010cc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did the cartoon dragon do when the lava flowed into the sea?\n(A) Flew into the volcano with the sea water wrapped around it\n(B) Drink water\n(C) Eat snacks\n(D) Play ball\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What did the cartoon dragon do when the lava flowed into the sea?\n(A) Flew into the volcano with the sea water wrapped around it\n(B) Drink water\n(C) Eat snacks\n(D) Play ball\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1304, "doc": {"video_name": "en_tv_4.mp4", "duration": 421.0, "question": "After the woman shot at the man, what happened to the woman?\n(A) The woman slipped and was caught by the man again\n(B) The woman successfully escaped\n(C) The woman was killed by others while running away\n(D) The woman was caught by the police while running away\n", "candidates": ["The woman slipped and was caught by the man again", "The woman successfully escaped", "The woman was killed by others while running away", "The woman was caught by the police while running away"], "answer": "A", "task_type": "plotQA", "question_id": "Q1304"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f4990cc1c48b7ed9291f5a436bd113693f9bb4342e8a0d417920a0b82816ffb1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "After the woman shot at the man, what happened to the woman?\n(A) The woman slipped and was caught by the man again\n(B) The woman successfully escaped\n(C) The woman was killed by others while running away\n(D) The woman was caught by the police while running away\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "After the woman shot at the man, what happened to the woman?\n(A) The woman slipped and was caught by the man again\n(B) The woman successfully escaped\n(C) The woman was killed by others while running away\n(D) The woman was caught by the police while running away\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1308, "doc": {"video_name": "movie101_56.mp4", "duration": 338.0, "question": "What is on the woman's table that makes the man angry?\n(A) Water bottle\n(B) Pass\n(C) ID card\n(D) Train ticket\n", "candidates": ["Water bottle", "Pass", "ID card", "Train ticket"], "answer": "B", "task_type": "plotQA", "question_id": "Q1308"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cabb4533b6a1ddb06d0f43892417446cc67b0452686bc6dc6cee98e8288ecab3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is on the woman's table that makes the man angry?\n(A) Water bottle\n(B) Pass\n(C) ID card\n(D) Train ticket\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is on the woman's table that makes the man angry?\n(A) Water bottle\n(B) Pass\n(C) ID card\n(D) Train ticket\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1312, "doc": {"video_name": "tomjerry_4.mp4", "duration": 311.0, "question": "Why was the cartoon cat thrown out of the door?\n(A) Because the cartoon cat couldn't catch the cartoon mouse\n(B) Because people thought the cartoon cat broke the vase\n(C) Because it's too ugly\n(D) Because it stole food\n", "candidates": ["Because the cartoon cat couldn't catch the cartoon mouse", "Because people thought the cartoon cat broke the vase", "Because it's too ugly", "Because it stole food"], "answer": "B", "task_type": "plotQA", "question_id": "Q1312"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6ec6c6f65cadf0e4c361296d5f85c202915c3b1a221b748284ad16a8470c0a42", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why was the cartoon cat thrown out of the door?\n(A) Because the cartoon cat couldn't catch the cartoon mouse\n(B) Because people thought the cartoon cat broke the vase\n(C) Because it's too ugly\n(D) Because it stole food\n", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "Why was the cartoon cat thrown out of the door?\n(A) Because the cartoon cat couldn't catch the cartoon mouse\n(B) Because people thought the cartoon cat broke the vase\n(C) Because it's too ugly\n(D) Because it stole food\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1316, "doc": {"video_name": "en_tv_34.mp4", "duration": 492.0, "question": "What color is the clothes of the woman who leaves after the conversation in the video?\n(A) Grey\n(B) Purple\n(C) White\n(D) Blue\n", "candidates": ["Grey", "Purple", "White", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1316"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f46e5841d96c0fdf203c82184015b9ecbec5768e9b9589109b039c77809b8c4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the clothes of the woman who leaves after the conversation in the video?\n(A) Grey\n(B) Purple\n(C) White\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the clothes of the woman who leaves after the conversation in the video?\n(A) Grey\n(B) Purple\n(C) White\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1320, "doc": {"video_name": "movie101_1.mp4", "duration": 214.0, "question": "Why did the children run out of the classroom?\n(A) To play games\n(B) Class is over\n(C) To watch two other children fight\n(D) To eat\n", "candidates": ["To play games", "Class is over", "To watch two other children fight", "To eat"], "answer": "C", "task_type": "plotQA", "question_id": "Q1320"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "36b790c66257f6cdbcd0c9e0b2b314211380d8dd06cd45bcc67e1c58ce01a7d8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why did the children run out of the classroom?\n(A) To play games\n(B) Class is over\n(C) To watch two other children fight\n(D) To eat\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Why did the children run out of the classroom?\n(A) To play games\n(B) Class is over\n(C) To watch two other children fight\n(D) To eat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1324, "doc": {"video_name": "xiaoliyu_2.mp4", "duration": 364.0, "question": "What does the cartoon frog use to take something from the cartoon seahorse's hand?\n(A) Tongue\n(B) Head\n(C) Hand\n(D) Foot\n", "candidates": ["Tongue", "Head", "Hand", "Foot"], "answer": "A", "task_type": "plotQA", "question_id": "Q1324"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4330bbc45d468ab5239a16e07c9a4b8c8d09bf676085ef624574b9d4aebb0383", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What does the cartoon frog use to take something from the cartoon seahorse's hand?\n(A) Tongue\n(B) Head\n(C) Hand\n(D) Foot\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What does the cartoon frog use to take something from the cartoon seahorse's hand?\n(A) Tongue\n(B) Head\n(C) Hand\n(D) Foot\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1328, "doc": {"video_name": "movie101_71.mp4", "duration": 227.0, "question": "What mode of transportation does the man at the end of the video use to pick up the woman?\n(A) Electric Scooter\n(B) Car\n(C) Bicycle\n(D) Train\n", "candidates": ["Electric Scooter", "Car", "Bicycle", "Train"], "answer": "B", "task_type": "plotQA", "question_id": "Q1328"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3e93a0e68c54f6ecf9cf422b32360aab2a4c7d7c5bf9615baa386d19b7211c16", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What mode of transportation does the man at the end of the video use to pick up the woman?\n(A) Electric Scooter\n(B) Car\n(C) Bicycle\n(D) Train\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What mode of transportation does the man at the end of the video use to pick up the woman?\n(A) Electric Scooter\n(B) Car\n(C) Bicycle\n(D) Train\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1332, "doc": {"video_name": "tomjerry_10.mp4", "duration": 302.0, "question": "What does the cartoon big mouse use to help the cartoon little mouse climb the table?\n(A) Tablecloth\n(B) Towel\n(C) Rope\n(D) Noodles\n", "candidates": ["Tablecloth", "Towel", "Rope", "Noodles"], "answer": "D", "task_type": "plotQA", "question_id": "Q1332"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e595198fb8cec0537d18803a712a168376c25a5abdb8b3f32f1a61db947750c6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What does the cartoon big mouse use to help the cartoon little mouse climb the table?\n(A) Tablecloth\n(B) Towel\n(C) Rope\n(D) Noodles\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "What does the cartoon big mouse use to help the cartoon little mouse climb the table?\n(A) Tablecloth\n(B) Towel\n(C) Rope\n(D) Noodles\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1336, "doc": {"video_name": "en_tv_33.mp4", "duration": 532.0, "question": "What color is the woman's clothing when she is running in the video?\n(A) White\n(B) Blue\n(C) Pink\n(D) Green\n", "candidates": ["White", "Blue", "Pink", "Green"], "answer": "A", "task_type": "plotQA", "question_id": "Q1336"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f343a37a2ef6502dd8ac01fec1eda2eaf0f87c155db3e2482a4ab39310a8e511", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the woman's clothing when she is running in the video?\n(A) White\n(B) Blue\n(C) Pink\n(D) Green\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the woman's clothing when she is running in the video?\n(A) White\n(B) Blue\n(C) Pink\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1340, "doc": {"video_name": "en_tv_33.mp4", "duration": 532.0, "question": "What fruit is on the table in the video?\n(A) Orange\n(B) Banana\n(C) Blueberries\n(D) Apple\n", "candidates": ["Orange", "Banana", "Blueberries", "Apple"], "answer": "C", "task_type": "plotQA", "question_id": "Q1340"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ecf4a26fa478eb9d583c09e00277982ddf2aaf8ad6bac14d623b8c054853b00b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What fruit is on the table in the video?\n(A) Orange\n(B) Banana\n(C) Blueberries\n(D) Apple\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What fruit is on the table in the video?\n(A) Orange\n(B) Banana\n(C) Blueberries\n(D) Apple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1344, "doc": {"video_name": "en_tv_7.mp4", "duration": 533.0, "question": "What do the woman in the leather jacket and the man with a scar on his face see when they enter the room?\n(A) A man sitting on a chair.\n(B) A man standing by the window.\n(C) A man dies at the edge of the window.\n(D) A man sitting on the bed.\n", "candidates": ["A man sitting on a chair.", "A man standing by the window.", "A man dies at the edge of the window.", "A man sitting on the bed."], "answer": "C", "task_type": "plotQA", "question_id": "Q1344"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "932b70cee501ebb9e0dc30f4355ba708e9442c00ad15919c18c64326c19d9a1f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What do the woman in the leather jacket and the man with a scar on his face see when they enter the room?\n(A) A man sitting on a chair.\n(B) A man standing by the window.\n(C) A man dies at the edge of the window.\n(D) A man sitting on the bed.\n", "task_type": "plotQA", "pred_answer": "C.", "answer": "C"}, "input": "What do the woman in the leather jacket and the man with a scar on his face see when they enter the room?\n(A) A man sitting on a chair.\n(B) A man standing by the window.\n(C) A man dies at the edge of the window.\n(D) A man sitting on the bed.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1348, "doc": {"video_name": "movie101_113.mp4", "duration": 500.0, "question": "Why does the man lie on the chair?\n(A) To receive treatment\n(B) The man is thinking\n(C) The man is stargazing\n(D) The man is sleeping\n", "candidates": ["To receive treatment", "The man is thinking", "The man is stargazing", "The man is sleeping"], "answer": "A", "task_type": "plotQA", "question_id": "Q1348"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0672fc2f4dc7041bb5d0f5c358dc1b9e462269818bd6c818d968c5de6bfd9eb0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why does the man lie on the chair?\n(A) To receive treatment\n(B) The man is thinking\n(C) The man is stargazing\n(D) The man is sleeping\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "Why does the man lie on the chair?\n(A) To receive treatment\n(B) The man is thinking\n(C) The man is stargazing\n(D) The man is sleeping\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1352, "doc": {"video_name": "movie101_32.mp4", "duration": 247.0, "question": "What color is the car knocking on the window and opening the door in the movie?\n(A) Red\n(B) Blue\n(C) Yellow\n(D) White\n", "candidates": ["Red", "Blue", "Yellow", "White"], "answer": "A", "task_type": "plotQA", "question_id": "Q1352"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "382971d9f0ecf462e799ce48e99d119f2f78c9f4d1c351dba90938f9cb3bd2f6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the car knocking on the window and opening the door in the movie?\n(A) Red\n(B) Blue\n(C) Yellow\n(D) White\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What color is the car knocking on the window and opening the door in the movie?\n(A) Red\n(B) Blue\n(C) Yellow\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1356, "doc": {"video_name": "movie101_77.mp4", "duration": 206.0, "question": "In the video, what instrument is being played in the audience?\n(A) Suona\n(B) Piano\n(C) Flute\n(D) Accordion\n", "candidates": ["Suona", "Piano", "Flute", "Accordion"], "answer": "B", "task_type": "plotQA", "question_id": "Q1356"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3bef173181bba23b24018403470d94cbef05640d69d9d6abbfb883056575c66c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the video, what instrument is being played in the audience?\n(A) Suona\n(B) Piano\n(C) Flute\n(D) Accordion\n", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "In the video, what instrument is being played in the audience?\n(A) Suona\n(B) Piano\n(C) Flute\n(D) Accordion\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1360, "doc": {"video_name": "en_tv_35.mp4", "duration": 505.0, "question": "What color is the man's hair at the beginning of the video?\n(A) Red\n(B) Blonde\n(C) White\n(D) Black\n", "candidates": ["Red", "Blonde", "White", "Black"], "answer": "B", "task_type": "plotQA", "question_id": "Q1360"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "139ac1705b115315ec8729942552811790139194eec8f0c5b5533f2e329f7941", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the man's hair at the beginning of the video?\n(A) Red\n(B) Blonde\n(C) White\n(D) Black\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What color is the man's hair at the beginning of the video?\n(A) Red\n(B) Blonde\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1364, "doc": {"video_name": "movie101_38.mp4", "duration": 539.0, "question": "What color is the dress of the woman who walks into the restaurant in the video?\n(A) Green\n(B) Black\n(C) Blue\n(D) Red\n", "candidates": ["Green", "Black", "Blue", "Red"], "answer": "D", "task_type": "plotQA", "question_id": "Q1364"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a487633e6c2f2e136d0b235c3cd8c18598038e9b7803e6d5ebbdae61f21e9e97", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the dress of the woman who walks into the restaurant in the video?\n(A) Green\n(B) Black\n(C) Blue\n(D) Red\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What color is the dress of the woman who walks into the restaurant in the video?\n(A) Green\n(B) Black\n(C) Blue\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1368, "doc": {"video_name": "en_tv_36.mp4", "duration": 574.0, "question": "At the end of the video, what color is the woman's phone?\n(A) White\n(B) Black\n(C) Blue\n(D) Purple\n", "candidates": ["White", "Black", "Blue", "Purple"], "answer": "D", "task_type": "plotQA", "question_id": "Q1368"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a7e448ca7e96db56378a55cede6f55647e5ade472dbec35e346ff3fe3de95d74", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "At the end of the video, what color is the woman's phone?\n(A) White\n(B) Black\n(C) Blue\n(D) Purple\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "At the end of the video, what color is the woman's phone?\n(A) White\n(B) Black\n(C) Blue\n(D) Purple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1372, "doc": {"video_name": "movie101_69.mp4", "duration": 385.0, "question": "What did the white-haired man do after receiving the phone call?\n(A) Dump the goods\n(B) Sleep\n(C) Drink\n(D) Eat\n", "candidates": ["Dump the goods", "Sleep", "Drink", "Eat"], "answer": "A", "task_type": "plotQA", "question_id": "Q1372"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d3b204ffc7a69c23299d601eaf01e4ef640538e4ddad7089b610094568cdbb7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did the white-haired man do after receiving the phone call?\n(A) Dump the goods\n(B) Sleep\n(C) Drink\n(D) Eat\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What did the white-haired man do after receiving the phone call?\n(A) Dump the goods\n(B) Sleep\n(C) Drink\n(D) Eat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1376, "doc": {"video_name": "movie101_3.mp4", "duration": 535.0, "question": "What was the final outcome of the person in the yellow vest and the other man with long hair?\n(A) One escaped and one was caught\n(B) They successfully escaped from the roof\n(C) Caught\n(D) Hid\n", "candidates": ["One escaped and one was caught", "They successfully escaped from the roof", "Caught", "Hid"], "answer": "B", "task_type": "plotQA", "question_id": "Q1376"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bbe76dd1d46b4bbdd435f5b5fc6a9cef39f906313e5a891996c99a530aa39ced", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What was the final outcome of the person in the yellow vest and the other man with long hair?\n(A) One escaped and one was caught\n(B) They successfully escaped from the roof\n(C) Caught\n(D) Hid\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What was the final outcome of the person in the yellow vest and the other man with long hair?\n(A) One escaped and one was caught\n(B) They successfully escaped from the roof\n(C) Caught\n(D) Hid\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1380, "doc": {"video_name": "en_tv_4.mp4", "duration": 421.0, "question": "Under the bridge tunnel, what did the woman do during her interaction with the man?\n(A) She shot at the man with a gun\n(B) The woman turned around and left the bridge tunnel\n(C) The woman hugged the man\n(D) The woman lost control of her emotions\n", "candidates": ["She shot at the man with a gun", "The woman turned around and left the bridge tunnel", "The woman hugged the man", "The woman lost control of her emotions"], "answer": "A", "task_type": "plotQA", "question_id": "Q1380"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "28ae631659a84a731e96033d0e6f8b656aa1c6c98ca7914aa340b63fca954899", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Under the bridge tunnel, what did the woman do during her interaction with the man?\n(A) She shot at the man with a gun\n(B) The woman turned around and left the bridge tunnel\n(C) The woman hugged the man\n(D) The woman lost control of her emotions\n", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "Under the bridge tunnel, what did the woman do during her interaction with the man?\n(A) She shot at the man with a gun\n(B) The woman turned around and left the bridge tunnel\n(C) The woman hugged the man\n(D) The woman lost control of her emotions\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1384, "doc": {"video_name": "movie101_61.mp4", "duration": 627.0, "question": "Why is the woman hiding from the police at the beginning of the video?\n(A) Because she is doing illegal work\n(B) Because she has a history of evading the police\n(C) Because she is being coerced by a villain\n(D) Because she is an undercover cop and doesn't want to blow her cover\n", "candidates": ["Because she is doing illegal work", "Because she has a history of evading the police", "Because she is being coerced by a villain", "Because she is an undercover cop and doesn't want to blow her cover"], "answer": "A", "task_type": "plotQA", "question_id": "Q1384"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7ac1eb65afdfc1ed9f4894288f1b584d3b5c54b66da4c9d3dc77f0a81b4bb753", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why is the woman hiding from the police at the beginning of the video?\n(A) Because she is doing illegal work\n(B) Because she has a history of evading the police\n(C) Because she is being coerced by a villain\n(D) Because she is an undercover cop and doesn't want to blow her cover\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "Why is the woman hiding from the police at the beginning of the video?\n(A) Because she is doing illegal work\n(B) Because she has a history of evading the police\n(C) Because she is being coerced by a villain\n(D) Because she is an undercover cop and doesn't want to blow her cover\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1388, "doc": {"video_name": "movie101_107.mp4", "duration": 471.0, "question": "What do the man in the grey vest and the man selling raw meat do at the marketplace at night?\n(A) Repair equipment\n(B) Steal things\n(C) Clean the marketplace\n(D) Sabotage the electrical switch\n", "candidates": ["Repair equipment", "Steal things", "Clean the marketplace", "Sabotage the electrical switch"], "answer": "D", "task_type": "plotQA", "question_id": "Q1388"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "187ffe72e6092c94472c32f21680b4600c7e6607c23d516728ce80c33ed56b54", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What do the man in the grey vest and the man selling raw meat do at the marketplace at night?\n(A) Repair equipment\n(B) Steal things\n(C) Clean the marketplace\n(D) Sabotage the electrical switch\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What do the man in the grey vest and the man selling raw meat do at the marketplace at night?\n(A) Repair equipment\n(B) Steal things\n(C) Clean the marketplace\n(D) Sabotage the electrical switch\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1392, "doc": {"video_name": "movie101_12.mp4", "duration": 346.0, "question": "What color of top does the woman who is doing the makeup in the video wear?\n(A) Yellow\n(B) Blue\n(C) Green\n(D) White\n", "candidates": ["Yellow", "Blue", "Green", "White"], "answer": "B", "task_type": "plotQA", "question_id": "Q1392"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "48a08888d37b63b6a4b096d561810e4ac82308bcb1723a04060935237a07ada8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color of top does the woman who is doing the makeup in the video wear?\n(A) Yellow\n(B) Blue\n(C) Green\n(D) White\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What color of top does the woman who is doing the makeup in the video wear?\n(A) Yellow\n(B) Blue\n(C) Green\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1396, "doc": {"video_name": "movie101_23.mp4", "duration": 473.0, "question": "What color is the suit that the man playing violin on the hot air balloon in the video is wearing?\n(A) Red\n(B) Yellow\n(C) Green\n(D) Black\n", "candidates": ["Red", "Yellow", "Green", "Black"], "answer": "D", "task_type": "plotQA", "question_id": "Q1396"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "acb7222b795290d4ea1563c3bffaddfad4eb2c3a1f55af44aa84a799fe46f6ee", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the suit that the man playing violin on the hot air balloon in the video is wearing?\n(A) Red\n(B) Yellow\n(C) Green\n(D) Black\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What color is the suit that the man playing violin on the hot air balloon in the video is wearing?\n(A) Red\n(B) Yellow\n(C) Green\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1400, "doc": {"video_name": "movie101_89.mp4", "duration": 677.0, "question": "What color is the old woman's clothing at the start of the video?\n(A) White\n(B) Black\n(C) Green\n(D) Brown\n", "candidates": ["White", "Black", "Green", "Brown"], "answer": "D", "task_type": "plotQA", "question_id": "Q1400"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f72a0c5fdb8219ab39d9e67d3f4276c20e5ec8c4cacd65c790f7d43b742bc127", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the old woman's clothing at the start of the video?\n(A) White\n(B) Black\n(C) Green\n(D) Brown\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the old woman's clothing at the start of the video?\n(A) White\n(B) Black\n(C) Green\n(D) Brown\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1404, "doc": {"video_name": "movie101_35.mp4", "duration": 481.0, "question": "What color are the shoes of the robot in the video?\n(A) Red\n(B) Purple\n(C) Green\n(D) Blue\n", "candidates": ["Red", "Purple", "Green", "Blue"], "answer": "A", "task_type": "plotQA", "question_id": "Q1404"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e88df302828e10d5a85b63c29dc37a8a8542681e11181976525d9188b4a5a098", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color are the shoes of the robot in the video?\n(A) Red\n(B) Purple\n(C) Green\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What color are the shoes of the robot in the video?\n(A) Red\n(B) Purple\n(C) Green\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1408, "doc": {"video_name": "en_tv_28.mp4", "duration": 575.0, "question": "What color is the old woman's clothes in the video?\n(A) Yellow\n(B) Black\n(C) Red\n(D) Purple\n", "candidates": ["Yellow", "Black", "Red", "Purple"], "answer": "D", "task_type": "plotQA", "question_id": "Q1408"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5c6a141909eff6537e8e0e1014245ceeb7d05a7eb92ce3a86fb36b94f7078180", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the old woman's clothes in the video?\n(A) Yellow\n(B) Black\n(C) Red\n(D) Purple\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What color is the old woman's clothes in the video?\n(A) Yellow\n(B) Black\n(C) Red\n(D) Purple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1412, "doc": {"video_name": "movie101_0.mp4", "duration": 789.0, "question": "Why did the three people appearing at the beginning of the video stop in the forest?\n(A) Rest\n(B) They want to observe the castle in the forest\n(C) Play\n(D) Sleep\n", "candidates": ["Rest", "They want to observe the castle in the forest", "Play", "Sleep"], "answer": "B", "task_type": "plotQA", "question_id": "Q1412"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ecb47a65936dd4e857d6fc136c0d4acd6247fed352b6d7a5cc7e1947422cf9af", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why did the three people appearing at the beginning of the video stop in the forest?\n(A) Rest\n(B) They want to observe the castle in the forest\n(C) Play\n(D) Sleep\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Why did the three people appearing at the beginning of the video stop in the forest?\n(A) Rest\n(B) They want to observe the castle in the forest\n(C) Play\n(D) Sleep\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1416, "doc": {"video_name": "movie101_113.mp4", "duration": 500.0, "question": "What happens to the man after the argument with the woman?\n(A) The man is drinking water\n(B) The man is smoking\n(C) The man is eating\n(D) He commits suicide with a gun\n", "candidates": ["The man is drinking water", "The man is smoking", "The man is eating", "He commits suicide with a gun"], "answer": "D", "task_type": "plotQA", "question_id": "Q1416"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "44090d664941670aa775751b6be1c913aeb09612671c7c9dce75e8ea95973ccf", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What happens to the man after the argument with the woman?\n(A) The man is drinking water\n(B) The man is smoking\n(C) The man is eating\n(D) He commits suicide with a gun\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What happens to the man after the argument with the woman?\n(A) The man is drinking water\n(B) The man is smoking\n(C) The man is eating\n(D) He commits suicide with a gun\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1420, "doc": {"video_name": "tomjerry_3.mp4", "duration": 303.0, "question": "What did the delivery man deliver?\n(A) Ham\n(B) Fish meat\n(C) Toy\n(D) A kitten\n", "candidates": ["Ham", "Fish meat", "Toy", "A kitten"], "answer": "D", "task_type": "plotQA", "question_id": "Q1420"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "17994bd3dc241b73b82e74dc3ebab0a97cbf9a22b4d5d2ea551310cf97f1855b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did the delivery man deliver?\n(A) Ham\n(B) Fish meat\n(C) Toy\n(D) A kitten\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What did the delivery man deliver?\n(A) Ham\n(B) Fish meat\n(C) Toy\n(D) A kitten\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1424, "doc": {"video_name": "movie101_16.mp4", "duration": 740.0, "question": "What kind of animals are being kept in the video?\n(A) Sheep\n(B) Dog\n(C) Cat\n(D) Pig\n", "candidates": ["Sheep", "Dog", "Cat", "Pig"], "answer": "A", "task_type": "plotQA", "question_id": "Q1424"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ed277a8fee2675dc642eee19769479e373c30e8aed5ec5158ca208a1e38479f6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What kind of animals are being kept in the video?\n(A) Sheep\n(B) Dog\n(C) Cat\n(D) Pig\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What kind of animals are being kept in the video?\n(A) Sheep\n(B) Dog\n(C) Cat\n(D) Pig\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1428, "doc": {"video_name": "movie101_9.mp4", "duration": 713.0, "question": "What is inside the can in the forest?\n(A) Insects\n(B) Grains and water\n(C) Meat\n(D) Candies\n", "candidates": ["Insects", "Grains and water", "Meat", "Candies"], "answer": "B", "task_type": "plotQA", "question_id": "Q1428"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43648dc9da237f019f4c8466f5fdb40d3b7f9df21247271f98bdab091dec1da8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is inside the can in the forest?\n(A) Insects\n(B) Grains and water\n(C) Meat\n(D) Candies\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is inside the can in the forest?\n(A) Insects\n(B) Grains and water\n(C) Meat\n(D) Candies\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1432, "doc": {"video_name": "en_tv_24.mp4", "duration": 482.0, "question": "What color is the lamp next to the two people chatting in the clip?\n(A) Red\n(B) Blue\n(C) Black\n(D) Green\n", "candidates": ["Red", "Blue", "Black", "Green"], "answer": "D", "task_type": "plotQA", "question_id": "Q1432"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07d63780ed616cc8f7c0837a0923411232327ee758596a2707261add8a15d3cd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the lamp next to the two people chatting in the clip?\n(A) Red\n(B) Blue\n(C) Black\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What color is the lamp next to the two people chatting in the clip?\n(A) Red\n(B) Blue\n(C) Black\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1436, "doc": {"video_name": "en_tv_37.mp4", "duration": 467.0, "question": "What color of clothes does the girl who lost her life wear?\n(A) Red\n(B) White\n(C) Blue\n(D) Green\n", "candidates": ["Red", "White", "Blue", "Green"], "answer": "B", "task_type": "plotQA", "question_id": "Q1436"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b79a3a6f4929a8fdc47ed5aeef1b0fb546cafe9b70de3c9a86764a595a3b6a81", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color of clothes does the girl who lost her life wear?\n(A) Red\n(B) White\n(C) Blue\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color of clothes does the girl who lost her life wear?\n(A) Red\n(B) White\n(C) Blue\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1440, "doc": {"video_name": "tomjerry_1.mp4", "duration": 311.0, "question": "From where does the cartoon mouse escape to the outside of the house?\n(A) Door\n(B) Tunnel\n(C) Hole in the wall\n(D) Window\n", "candidates": ["Door", "Tunnel", "Hole in the wall", "Window"], "answer": "D", "task_type": "plotQA", "question_id": "Q1440"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "681b7512130dc4ea6154e20ae3671f5fe59ae7b5f36e59fb9cab6326eabe7f2d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "From where does the cartoon mouse escape to the outside of the house?\n(A) Door\n(B) Tunnel\n(C) Hole in the wall\n(D) Window\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "From where does the cartoon mouse escape to the outside of the house?\n(A) Door\n(B) Tunnel\n(C) Hole in the wall\n(D) Window\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1444, "doc": {"video_name": "movie101_29.mp4", "duration": 482.0, "question": "What color is the hat the man in the movie is wearing?\n(A) Blue\n(B) White\n(C) Black\n(D) Green\n", "candidates": ["Blue", "White", "Black", "Green"], "answer": "C", "task_type": "plotQA", "question_id": "Q1444"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6a17c1654c7ee4f6d880f0a65875a5e2616c50f40bcab3a3047b324173460abc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the hat the man in the movie is wearing?\n(A) Blue\n(B) White\n(C) Black\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the hat the man in the movie is wearing?\n(A) Blue\n(B) White\n(C) Black\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1448, "doc": {"video_name": "movie101_21.mp4", "duration": 289.0, "question": "How many people are on the pirate ship?\n(A) Six\n(B) Three\n(C) Three\n(D) Eight\n", "candidates": ["Six", "Three", "Three", "Eight"], "answer": "B", "task_type": "plotQA", "question_id": "Q1448"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "96d0b5ccee90f30e40d1f3f83eb8e1c92bb397fc30a745f532d5774bf7e4992d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many people are on the pirate ship?\n(A) Six\n(B) Three\n(C) Three\n(D) Eight\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "How many people are on the pirate ship?\n(A) Six\n(B) Three\n(C) Three\n(D) Eight\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1452, "doc": {"video_name": "movie101_52.mp4", "duration": 338.0, "question": "At the beginning of the video, what are the two soldiers looking for on the battlefield?\n(A) A letter\n(B) A watch\n(C) A shoe\n(D) A belt\n", "candidates": ["A letter", "A watch", "A shoe", "A belt"], "answer": "B", "task_type": "plotQA", "question_id": "Q1452"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "be57ef81487c70926e8ee581c9f1747b12fa940f56f9a049635e227ca7f69dee", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "At the beginning of the video, what are the two soldiers looking for on the battlefield?\n(A) A letter\n(B) A watch\n(C) A shoe\n(D) A belt\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "At the beginning of the video, what are the two soldiers looking for on the battlefield?\n(A) A letter\n(B) A watch\n(C) A shoe\n(D) A belt\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1456, "doc": {"video_name": "xiaoliyu_4.mp4", "duration": 399.0, "question": "Why is the cartoon shark chasing the cartoon carp?\n(A) To snatch teeth\n(B) Attack\n(C) Play\n(D) Snatch food\n", "candidates": ["To snatch teeth", "Attack", "Play", "Snatch food"], "answer": "A", "task_type": "plotQA", "question_id": "Q1456"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "88d0adc04df6590e9cb271ffba3484b80742c3d3d24c10dbbe133a63412672f7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why is the cartoon shark chasing the cartoon carp?\n(A) To snatch teeth\n(B) Attack\n(C) Play\n(D) Snatch food\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "Why is the cartoon shark chasing the cartoon carp?\n(A) To snatch teeth\n(B) Attack\n(C) Play\n(D) Snatch food\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1460, "doc": {"video_name": "en_tv_5.mp4", "duration": 491.0, "question": "What happened during the police search process?\n(A) Knocked down by a man and a woman\n(B) Hit by a car\n(C) Shot by a gun\n(D) Bitten by a dog\n", "candidates": ["Knocked down by a man and a woman", "Hit by a car", "Shot by a gun", "Bitten by a dog"], "answer": "A", "task_type": "plotQA", "question_id": "Q1460"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d5a6f1017babd6deda1dac9e4c236178381b1822fe1f01bf34016d70d4e526d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What happened during the police search process?\n(A) Knocked down by a man and a woman\n(B) Hit by a car\n(C) Shot by a gun\n(D) Bitten by a dog\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What happened during the police search process?\n(A) Knocked down by a man and a woman\n(B) Hit by a car\n(C) Shot by a gun\n(D) Bitten by a dog\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1464, "doc": {"video_name": "movie101_68.mp4", "duration": 445.0, "question": "What color car hit the woman in the video?\n(A) Red\n(B) White\n(C) Blue\n(D) Black\n", "candidates": ["Red", "White", "Blue", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1464"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "58ae201af421b526351987b0674ced39d7a42249e1b749b9feb985b568f2ea27", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color car hit the woman in the video?\n(A) Red\n(B) White\n(C) Blue\n(D) Black\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What color car hit the woman in the video?\n(A) Red\n(B) White\n(C) Blue\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1468, "doc": {"video_name": "en_tv_19.mp4", "duration": 583.0, "question": "What color is the top of the woman talking to the black police officer in the movie?\n(A) Green\n(B) Black\n(C) White\n(D) Purple\n", "candidates": ["Green", "Black", "White", "Purple"], "answer": "D", "task_type": "plotQA", "question_id": "Q1468"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "484de2c73d2a855a2c7cd35c766357075210e09e6fdb1008d358d806f1976cb2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the top of the woman talking to the black police officer in the movie?\n(A) Green\n(B) Black\n(C) White\n(D) Purple\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What color is the top of the woman talking to the black police officer in the movie?\n(A) Green\n(B) Black\n(C) White\n(D) Purple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1472, "doc": {"video_name": "haimian_4.mp4", "duration": 323.0, "question": "How does the cartoon sponge walk after entering the room?\n(A) Swaggering\n(B) Tip-toeing\n(C) Walking with pause\n(D) Hopping\n", "candidates": ["Swaggering", "Tip-toeing", "Walking with pause", "Hopping"], "answer": "B", "task_type": "plotQA", "question_id": "Q1472"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a1f34d28c35ab7fe3be5085b7ece12b4d3e8c748389e041e573ae66ec9b67670", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How does the cartoon sponge walk after entering the room?\n(A) Swaggering\n(B) Tip-toeing\n(C) Walking with pause\n(D) Hopping\n", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "How does the cartoon sponge walk after entering the room?\n(A) Swaggering\n(B) Tip-toeing\n(C) Walking with pause\n(D) Hopping\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1476, "doc": {"video_name": "haimian_10.mp4", "duration": 361.0, "question": "Why is the cartoon sponge going up and down in the air?\n(A) Because the cartoon sponge is looking for something on the roof\n(B) Because the cartoon sponge is dancing\n(C) Because the cartoon starfish is dancing\n(D) Because the cartoon starfish is running back and forth on the roof\n", "candidates": ["Because the cartoon sponge is looking for something on the roof", "Because the cartoon sponge is dancing", "Because the cartoon starfish is dancing", "Because the cartoon starfish is running back and forth on the roof"], "answer": "D", "task_type": "plotQA", "question_id": "Q1476"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d857852564a92322ffdbd1e9d4fd1e269c7a0e44d2a5c4842c0ec93168975b40", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why is the cartoon sponge going up and down in the air?\n(A) Because the cartoon sponge is looking for something on the roof\n(B) Because the cartoon sponge is dancing\n(C) Because the cartoon starfish is dancing\n(D) Because the cartoon starfish is running back and forth on the roof\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Why is the cartoon sponge going up and down in the air?\n(A) Because the cartoon sponge is looking for something on the roof\n(B) Because the cartoon sponge is dancing\n(C) Because the cartoon starfish is dancing\n(D) Because the cartoon starfish is running back and forth on the roof\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1480, "doc": {"video_name": "movie101_39.mp4", "duration": 473.0, "question": "What color is the dress of the woman coming down from the eaves?\n(A) White\n(B) Red\n(C) Black\n(D) Blue\n", "candidates": ["White", "Red", "Black", "Blue"], "answer": "C", "task_type": "plotQA", "question_id": "Q1480"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ade970c6d9e66940bccbcbad7500b663322e7a7d839add7990391ffccd9c3d5a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the dress of the woman coming down from the eaves?\n(A) White\n(B) Red\n(C) Black\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What color is the dress of the woman coming down from the eaves?\n(A) White\n(B) Red\n(C) Black\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1484, "doc": {"video_name": "movie101_58.mp4", "duration": 605.0, "question": "What is the man's reaction after the reporter asks several questions in a row?\n(A) Pleased\n(B) Happy\n(C) Gets angry and ends the interview\n(D) Sad\n", "candidates": ["Pleased", "Happy", "Gets angry and ends the interview", "Sad"], "answer": "C", "task_type": "plotQA", "question_id": "Q1484"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "09330b4722c2772b01c5a594bc006d8e214644ef272def3cd8cebb19a5060ab8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the man's reaction after the reporter asks several questions in a row?\n(A) Pleased\n(B) Happy\n(C) Gets angry and ends the interview\n(D) Sad\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What is the man's reaction after the reporter asks several questions in a row?\n(A) Pleased\n(B) Happy\n(C) Gets angry and ends the interview\n(D) Sad\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1488, "doc": {"video_name": "tomjerry_7.mp4", "duration": 306.0, "question": "What is inside the white golf ball?\n(A) Chocolate\n(B) A wooden ball\n(C) A bomb\n(D) Nothing\n", "candidates": ["Chocolate", "A wooden ball", "A bomb", "Nothing"], "answer": "C", "task_type": "plotQA", "question_id": "Q1488"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1703206c9b507e8935e3cea1a552bb8890bc83a9598a98f0a5a1acf01c0dbafd", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is inside the white golf ball?\n(A) Chocolate\n(B) A wooden ball\n(C) A bomb\n(D) Nothing\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What is inside the white golf ball?\n(A) Chocolate\n(B) A wooden ball\n(C) A bomb\n(D) Nothing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1492, "doc": {"video_name": "en_tv_7.mp4", "duration": 533.0, "question": "What happens to the man and woman in the parking lot after they separate?\n(A) They fight with strangers.\n(B) The woman helps a little girl.\n(C) The woman has a car accident.\n(D) The man runs into an old friend in the parking lot.\n", "candidates": ["They fight with strangers.", "The woman helps a little girl.", "The woman has a car accident.", "The man runs into an old friend in the parking lot."], "answer": "A", "task_type": "plotQA", "question_id": "Q1492"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D."]], "filtered_resps": ["D."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bdc329bf03cade61e6b422aaeb0d3d528fc7ac5ed064c92328d8a9a5cb5e318f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What happens to the man and woman in the parking lot after they separate?\n(A) They fight with strangers.\n(B) The woman helps a little girl.\n(C) The woman has a car accident.\n(D) The man runs into an old friend in the parking lot.\n", "task_type": "plotQA", "pred_answer": "D.", "answer": "A"}, "input": "What happens to the man and woman in the parking lot after they separate?\n(A) They fight with strangers.\n(B) The woman helps a little girl.\n(C) The woman has a car accident.\n(D) The man runs into an old friend in the parking lot.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1496, "doc": {"video_name": "en_tv_34.mp4", "duration": 492.0, "question": "What is the mood of the woman who leaves after the conversation in the video?\n(A) Scared\n(B) Angry\n(C) Crying\n(D) Happy\n", "candidates": ["Scared", "Angry", "Crying", "Happy"], "answer": "B", "task_type": "plotQA", "question_id": "Q1496"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "567a3f2d839b95bf9abc9164a7f54189225c435bd1f600b22b85988a32dfdb5f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the mood of the woman who leaves after the conversation in the video?\n(A) Scared\n(B) Angry\n(C) Crying\n(D) Happy\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the mood of the woman who leaves after the conversation in the video?\n(A) Scared\n(B) Angry\n(C) Crying\n(D) Happy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1500, "doc": {"video_name": "en_tv_10.mp4", "duration": 545.0, "question": "At the end of the video, what is the mood of the woman in the plaid shirt when she talks to the man with a scar on his face?\n(A) Excited\n(B) Scared\n(C) Excited\n(D) Nervous\n", "candidates": ["Excited", "Scared", "Excited", "Nervous"], "answer": "A", "task_type": "plotQA", "question_id": "Q1500"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "627a777d825121b0e14f09553e1758de5981d16751736b3612b3152e4be24493", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "At the end of the video, what is the mood of the woman in the plaid shirt when she talks to the man with a scar on his face?\n(A) Excited\n(B) Scared\n(C) Excited\n(D) Nervous\n", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "At the end of the video, what is the mood of the woman in the plaid shirt when she talks to the man with a scar on his face?\n(A) Excited\n(B) Scared\n(C) Excited\n(D) Nervous\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1504, "doc": {"video_name": "movie101_27.mp4", "duration": 374.0, "question": "What color is the shirt worn by the man while chatting with the woman in the office?\n(A) Blue\n(B) White\n(C) Green\n(D) Purple\n", "candidates": ["Blue", "White", "Green", "Purple"], "answer": "D", "task_type": "plotQA", "question_id": "Q1504"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ebaa540108e47fa0bf452511ebdb7723b3dfacd1e780a7ced18eab7f420682cb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the shirt worn by the man while chatting with the woman in the office?\n(A) Blue\n(B) White\n(C) Green\n(D) Purple\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the shirt worn by the man while chatting with the woman in the office?\n(A) Blue\n(B) White\n(C) Green\n(D) Purple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1508, "doc": {"video_name": "tomjerry_10.mp4", "duration": 302.0, "question": "Why did the cartoon little mouse turn into a sphere?\n(A) Because it swallowed an orange\n(B) Because it swallowed an apple\n(C) Because it swallowed a tomato\n(D) Because it swallowed a peach\n", "candidates": ["Because it swallowed an orange", "Because it swallowed an apple", "Because it swallowed a tomato", "Because it swallowed a peach"], "answer": "A", "task_type": "plotQA", "question_id": "Q1508"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3b050ef83b08d675597641aa09927387a5c4405ffdb97e106c1dcf12193a91e7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why did the cartoon little mouse turn into a sphere?\n(A) Because it swallowed an orange\n(B) Because it swallowed an apple\n(C) Because it swallowed a tomato\n(D) Because it swallowed a peach\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "Why did the cartoon little mouse turn into a sphere?\n(A) Because it swallowed an orange\n(B) Because it swallowed an apple\n(C) Because it swallowed a tomato\n(D) Because it swallowed a peach\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1512, "doc": {"video_name": "movie101_101.mp4", "duration": 318.0, "question": "In the desert, why was the person in the white uniform being helped to walk?\n(A) Because a tsunami occurred\n(B) Because a sandstorm occurred\n(C) His leg was injured\n(D) Because an earthquake occurred\n", "candidates": ["Because a tsunami occurred", "Because a sandstorm occurred", "His leg was injured", "Because an earthquake occurred"], "answer": "C", "task_type": "plotQA", "question_id": "Q1512"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e6e1306a745c80737730c44034235a968f91541fce5e829e907e8727efab88d8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the desert, why was the person in the white uniform being helped to walk?\n(A) Because a tsunami occurred\n(B) Because a sandstorm occurred\n(C) His leg was injured\n(D) Because an earthquake occurred\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "In the desert, why was the person in the white uniform being helped to walk?\n(A) Because a tsunami occurred\n(B) Because a sandstorm occurred\n(C) His leg was injured\n(D) Because an earthquake occurred\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1516, "doc": {"video_name": "movie101_65.mp4", "duration": 611.0, "question": "What happened to the painting of the fat man in the video?\n(A) Torn\n(B) Smeared\n(C) Burned\n(D) Disappeared\n", "candidates": ["Torn", "Smeared", "Burned", "Disappeared"], "answer": "C", "task_type": "plotQA", "question_id": "Q1516"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2e75ffa40f7dab087cfae846173ed7f8053a426f526447b444ab700d9ed2c6a4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What happened to the painting of the fat man in the video?\n(A) Torn\n(B) Smeared\n(C) Burned\n(D) Disappeared\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "What happened to the painting of the fat man in the video?\n(A) Torn\n(B) Smeared\n(C) Burned\n(D) Disappeared\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1520, "doc": {"video_name": "haimian_7.mp4", "duration": 426.0, "question": "Why did the Cartoon Sponge turn into a sphere?\n(A) Because the Cartoon Octopus wanted to use it to shoot a basket\n(B) Because it can help it escape quickly\n(C) Because the Cartoon Shark wanted to use it to shoot a basket\n(D) Because it curled up into a ball out of fear\n", "candidates": ["Because the Cartoon Octopus wanted to use it to shoot a basket", "Because it can help it escape quickly", "Because the Cartoon Shark wanted to use it to shoot a basket", "Because it curled up into a ball out of fear"], "answer": "C", "task_type": "plotQA", "question_id": "Q1520"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c4ce3956d237b95b1353e43ca100b255132c3de83bac481cda938e8200608bac", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why did the Cartoon Sponge turn into a sphere?\n(A) Because the Cartoon Octopus wanted to use it to shoot a basket\n(B) Because it can help it escape quickly\n(C) Because the Cartoon Shark wanted to use it to shoot a basket\n(D) Because it curled up into a ball out of fear\n", "task_type": "plotQA", "pred_answer": "D", "answer": "C"}, "input": "Why did the Cartoon Sponge turn into a sphere?\n(A) Because the Cartoon Octopus wanted to use it to shoot a basket\n(B) Because it can help it escape quickly\n(C) Because the Cartoon Shark wanted to use it to shoot a basket\n(D) Because it curled up into a ball out of fear\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1524, "doc": {"video_name": "en_tv_26.mp4", "duration": 522.0, "question": "What color is the man's top who is performing on stage at the amusement park?\n(A) Pink\n(B) Black\n(C) Red\n(D) White\n", "candidates": ["Pink", "Black", "Red", "White"], "answer": "B", "task_type": "plotQA", "question_id": "Q1524"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2eff96d4f738b9ac199caa9814fc539550e1cf02630fe2bd5d8a5d90dc06f005", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the man's top who is performing on stage at the amusement park?\n(A) Pink\n(B) Black\n(C) Red\n(D) White\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the man's top who is performing on stage at the amusement park?\n(A) Pink\n(B) Black\n(C) Red\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1528, "doc": {"video_name": "movie101_113.mp4", "duration": 500.0, "question": "What does the man being treated see during his visit?\n(A) A banana\n(B) An apple\n(C) A man and a woman having an argument\n(D) A pineapple\n", "candidates": ["A banana", "An apple", "A man and a woman having an argument", "A pineapple"], "answer": "C", "task_type": "plotQA", "question_id": "Q1528"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5966c60c1ab63ba7531a148b490f8201352dca4065fcc3bd371578c1344f4683", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What does the man being treated see during his visit?\n(A) A banana\n(B) An apple\n(C) A man and a woman having an argument\n(D) A pineapple\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What does the man being treated see during his visit?\n(A) A banana\n(B) An apple\n(C) A man and a woman having an argument\n(D) A pineapple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1532, "doc": {"video_name": "movie101_57.mp4", "duration": 753.0, "question": "What mode of transportation does the woman in the video use?\n(A) Bus\n(B) Bicycle\n(C) Car\n(D) Electric scooter\n", "candidates": ["Bus", "Bicycle", "Car", "Electric scooter"], "answer": "D", "task_type": "plotQA", "question_id": "Q1532"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b9a248bd17e383f2ca051e2e328ed5d997bd73ccb7c4a63859bbf78604125698", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What mode of transportation does the woman in the video use?\n(A) Bus\n(B) Bicycle\n(C) Car\n(D) Electric scooter\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What mode of transportation does the woman in the video use?\n(A) Bus\n(B) Bicycle\n(C) Car\n(D) Electric scooter\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1536, "doc": {"video_name": "movie101_10.mp4", "duration": 716.0, "question": "What color is the watch worn by the character in the video?\n(A) Red\n(B) Blue\n(C) Black\n(D) White\n", "candidates": ["Red", "Blue", "Black", "White"], "answer": "C", "task_type": "plotQA", "question_id": "Q1536"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7b5c1b3a103663a53b87706dcfac49326b75ab80699fbfc96face0218e7e6bad", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the watch worn by the character in the video?\n(A) Red\n(B) Blue\n(C) Black\n(D) White\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the watch worn by the character in the video?\n(A) Red\n(B) Blue\n(C) Black\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1540, "doc": {"video_name": "haimian_1.mp4", "duration": 305.0, "question": "What is the cartoon sponge doing lying in bed?\n(A) Watching TV\n(B) Chatting\n(C) Playing games\n(D) Sleeping\n", "candidates": ["Watching TV", "Chatting", "Playing games", "Sleeping"], "answer": "A", "task_type": "plotQA", "question_id": "Q1540"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1a3286b20ed3c2288d2033adf2b5197fb33e28ecd168fb43d575e180a1b31b14", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the cartoon sponge doing lying in bed?\n(A) Watching TV\n(B) Chatting\n(C) Playing games\n(D) Sleeping\n", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "What is the cartoon sponge doing lying in bed?\n(A) Watching TV\n(B) Chatting\n(C) Playing games\n(D) Sleeping\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1544, "doc": {"video_name": "movie101_24.mp4", "duration": 311.0, "question": "What color is the mountain in the background of the video?\n(A) Blue\n(B) White\n(C) Green\n(D) Brown\n", "candidates": ["Blue", "White", "Green", "Brown"], "answer": "A", "task_type": "plotQA", "question_id": "Q1544"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "522e7d9e84056fe151668b26a1379564169f2223942c2feab67ae06745af382b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the mountain in the background of the video?\n(A) Blue\n(B) White\n(C) Green\n(D) Brown\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What color is the mountain in the background of the video?\n(A) Blue\n(B) White\n(C) Green\n(D) Brown\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1548, "doc": {"video_name": "movie101_59.mp4", "duration": 458.0, "question": "Why was the man's ear cut off in the video?\n(A) Because he got into a fight with someone\n(B) Another man accidentally cut the wrong person\n(C) Because he violated some agreement and was punished\n(D) He was retaliated against by others\n", "candidates": ["Because he got into a fight with someone", "Another man accidentally cut the wrong person", "Because he violated some agreement and was punished", "He was retaliated against by others"], "answer": "B", "task_type": "plotQA", "question_id": "Q1548"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8bb97abaee5878503860d4c0e28f8a47c6e255eefd60b771abe180ec9a5cbe7d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why was the man's ear cut off in the video?\n(A) Because he got into a fight with someone\n(B) Another man accidentally cut the wrong person\n(C) Because he violated some agreement and was punished\n(D) He was retaliated against by others\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "Why was the man's ear cut off in the video?\n(A) Because he got into a fight with someone\n(B) Another man accidentally cut the wrong person\n(C) Because he violated some agreement and was punished\n(D) He was retaliated against by others\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1552, "doc": {"video_name": "tomjerry_2.mp4", "duration": 303.0, "question": "What color blanket is the cartoon cat wrapped in when closing the door?\n(A) Green\n(B) Deep pink\n(C) Yellow\n(D) Blue\n", "candidates": ["Green", "Deep pink", "Yellow", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1552"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1bde0adaa0afac5a3434986c54baf0eea6840af9cdc10905d5865417f70b0043", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color blanket is the cartoon cat wrapped in when closing the door?\n(A) Green\n(B) Deep pink\n(C) Yellow\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color blanket is the cartoon cat wrapped in when closing the door?\n(A) Green\n(B) Deep pink\n(C) Yellow\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1556, "doc": {"video_name": "movie101_15.mp4", "duration": 441.0, "question": "What color is the table in the boys' dormitory in the video?\n(A) Yellow\n(B) Blue\n(C) White\n(D) Green\n", "candidates": ["Yellow", "Blue", "White", "Green"], "answer": "C", "task_type": "plotQA", "question_id": "Q1556"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "920133909124b03119fab935787347fde8c9e37c7821320f3c8c149dac6ab51d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the table in the boys' dormitory in the video?\n(A) Yellow\n(B) Blue\n(C) White\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the table in the boys' dormitory in the video?\n(A) Yellow\n(B) Blue\n(C) White\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1560, "doc": {"video_name": "movie101_86.mp4", "duration": 611.0, "question": "What is the mood of the man at the beginning of the video?\n(A) Wronged\n(B) Neutral\n(C) Sad\n(D) Joyful\n", "candidates": ["Wronged", "Neutral", "Sad", "Joyful"], "answer": "C", "task_type": "plotQA", "question_id": "Q1560"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9909026fdd4b0955952caa4932e6b5dbff4b2719151a242f55adf356d1c1ab6b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the mood of the man at the beginning of the video?\n(A) Wronged\n(B) Neutral\n(C) Sad\n(D) Joyful\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What is the mood of the man at the beginning of the video?\n(A) Wronged\n(B) Neutral\n(C) Sad\n(D) Joyful\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1564, "doc": {"video_name": "movie101_0.mp4", "duration": 789.0, "question": "Why was there an explosion on the city street?\n(A) Gas explosion\n(B) There's a bomb\n(C) A car accident caused the car to explode\n(D) Fire\n", "candidates": ["Gas explosion", "There's a bomb", "A car accident caused the car to explode", "Fire"], "answer": "C", "task_type": "plotQA", "question_id": "Q1564"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "79f928f0125e0d5c84bc1c6d0dd9ee9589998a42cd9bbb62059e8af8e682abfe", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why was there an explosion on the city street?\n(A) Gas explosion\n(B) There's a bomb\n(C) A car accident caused the car to explode\n(D) Fire\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Why was there an explosion on the city street?\n(A) Gas explosion\n(B) There's a bomb\n(C) A car accident caused the car to explode\n(D) Fire\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1568, "doc": {"video_name": "movie101_8.mp4", "duration": 750.0, "question": "Where does the fight at the beginning of the video take place?\n(A) On the island\n(B) In the desert\n(C) On the grassland\n(D) In the forest\n", "candidates": ["On the island", "In the desert", "On the grassland", "In the forest"], "answer": "D", "task_type": "plotQA", "question_id": "Q1568"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5ff356123bdb5c5128b2c7c92c1c728bebbcb8f277b6e4a4c50ecbb7c7951943", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where does the fight at the beginning of the video take place?\n(A) On the island\n(B) In the desert\n(C) On the grassland\n(D) In the forest\n", "task_type": "plotQA", "pred_answer": "C.", "answer": "D"}, "input": "Where does the fight at the beginning of the video take place?\n(A) On the island\n(B) In the desert\n(C) On the grassland\n(D) In the forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1572, "doc": {"video_name": "movie101_75.mp4", "duration": 669.0, "question": "What is the animal at the beginning of the video?\n(A) Cat\n(B) Cow\n(C) Dog\n(D) Horse\n", "candidates": ["Cat", "Cow", "Dog", "Horse"], "answer": "C", "task_type": "plotQA", "question_id": "Q1572"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "795529583d4810138d1f19e184fae50da54920db419bc1190511e30cf27e1564", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the animal at the beginning of the video?\n(A) Cat\n(B) Cow\n(C) Dog\n(D) Horse\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What is the animal at the beginning of the video?\n(A) Cat\n(B) Cow\n(C) Dog\n(D) Horse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1576, "doc": {"video_name": "tomjerry_6.mp4", "duration": 313.0, "question": "How did the golf ball come out of the hole?\n(A) It was taken out by the cartoon mouse\n(B) It was taken out by the cartoon cat\n(C) It rolled out\n(D) It bounced out\n", "candidates": ["It was taken out by the cartoon mouse", "It was taken out by the cartoon cat", "It rolled out", "It bounced out"], "answer": "A", "task_type": "plotQA", "question_id": "Q1576"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ef5bca79437016adc84ea807f918bc1361ab239cac226cd9f46182dc732f33cf", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How did the golf ball come out of the hole?\n(A) It was taken out by the cartoon mouse\n(B) It was taken out by the cartoon cat\n(C) It rolled out\n(D) It bounced out\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "How did the golf ball come out of the hole?\n(A) It was taken out by the cartoon mouse\n(B) It was taken out by the cartoon cat\n(C) It rolled out\n(D) It bounced out\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1580, "doc": {"video_name": "movie101_105.mp4", "duration": 744.0, "question": "Why did the person hiding in the truck run in a hurry?\n(A) Because there was an earthquake\n(B) Because there was a flood\n(C) Because there was a tsunami\n(D) Because there was a sandstorm\n", "candidates": ["Because there was an earthquake", "Because there was a flood", "Because there was a tsunami", "Because there was a sandstorm"], "answer": "A", "task_type": "plotQA", "question_id": "Q1580"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fa786354f60cd1a57e36f98f04dc85a76da3cc273fdd3c8e8c0378497d99f54d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why did the person hiding in the truck run in a hurry?\n(A) Because there was an earthquake\n(B) Because there was a flood\n(C) Because there was a tsunami\n(D) Because there was a sandstorm\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "Why did the person hiding in the truck run in a hurry?\n(A) Because there was an earthquake\n(B) Because there was a flood\n(C) Because there was a tsunami\n(D) Because there was a sandstorm\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1584, "doc": {"video_name": "movie101_35.mp4", "duration": 481.0, "question": "What color is the clothes of the man in the house in the video?\n(A) Black\n(B) White\n(C) Green\n(D) Yellow\n", "candidates": ["Black", "White", "Green", "Yellow"], "answer": "A", "task_type": "plotQA", "question_id": "Q1584"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a728fa5681956503fb6d6f9ea4418490c042630ee2b77397a4483aa7febad9b0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the clothes of the man in the house in the video?\n(A) Black\n(B) White\n(C) Green\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the clothes of the man in the house in the video?\n(A) Black\n(B) White\n(C) Green\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1588, "doc": {"video_name": "haimian_2.mp4", "duration": 356.0, "question": "What did the cartoon octopus give to the cartoon sponge before going on stage?\n(A) A piece of clothing\n(B) A scarf\n(C) A mop\n(D) A towel\n", "candidates": ["A piece of clothing", "A scarf", "A mop", "A towel"], "answer": "C", "task_type": "plotQA", "question_id": "Q1588"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7aa1db0c492b36674f39e74663f555a3286f4130dae89bdc92ae8efadc6c0497", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did the cartoon octopus give to the cartoon sponge before going on stage?\n(A) A piece of clothing\n(B) A scarf\n(C) A mop\n(D) A towel\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What did the cartoon octopus give to the cartoon sponge before going on stage?\n(A) A piece of clothing\n(B) A scarf\n(C) A mop\n(D) A towel\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1592, "doc": {"video_name": "movie101_75.mp4", "duration": 669.0, "question": "What color is the dress that the person is wearing at the beginning of the video?\n(A) White\n(B) Blue\n(C) Black\n(D) Green\n", "candidates": ["White", "Blue", "Black", "Green"], "answer": "A", "task_type": "plotQA", "question_id": "Q1592"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "20bba0142e24e7dda2857fe3cd0e9dfa43f8f8bac92436905169736e0430aa06", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the dress that the person is wearing at the beginning of the video?\n(A) White\n(B) Blue\n(C) Black\n(D) Green\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the dress that the person is wearing at the beginning of the video?\n(A) White\n(B) Blue\n(C) Black\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1596, "doc": {"video_name": "tomjerry_6.mp4", "duration": 313.0, "question": "What did the cartoon mouse do to the cartoon cat's tail?\n(A) Cut it off\n(B) Trimmed the fur\n(C) Used it as a candle\n(D) Nailed it\n", "candidates": ["Cut it off", "Trimmed the fur", "Used it as a candle", "Nailed it"], "answer": "C", "task_type": "plotQA", "question_id": "Q1596"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3466837e70059b384145d92d12c26628ef4ef569a3d69b407ed429f3d934f65e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did the cartoon mouse do to the cartoon cat's tail?\n(A) Cut it off\n(B) Trimmed the fur\n(C) Used it as a candle\n(D) Nailed it\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What did the cartoon mouse do to the cartoon cat's tail?\n(A) Cut it off\n(B) Trimmed the fur\n(C) Used it as a candle\n(D) Nailed it\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1600, "doc": {"video_name": "en_tv_24.mp4", "duration": 482.0, "question": "What color is the police car in the video?\n(A) White\n(B) Blue\n(C) Red\n(D) Black\n", "candidates": ["White", "Blue", "Red", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1600"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2261f844cab43877be083a6ffbcdf6ecb3c1109d227fb7371998f9cfb5d411a9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the police car in the video?\n(A) White\n(B) Blue\n(C) Red\n(D) Black\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the police car in the video?\n(A) White\n(B) Blue\n(C) Red\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1604, "doc": {"video_name": "movie101_40.mp4", "duration": 450.0, "question": "What color are the hats people are wearing at the beginning of the video?\n(A) Blue\n(B) Black\n(C) Green\n(D) Red\n", "candidates": ["Blue", "Black", "Green", "Red"], "answer": "B", "task_type": "plotQA", "question_id": "Q1604"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "69305a1b2a3a733f0c5e5dad7a6c2d34b35c6f994a120694b1db3c6517dc06fa", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color are the hats people are wearing at the beginning of the video?\n(A) Blue\n(B) Black\n(C) Green\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color are the hats people are wearing at the beginning of the video?\n(A) Blue\n(B) Black\n(C) Green\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1608, "doc": {"video_name": "movie101_83.mp4", "duration": 640.0, "question": "What is the man's profession at the end of the video?\n(A) Worker\n(B) Chef\n(C) Teacher\n(D) Doctor\n", "candidates": ["Worker", "Chef", "Teacher", "Doctor"], "answer": "B", "task_type": "plotQA", "question_id": "Q1608"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e00471655b4ce8372e7225495bd8bb27c5023d046dbeaddea7d22591ceced824", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the man's profession at the end of the video?\n(A) Worker\n(B) Chef\n(C) Teacher\n(D) Doctor\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the man's profession at the end of the video?\n(A) Worker\n(B) Chef\n(C) Teacher\n(D) Doctor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1612, "doc": {"video_name": "movie101_29.mp4", "duration": 482.0, "question": "How many women appear in the video?\n(A) Four\n(B) Two\n(C) Three\n(D) One\n", "candidates": ["Four", "Two", "Three", "One"], "answer": "B", "task_type": "plotQA", "question_id": "Q1612"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1b3fd67345dc6d5205598e849841abb273886c2d4e7611f2d9a8f775727c6ce9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many women appear in the video?\n(A) Four\n(B) Two\n(C) Three\n(D) One\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "How many women appear in the video?\n(A) Four\n(B) Two\n(C) Three\n(D) One\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1616, "doc": {"video_name": "movie101_21.mp4", "duration": 289.0, "question": "What color is the coat worn by the girl braiding her hair in the video?\n(A) White\n(B) Red\n(C) Blue\n(D) Yellow\n", "candidates": ["White", "Red", "Blue", "Yellow"], "answer": "B", "task_type": "plotQA", "question_id": "Q1616"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7f76370f4b400ab133be60e5d6655b7545efba4d874ca4a933ae9e0a74fbc797", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the coat worn by the girl braiding her hair in the video?\n(A) White\n(B) Red\n(C) Blue\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the coat worn by the girl braiding her hair in the video?\n(A) White\n(B) Red\n(C) Blue\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1620, "doc": {"video_name": "tomjerry_11.mp4", "duration": 322.0, "question": "Why is the grey cartoon cat making a phone call?\n(A) To communicate with the doctor\n(B) To inquire about the situation\n(C) To call other cartoon cats\n(D) To communicate with the owner\n", "candidates": ["To communicate with the doctor", "To inquire about the situation", "To call other cartoon cats", "To communicate with the owner"], "answer": "C", "task_type": "plotQA", "question_id": "Q1620"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ecc8f9ace557980c4435d3883e0616d7bae90cfd43eb53f131397d1a6ccdc07b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why is the grey cartoon cat making a phone call?\n(A) To communicate with the doctor\n(B) To inquire about the situation\n(C) To call other cartoon cats\n(D) To communicate with the owner\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Why is the grey cartoon cat making a phone call?\n(A) To communicate with the doctor\n(B) To inquire about the situation\n(C) To call other cartoon cats\n(D) To communicate with the owner\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1624, "doc": {"video_name": "movie101_22.mp4", "duration": 335.0, "question": "What animal is pulling the gears in the video?\n(A) Horse\n(B) Donkey\n(C) Sheep\n(D) Cow\n", "candidates": ["Horse", "Donkey", "Sheep", "Cow"], "answer": "A", "task_type": "plotQA", "question_id": "Q1624"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "642e3209ec28b17eaf94bd1f08daafce1a333e5670e65333a7d12e6166d0a89f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What animal is pulling the gears in the video?\n(A) Horse\n(B) Donkey\n(C) Sheep\n(D) Cow\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What animal is pulling the gears in the video?\n(A) Horse\n(B) Donkey\n(C) Sheep\n(D) Cow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1628, "doc": {"video_name": "tomjerry_3.mp4", "duration": 303.0, "question": "What did the cartoon cat put under the carpet?\n(A) Cucumber\n(B) Tomato\n(C) Onion\n(D) Potato\n", "candidates": ["Cucumber", "Tomato", "Onion", "Potato"], "answer": "B", "task_type": "plotQA", "question_id": "Q1628"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0aa896bf2b02eec113b18a1179306ff173c00e3dd44dd5adcc78bd4a720bb6c2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did the cartoon cat put under the carpet?\n(A) Cucumber\n(B) Tomato\n(C) Onion\n(D) Potato\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What did the cartoon cat put under the carpet?\n(A) Cucumber\n(B) Tomato\n(C) Onion\n(D) Potato\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1632, "doc": {"video_name": "movie101_8.mp4", "duration": 750.0, "question": "What color is the fur of the little monster in the video?\n(A) Green\n(B) Blue\n(C) Red\n(D) Yellow\n", "candidates": ["Green", "Blue", "Red", "Yellow"], "answer": "A", "task_type": "plotQA", "question_id": "Q1632"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c864d140a333c3d9cda1f7132f9eb9b1f8a060d69a9dad54bfff9a9de108dcee", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the fur of the little monster in the video?\n(A) Green\n(B) Blue\n(C) Red\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the fur of the little monster in the video?\n(A) Green\n(B) Blue\n(C) Red\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1636, "doc": {"video_name": "movie101_53.mp4", "duration": 328.0, "question": "The change in indoor activities of the man in the blue jacket and the woman in the red sweater\n(A) Frolicking and messing around turns into hugging and kissing\n(B) Drinking together\n(C) Watching TV together\n(D) Eating together\n", "candidates": ["Frolicking and messing around turns into hugging and kissing", "Drinking together", "Watching TV together", "Eating together"], "answer": "A", "task_type": "plotQA", "question_id": "Q1636"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e0497882b391c36f64b001ae00006d2d87ef289ee5ed37f6fb393730fdeecea8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "The change in indoor activities of the man in the blue jacket and the woman in the red sweater\n(A) Frolicking and messing around turns into hugging and kissing\n(B) Drinking together\n(C) Watching TV together\n(D) Eating together\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "The change in indoor activities of the man in the blue jacket and the woman in the red sweater\n(A) Frolicking and messing around turns into hugging and kissing\n(B) Drinking together\n(C) Watching TV together\n(D) Eating together\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1640, "doc": {"video_name": "tomjerry_7.mp4", "duration": 306.0, "question": "Why is the cartoon cat sitting in front of the window?\n(A) To enjoy the view\n(B) To watch the white cartoon female cat\n(C) Feeling lost\n(D) To make a phone call\n", "candidates": ["To enjoy the view", "To watch the white cartoon female cat", "Feeling lost", "To make a phone call"], "answer": "B", "task_type": "plotQA", "question_id": "Q1640"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "415fc73fd73028916c4bc54682f83a0dfe1e5dc66c670370cada43ccada8b006", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why is the cartoon cat sitting in front of the window?\n(A) To enjoy the view\n(B) To watch the white cartoon female cat\n(C) Feeling lost\n(D) To make a phone call\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Why is the cartoon cat sitting in front of the window?\n(A) To enjoy the view\n(B) To watch the white cartoon female cat\n(C) Feeling lost\n(D) To make a phone call\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1644, "doc": {"video_name": "movie101_22.mp4", "duration": 335.0, "question": "What color is the hat worn by the person in the video?\n(A) Black\n(B) White\n(C) Blue\n(D) Red\n", "candidates": ["Black", "White", "Blue", "Red"], "answer": "A", "task_type": "plotQA", "question_id": "Q1644"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "da3d26023439f05f963bf19cd81ff9266a2807ebe23c2c0cf1e96e6303ee56d0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the hat worn by the person in the video?\n(A) Black\n(B) White\n(C) Blue\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What color is the hat worn by the person in the video?\n(A) Black\n(B) White\n(C) Blue\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1648, "doc": {"video_name": "en_tv_14.mp4", "duration": 459.0, "question": "What color is the coat of the woman walking in the prison corridor?\n(A) Black\n(B) Yellow\n(C) Red\n(D) White\n", "candidates": ["Black", "Yellow", "Red", "White"], "answer": "A", "task_type": "plotQA", "question_id": "Q1648"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d88daa884a25a72976675130445a647e933d2e08315fb74e8fe24eaa59a9661", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the coat of the woman walking in the prison corridor?\n(A) Black\n(B) Yellow\n(C) Red\n(D) White\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the coat of the woman walking in the prison corridor?\n(A) Black\n(B) Yellow\n(C) Red\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1652, "doc": {"video_name": "AWA-6.mp4", "duration": 450.0, "question": "What is the main background of the video?\n(A) Grassland\n(B) Lake\n(C) Ocean\n(D) Desert\n", "candidates": ["Grassland", "Lake", "Ocean", "Desert"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1652"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "51338ffd3d543f35e5bb6fcb3671b462fd96f83df383235dfe2a4aaf3ae88ec2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main background of the video?\n(A) Grassland\n(B) Lake\n(C) Ocean\n(D) Desert\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the main background of the video?\n(A) Grassland\n(B) Lake\n(C) Ocean\n(D) Desert\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1656, "doc": {"video_name": "movie101_74.mp4", "duration": 323.0, "question": "What is the genre of this film?\n(A) Sci-Fi\n(B) Romance\n(C) Action\n(D) Mystery\n", "candidates": ["Sci-Fi", "Romance", "Action", "Mystery"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1656"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "606f1c77ee9bc6f0be6bb4e19cf85307e3017222753ea63bebea56f9cbc262c3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the genre of this film?\n(A) Sci-Fi\n(B) Romance\n(C) Action\n(D) Mystery\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the genre of this film?\n(A) Sci-Fi\n(B) Romance\n(C) Action\n(D) Mystery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1660, "doc": {"video_name": "haimian_1.mp4", "duration": 305.0, "question": "What is the background of the video?\n(A) Desert\n(B) Undersea\n(C) Forest\n(D) Beach\n", "candidates": ["Desert", "Undersea", "Forest", "Beach"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1660"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cb2444df60739f80f7e6ca6b3aa6ff6588a8795386cce5b8a3239623e772e6c3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the background of the video?\n(A) Desert\n(B) Undersea\n(C) Forest\n(D) Beach\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the background of the video?\n(A) Desert\n(B) Undersea\n(C) Forest\n(D) Beach\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1664, "doc": {"video_name": "203.mp4", "duration": 480.0, "question": "What type of video is this?\n(A) Science Fiction\n(B) Animals\n(C) Romance\n(D) Comedy\n", "candidates": ["Science Fiction", "Animals", "Romance", "Comedy"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1664"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6ca381df9fbc707cf0a45edd1ec967099ace21e06ea2462cb744a028bb4eae3c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Science Fiction\n(B) Animals\n(C) Romance\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Science Fiction\n(B) Animals\n(C) Romance\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1668, "doc": {"video_name": "movie101_56.mp4", "duration": 338.0, "question": "What is the main setting of the video?\n(A) Countryside\n(B) Desert\n(C) City\n(D) Seaside\n", "candidates": ["Countryside", "Desert", "City", "Seaside"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1668"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3958c14844211ac9e789d9432823dcc6e3b96cac5b3339ed27fb55f06477be2b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the main setting of the video?\n(A) Countryside\n(B) Desert\n(C) City\n(D) Seaside\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the main setting of the video?\n(A) Countryside\n(B) Desert\n(C) City\n(D) Seaside\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1672, "doc": {"video_name": "en_tv_20.mp4", "duration": 426.0, "question": "In what setting does the scene in the video take place?\n(A) Forest\n(B) Snowy Mountain\n(C) City\n(D) Island\n", "candidates": ["Forest", "Snowy Mountain", "City", "Island"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1672"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8a5300f825ffb22de63f84ce663c1920702d7ab9fb1694d79856bcf201219d71", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what setting does the scene in the video take place?\n(A) Forest\n(B) Snowy Mountain\n(C) City\n(D) Island\n", "task_type": "topic_reasoning", "pred_answer": "C.", "answer": "C"}, "input": "In what setting does the scene in the video take place?\n(A) Forest\n(B) Snowy Mountain\n(C) City\n(D) Island\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1676, "doc": {"video_name": "215.mp4", "duration": 480.0, "question": "What type of video is this?\n(A) It is a video documenting daily life.\n(B) It is a video documenting traditional customs.\n(C) It is a video documenting food.\n(D) It is a video documenting nature.\n", "candidates": ["It is a video documenting daily life.", "It is a video documenting traditional customs.", "It is a video documenting food.", "It is a video documenting nature."], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1676"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D."]], "filtered_resps": ["D."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b4c9d3697806eb71a3aa6541eba6ee6f9ff1b5c9fabbd920dab53344d589c8e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) It is a video documenting daily life.\n(B) It is a video documenting traditional customs.\n(C) It is a video documenting food.\n(D) It is a video documenting nature.\n", "task_type": "topic_reasoning", "pred_answer": "D.", "answer": "D"}, "input": "What type of video is this?\n(A) It is a video documenting daily life.\n(B) It is a video documenting traditional customs.\n(C) It is a video documenting food.\n(D) It is a video documenting nature.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1680, "doc": {"video_name": "AWA-17.mp4", "duration": 450.0, "question": "What type of video is this?\n(A) Comedy\n(B) Animal\n(C) Science Fiction\n(D) Action\n", "candidates": ["Comedy", "Animal", "Science Fiction", "Action"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1680"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "371363796dfeea6029516805cee829ccb83e13e4d40e7a06b19d2c74be68e146", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Comedy\n(B) Animal\n(C) Science Fiction\n(D) Action\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Comedy\n(B) Animal\n(C) Science Fiction\n(D) Action\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1684, "doc": {"video_name": "en_tv_12.mp4", "duration": 587.0, "question": "What type of video is this?\n(A) Advertisement video\n(B) Daily life documentary\n(C) Animation\n(D) Musical\n", "candidates": ["Advertisement video", "Daily life documentary", "Animation", "Musical"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1684"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a7378ab28eafa2ff93ccdd4060244065d09de3fa2bbaf70f1cdfbe4a19e3435d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Advertisement video\n(B) Daily life documentary\n(C) Animation\n(D) Musical\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Advertisement video\n(B) Daily life documentary\n(C) Animation\n(D) Musical\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1688, "doc": {"video_name": "en_tv_7.mp4", "duration": 533.0, "question": "What type of video is this?\n(A) Animation\n(B) Music video\n(C) Movie clip\n(D) Documentary\n", "candidates": ["Animation", "Music video", "Movie clip", "Documentary"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1688"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d0f96b4c1e1a704abb7f917c44ea3d41c89ab035abb3f0a19dd3a136e27c2c0d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Animation\n(B) Music video\n(C) Movie clip\n(D) Documentary\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of video is this?\n(A) Animation\n(B) Music video\n(C) Movie clip\n(D) Documentary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1692, "doc": {"video_name": "ego_12.mp4", "duration": 480.02, "question": "Who is captured in the video?\n(A) Elderly\n(B) Child\n(C) Man\n(D) Young woman\n", "candidates": ["Elderly", "Child", "Man", "Young woman"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1692"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a7694b1154f940ed4b9564185cad061fccc152af650d2e099edf0f8a4bd05664", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Who is captured in the video?\n(A) Elderly\n(B) Child\n(C) Man\n(D) Young woman\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "D"}, "input": "Who is captured in the video?\n(A) Elderly\n(B) Child\n(C) Man\n(D) Young woman\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1696, "doc": {"video_name": "movie101_70.mp4", "duration": 361.0, "question": "What event is mainly narrated in the video?\n(A) Theft\n(B) Romance\n(C) Dance\n(D) Chase\n", "candidates": ["Theft", "Romance", "Dance", "Chase"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1696"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "de0609dc9c6223c411988b1c60b1a9cbbee0afc84cd46372a8237671238a12f3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What event is mainly narrated in the video?\n(A) Theft\n(B) Romance\n(C) Dance\n(D) Chase\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "A"}, "input": "What event is mainly narrated in the video?\n(A) Theft\n(B) Romance\n(C) Dance\n(D) Chase\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1700, "doc": {"video_name": "AWD-1.mp4", "duration": 450.13, "question": "What type of video is this?\n(A) Nature Science\n(B) History Culture\n(C) Traditional Festival\n(D) Food Flavor\n", "candidates": ["Nature Science", "History Culture", "Traditional Festival", "Food Flavor"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1700"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7bbaf5b13fa0f7d7f0982cf05a29e5cec64b64b5de222317ae4b5b8154a61f26", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Nature Science\n(B) History Culture\n(C) Traditional Festival\n(D) Food Flavor\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What type of video is this?\n(A) Nature Science\n(B) History Culture\n(C) Traditional Festival\n(D) Food Flavor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1704, "doc": {"video_name": "movie101_28.mp4", "duration": 234.0, "question": "What is the genre of this movie clip?\n(A) Horror\n(B) Science Fiction\n(C) War\n(D) Comedy\n", "candidates": ["Horror", "Science Fiction", "War", "Comedy"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1704"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "04084313d338c8f597ae95c44c528358ceccf0b9bd000169d0e0a620df38f2ba", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the genre of this movie clip?\n(A) Horror\n(B) Science Fiction\n(C) War\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "B"}, "input": "What is the genre of this movie clip?\n(A) Horror\n(B) Science Fiction\n(C) War\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1708, "doc": {"video_name": "movie101_29.mp4", "duration": 482.0, "question": "What kind of weather is depicted in the video?\n(A) Foggy\n(B) Rainy\n(C) Snowy\n(D) Sunny\n", "candidates": ["Foggy", "Rainy", "Snowy", "Sunny"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1708"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1c29ffee2c7633f8b12bce3c77f21797fcc44c170fea0c508aeb6311d173499", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What kind of weather is depicted in the video?\n(A) Foggy\n(B) Rainy\n(C) Snowy\n(D) Sunny\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "D"}, "input": "What kind of weather is depicted in the video?\n(A) Foggy\n(B) Rainy\n(C) Snowy\n(D) Sunny\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1712, "doc": {"video_name": "movie101_108.mp4", "duration": 330.0, "question": "Which scene does not appear in the video?\n(A) Bathroom\n(B) Playground\n(C) Mountain road\n(D) Auditorium\n", "candidates": ["Bathroom", "Playground", "Mountain road", "Auditorium"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1712"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eb693274ea6c3f18fe643783052731d40dabc97936d63f0dce2169a97047832c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which scene does not appear in the video?\n(A) Bathroom\n(B) Playground\n(C) Mountain road\n(D) Auditorium\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "D"}, "input": "Which scene does not appear in the video?\n(A) Bathroom\n(B) Playground\n(C) Mountain road\n(D) Auditorium\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1716, "doc": {"video_name": "movie101_40.mp4", "duration": 450.0, "question": "In what setting does the scene in the video take place?\n(A) Snowy Mountain\n(B) Forest\n(C) Island\n(D) City\n", "candidates": ["Snowy Mountain", "Forest", "Island", "City"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1716"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ab4ae6fa4d178638fef110e925cf9d39e3cc9ecf2f75fb3e668e735dae35fdf0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what setting does the scene in the video take place?\n(A) Snowy Mountain\n(B) Forest\n(C) Island\n(D) City\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "D"}, "input": "In what setting does the scene in the video take place?\n(A) Snowy Mountain\n(B) Forest\n(C) Island\n(D) City\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1720, "doc": {"video_name": "230.mp4", "duration": 480.0, "question": "What kind of video is this?\n(A) This is a video related to nature\n(B) This is a video related to traditional culture\n(C) This is a video related to food\n(D) This is a video related to transportation\n", "candidates": ["This is a video related to nature", "This is a video related to traditional culture", "This is a video related to food", "This is a video related to transportation"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1720"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "56a4ef4cca272c5499447409ba7babcd51ab682e8716708c96ef8709582c71fd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What kind of video is this?\n(A) This is a video related to nature\n(B) This is a video related to traditional culture\n(C) This is a video related to food\n(D) This is a video related to transportation\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What kind of video is this?\n(A) This is a video related to nature\n(B) This is a video related to traditional culture\n(C) This is a video related to food\n(D) This is a video related to transportation\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1724, "doc": {"video_name": "movie101_19.mp4", "duration": 477.0, "question": "What genre of movie does the scene in the video belong to?\n(A) Action\n(B) Science Fiction\n(C) Comedy\n(D) Documentary\n", "candidates": ["Action", "Science Fiction", "Comedy", "Documentary"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1724"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7731458cc600fcfac707efa7f891842117c2a8efe140da11915cede351179317", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What genre of movie does the scene in the video belong to?\n(A) Action\n(B) Science Fiction\n(C) Comedy\n(D) Documentary\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What genre of movie does the scene in the video belong to?\n(A) Action\n(B) Science Fiction\n(C) Comedy\n(D) Documentary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1728, "doc": {"video_name": "en_tv_26.mp4", "duration": 522.0, "question": "What genre of movie is the clip in the video from?\n(A) War\n(B) Action\n(C) Horror\n(D) Documentary\n", "candidates": ["War", "Action", "Horror", "Documentary"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1728"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4370feabcd4484bd023ec61c45b6c7468e639f6c9325e6530af7b4ed7cc96d8c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What genre of movie is the clip in the video from?\n(A) War\n(B) Action\n(C) Horror\n(D) Documentary\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "B"}, "input": "What genre of movie is the clip in the video from?\n(A) War\n(B) Action\n(C) Horror\n(D) Documentary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1732, "doc": {"video_name": "AWD-1.mp4", "duration": 450.13, "question": "What is the background of the video?\n(A) Forest\n(B) Ocean\n(C) Desert\n(D) Glacier\n", "candidates": ["Forest", "Ocean", "Desert", "Glacier"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1732"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "879b3515180f3bf4044ae6d642e4e20939a74e6ba9e97e8feeb657e9c3fd9e67", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the background of the video?\n(A) Forest\n(B) Ocean\n(C) Desert\n(D) Glacier\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the background of the video?\n(A) Forest\n(B) Ocean\n(C) Desert\n(D) Glacier\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1736, "doc": {"video_name": "AWB-4.mp4", "duration": 450.0, "question": "What is the main setting of the video?\n(A) Grassland\n(B) Desert\n(C) Ocean\n(D) City\n", "candidates": ["Grassland", "Desert", "Ocean", "City"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1736"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6557e890d0acc59261321b9fff5170f4c830fd8ed6824408c84ff38d4f53a7ab", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main setting of the video?\n(A) Grassland\n(B) Desert\n(C) Ocean\n(D) City\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the main setting of the video?\n(A) Grassland\n(B) Desert\n(C) Ocean\n(D) City\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1740, "doc": {"video_name": "movie101_51.mp4", "duration": 796.0, "question": "What kind of clothing is the old man preparing food on the street wearing?\n(A) Sportswear\n(B) Zhongshan suit\n(C) Japanese clothing\n(D) Casual wear\n", "candidates": ["Sportswear", "Zhongshan suit", "Japanese clothing", "Casual wear"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1740"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fe578428a2bcefe6d2cbe86763a17c512bb5523fb11b60745928b082faa80d75", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What kind of clothing is the old man preparing food on the street wearing?\n(A) Sportswear\n(B) Zhongshan suit\n(C) Japanese clothing\n(D) Casual wear\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "C"}, "input": "What kind of clothing is the old man preparing food on the street wearing?\n(A) Sportswear\n(B) Zhongshan suit\n(C) Japanese clothing\n(D) Casual wear\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1744, "doc": {"video_name": "movie101_107.mp4", "duration": 471.0, "question": "What is the main setting of the video?\n(A) Marketplace\n(B) Park\n(C) Office\n(D) Stadium\n", "candidates": ["Marketplace", "Park", "Office", "Stadium"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1744"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6429ac35cd831ac7787ed616ca8af1ed7a71168c1f781c0af3274b91ed5c17ed", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main setting of the video?\n(A) Marketplace\n(B) Park\n(C) Office\n(D) Stadium\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the main setting of the video?\n(A) Marketplace\n(B) Park\n(C) Office\n(D) Stadium\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1748, "doc": {"video_name": "movie101_15.mp4", "duration": 441.0, "question": "What animal appears in the video?\n(A) Dog\n(B) Cat\n(C) Llama\n(D) Sheep\n", "candidates": ["Dog", "Cat", "Llama", "Sheep"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1748"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "902aca396e45be80c1ba5d252a8c8a834d0caafa8abd2cfeff30855195c363ab", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What animal appears in the video?\n(A) Dog\n(B) Cat\n(C) Llama\n(D) Sheep\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What animal appears in the video?\n(A) Dog\n(B) Cat\n(C) Llama\n(D) Sheep\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1752, "doc": {"video_name": "ego_1.mp4", "duration": 480.0, "question": "In this first-person perspective video, what is the main activity the first-person character is doing?\n(A) Sawing wood\n(B) Watering the lawn\n(C) Repairing pipes\n(D) Installing wooden boards\n", "candidates": ["Sawing wood", "Watering the lawn", "Repairing pipes", "Installing wooden boards"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1752"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cca211abe37d1b6fb0f1f5df9e9a8d21b355510b1044de078cbaee89e9efddf8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this first-person perspective video, what is the main activity the first-person character is doing?\n(A) Sawing wood\n(B) Watering the lawn\n(C) Repairing pipes\n(D) Installing wooden boards\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "In this first-person perspective video, what is the main activity the first-person character is doing?\n(A) Sawing wood\n(B) Watering the lawn\n(C) Repairing pipes\n(D) Installing wooden boards\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1756, "doc": {"video_name": "AWA-17.mp4", "duration": 450.0, "question": "What is the scene of the video?\n(A) Ocean\n(B) Grassland\n(C) Desert\n(D) Sky\n", "candidates": ["Ocean", "Grassland", "Desert", "Sky"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1756"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b9567205832f56e4a5167ed36c5c670a6a429c6fdd961bb865c5184e717e047", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the scene of the video?\n(A) Ocean\n(B) Grassland\n(C) Desert\n(D) Sky\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the scene of the video?\n(A) Ocean\n(B) Grassland\n(C) Desert\n(D) Sky\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1760, "doc": {"video_name": "movie101_102.mp4", "duration": 607.0, "question": "Where does the main event in the video take place?\n(A) School\n(B) Temple\n(C) Desert\n(D) Forest\n", "candidates": ["School", "Temple", "Desert", "Forest"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1760"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2b92c2ea6d86b38ca7857f52712a6d2add8d308c3c43d0b1a78caef7b12dedbb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where does the main event in the video take place?\n(A) School\n(B) Temple\n(C) Desert\n(D) Forest\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "Where does the main event in the video take place?\n(A) School\n(B) Temple\n(C) Desert\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1764, "doc": {"video_name": "238.mp4", "duration": 480.0, "question": "What type of video is this?\n(A) Science Fiction\n(B) Comedy\n(C) Romance\n(D) Nature Documentary\n", "candidates": ["Science Fiction", "Comedy", "Romance", "Nature Documentary"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1764"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "00d2d9536e4762cb8285f7c7324d6646c382df745f7a6babde979ae56d743a1f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Science Fiction\n(B) Comedy\n(C) Romance\n(D) Nature Documentary\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of video is this?\n(A) Science Fiction\n(B) Comedy\n(C) Romance\n(D) Nature Documentary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1768, "doc": {"video_name": "en_tv_10.mp4", "duration": 545.0, "question": "What type of video is this?\n(A) Documentary\n(B) Cartoon\n(C) Movie clip\n(D) Stage play\n", "candidates": ["Documentary", "Cartoon", "Movie clip", "Stage play"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1768"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "174984500a6b212ecbcdf325406467286a3042bb30194a91daaefb3b18ffd1c0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Documentary\n(B) Cartoon\n(C) Movie clip\n(D) Stage play\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of video is this?\n(A) Documentary\n(B) Cartoon\n(C) Movie clip\n(D) Stage play\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1772, "doc": {"video_name": "AWA-15.mp4", "duration": 450.0, "question": "What is this video related to?\n(A) Lifestyle\n(B) Wildlife\n(C) Food Flavors\n(D) Traditional Festivals\n", "candidates": ["Lifestyle", "Wildlife", "Food Flavors", "Traditional Festivals"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1772"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "920de6de2c2987d80b13626cc5aaceb89bc695196a57d499f5779aa2ba13eb09", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is this video related to?\n(A) Lifestyle\n(B) Wildlife\n(C) Food Flavors\n(D) Traditional Festivals\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is this video related to?\n(A) Lifestyle\n(B) Wildlife\n(C) Food Flavors\n(D) Traditional Festivals\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1776, "doc": {"video_name": "ego_25.mp4", "duration": 480.0, "question": "What is the first-person character mainly doing in this first-person video?\n(A) Riding a horse\n(B) Riding a motorcycle\n(C) Riding a bicycle\n(D) Riding a tricycle\n", "candidates": ["Riding a horse", "Riding a motorcycle", "Riding a bicycle", "Riding a tricycle"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1776"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "161bfde4c2b7df2f2e4d6e8acc1aac4fa32dc4b3a75e9355f1e5d13b71e96c4f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the first-person character mainly doing in this first-person video?\n(A) Riding a horse\n(B) Riding a motorcycle\n(C) Riding a bicycle\n(D) Riding a tricycle\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the first-person character mainly doing in this first-person video?\n(A) Riding a horse\n(B) Riding a motorcycle\n(C) Riding a bicycle\n(D) Riding a tricycle\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1780, "doc": {"video_name": "haimian_9.mp4", "duration": 444.0, "question": "What type of video is this?\n(A) Thriller\n(B) Mystery\n(C) Cartoon\n(D) Sci-Fi\n", "candidates": ["Thriller", "Mystery", "Cartoon", "Sci-Fi"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1780"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6ce087332483b1c3ec22b7adc848551916d7140580a918a42a45205b060782d2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Thriller\n(B) Mystery\n(C) Cartoon\n(D) Sci-Fi\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of video is this?\n(A) Thriller\n(B) Mystery\n(C) Cartoon\n(D) Sci-Fi\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1784, "doc": {"video_name": "movie101_71.mp4", "duration": 227.0, "question": "What event is primarily depicted in the video?\n(A) Prison Break\n(B) Romance\n(C) Dance\n(D) Competition\n", "candidates": ["Prison Break", "Romance", "Dance", "Competition"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1784"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "350bf94de79dde08a130616cd07b1c9b0a084045c949633b29f41216d60b0227", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What event is primarily depicted in the video?\n(A) Prison Break\n(B) Romance\n(C) Dance\n(D) Competition\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What event is primarily depicted in the video?\n(A) Prison Break\n(B) Romance\n(C) Dance\n(D) Competition\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1788, "doc": {"video_name": "AWE-3.mp4", "duration": 450.0, "question": "What type of video is this?\n(A) Food Flavor\n(B) Natural Science\n(C) Traditional Festivals\n(D) Historical Culture\n", "candidates": ["Food Flavor", "Natural Science", "Traditional Festivals", "Historical Culture"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1788"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9fa03de316570a543b5bc6747cc74c4c055e0f765256e978d71faf750944b11c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Food Flavor\n(B) Natural Science\n(C) Traditional Festivals\n(D) Historical Culture\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Food Flavor\n(B) Natural Science\n(C) Traditional Festivals\n(D) Historical Culture\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1792, "doc": {"video_name": "movie101_62.mp4", "duration": 601.0, "question": "What is the setting of the video?\n(A) Grassland\n(B) Ocean\n(C) Desert\n(D) Forest\n", "candidates": ["Grassland", "Ocean", "Desert", "Forest"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1792"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "073fff4b0b539f8ef1d1a5c5991f21b6d84fa98a45d7551066120a0c34a24fa8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the setting of the video?\n(A) Grassland\n(B) Ocean\n(C) Desert\n(D) Forest\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the setting of the video?\n(A) Grassland\n(B) Ocean\n(C) Desert\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1796, "doc": {"video_name": "ego_26.mp4", "duration": 375.37, "question": "In this first-person perspective video, what is the main activity of the person in the first-person perspective?\n(A) Buying a bicycle\n(B) Repairing a car\n(C) Repairing a bicycle\n(D) Riding a bicycle\n", "candidates": ["Buying a bicycle", "Repairing a car", "Repairing a bicycle", "Riding a bicycle"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1796"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6ff7e19d72a4a32d1835602641e13dd6037c37d9cdaf77eb43e28c3bed2c70d2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this first-person perspective video, what is the main activity of the person in the first-person perspective?\n(A) Buying a bicycle\n(B) Repairing a car\n(C) Repairing a bicycle\n(D) Riding a bicycle\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "In this first-person perspective video, what is the main activity of the person in the first-person perspective?\n(A) Buying a bicycle\n(B) Repairing a car\n(C) Repairing a bicycle\n(D) Riding a bicycle\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1800, "doc": {"video_name": "AWB-7.mp4", "duration": 450.0, "question": "What type of video is this?\n(A) Animals\n(B) Comedy\n(C) Science Fiction\n(D) Romance\n", "candidates": ["Animals", "Comedy", "Science Fiction", "Romance"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1800"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1ed7b9973d9b496d5ff0dd8f39be5c70783272a73adcacb4b0acf84b7dd95d05", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Animals\n(B) Comedy\n(C) Science Fiction\n(D) Romance\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What type of video is this?\n(A) Animals\n(B) Comedy\n(C) Science Fiction\n(D) Romance\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1804, "doc": {"video_name": "movie101_51.mp4", "duration": 796.0, "question": "In the beginning of the video, under what conditions is the crane working?\n(A) At noon\n(B) In the morning\n(C) At night\n(D) In the afternoon\n", "candidates": ["At noon", "In the morning", "At night", "In the afternoon"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1804"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a7d4fe543f19888c30d668f6b953ba25c221b9822ec16c3f5160f56200f17c12", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the beginning of the video, under what conditions is the crane working?\n(A) At noon\n(B) In the morning\n(C) At night\n(D) In the afternoon\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "In the beginning of the video, under what conditions is the crane working?\n(A) At noon\n(B) In the morning\n(C) At night\n(D) In the afternoon\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1808, "doc": {"video_name": "movie101_25.mp4", "duration": 737.0, "question": "What is the weather in the scene of the video?\n(A) Sunny day\n(B) Snowy day\n(C) Rainy day\n(D) Foggy day\n", "candidates": ["Sunny day", "Snowy day", "Rainy day", "Foggy day"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1808"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c64a0ecfc4d7c7fbcb69d87f4606d2bacacbb17efa19100ee4abfede8a465b76", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the weather in the scene of the video?\n(A) Sunny day\n(B) Snowy day\n(C) Rainy day\n(D) Foggy day\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "A"}, "input": "What is the weather in the scene of the video?\n(A) Sunny day\n(B) Snowy day\n(C) Rainy day\n(D) Foggy day\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1812, "doc": {"video_name": "AWC-10.mp4", "duration": 450.0, "question": "What is the character in the video studying?\n(A) Whales\n(B) Sea Turtles\n(C) Birds\n(D) Dinosaurs\n", "candidates": ["Whales", "Sea Turtles", "Birds", "Dinosaurs"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1812"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b6ea59a598a6ce062a4bb6b40118bfa52f2d13aba5778078c889282a3c9e6021", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the character in the video studying?\n(A) Whales\n(B) Sea Turtles\n(C) Birds\n(D) Dinosaurs\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "D"}, "input": "What is the character in the video studying?\n(A) Whales\n(B) Sea Turtles\n(C) Birds\n(D) Dinosaurs\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1816, "doc": {"video_name": "en_tv_15.mp4", "duration": 541.0, "question": "What genre of movie is the clip in the video from?\n(A) Action movie\n(B) Documentary\n(C) Horror movie\n(D) War movie\n", "candidates": ["Action movie", "Documentary", "Horror movie", "War movie"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1816"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d6ca36d1687e7cc2373406a3446230e4d448b1426313439b5930760faa41f81e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What genre of movie is the clip in the video from?\n(A) Action movie\n(B) Documentary\n(C) Horror movie\n(D) War movie\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "A"}, "input": "What genre of movie is the clip in the video from?\n(A) Action movie\n(B) Documentary\n(C) Horror movie\n(D) War movie\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1820, "doc": {"video_name": "232.mp4", "duration": 480.0, "question": "Who is the person doing the explanation in the video?\n(A) It's a man\n(B) It's a woman\n(C) It's a child\n(D) It's an old person\n", "candidates": ["It's a man", "It's a woman", "It's a child", "It's an old person"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1820"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "825643d27945742e56cf6db6627bd7e0f3193b549c82721aa7b507fb7b1fbfeb", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Who is the person doing the explanation in the video?\n(A) It's a man\n(B) It's a woman\n(C) It's a child\n(D) It's an old person\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "Who is the person doing the explanation in the video?\n(A) It's a man\n(B) It's a woman\n(C) It's a child\n(D) It's an old person\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1824, "doc": {"video_name": "movie101_30.mp4", "duration": 726.0, "question": "In what scenario does the scene in the video take place?\n(A) Snowy Mountain\n(B) Forest\n(C) City\n(D) Island\n", "candidates": ["Snowy Mountain", "Forest", "City", "Island"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1824"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c7b90fba117bcf890df4a158745d47d39541f9b01a1082a4d67b54c0eda0284e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what scenario does the scene in the video take place?\n(A) Snowy Mountain\n(B) Forest\n(C) City\n(D) Island\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "In what scenario does the scene in the video take place?\n(A) Snowy Mountain\n(B) Forest\n(C) City\n(D) Island\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1828, "doc": {"video_name": "movie101_17.mp4", "duration": 613.0, "question": "What is the weather in the scene of the video?\n(A) Rainy\n(B) Foggy\n(C) Sunny\n(D) Snowy\n", "candidates": ["Rainy", "Foggy", "Sunny", "Snowy"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1828"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a74173de081e7ce755a2c03078885fea57fe18fb2833a6ba7a59baf23cc7cda4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the weather in the scene of the video?\n(A) Rainy\n(B) Foggy\n(C) Sunny\n(D) Snowy\n", "task_type": "topic_reasoning", "pred_answer": "C.", "answer": "C"}, "input": "What is the weather in the scene of the video?\n(A) Rainy\n(B) Foggy\n(C) Sunny\n(D) Snowy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1832, "doc": {"video_name": "en_tv_14.mp4", "duration": 459.0, "question": "What is the setting of the clip in the video?\n(A) Grassland\n(B) Snowy mountain\n(C) Island\n(D) City\n", "candidates": ["Grassland", "Snowy mountain", "Island", "City"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1832"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d90cd8d9586512265b637c6e2a2780901e2ef089d5d08b224d6c73552ca870ab", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the setting of the clip in the video?\n(A) Grassland\n(B) Snowy mountain\n(C) Island\n(D) City\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the setting of the clip in the video?\n(A) Grassland\n(B) Snowy mountain\n(C) Island\n(D) City\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1836, "doc": {"video_name": "en_tv_16.mp4", "duration": 552.0, "question": "What genre of movie is the clip in the video from?\n(A) Action movie\n(B) Documentary\n(C) Horror movie\n(D) War movie\n", "candidates": ["Action movie", "Documentary", "Horror movie", "War movie"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1836"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d6ca36d1687e7cc2373406a3446230e4d448b1426313439b5930760faa41f81e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What genre of movie is the clip in the video from?\n(A) Action movie\n(B) Documentary\n(C) Horror movie\n(D) War movie\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What genre of movie is the clip in the video from?\n(A) Action movie\n(B) Documentary\n(C) Horror movie\n(D) War movie\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1840, "doc": {"video_name": "tomjerry_6.mp4", "duration": 313.0, "question": "What type of video is this?\n(A) Cartoon animation\n(B) Thriller mystery\n(C) Ethics\n(D) Science fiction\n", "candidates": ["Cartoon animation", "Thriller mystery", "Ethics", "Science fiction"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1840"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8159aec7fc2b9e9f35d55321ecff298503f7d1113e2638d114b2e875148ccb27", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Cartoon animation\n(B) Thriller mystery\n(C) Ethics\n(D) Science fiction\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What type of video is this?\n(A) Cartoon animation\n(B) Thriller mystery\n(C) Ethics\n(D) Science fiction\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1844, "doc": {"video_name": "AWB-7.mp4", "duration": 450.0, "question": "What is the main setting of the video?\n(A) Grassland\n(B) City\n(C) Ocean\n(D) Desert\n", "candidates": ["Grassland", "City", "Ocean", "Desert"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1844"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc8f9233304e349f6b6f0bb4c594015c407b44c668cdbdb106071327c57cf696", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main setting of the video?\n(A) Grassland\n(B) City\n(C) Ocean\n(D) Desert\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the main setting of the video?\n(A) Grassland\n(B) City\n(C) Ocean\n(D) Desert\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1848, "doc": {"video_name": "movie101_107.mp4", "duration": 471.0, "question": "Who is the main character of the video?\n(A) An old man with white hair\n(B) A man in green clothes\n(C) The woman selling fish\n(D) A young child\n", "candidates": ["An old man with white hair", "A man in green clothes", "The woman selling fish", "A young child"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1848"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ae349cd5687dd8cc7c1b6749794925cc5c5a2836f20245c0f28f9bd90905a40", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who is the main character of the video?\n(A) An old man with white hair\n(B) A man in green clothes\n(C) The woman selling fish\n(D) A young child\n", "task_type": "topic_reasoning", "pred_answer": "C.", "answer": "C"}, "input": "Who is the main character of the video?\n(A) An old man with white hair\n(B) A man in green clothes\n(C) The woman selling fish\n(D) A young child\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1852, "doc": {"video_name": "216.mp4", "duration": 480.0, "question": "What type of video is this?\n(A) Romance\n(B) Nature\n(C) Sci-fi\n(D) Comedy\n", "candidates": ["Romance", "Nature", "Sci-fi", "Comedy"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1852"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e1e29dd8b7080fb72bab0e9add003e4598e5e8dc36faa1f311f6e8ce83896e76", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Romance\n(B) Nature\n(C) Sci-fi\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Romance\n(B) Nature\n(C) Sci-fi\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1856, "doc": {"video_name": "ego_3.mp4", "duration": 418.06, "question": "In this first-person video, what is the first-person character doing?\n(A) Trying on jewelry\n(B) Trying on a tie\n(C) Trying on clothes\n(D) Trying on shoes\n", "candidates": ["Trying on jewelry", "Trying on a tie", "Trying on clothes", "Trying on shoes"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1856"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1eb9057da6f95f6c8ab864ced2788e2a3d42ffd2d989a96a8d3f07ba0b9e6853", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this first-person video, what is the first-person character doing?\n(A) Trying on jewelry\n(B) Trying on a tie\n(C) Trying on clothes\n(D) Trying on shoes\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "In this first-person video, what is the first-person character doing?\n(A) Trying on jewelry\n(B) Trying on a tie\n(C) Trying on clothes\n(D) Trying on shoes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1860, "doc": {"video_name": "movie101_14.mp4", "duration": 465.0, "question": "What is the weather in the scene of the video?\n(A) Rainy\n(B) Foggy\n(C) Sunny\n(D) Snowy\n", "candidates": ["Rainy", "Foggy", "Sunny", "Snowy"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1860"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a74173de081e7ce755a2c03078885fea57fe18fb2833a6ba7a59baf23cc7cda4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the weather in the scene of the video?\n(A) Rainy\n(B) Foggy\n(C) Sunny\n(D) Snowy\n", "task_type": "topic_reasoning", "pred_answer": "C.", "answer": "C"}, "input": "What is the weather in the scene of the video?\n(A) Rainy\n(B) Foggy\n(C) Sunny\n(D) Snowy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1864, "doc": {"video_name": "en_tv_38.mp4", "duration": 410.0, "question": "What type of film is this?\n(A) Thriller\n(B) Comedy\n(C) Action\n(D) Romance\n", "candidates": ["Thriller", "Comedy", "Action", "Romance"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1864"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9d79a471adccf13e277b776265613978c5c9637fce01eec4bb35343de1df6b00", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Thriller\n(B) Comedy\n(C) Action\n(D) Romance\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of film is this?\n(A) Thriller\n(B) Comedy\n(C) Action\n(D) Romance\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1868, "doc": {"video_name": "movie101_34.mp4", "duration": 748.0, "question": "In what setting does the scene in the video take place?\n(A) Forest\n(B) Island\n(C) Snowy Mountain\n(D) Town\n", "candidates": ["Forest", "Island", "Snowy Mountain", "Town"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1868"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e52404d5f262242c29be73c79e4f7954aa03373b3707a0cc8f6c9037b49e9ab4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what setting does the scene in the video take place?\n(A) Forest\n(B) Island\n(C) Snowy Mountain\n(D) Town\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "In what setting does the scene in the video take place?\n(A) Forest\n(B) Island\n(C) Snowy Mountain\n(D) Town\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1872, "doc": {"video_name": "haimian_4.mp4", "duration": 323.0, "question": "What is the background of the video?\n(A) Forest\n(B) Desert\n(C) Gobi\n(D) Under the sea\n", "candidates": ["Forest", "Desert", "Gobi", "Under the sea"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1872"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7753c1e11e81d7f7ced5a922cd93952c818ad6360c6c2bb4c93e640a0f1033da", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the background of the video?\n(A) Forest\n(B) Desert\n(C) Gobi\n(D) Under the sea\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the background of the video?\n(A) Forest\n(B) Desert\n(C) Gobi\n(D) Under the sea\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1876, "doc": {"video_name": "game_19.mp4", "duration": 203.73, "question": "What animal appears in this gameplay video?\n(A) Parrots\n(B) Wolves\n(C) Ocelots\n(D) Chickens\n", "candidates": ["Parrots", "Wolves", "Ocelots", "Chickens"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1876"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0fa5b395cd3b2942462f16df94022ee2ca768e6c285a883d3080a00274b17e6a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What animal appears in this gameplay video?\n(A) Parrots\n(B) Wolves\n(C) Ocelots\n(D) Chickens\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What animal appears in this gameplay video?\n(A) Parrots\n(B) Wolves\n(C) Ocelots\n(D) Chickens\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1880, "doc": {"video_name": "game_41.mp4", "duration": 312.45, "question": "What is this video about?\n(A) A person in the game planting trees by the lake\n(B) A person in the game building a structure by the lake\n(C) A person in the game taking care of pets\n(D) A documentary about humans and nature\n", "candidates": ["A person in the game planting trees by the lake", "A person in the game building a structure by the lake", "A person in the game taking care of pets", "A documentary about humans and nature"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1880"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "789a5c146fff3e1121f560bea40b66fa245c19b35b7e74c9f178546b8e4b5157", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is this video about?\n(A) A person in the game planting trees by the lake\n(B) A person in the game building a structure by the lake\n(C) A person in the game taking care of pets\n(D) A documentary about humans and nature\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is this video about?\n(A) A person in the game planting trees by the lake\n(B) A person in the game building a structure by the lake\n(C) A person in the game taking care of pets\n(D) A documentary about humans and nature\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1884, "doc": {"video_name": "game_15.mp4", "duration": 335.32, "question": "What is the object being built in this video?\n(A) A wall and a trench behind it\n(B) A house and a garden\n(C) A bridge and a river\n(D) A tower and a moat\n", "candidates": ["A wall and a trench behind it", "A house and a garden", "A bridge and a river", "A tower and a moat"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1884"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "41383cabc8ba0ff7a17c7c06ba41064432ea4e905863f1286c139d5e82318a4b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the object being built in this video?\n(A) A wall and a trench behind it\n(B) A house and a garden\n(C) A bridge and a river\n(D) A tower and a moat\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the object being built in this video?\n(A) A wall and a trench behind it\n(B) A house and a garden\n(C) A bridge and a river\n(D) A tower and a moat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1888, "doc": {"video_name": "game_46.mp4", "duration": 208.01, "question": "What is this video about?\n(A) A person performing a song\n(B) A person doing a live product promotion\n(C) A game tutorial video\n(D) A cartoon animation\n", "candidates": ["A person performing a song", "A person doing a live product promotion", "A game tutorial video", "A cartoon animation"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1888"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2e4ba5389f5c37cdf5908ac0c3938baec5ed4f412335c0c9584088bcf5aa648a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is this video about?\n(A) A person performing a song\n(B) A person doing a live product promotion\n(C) A game tutorial video\n(D) A cartoon animation\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is this video about?\n(A) A person performing a song\n(B) A person doing a live product promotion\n(C) A game tutorial video\n(D) A cartoon animation\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1892, "doc": {"video_name": "game_13.mp4", "duration": 193.84, "question": "What is the main action performed by the character in this video?\n(A) Attacking\n(B) Crafting\n(C) Trading\n(D) Exploring\n", "candidates": ["Attacking", "Crafting", "Trading", "Exploring"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1892"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0b8bb35dfe62884cec5e008d0675a750edcbf232d840e5350d80ff8c90723d86", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main action performed by the character in this video?\n(A) Attacking\n(B) Crafting\n(C) Trading\n(D) Exploring\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "A"}, "input": "What is the main action performed by the character in this video?\n(A) Attacking\n(B) Crafting\n(C) Trading\n(D) Exploring\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1896, "doc": {"video_name": "game_37.mp4", "duration": 259.07, "question": "What is the video demonstrating?\n(A) How to build a pool in the game\n(B) How to build a tiny fishing hut in the game\n(C) A cartoon animation\n(D) How to catch fish in the game\n", "candidates": ["How to build a pool in the game", "How to build a tiny fishing hut in the game", "A cartoon animation", "How to catch fish in the game"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1896"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a38a2aceba8ced32acb71752fe2d162f324f5558178c49ff8caff6c05100ba5a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the video demonstrating?\n(A) How to build a pool in the game\n(B) How to build a tiny fishing hut in the game\n(C) A cartoon animation\n(D) How to catch fish in the game\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the video demonstrating?\n(A) How to build a pool in the game\n(B) How to build a tiny fishing hut in the game\n(C) A cartoon animation\n(D) How to catch fish in the game\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1900, "doc": {"video_name": "game_42.mp4", "duration": 1173.14, "question": "What is this video about?\n(A) A person demonstrating how to win in a shooting game\n(B) Someone live-streaming a singing performance\n(C) A person demonstrating how to cook in the game\n(D) A person demonstrating how to build a device in the game\n", "candidates": ["A person demonstrating how to win in a shooting game", "Someone live-streaming a singing performance", "A person demonstrating how to cook in the game", "A person demonstrating how to build a device in the game"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1900"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4c49ccf6d0d12de68a1f2b016fdfd893867e7418098b7534e006d576ffa82195", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is this video about?\n(A) A person demonstrating how to win in a shooting game\n(B) Someone live-streaming a singing performance\n(C) A person demonstrating how to cook in the game\n(D) A person demonstrating how to build a device in the game\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is this video about?\n(A) A person demonstrating how to win in a shooting game\n(B) Someone live-streaming a singing performance\n(C) A person demonstrating how to cook in the game\n(D) A person demonstrating how to build a device in the game\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1904, "doc": {"video_name": "game_25.mp4", "duration": 189.89, "question": "What is the game character doing?\n(A) Making a scarecrow\n(B) Fighting with enemies\n(C) Collecting resources\n(D) Building a house\n", "candidates": ["Making a scarecrow", "Fighting with enemies", "Collecting resources", "Building a house"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1904"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5b73fac1d6801516bb9180d4f5dfece384bb64b059d7fee2e1f574b0b6efd70c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the game character doing?\n(A) Making a scarecrow\n(B) Fighting with enemies\n(C) Collecting resources\n(D) Building a house\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the game character doing?\n(A) Making a scarecrow\n(B) Fighting with enemies\n(C) Collecting resources\n(D) Building a house\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1908, "doc": {"video_name": "game_27.mp4", "duration": 200.37, "question": "What is the protagonist of the game doing?\n(A) Constructing a sandcastle on the beach\n(B) Setting up a tent at a coastal campsite\n(C) Exploring a cave\n(D) Building a house by the seaside\n", "candidates": ["Constructing a sandcastle on the beach", "Setting up a tent at a coastal campsite", "Exploring a cave", "Building a house by the seaside"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1908"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9f304884bb29beed1a981c0d2def29ac868f9824ae6e06f5978cad23038ee769", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the protagonist of the game doing?\n(A) Constructing a sandcastle on the beach\n(B) Setting up a tent at a coastal campsite\n(C) Exploring a cave\n(D) Building a house by the seaside\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the protagonist of the game doing?\n(A) Constructing a sandcastle on the beach\n(B) Setting up a tent at a coastal campsite\n(C) Exploring a cave\n(D) Building a house by the seaside\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1912, "doc": {"video_name": "game_1.mp4", "duration": 313.33, "question": "What shape is the object built by the main character in the video?\n(A) circle\n(B) triangle\n(C) rectangle\n(D) heart\n", "candidates": ["circle", "triangle", "rectangle", "heart"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1912"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "109964c007db083b6fa8b624e435b13df9d3785cfa9e77fc76301e8fd5e00aae", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What shape is the object built by the main character in the video?\n(A) circle\n(B) triangle\n(C) rectangle\n(D) heart\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "D"}, "input": "What shape is the object built by the main character in the video?\n(A) circle\n(B) triangle\n(C) rectangle\n(D) heart\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1916, "doc": {"video_name": "order_151.mp4", "duration": 7590.67, "question": "Arrange the following events from the video in the correct chronological order: (1)The woman starts working on her nails using bottles from a box next to her; (2)The words \"Love Food & Money with Angie Greenup\" appears on screen; (3)Her twitter handle and subscribe screen are shown while she holds her dogs; (4)The woman speaks to the camera from her living room while her dogs play fight behind her.\n(A) 2->4->1->3\n(B) 2->1->4->3\n(C) 4->2->1->3\n(D) 1->2->4->3\n", "candidates": ["2->4->1->3", "2->1->4->3", "4->2->1->3", "1->2->4->3"], "answer": "A", "task_type": "order", "question_id": "Q1916"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ab313748899f7df256f77e4de554c32a0183ba7cae4d5d789be97aa9dd6e532e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The woman starts working on her nails using bottles from a box next to her; (2)The words \"Love Food & Money with Angie Greenup\" appears on screen; (3)Her twitter handle and subscribe screen are shown while she holds her dogs; (4)The woman speaks to the camera from her living room while her dogs play fight behind her.\n(A) 2->4->1->3\n(B) 2->1->4->3\n(C) 4->2->1->3\n(D) 1->2->4->3\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The woman starts working on her nails using bottles from a box next to her; (2)The words \"Love Food & Money with Angie Greenup\" appears on screen; (3)Her twitter handle and subscribe screen are shown while she holds her dogs; (4)The woman speaks to the camera from her living room while her dogs play fight behind her.\n(A) 2->4->1->3\n(B) 2->1->4->3\n(C) 4->2->1->3\n(D) 1->2->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1920, "doc": {"video_name": "order_302.mp4", "duration": 490.05999999999995, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) javelin throw --> stomping grapes --> baking cookies --> playing trombone\n(B) playing trombone --> baking cookies --> stomping grapes --> javelin throw\n(C) javelin throw --> stomping grapes --> playing trombone --> baking cookies\n(D) stomping grapes --> playing trombone --> baking cookies --> javelin throw\n", "candidates": ["javelin throw --> stomping grapes --> baking cookies --> playing trombone", "playing trombone --> baking cookies --> stomping grapes --> javelin throw", "javelin throw --> stomping grapes --> playing trombone --> baking cookies", "stomping grapes --> playing trombone --> baking cookies --> javelin throw"], "answer": "D", "task_type": "order", "question_id": "Q1920"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ab567feeba32bc70950e69ad325717e73d474f210eb3a6ac64dc5f78be0eabfb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) javelin throw --> stomping grapes --> baking cookies --> playing trombone\n(B) playing trombone --> baking cookies --> stomping grapes --> javelin throw\n(C) javelin throw --> stomping grapes --> playing trombone --> baking cookies\n(D) stomping grapes --> playing trombone --> baking cookies --> javelin throw\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) javelin throw --> stomping grapes --> baking cookies --> playing trombone\n(B) playing trombone --> baking cookies --> stomping grapes --> javelin throw\n(C) javelin throw --> stomping grapes --> playing trombone --> baking cookies\n(D) stomping grapes --> playing trombone --> baking cookies --> javelin throw\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1924, "doc": {"video_name": "order_115.mp4", "duration": 706.6700000000001, "question": "Arrange the following events from the video in the correct chronological order: (1)A lady in black reads names a man hands her and passes out prize buckets to the kids; (2)A group of kids is building a moat filled with water around a sand castle; (3)We see an opening title screen; (4)We see kids across the beach working on their castles in the wet sand.\n(A) 3->2->4->1\n(B) 4->1->2->3\n(C) 2->3->1->4\n(D) 1->4->2->3\n", "candidates": ["3->2->4->1", "4->1->2->3", "2->3->1->4", "1->4->2->3"], "answer": "A", "task_type": "order", "question_id": "Q1924"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bd110cd4056f44244a0ec32ab832ac08676309c05e649fe6a4b9f3bc4c9037ed", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A lady in black reads names a man hands her and passes out prize buckets to the kids; (2)A group of kids is building a moat filled with water around a sand castle; (3)We see an opening title screen; (4)We see kids across the beach working on their castles in the wet sand.\n(A) 3->2->4->1\n(B) 4->1->2->3\n(C) 2->3->1->4\n(D) 1->4->2->3\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A lady in black reads names a man hands her and passes out prize buckets to the kids; (2)A group of kids is building a moat filled with water around a sand castle; (3)We see an opening title screen; (4)We see kids across the beach working on their castles in the wet sand.\n(A) 3->2->4->1\n(B) 4->1->2->3\n(C) 2->3->1->4\n(D) 1->4->2->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1928, "doc": {"video_name": "order_78.mp4", "duration": 2241.51, "question": "Arrange the following events from the video in the correct chronological order: (1)The man melts the wax with the tool and wipes the ski; (2)The man adds a substance from a jug to the ski and wipes it with a paper towel; (3)The man exchanges skis and waxes the second one with the tool; (4)The man then scrapes the wax off the ski and uses a different tools and paper towel on the ski.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "1->2->3->4", "3->2->1->4", "2->1->3->4"], "answer": "D", "task_type": "order", "question_id": "Q1928"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "18ec2493e82b01379923b436d11bf509bcc89e6e12d52f691f4393750ae6e8e0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man melts the wax with the tool and wipes the ski; (2)The man adds a substance from a jug to the ski and wipes it with a paper towel; (3)The man exchanges skis and waxes the second one with the tool; (4)The man then scrapes the wax off the ski and uses a different tools and paper towel on the ski.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man melts the wax with the tool and wipes the ski; (2)The man adds a substance from a jug to the ski and wipes it with a paper towel; (3)The man exchanges skis and waxes the second one with the tool; (4)The man then scrapes the wax off the ski and uses a different tools and paper towel on the ski.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1932, "doc": {"video_name": "order_74.mp4", "duration": 684.15, "question": "Arrange the following events from the video in the correct chronological order: (1)A shirtless man lifts a ball onto one shoulder; (2)A series of tug of war matches are shown; (3)A third man flips a heavy tire; (4)Individuals are shown exercising with weights, kegs, or tires in a parking lot.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 3->1->2->4\n(D) 1->3->4->2\n", "candidates": ["1->2->3->4", "2->1->3->4", "3->1->2->4", "1->3->4->2"], "answer": "D", "task_type": "order", "question_id": "Q1932"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dc834e573d26afdd9724c0a96565670c0448d18cede37d6a3c6714aeb0fd229b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A shirtless man lifts a ball onto one shoulder; (2)A series of tug of war matches are shown; (3)A third man flips a heavy tire; (4)Individuals are shown exercising with weights, kegs, or tires in a parking lot.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 3->1->2->4\n(D) 1->3->4->2\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A shirtless man lifts a ball onto one shoulder; (2)A series of tug of war matches are shown; (3)A third man flips a heavy tire; (4)Individuals are shown exercising with weights, kegs, or tires in a parking lot.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 3->1->2->4\n(D) 1->3->4->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1936, "doc": {"video_name": "order_210.mp4", "duration": 777.0500000000001, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> jetskiing --> javelin throw --> playing trombone\n(B) jetskiing --> playing trombone --> javelin throw --> cooking sausages\n(C) jetskiing --> cooking sausages --> javelin throw --> playing trombone\n(D) cooking sausages --> javelin throw --> jetskiing --> playing trombone\n", "candidates": ["cooking sausages --> jetskiing --> javelin throw --> playing trombone", "jetskiing --> playing trombone --> javelin throw --> cooking sausages", "jetskiing --> cooking sausages --> javelin throw --> playing trombone", "cooking sausages --> javelin throw --> jetskiing --> playing trombone"], "answer": "B", "task_type": "order", "question_id": "Q1936"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "38a9c384bc7eddaf9ea26fcdbf225e5054de8c72c8d576a0135d1f239930c77d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> jetskiing --> javelin throw --> playing trombone\n(B) jetskiing --> playing trombone --> javelin throw --> cooking sausages\n(C) jetskiing --> cooking sausages --> javelin throw --> playing trombone\n(D) cooking sausages --> javelin throw --> jetskiing --> playing trombone\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> jetskiing --> javelin throw --> playing trombone\n(B) jetskiing --> playing trombone --> javelin throw --> cooking sausages\n(C) jetskiing --> cooking sausages --> javelin throw --> playing trombone\n(D) cooking sausages --> javelin throw --> jetskiing --> playing trombone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1940, "doc": {"video_name": "order_269.mp4", "duration": 485.98999999999995, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) jetskiing --> abseiling --> water sliding --> playing trombone\n(B) playing trombone --> jetskiing --> water sliding --> abseiling\n(C) playing trombone --> abseiling --> jetskiing --> water sliding\n(D) abseiling --> playing trombone --> jetskiing --> water sliding\n", "candidates": ["jetskiing --> abseiling --> water sliding --> playing trombone", "playing trombone --> jetskiing --> water sliding --> abseiling", "playing trombone --> abseiling --> jetskiing --> water sliding", "abseiling --> playing trombone --> jetskiing --> water sliding"], "answer": "B", "task_type": "order", "question_id": "Q1940"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6c65c6c3971e34e4340bb707c874fa53e521a8dc688b7caf2dcc10c2e05c2f17", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) jetskiing --> abseiling --> water sliding --> playing trombone\n(B) playing trombone --> jetskiing --> water sliding --> abseiling\n(C) playing trombone --> abseiling --> jetskiing --> water sliding\n(D) abseiling --> playing trombone --> jetskiing --> water sliding\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) jetskiing --> abseiling --> water sliding --> playing trombone\n(B) playing trombone --> jetskiing --> water sliding --> abseiling\n(C) playing trombone --> abseiling --> jetskiing --> water sliding\n(D) abseiling --> playing trombone --> jetskiing --> water sliding\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1944, "doc": {"video_name": "order_294.mp4", "duration": 490.10999999999996, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) shredding paper --> riding mule --> baking cookies --> milking cow\n(B) shredding paper --> baking cookies --> riding mule --> milking cow\n(C) riding mule --> shredding paper --> milking cow --> baking cookies\n(D) shredding paper --> milking cow --> baking cookies --> riding mule\n", "candidates": ["shredding paper --> riding mule --> baking cookies --> milking cow", "shredding paper --> baking cookies --> riding mule --> milking cow", "riding mule --> shredding paper --> milking cow --> baking cookies", "shredding paper --> milking cow --> baking cookies --> riding mule"], "answer": "A", "task_type": "order", "question_id": "Q1944"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dce40b39a937fe51e318cdf1ad7f6c9f019093648502cd01f459472250c73405", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) shredding paper --> riding mule --> baking cookies --> milking cow\n(B) shredding paper --> baking cookies --> riding mule --> milking cow\n(C) riding mule --> shredding paper --> milking cow --> baking cookies\n(D) shredding paper --> milking cow --> baking cookies --> riding mule\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) shredding paper --> riding mule --> baking cookies --> milking cow\n(B) shredding paper --> baking cookies --> riding mule --> milking cow\n(C) riding mule --> shredding paper --> milking cow --> baking cookies\n(D) shredding paper --> milking cow --> baking cookies --> riding mule\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1948, "doc": {"video_name": "order_15.mp4", "duration": 711.2, "question": "Arrange the following events from the video in the correct chronological order: (1)People crash into the bottom of a bridge; (2)People are sitting on a raft going down a river; (3)People are walking across the water and down a trail; (4)People are carrying their raft and get into a van.\n(A) 1->3->2->4\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 2->3->1->4\n", "candidates": ["1->3->2->4", "1->2->3->4", "2->1->3->4", "2->3->1->4"], "answer": "C", "task_type": "order", "question_id": "Q1948"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "30eee3055833f193592c8f61166d117c4175b39ebbcee70b86bd3dd24608abee", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)People crash into the bottom of a bridge; (2)People are sitting on a raft going down a river; (3)People are walking across the water and down a trail; (4)People are carrying their raft and get into a van.\n(A) 1->3->2->4\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 2->3->1->4\n", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)People crash into the bottom of a bridge; (2)People are sitting on a raft going down a river; (3)People are walking across the water and down a trail; (4)People are carrying their raft and get into a van.\n(A) 1->3->2->4\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 2->3->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1952, "doc": {"video_name": "order_339.mp4", "duration": 486.78, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> paragliding --> riding mule --> jetskiing\n(B) playing trombone --> riding mule --> jetskiing --> paragliding\n(C) paragliding --> riding mule --> jetskiing --> playing trombone\n(D) paragliding --> jetskiing --> riding mule --> playing trombone\n", "candidates": ["playing trombone --> paragliding --> riding mule --> jetskiing", "playing trombone --> riding mule --> jetskiing --> paragliding", "paragliding --> riding mule --> jetskiing --> playing trombone", "paragliding --> jetskiing --> riding mule --> playing trombone"], "answer": "D", "task_type": "order", "question_id": "Q1952"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b87b660c39e9ec6e1a8bf0a75c251004ef12cd34aa9ff429a1a95e0a7da302e4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> paragliding --> riding mule --> jetskiing\n(B) playing trombone --> riding mule --> jetskiing --> paragliding\n(C) paragliding --> riding mule --> jetskiing --> playing trombone\n(D) paragliding --> jetskiing --> riding mule --> playing trombone\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> paragliding --> riding mule --> jetskiing\n(B) playing trombone --> riding mule --> jetskiing --> paragliding\n(C) paragliding --> riding mule --> jetskiing --> playing trombone\n(D) paragliding --> jetskiing --> riding mule --> playing trombone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1956, "doc": {"video_name": "order_11.mp4", "duration": 707.69, "question": "Arrange the following events from the video in the correct chronological order: (1)The man attempts to walk across the rope but falls and holds onto the rope; (2)A seal sits on a rock near an ocean; (3)The man films from a beach cliff next to a tent; (4)The man walks across the rope all the way to the attached rock.\n(A) 2->1->4->3\n(B) 1->2->3->4\n(C) 3->1->2->4\n(D) 2->4->1->3\n", "candidates": ["2->1->4->3", "1->2->3->4", "3->1->2->4", "2->4->1->3"], "answer": "A", "task_type": "order", "question_id": "Q1956"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd09f57b376f3f5939ee4f740073678726c02c18f9205b68587d332dbbb2a98c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man attempts to walk across the rope but falls and holds onto the rope; (2)A seal sits on a rock near an ocean; (3)The man films from a beach cliff next to a tent; (4)The man walks across the rope all the way to the attached rock.\n(A) 2->1->4->3\n(B) 1->2->3->4\n(C) 3->1->2->4\n(D) 2->4->1->3\n", "task_type": "order", "pred_answer": "D", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man attempts to walk across the rope but falls and holds onto the rope; (2)A seal sits on a rock near an ocean; (3)The man films from a beach cliff next to a tent; (4)The man walks across the rope all the way to the attached rock.\n(A) 2->1->4->3\n(B) 1->2->3->4\n(C) 3->1->2->4\n(D) 2->4->1->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1960, "doc": {"video_name": "order_346.mp4", "duration": 5786.9, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) abseiling --> javelin throw --> pole vault --> cleaning toilet\n(B) pole vault --> cleaning toilet --> javelin throw --> abseiling\n(C) abseiling --> javelin throw --> cleaning toilet --> pole vault\n(D) javelin throw --> abseiling --> cleaning toilet --> pole vault\n", "candidates": ["abseiling --> javelin throw --> pole vault --> cleaning toilet", "pole vault --> cleaning toilet --> javelin throw --> abseiling", "abseiling --> javelin throw --> cleaning toilet --> pole vault", "javelin throw --> abseiling --> cleaning toilet --> pole vault"], "answer": "B", "task_type": "order", "question_id": "Q1960"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fead23d29b33e75bb2d50d7e2084fc3047f23aabb4bc3aba50f1f45765421141", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) abseiling --> javelin throw --> pole vault --> cleaning toilet\n(B) pole vault --> cleaning toilet --> javelin throw --> abseiling\n(C) abseiling --> javelin throw --> cleaning toilet --> pole vault\n(D) javelin throw --> abseiling --> cleaning toilet --> pole vault\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) abseiling --> javelin throw --> pole vault --> cleaning toilet\n(B) pole vault --> cleaning toilet --> javelin throw --> abseiling\n(C) abseiling --> javelin throw --> cleaning toilet --> pole vault\n(D) javelin throw --> abseiling --> cleaning toilet --> pole vault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1964, "doc": {"video_name": "order_290.mp4", "duration": 504.38, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) clean and jerk --> jetskiing --> pole vault --> javelin throw\n(B) jetskiing --> pole vault --> clean and jerk --> javelin throw\n(C) pole vault --> jetskiing --> clean and jerk --> javelin throw\n(D) javelin throw --> jetskiing --> pole vault --> clean and jerk\n", "candidates": ["clean and jerk --> jetskiing --> pole vault --> javelin throw", "jetskiing --> pole vault --> clean and jerk --> javelin throw", "pole vault --> jetskiing --> clean and jerk --> javelin throw", "javelin throw --> jetskiing --> pole vault --> clean and jerk"], "answer": "C", "task_type": "order", "question_id": "Q1964"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6eb47dbe503aa2b4e1cdeb154a279f5de3d56bc3ec3eb064ead1fa7c328f79e3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) clean and jerk --> jetskiing --> pole vault --> javelin throw\n(B) jetskiing --> pole vault --> clean and jerk --> javelin throw\n(C) pole vault --> jetskiing --> clean and jerk --> javelin throw\n(D) javelin throw --> jetskiing --> pole vault --> clean and jerk\n", "task_type": "order", "pred_answer": "A", "answer": "C"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) clean and jerk --> jetskiing --> pole vault --> javelin throw\n(B) jetskiing --> pole vault --> clean and jerk --> javelin throw\n(C) pole vault --> jetskiing --> clean and jerk --> javelin throw\n(D) javelin throw --> jetskiing --> pole vault --> clean and jerk\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1968, "doc": {"video_name": "order_282.mp4", "duration": 490.03999999999996, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> abseiling --> zumba --> cooking sausages\n(B) abseiling --> cooking sausages --> zumba --> jetskiing\n(C) abseiling --> zumba --> cooking sausages --> jetskiing\n(D) cooking sausages --> abseiling --> zumba --> jetskiing\n", "candidates": ["jetskiing --> abseiling --> zumba --> cooking sausages", "abseiling --> cooking sausages --> zumba --> jetskiing", "abseiling --> zumba --> cooking sausages --> jetskiing", "cooking sausages --> abseiling --> zumba --> jetskiing"], "answer": "B", "task_type": "order", "question_id": "Q1968"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6039f20767f9f84f95e345973937d16dbd16e4967fbedb605802441269ac6ca5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> abseiling --> zumba --> cooking sausages\n(B) abseiling --> cooking sausages --> zumba --> jetskiing\n(C) abseiling --> zumba --> cooking sausages --> jetskiing\n(D) cooking sausages --> abseiling --> zumba --> jetskiing\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> abseiling --> zumba --> cooking sausages\n(B) abseiling --> cooking sausages --> zumba --> jetskiing\n(C) abseiling --> zumba --> cooking sausages --> jetskiing\n(D) cooking sausages --> abseiling --> zumba --> jetskiing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1972, "doc": {"video_name": "order_239.mp4", "duration": 515.76, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) abseiling --> water sliding --> baking cookies --> jetskiing\n(B) abseiling --> water sliding --> jetskiing --> baking cookies\n(C) water sliding --> jetskiing --> baking cookies --> abseiling\n(D) jetskiing --> abseiling --> water sliding --> baking cookies\n", "candidates": ["abseiling --> water sliding --> baking cookies --> jetskiing", "abseiling --> water sliding --> jetskiing --> baking cookies", "water sliding --> jetskiing --> baking cookies --> abseiling", "jetskiing --> abseiling --> water sliding --> baking cookies"], "answer": "C", "task_type": "order", "question_id": "Q1972"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c8e6b94aa17167898189ccc40582f74fd08999b0f1c3a8c8c2c635ced18e850a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) abseiling --> water sliding --> baking cookies --> jetskiing\n(B) abseiling --> water sliding --> jetskiing --> baking cookies\n(C) water sliding --> jetskiing --> baking cookies --> abseiling\n(D) jetskiing --> abseiling --> water sliding --> baking cookies\n", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) abseiling --> water sliding --> baking cookies --> jetskiing\n(B) abseiling --> water sliding --> jetskiing --> baking cookies\n(C) water sliding --> jetskiing --> baking cookies --> abseiling\n(D) jetskiing --> abseiling --> water sliding --> baking cookies\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1976, "doc": {"video_name": "order_246.mp4", "duration": 600.03, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) making jewelry --> riding mule --> playing trombone --> shredding paper\n(B) shredding paper --> riding mule --> making jewelry --> playing trombone\n(C) making jewelry --> shredding paper --> playing trombone --> riding mule\n(D) playing trombone --> riding mule --> making jewelry --> shredding paper\n", "candidates": ["making jewelry --> riding mule --> playing trombone --> shredding paper", "shredding paper --> riding mule --> making jewelry --> playing trombone", "making jewelry --> shredding paper --> playing trombone --> riding mule", "playing trombone --> riding mule --> making jewelry --> shredding paper"], "answer": "A", "task_type": "order", "question_id": "Q1976"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7f73ba187ab8de02446f2a53daeb560f6f20ec8dbf6f00e095b718a29505b6c9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) making jewelry --> riding mule --> playing trombone --> shredding paper\n(B) shredding paper --> riding mule --> making jewelry --> playing trombone\n(C) making jewelry --> shredding paper --> playing trombone --> riding mule\n(D) playing trombone --> riding mule --> making jewelry --> shredding paper\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) making jewelry --> riding mule --> playing trombone --> shredding paper\n(B) shredding paper --> riding mule --> making jewelry --> playing trombone\n(C) making jewelry --> shredding paper --> playing trombone --> riding mule\n(D) playing trombone --> riding mule --> making jewelry --> shredding paper\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1980, "doc": {"video_name": "order_165.mp4", "duration": 986.37, "question": "Arrange the following events from the video in the correct chronological order: (1)Man is on lake side talking to the camera like other couples as well; (2)Man is talking to the camera; (3)People are kayaking on calm river and have a good picnic day; (4)People are standing on a side of a rock wall.\n(A) 1->2->3->4\n(B) 2->1->4->3\n(C) 4->2->1->3\n(D) 2->4->1->3\n", "candidates": ["1->2->3->4", "2->1->4->3", "4->2->1->3", "2->4->1->3"], "answer": "D", "task_type": "order", "question_id": "Q1980"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "184f2ecdb2795add533fdfce074116bb7d08018b43d71dacf8a1c77be0e99777", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Man is on lake side talking to the camera like other couples as well; (2)Man is talking to the camera; (3)People are kayaking on calm river and have a good picnic day; (4)People are standing on a side of a rock wall.\n(A) 1->2->3->4\n(B) 2->1->4->3\n(C) 4->2->1->3\n(D) 2->4->1->3\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Man is on lake side talking to the camera like other couples as well; (2)Man is talking to the camera; (3)People are kayaking on calm river and have a good picnic day; (4)People are standing on a side of a rock wall.\n(A) 1->2->3->4\n(B) 2->1->4->3\n(C) 4->2->1->3\n(D) 2->4->1->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1984, "doc": {"video_name": "order_316.mp4", "duration": 479.47, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> stomping grapes --> clean and jerk --> zumba\n(B) zumba --> baking cookies --> clean and jerk --> stomping grapes\n(C) baking cookies --> zumba --> clean and jerk --> stomping grapes\n(D) clean and jerk --> stomping grapes --> baking cookies --> zumba\n", "candidates": ["baking cookies --> stomping grapes --> clean and jerk --> zumba", "zumba --> baking cookies --> clean and jerk --> stomping grapes", "baking cookies --> zumba --> clean and jerk --> stomping grapes", "clean and jerk --> stomping grapes --> baking cookies --> zumba"], "answer": "B", "task_type": "order", "question_id": "Q1984"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "20478d98ced0fd5e725ae0dc325ca6baf1647b3df006aa677cd466c954a60f26", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> stomping grapes --> clean and jerk --> zumba\n(B) zumba --> baking cookies --> clean and jerk --> stomping grapes\n(C) baking cookies --> zumba --> clean and jerk --> stomping grapes\n(D) clean and jerk --> stomping grapes --> baking cookies --> zumba\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> stomping grapes --> clean and jerk --> zumba\n(B) zumba --> baking cookies --> clean and jerk --> stomping grapes\n(C) baking cookies --> zumba --> clean and jerk --> stomping grapes\n(D) clean and jerk --> stomping grapes --> baking cookies --> zumba\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1988, "doc": {"video_name": "order_172.mp4", "duration": 6510.83, "question": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "candidates": ["2->1->3->4", "3->2->1->4", "4->3->2->1", "1->2->3->4"], "answer": "D", "task_type": "order", "question_id": "Q1988"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4926aa3e27d0f00a64f91b3cab29f040dd1eea54cceefcfc7184f37b0da3f7d9", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "A", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1992, "doc": {"video_name": "order_64.mp4", "duration": 674.54, "question": "Arrange the following events from the video in the correct chronological order: (1)The action of the lures is shown underwater as several different fish go after the lures; (2)Several men show off the different lures they are using for ice fishing; (3)The video ends with the closing credits and Graphics shown on the screen; (4)An introduction comes onto the screen for a video about fishing lures.\n(A) 3->2->1->4\n(B) 2->1->3->4\n(C) 1->2->3->4\n(D) 4->2->1->3\n", "candidates": ["3->2->1->4", "2->1->3->4", "1->2->3->4", "4->2->1->3"], "answer": "D", "task_type": "order", "question_id": "Q1992"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "342517675f7641556782b9cbdadfb47515176dbcedc8bc1c6a17105047c8fceb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The action of the lures is shown underwater as several different fish go after the lures; (2)Several men show off the different lures they are using for ice fishing; (3)The video ends with the closing credits and Graphics shown on the screen; (4)An introduction comes onto the screen for a video about fishing lures.\n(A) 3->2->1->4\n(B) 2->1->3->4\n(C) 1->2->3->4\n(D) 4->2->1->3\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The action of the lures is shown underwater as several different fish go after the lures; (2)Several men show off the different lures they are using for ice fishing; (3)The video ends with the closing credits and Graphics shown on the screen; (4)An introduction comes onto the screen for a video about fishing lures.\n(A) 3->2->1->4\n(B) 2->1->3->4\n(C) 1->2->3->4\n(D) 4->2->1->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1996, "doc": {"video_name": "order_323.mp4", "duration": 378.04, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> stomping grapes --> shredding paper --> abseiling\n(B) shredding paper --> javelin throw --> abseiling --> stomping grapes\n(C) abseiling --> javelin throw --> stomping grapes --> shredding paper\n(D) shredding paper --> stomping grapes --> javelin throw --> abseiling\n", "candidates": ["javelin throw --> stomping grapes --> shredding paper --> abseiling", "shredding paper --> javelin throw --> abseiling --> stomping grapes", "abseiling --> javelin throw --> stomping grapes --> shredding paper", "shredding paper --> stomping grapes --> javelin throw --> abseiling"], "answer": "B", "task_type": "order", "question_id": "Q1996"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c667b8710de881b88969cd07474f7504c93d3a73cc4e39c0a39555af0d76dbdf", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> stomping grapes --> shredding paper --> abseiling\n(B) shredding paper --> javelin throw --> abseiling --> stomping grapes\n(C) abseiling --> javelin throw --> stomping grapes --> shredding paper\n(D) shredding paper --> stomping grapes --> javelin throw --> abseiling\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> stomping grapes --> shredding paper --> abseiling\n(B) shredding paper --> javelin throw --> abseiling --> stomping grapes\n(C) abseiling --> javelin throw --> stomping grapes --> shredding paper\n(D) shredding paper --> stomping grapes --> javelin throw --> abseiling\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2000, "doc": {"video_name": "order_60.mp4", "duration": 688.3399999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)The chef pours soy sauce into the cup; (2)The chef shows off shredded garlic before throwing it into the cup; (3)The chef grabs a bowl of salad and shows it off; (4)The chef grabs a cup of nuts and throws it on top of a salad.\n(A) 3->4->1->2\n(B) 2->1->3->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "candidates": ["3->4->1->2", "2->1->3->4", "4->3->2->1", "1->2->3->4"], "answer": "A", "task_type": "order", "question_id": "Q2000"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "21d3f6cba62274df3f6d14f2ae907a8b7af2c6461610b59943f961b1cff44422", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The chef pours soy sauce into the cup; (2)The chef shows off shredded garlic before throwing it into the cup; (3)The chef grabs a bowl of salad and shows it off; (4)The chef grabs a cup of nuts and throws it on top of a salad.\n(A) 3->4->1->2\n(B) 2->1->3->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The chef pours soy sauce into the cup; (2)The chef shows off shredded garlic before throwing it into the cup; (3)The chef grabs a bowl of salad and shows it off; (4)The chef grabs a cup of nuts and throws it on top of a salad.\n(A) 3->4->1->2\n(B) 2->1->3->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2004, "doc": {"video_name": "order_252.mp4", "duration": 532.0899999999999, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> carving pumpkin --> playing harp --> cleaning toilet\n(B) playing trombone --> cleaning toilet --> carving pumpkin --> playing harp\n(C) cleaning toilet --> playing trombone --> playing harp --> carving pumpkin\n(D) playing harp --> carving pumpkin --> playing trombone --> cleaning toilet\n", "candidates": ["playing trombone --> carving pumpkin --> playing harp --> cleaning toilet", "playing trombone --> cleaning toilet --> carving pumpkin --> playing harp", "cleaning toilet --> playing trombone --> playing harp --> carving pumpkin", "playing harp --> carving pumpkin --> playing trombone --> cleaning toilet"], "answer": "A", "task_type": "order", "question_id": "Q2004"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "275e1bf73c016cd427449330bb58bacc1b02f0dfe62f31e686c900cf2350cc4e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> carving pumpkin --> playing harp --> cleaning toilet\n(B) playing trombone --> cleaning toilet --> carving pumpkin --> playing harp\n(C) cleaning toilet --> playing trombone --> playing harp --> carving pumpkin\n(D) playing harp --> carving pumpkin --> playing trombone --> cleaning toilet\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> carving pumpkin --> playing harp --> cleaning toilet\n(B) playing trombone --> cleaning toilet --> carving pumpkin --> playing harp\n(C) cleaning toilet --> playing trombone --> playing harp --> carving pumpkin\n(D) playing harp --> carving pumpkin --> playing trombone --> cleaning toilet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2008, "doc": {"video_name": "order_171.mp4", "duration": 8177.369999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)Water is added to the cup and more limes are squeezed in by hand; (2)The small bowls of salt are arranged and limes are sliced in halves; (3)The cup is stirred with more water and a set of cups filled with the refreshment are seen; (4)The limes are juiced into a cup using a hand held press.\n(A) 1->2->4->3\n(B) 2->4->1->3\n(C) 2->1->4->3\n(D) 4->2->1->3\n", "candidates": ["1->2->4->3", "2->4->1->3", "2->1->4->3", "4->2->1->3"], "answer": "B", "task_type": "order", "question_id": "Q2008"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "defaa9f217952433fa33cb17e72ca78de3d43713c448fcce714980dd754d671e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Water is added to the cup and more limes are squeezed in by hand; (2)The small bowls of salt are arranged and limes are sliced in halves; (3)The cup is stirred with more water and a set of cups filled with the refreshment are seen; (4)The limes are juiced into a cup using a hand held press.\n(A) 1->2->4->3\n(B) 2->4->1->3\n(C) 2->1->4->3\n(D) 4->2->1->3\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Water is added to the cup and more limes are squeezed in by hand; (2)The small bowls of salt are arranged and limes are sliced in halves; (3)The cup is stirred with more water and a set of cups filled with the refreshment are seen; (4)The limes are juiced into a cup using a hand held press.\n(A) 1->2->4->3\n(B) 2->4->1->3\n(C) 2->1->4->3\n(D) 4->2->1->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2012, "doc": {"video_name": "order_153.mp4", "duration": 828.52, "question": "Arrange the following events from the video in the correct chronological order: (1)A man picks up the baby from the pool; (2)A person carries two bags out of a house; (3)A baby falls into the swimming pool; (4)A dog walks out of a house.\n(A) 2->4->3->1\n(B) 1->3->2->4\n(C) 4->2->1->3\n(D) 3->1->2->4\n", "candidates": ["2->4->3->1", "1->3->2->4", "4->2->1->3", "3->1->2->4"], "answer": "A", "task_type": "order", "question_id": "Q2012"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "01454774d0c13bd74ae55cc538b92742da11db27c823b90a46aafe407a7d1903", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A man picks up the baby from the pool; (2)A person carries two bags out of a house; (3)A baby falls into the swimming pool; (4)A dog walks out of a house.\n(A) 2->4->3->1\n(B) 1->3->2->4\n(C) 4->2->1->3\n(D) 3->1->2->4\n", "task_type": "order", "pred_answer": "D", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A man picks up the baby from the pool; (2)A person carries two bags out of a house; (3)A baby falls into the swimming pool; (4)A dog walks out of a house.\n(A) 2->4->3->1\n(B) 1->3->2->4\n(C) 4->2->1->3\n(D) 3->1->2->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2016, "doc": {"video_name": "order_135.mp4", "duration": 421.76, "question": "Arrange the following events from the video in the correct chronological order: (1)A young and a kid are doing balance in a balance rope; (2)People are walking in a bridge to see a competition of men doing tricks on top of a balance rope; (3)A man is jumping and doing tricks in a balance rope above a cold river; (4)The boy is in a competition in snowy path doing tricks on a balance rope with people behind a fence watching him.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "3->2->1->4", "1->2->3->4", "2->1->3->4"], "answer": "C", "task_type": "order", "question_id": "Q2016"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "09b962575a5778a31c3a0222fa3fce97a14d88df2c6318354e050e4fa42ddfcc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A young and a kid are doing balance in a balance rope; (2)People are walking in a bridge to see a competition of men doing tricks on top of a balance rope; (3)A man is jumping and doing tricks in a balance rope above a cold river; (4)The boy is in a competition in snowy path doing tricks on a balance rope with people behind a fence watching him.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A young and a kid are doing balance in a balance rope; (2)People are walking in a bridge to see a competition of men doing tricks on top of a balance rope; (3)A man is jumping and doing tricks in a balance rope above a cold river; (4)The boy is in a competition in snowy path doing tricks on a balance rope with people behind a fence watching him.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2020, "doc": {"video_name": "order_98.mp4", "duration": 840.4, "question": "Arrange the following events from the video in the correct chronological order: (1)A person in a red coat cleans the snow off their car; (2)The trunk of the car is lifted open; (3)A person in a tan coat cleans off the front of the car; (4)A man in a white jacket starts to clear the snow off of another car.\n(A) 3->2->1->4\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 4->3->2->1\n", "candidates": ["3->2->1->4", "1->2->3->4", "2->1->3->4", "4->3->2->1"], "answer": "B", "task_type": "order", "question_id": "Q2020"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "de4cf2cffbc9235f20000a4943f5f14b9d30b12f881d3f7bc21d55033770f72b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A person in a red coat cleans the snow off their car; (2)The trunk of the car is lifted open; (3)A person in a tan coat cleans off the front of the car; (4)A man in a white jacket starts to clear the snow off of another car.\n(A) 3->2->1->4\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A person in a red coat cleans the snow off their car; (2)The trunk of the car is lifted open; (3)A person in a tan coat cleans off the front of the car; (4)A man in a white jacket starts to clear the snow off of another car.\n(A) 3->2->1->4\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2024, "doc": {"video_name": "order_96.mp4", "duration": 810.4, "question": "Arrange the following events from the video in the correct chronological order: (1)A person in a red coat cleans the snow off their car; (2)The trunk of the car is lifted open; (3)A person in a tan coat cleans off the front of the car; (4)A man in a white jacket starts to clear the snow off of another car.\n(A) 3->2->1->4\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "candidates": ["3->2->1->4", "4->3->2->1", "2->1->3->4", "1->2->3->4"], "answer": "D", "task_type": "order", "question_id": "Q2024"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "58873f77afc01f550489523c83dc91bac4d164480f1408e7e9132bd332017a97", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A person in a red coat cleans the snow off their car; (2)The trunk of the car is lifted open; (3)A person in a tan coat cleans off the front of the car; (4)A man in a white jacket starts to clear the snow off of another car.\n(A) 3->2->1->4\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A person in a red coat cleans the snow off their car; (2)The trunk of the car is lifted open; (3)A person in a tan coat cleans off the front of the car; (4)A man in a white jacket starts to clear the snow off of another car.\n(A) 3->2->1->4\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2028, "doc": {"video_name": "order_94.mp4", "duration": 707.6, "question": "Arrange the following events from the video in the correct chronological order: (1) The marching band aligns in the street with their instruments; (2) A man passes in front of the marching band holding a camera; (3) The marching band performs in a field and in a gym, moving around while playing; (4) The marching band performs in front of a building and other places.\n(A) 1->3->2->4\n(B) 4->2->1->3\n(C) 2->4->3->1\n(D) 3->1->4->2\n", "candidates": ["1->3->2->4", "4->2->1->3", "2->4->3->1", "3->1->4->2"], "answer": "D", "task_type": "order", "question_id": "Q2028"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d13a92fdf37323fb6cd0f9cd8ba934d8bb3cd4faeff1f7e90359a9395107f277", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) The marching band aligns in the street with their instruments; (2) A man passes in front of the marching band holding a camera; (3) The marching band performs in a field and in a gym, moving around while playing; (4) The marching band performs in front of a building and other places.\n(A) 1->3->2->4\n(B) 4->2->1->3\n(C) 2->4->3->1\n(D) 3->1->4->2\n", "task_type": "order", "pred_answer": "A", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1) The marching band aligns in the street with their instruments; (2) A man passes in front of the marching band holding a camera; (3) The marching band performs in a field and in a gym, moving around while playing; (4) The marching band performs in front of a building and other places.\n(A) 1->3->2->4\n(B) 4->2->1->3\n(C) 2->4->3->1\n(D) 3->1->4->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2032, "doc": {"video_name": "order_90.mp4", "duration": 703.3499999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)Man is on lake side talking to the camera like other couples as well; (2)Man is talking to the camera; (3)People are kayaking on calm river and have a good picnic day; (4)People are standing on a side of a rock wall.\n(A) 4->2->1->3\n(B) 2->1->4->3\n(C) 2->4->1->3\n(D) 1->2->3->4\n", "candidates": ["4->2->1->3", "2->1->4->3", "2->4->1->3", "1->2->3->4"], "answer": "C", "task_type": "order", "question_id": "Q2032"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bfd47a10ee05c0e9e29674e700d1058b82437549946e320101cae7a3bc672474", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Man is on lake side talking to the camera like other couples as well; (2)Man is talking to the camera; (3)People are kayaking on calm river and have a good picnic day; (4)People are standing on a side of a rock wall.\n(A) 4->2->1->3\n(B) 2->1->4->3\n(C) 2->4->1->3\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Man is on lake side talking to the camera like other couples as well; (2)Man is talking to the camera; (3)People are kayaking on calm river and have a good picnic day; (4)People are standing on a side of a rock wall.\n(A) 4->2->1->3\n(B) 2->1->4->3\n(C) 2->4->1->3\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2036, "doc": {"video_name": "order_141.mp4", "duration": 518.43, "question": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 3->4->1->2\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 2->1->4->3\n", "candidates": ["3->4->1->2", "1->2->3->4", "4->3->2->1", "2->1->4->3"], "answer": "D", "task_type": "order", "question_id": "Q2036"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a2e94c32a9fed742f18acb24c2be3a316f8432bd180050a878985d0f50853ad", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 3->4->1->2\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 2->1->4->3\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 3->4->1->2\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 2->1->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2040, "doc": {"video_name": "order_57.mp4", "duration": 672.89, "question": "Arrange the following events from the video in the correct chronological order: (1)The boy begins hopping on the squares, starting from his driveway; (2)The girl joins him near the sidewalk and walks along his side as he hops across the squares; (3)He hops till he reaches the end of the sidewalk which marks the end of the hopscotch squares; (4)After he's done hopping he smiles and begins walking back.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "candidates": ["1->2->3->4", "4->3->2->1", "3->2->1->4", "2->1->3->4"], "answer": "A", "task_type": "order", "question_id": "Q2040"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ffbf51d1926ca741d5655cce63d1f3d5b805e946ad65ebfb1ae46224dae5a3a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The boy begins hopping on the squares, starting from his driveway; (2)The girl joins him near the sidewalk and walks along his side as he hops across the squares; (3)He hops till he reaches the end of the sidewalk which marks the end of the hopscotch squares; (4)After he's done hopping he smiles and begins walking back.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The boy begins hopping on the squares, starting from his driveway; (2)The girl joins him near the sidewalk and walks along his side as he hops across the squares; (3)He hops till he reaches the end of the sidewalk which marks the end of the hopscotch squares; (4)After he's done hopping he smiles and begins walking back.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 3->2->1->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2044, "doc": {"video_name": "order_54.mp4", "duration": 665.54, "question": "Arrange the following events from the video in the correct chronological order: (1)A man picks up the baby from the pool; (2)A person carries two bags out of a house; (3)A baby falls into the swimming pool; (4)A dog walks out of a house.\n(A) 3->1->2->4\n(B) 4->2->1->3\n(C) 2->4->3->1\n(D) 1->3->2->4\n", "candidates": ["3->1->2->4", "4->2->1->3", "2->4->3->1", "1->3->2->4"], "answer": "C", "task_type": "order", "question_id": "Q2044"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4fed34e42a426659a36b0eb401901ab99a5f78f1aa0f435ec389b1600679e803", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A man picks up the baby from the pool; (2)A person carries two bags out of a house; (3)A baby falls into the swimming pool; (4)A dog walks out of a house.\n(A) 3->1->2->4\n(B) 4->2->1->3\n(C) 2->4->3->1\n(D) 1->3->2->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A man picks up the baby from the pool; (2)A person carries two bags out of a house; (3)A baby falls into the swimming pool; (4)A dog walks out of a house.\n(A) 3->1->2->4\n(B) 4->2->1->3\n(C) 2->4->3->1\n(D) 1->3->2->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2048, "doc": {"video_name": "order_51.mp4", "duration": 665.78, "question": "Arrange the following events from the video in the correct chronological order: (1)A guy sits and talks inside; (2)A man surfs on a body of water; (3)A lady spins on skates; (4)The credits of the video are shown.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "candidates": ["2->1->3->4", "3->2->1->4", "4->3->2->1", "1->2->3->4"], "answer": "D", "task_type": "order", "question_id": "Q2048"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cd84d81fb2451a58b753427b8f91ce49978663af9fb694e8b6084191ee5db66e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A guy sits and talks inside; (2)A man surfs on a body of water; (3)A lady spins on skates; (4)The credits of the video are shown.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "A", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A guy sits and talks inside; (2)A man surfs on a body of water; (3)A lady spins on skates; (4)The credits of the video are shown.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2052, "doc": {"video_name": "order_284.mp4", "duration": 593.3299999999999, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) milking cow --> shredding paper --> carving pumpkin --> pole vault\n(B) pole vault --> shredding paper --> carving pumpkin --> milking cow\n(C) carving pumpkin --> milking cow --> shredding paper --> pole vault\n(D) pole vault --> milking cow --> shredding paper --> carving pumpkin\n", "candidates": ["milking cow --> shredding paper --> carving pumpkin --> pole vault", "pole vault --> shredding paper --> carving pumpkin --> milking cow", "carving pumpkin --> milking cow --> shredding paper --> pole vault", "pole vault --> milking cow --> shredding paper --> carving pumpkin"], "answer": "B", "task_type": "order", "question_id": "Q2052"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7350f14db4c5f7c5c592e0baf4180b3915f019a604495c1a0f89d8245fae67cc", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) milking cow --> shredding paper --> carving pumpkin --> pole vault\n(B) pole vault --> shredding paper --> carving pumpkin --> milking cow\n(C) carving pumpkin --> milking cow --> shredding paper --> pole vault\n(D) pole vault --> milking cow --> shredding paper --> carving pumpkin\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) milking cow --> shredding paper --> carving pumpkin --> pole vault\n(B) pole vault --> shredding paper --> carving pumpkin --> milking cow\n(C) carving pumpkin --> milking cow --> shredding paper --> pole vault\n(D) pole vault --> milking cow --> shredding paper --> carving pumpkin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2056, "doc": {"video_name": "order_111.mp4", "duration": 675.0699999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "3->2->1->4", "1->2->3->4", "2->1->3->4"], "answer": "C", "task_type": "order", "question_id": "Q2056"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4677d4d5dc730fb53a9055b734516f512bd93efa66417cd871ba790732bf7a71", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2060, "doc": {"video_name": "order_88.mp4", "duration": 705.1400000000001, "question": "Arrange the following events from the video in the correct chronological order: (1) The slack line athletes finish competing and shake hands at the finish; (2) A bald headed man performs tricks on a yellow trick line; (3) Two men meet to compete in a slack line competition; (4) The camera pans back to the bald man performing trick on the slack line and back to the man wearing a baseball cap performing tricks on the traditional line.\n(A) 2->3->1->4\n(B) 1->2->3->4\n(C) 3->2->4->1\n(D) 4->3->2->1\n", "candidates": ["2->3->1->4", "1->2->3->4", "3->2->4->1", "4->3->2->1"], "answer": "C", "task_type": "order", "question_id": "Q2060"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "647c5db4f81a289ef39a46e3392688b3e7b0251e859a5a5633c91b1ef68b6731", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) The slack line athletes finish competing and shake hands at the finish; (2) A bald headed man performs tricks on a yellow trick line; (3) Two men meet to compete in a slack line competition; (4) The camera pans back to the bald man performing trick on the slack line and back to the man wearing a baseball cap performing tricks on the traditional line.\n(A) 2->3->1->4\n(B) 1->2->3->4\n(C) 3->2->4->1\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1) The slack line athletes finish competing and shake hands at the finish; (2) A bald headed man performs tricks on a yellow trick line; (3) Two men meet to compete in a slack line competition; (4) The camera pans back to the bald man performing trick on the slack line and back to the man wearing a baseball cap performing tricks on the traditional line.\n(A) 2->3->1->4\n(B) 1->2->3->4\n(C) 3->2->4->1\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2064, "doc": {"video_name": "order_86.mp4", "duration": 673.98, "question": "Arrange the following events from the video in the correct chronological order: (1)The man shows how to work the monkey bars; (2)A male fitness trainer from Iron Edge is about to demonstrate various workouts using bars; (3)The trainer shows how to maneuver straight bar, pulleys, and medicine ball; (4)A workout regimen is displayed as part of the conclusion of the demonstration.\n(A) 3->2->1->4\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "candidates": ["3->2->1->4", "4->3->2->1", "2->1->3->4", "1->2->3->4"], "answer": "C", "task_type": "order", "question_id": "Q2064"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "acea07737985f08453b6f147dc73201f33ea5d0b674959b6ec5dd6ee072ff182", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man shows how to work the monkey bars; (2)A male fitness trainer from Iron Edge is about to demonstrate various workouts using bars; (3)The trainer shows how to maneuver straight bar, pulleys, and medicine ball; (4)A workout regimen is displayed as part of the conclusion of the demonstration.\n(A) 3->2->1->4\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man shows how to work the monkey bars; (2)A male fitness trainer from Iron Edge is about to demonstrate various workouts using bars; (3)The trainer shows how to maneuver straight bar, pulleys, and medicine ball; (4)A workout regimen is displayed as part of the conclusion of the demonstration.\n(A) 3->2->1->4\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2068, "doc": {"video_name": "order_84.mp4", "duration": 673.98, "question": "Arrange the following events from the video in the correct chronological order: (1)The man shows how to work the monkey bars; (2)A male fitness trainer from Iron Edge is about to demonstrate various workouts using bars; (3)The trainer shows how to maneuver straight bar, pulleys, and medicine ball; (4)A workout regimen is displayed as part of the conclusion of the demonstration.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "1->2->3->4", "3->2->1->4", "2->1->3->4"], "answer": "D", "task_type": "order", "question_id": "Q2068"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2a4cd7c9621cbb43a4255cff97d6e7d89a1a2520f309264637a3dedbd869ff74", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man shows how to work the monkey bars; (2)A male fitness trainer from Iron Edge is about to demonstrate various workouts using bars; (3)The trainer shows how to maneuver straight bar, pulleys, and medicine ball; (4)A workout regimen is displayed as part of the conclusion of the demonstration.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man shows how to work the monkey bars; (2)A male fitness trainer from Iron Edge is about to demonstrate various workouts using bars; (3)The trainer shows how to maneuver straight bar, pulleys, and medicine ball; (4)A workout regimen is displayed as part of the conclusion of the demonstration.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2072, "doc": {"video_name": "order_247.mp4", "duration": 396.33, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cooking sausages --> jetskiing --> cleaning toilet --> abseiling\n(B) abseiling --> jetskiing --> cooking sausages --> cleaning toilet\n(C) jetskiing --> abseiling --> cleaning toilet --> cooking sausages\n(D) abseiling --> cooking sausages --> cleaning toilet --> jetskiing\n", "candidates": ["cooking sausages --> jetskiing --> cleaning toilet --> abseiling", "abseiling --> jetskiing --> cooking sausages --> cleaning toilet", "jetskiing --> abseiling --> cleaning toilet --> cooking sausages", "abseiling --> cooking sausages --> cleaning toilet --> jetskiing"], "answer": "C", "task_type": "order", "question_id": "Q2072"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ce7006ec709e576a2485d1b3f2acea916051858ab466a47c3c7d6afcf315c02c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cooking sausages --> jetskiing --> cleaning toilet --> abseiling\n(B) abseiling --> jetskiing --> cooking sausages --> cleaning toilet\n(C) jetskiing --> abseiling --> cleaning toilet --> cooking sausages\n(D) abseiling --> cooking sausages --> cleaning toilet --> jetskiing\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cooking sausages --> jetskiing --> cleaning toilet --> abseiling\n(B) abseiling --> jetskiing --> cooking sausages --> cleaning toilet\n(C) jetskiing --> abseiling --> cleaning toilet --> cooking sausages\n(D) abseiling --> cooking sausages --> cleaning toilet --> jetskiing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2076, "doc": {"video_name": "order_254.mp4", "duration": 463.04999999999995, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) clean and jerk --> cooking sausages --> playing harp --> making jewelry\n(B) cooking sausages --> making jewelry --> clean and jerk --> playing harp\n(C) playing harp --> cooking sausages --> clean and jerk --> making jewelry\n(D) cooking sausages --> making jewelry --> playing harp --> clean and jerk\n", "candidates": ["clean and jerk --> cooking sausages --> playing harp --> making jewelry", "cooking sausages --> making jewelry --> clean and jerk --> playing harp", "playing harp --> cooking sausages --> clean and jerk --> making jewelry", "cooking sausages --> making jewelry --> playing harp --> clean and jerk"], "answer": "C", "task_type": "order", "question_id": "Q2076"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1bc7347a1be1555dffdc440a3c7fcc627909dadb6c3d9659539783ba07a573c9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) clean and jerk --> cooking sausages --> playing harp --> making jewelry\n(B) cooking sausages --> making jewelry --> clean and jerk --> playing harp\n(C) playing harp --> cooking sausages --> clean and jerk --> making jewelry\n(D) cooking sausages --> making jewelry --> playing harp --> clean and jerk\n", "task_type": "order", "pred_answer": "D", "answer": "C"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) clean and jerk --> cooking sausages --> playing harp --> making jewelry\n(B) cooking sausages --> making jewelry --> clean and jerk --> playing harp\n(C) playing harp --> cooking sausages --> clean and jerk --> making jewelry\n(D) cooking sausages --> making jewelry --> playing harp --> clean and jerk\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2080, "doc": {"video_name": "order_1.mp4", "duration": 665.74, "question": "Arrange the following events from the video in the correct chronological order: (1)A young and a kid are doing balance in a balance rope; (2)People are walking in a bridge to see a competition of men doing tricks on top of a balance rope; (3)A man is jumping and doing tricks in a balance rope above a cold river; (4)The boy is in a competition in snowy path doing tricks on a balance rope with people behind a fence watching him.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 3->2->1->4\n", "candidates": ["4->3->2->1", "1->2->3->4", "2->1->3->4", "3->2->1->4"], "answer": "B", "task_type": "order", "question_id": "Q2080"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07c35de5c803dfa04dcde1a43e4572d1dab69eb1951e14c39551aee31e8ff734", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A young and a kid are doing balance in a balance rope; (2)People are walking in a bridge to see a competition of men doing tricks on top of a balance rope; (3)A man is jumping and doing tricks in a balance rope above a cold river; (4)The boy is in a competition in snowy path doing tricks on a balance rope with people behind a fence watching him.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 3->2->1->4\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A young and a kid are doing balance in a balance rope; (2)People are walking in a bridge to see a competition of men doing tricks on top of a balance rope; (3)A man is jumping and doing tricks in a balance rope above a cold river; (4)The boy is in a competition in snowy path doing tricks on a balance rope with people behind a fence watching him.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 3->2->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2084, "doc": {"video_name": "order_46.mp4", "duration": 673.61, "question": "Arrange the following events from the video in the correct chronological order: (1)The man in red cap stands outside a barbershop talking; (2)The man pretends to be asleep during his haircut; (3)The man points out the cameras and explains it to the barber; (4)The man appears to fall out of the chair.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 1->2->4->3\n(D) 1->3->2->4\n", "candidates": ["2->1->3->4", "3->2->1->4", "1->2->4->3", "1->3->2->4"], "answer": "C", "task_type": "order", "question_id": "Q2084"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "45adfc51f2b0b906940c055c430953ba10551b09bb1f52a8b68c8ee4a6c28696", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man in red cap stands outside a barbershop talking; (2)The man pretends to be asleep during his haircut; (3)The man points out the cameras and explains it to the barber; (4)The man appears to fall out of the chair.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 1->2->4->3\n(D) 1->3->2->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man in red cap stands outside a barbershop talking; (2)The man pretends to be asleep during his haircut; (3)The man points out the cameras and explains it to the barber; (4)The man appears to fall out of the chair.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 1->2->4->3\n(D) 1->3->2->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2088, "doc": {"video_name": "order_43.mp4", "duration": 677.76, "question": "Arrange the following events from the video in the correct chronological order: (1)The bowler from the blue team hits an overhand ball to the batter; (2)The bowler raises his hand to claim that the batter has not made a run; (3)The bowler causes the batter to get out by hitting the stumps behind him, the entire team cheers; (4)The batter walks out and another batter from his team comes on the field.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 3->2->1->4\n(D) 4->3->2->1\n", "candidates": ["1->2->3->4", "2->1->3->4", "3->2->1->4", "4->3->2->1"], "answer": "A", "task_type": "order", "question_id": "Q2088"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e90ea2ca55945050f5d9c2d4e6f4e4f2e224cae008daf95e30fe9270ad0364ff", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The bowler from the blue team hits an overhand ball to the batter; (2)The bowler raises his hand to claim that the batter has not made a run; (3)The bowler causes the batter to get out by hitting the stumps behind him, the entire team cheers; (4)The batter walks out and another batter from his team comes on the field.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 3->2->1->4\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The bowler from the blue team hits an overhand ball to the batter; (2)The bowler raises his hand to claim that the batter has not made a run; (3)The bowler causes the batter to get out by hitting the stumps behind him, the entire team cheers; (4)The batter walks out and another batter from his team comes on the field.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 3->2->1->4\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2092, "doc": {"video_name": "order_162.mp4", "duration": 502.59999999999997, "question": "Arrange the following events from the video in the correct chronological order: (1)The instructor finishes the class; (2)A woman's indoor aerobic class is in process; (3)The logo 'Zumba Toning' appears on screen; (4)The camera briefly scans to the mirrored wall and then back to the class.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 3->2->4->1\n(D) 4->3->2->1\n", "candidates": ["2->1->3->4", "1->2->3->4", "3->2->4->1", "4->3->2->1"], "answer": "C", "task_type": "order", "question_id": "Q2092"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5922cd2b93a220b74b39294dccf33fad55eff291bd8c5ad9c9bdb1633a38c4d7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The instructor finishes the class; (2)A woman's indoor aerobic class is in process; (3)The logo 'Zumba Toning' appears on screen; (4)The camera briefly scans to the mirrored wall and then back to the class.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 3->2->4->1\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The instructor finishes the class; (2)A woman's indoor aerobic class is in process; (3)The logo 'Zumba Toning' appears on screen; (4)The camera briefly scans to the mirrored wall and then back to the class.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 3->2->4->1\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2096, "doc": {"video_name": "order_154.mp4", "duration": 511.90999999999997, "question": "Arrange the following events from the video in the correct chronological order: (1)The boy begins hopping on the squares, starting from his driveway; (2)The girl joins him near the sidewalk and walks along his side as he hops across the squares; (3)He hops till he reaches the end of the sidewalk which marks the end of the hopscotch squares; (4)After he's done hopping he smiles and begins walking back.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "candidates": ["4->3->2->1", "3->2->1->4", "2->1->3->4", "1->2->3->4"], "answer": "D", "task_type": "order", "question_id": "Q2096"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d97e8087491108cacc659b660a54dd89076518eb5e7e8ec28f27d4ffc578396", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The boy begins hopping on the squares, starting from his driveway; (2)The girl joins him near the sidewalk and walks along his side as he hops across the squares; (3)He hops till he reaches the end of the sidewalk which marks the end of the hopscotch squares; (4)After he's done hopping he smiles and begins walking back.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The boy begins hopping on the squares, starting from his driveway; (2)The girl joins him near the sidewalk and walks along his side as he hops across the squares; (3)He hops till he reaches the end of the sidewalk which marks the end of the hopscotch squares; (4)After he's done hopping he smiles and begins walking back.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 2->1->3->4\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2100, "doc": {"video_name": "order_242.mp4", "duration": 647.76, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) clean and jerk --> shredding paper --> cooking sausages --> paragliding\n(B) cooking sausages --> shredding paper --> clean and jerk --> paragliding\n(C) shredding paper --> paragliding --> cooking sausages --> clean and jerk\n(D) paragliding --> shredding paper --> cooking sausages --> clean and jerk\n", "candidates": ["clean and jerk --> shredding paper --> cooking sausages --> paragliding", "cooking sausages --> shredding paper --> clean and jerk --> paragliding", "shredding paper --> paragliding --> cooking sausages --> clean and jerk", "paragliding --> shredding paper --> cooking sausages --> clean and jerk"], "answer": "B", "task_type": "order", "question_id": "Q2100"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c42ec4b965dbe07b2aa13e10e79730c2051d2ddecf84de484756eca527e53eb6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) clean and jerk --> shredding paper --> cooking sausages --> paragliding\n(B) cooking sausages --> shredding paper --> clean and jerk --> paragliding\n(C) shredding paper --> paragliding --> cooking sausages --> clean and jerk\n(D) paragliding --> shredding paper --> cooking sausages --> clean and jerk\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) clean and jerk --> shredding paper --> cooking sausages --> paragliding\n(B) cooking sausages --> shredding paper --> clean and jerk --> paragliding\n(C) shredding paper --> paragliding --> cooking sausages --> clean and jerk\n(D) paragliding --> shredding paper --> cooking sausages --> clean and jerk\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2104, "doc": {"video_name": "order_298.mp4", "duration": 490.03, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> shredding paper --> milking cow --> stomping grapes\n(B) milking cow --> stomping grapes --> riding mule --> shredding paper\n(C) milking cow --> riding mule --> stomping grapes --> shredding paper\n(D) stomping grapes --> milking cow --> shredding paper --> riding mule\n", "candidates": ["riding mule --> shredding paper --> milking cow --> stomping grapes", "milking cow --> stomping grapes --> riding mule --> shredding paper", "milking cow --> riding mule --> stomping grapes --> shredding paper", "stomping grapes --> milking cow --> shredding paper --> riding mule"], "answer": "A", "task_type": "order", "question_id": "Q2104"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "588e9db687d1236a836736f965bbaf9afd2f334695c6edf2b900ac7a6271ec8b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> shredding paper --> milking cow --> stomping grapes\n(B) milking cow --> stomping grapes --> riding mule --> shredding paper\n(C) milking cow --> riding mule --> stomping grapes --> shredding paper\n(D) stomping grapes --> milking cow --> shredding paper --> riding mule\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> shredding paper --> milking cow --> stomping grapes\n(B) milking cow --> stomping grapes --> riding mule --> shredding paper\n(C) milking cow --> riding mule --> stomping grapes --> shredding paper\n(D) stomping grapes --> milking cow --> shredding paper --> riding mule\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2108, "doc": {"video_name": "order_231.mp4", "duration": 588.06, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> baking cookies --> milking cow --> making jewelry\n(B) making jewelry --> cooking sausages --> milking cow --> baking cookies\n(C) cooking sausages --> making jewelry --> baking cookies --> milking cow\n(D) baking cookies --> making jewelry --> milking cow --> cooking sausages\n", "candidates": ["cooking sausages --> baking cookies --> milking cow --> making jewelry", "making jewelry --> cooking sausages --> milking cow --> baking cookies", "cooking sausages --> making jewelry --> baking cookies --> milking cow", "baking cookies --> making jewelry --> milking cow --> cooking sausages"], "answer": "A", "task_type": "order", "question_id": "Q2108"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "677010c662cfe98f6a7ef733e5c30c469cab3983ceb48fa0096e21587a858327", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> baking cookies --> milking cow --> making jewelry\n(B) making jewelry --> cooking sausages --> milking cow --> baking cookies\n(C) cooking sausages --> making jewelry --> baking cookies --> milking cow\n(D) baking cookies --> making jewelry --> milking cow --> cooking sausages\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> baking cookies --> milking cow --> making jewelry\n(B) making jewelry --> cooking sausages --> milking cow --> baking cookies\n(C) cooking sausages --> making jewelry --> baking cookies --> milking cow\n(D) baking cookies --> making jewelry --> milking cow --> cooking sausages\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2112, "doc": {"video_name": "order_220.mp4", "duration": 490.04, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> water sliding --> abseiling --> jetskiing\n(B) abseiling --> baking cookies --> jetskiing --> water sliding\n(C) jetskiing --> water sliding --> abseiling --> baking cookies\n(D) abseiling --> jetskiing --> water sliding --> baking cookies\n", "candidates": ["baking cookies --> water sliding --> abseiling --> jetskiing", "abseiling --> baking cookies --> jetskiing --> water sliding", "jetskiing --> water sliding --> abseiling --> baking cookies", "abseiling --> jetskiing --> water sliding --> baking cookies"], "answer": "A", "task_type": "order", "question_id": "Q2112"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "23d851dedbfc70915a6bbfbdf0b12296bbf7075373d74b22e290a3c7406fbcd3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> water sliding --> abseiling --> jetskiing\n(B) abseiling --> baking cookies --> jetskiing --> water sliding\n(C) jetskiing --> water sliding --> abseiling --> baking cookies\n(D) abseiling --> jetskiing --> water sliding --> baking cookies\n", "task_type": "order", "pred_answer": "D", "answer": "A"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> water sliding --> abseiling --> jetskiing\n(B) abseiling --> baking cookies --> jetskiing --> water sliding\n(C) jetskiing --> water sliding --> abseiling --> baking cookies\n(D) abseiling --> jetskiing --> water sliding --> baking cookies\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2116, "doc": {"video_name": "order_349.mp4", "duration": 280.06, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> making jewelry --> zumba --> jetskiing\n(B) jetskiing --> making jewelry --> zumba --> baking cookies\n(C) jetskiing --> baking cookies --> making jewelry --> zumba\n(D) baking cookies --> zumba --> jetskiing --> making jewelry\n", "candidates": ["baking cookies --> making jewelry --> zumba --> jetskiing", "jetskiing --> making jewelry --> zumba --> baking cookies", "jetskiing --> baking cookies --> making jewelry --> zumba", "baking cookies --> zumba --> jetskiing --> making jewelry"], "answer": "A", "task_type": "order", "question_id": "Q2116"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3afd27fd58ca2bebb538eb8addfe5918dcdebd9ecd715fa684ebf07ab41b86ca", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> making jewelry --> zumba --> jetskiing\n(B) jetskiing --> making jewelry --> zumba --> baking cookies\n(C) jetskiing --> baking cookies --> making jewelry --> zumba\n(D) baking cookies --> zumba --> jetskiing --> making jewelry\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> making jewelry --> zumba --> jetskiing\n(B) jetskiing --> making jewelry --> zumba --> baking cookies\n(C) jetskiing --> baking cookies --> making jewelry --> zumba\n(D) baking cookies --> zumba --> jetskiing --> making jewelry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2120, "doc": {"video_name": "order_268.mp4", "duration": 663.8199999999999, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> clean and jerk --> riding mule --> pole vault\n(B) clean and jerk --> tossing coin --> pole vault --> riding mule\n(C) pole vault --> clean and jerk --> riding mule --> tossing coin\n(D) clean and jerk --> tossing coin --> riding mule --> pole vault\n", "candidates": ["tossing coin --> clean and jerk --> riding mule --> pole vault", "clean and jerk --> tossing coin --> pole vault --> riding mule", "pole vault --> clean and jerk --> riding mule --> tossing coin", "clean and jerk --> tossing coin --> riding mule --> pole vault"], "answer": "D", "task_type": "order", "question_id": "Q2120"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "348b4078c011e9b5ca1eaec5096aaf9ecc5882f42c59dc2d7372b7fed9cf05b5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> clean and jerk --> riding mule --> pole vault\n(B) clean and jerk --> tossing coin --> pole vault --> riding mule\n(C) pole vault --> clean and jerk --> riding mule --> tossing coin\n(D) clean and jerk --> tossing coin --> riding mule --> pole vault\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> clean and jerk --> riding mule --> pole vault\n(B) clean and jerk --> tossing coin --> pole vault --> riding mule\n(C) pole vault --> clean and jerk --> riding mule --> tossing coin\n(D) clean and jerk --> tossing coin --> riding mule --> pole vault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2124, "doc": {"video_name": "order_113.mp4", "duration": 675.0699999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 4->3->2->1\n", "candidates": ["2->1->3->4", "3->2->1->4", "1->2->3->4", "4->3->2->1"], "answer": "C", "task_type": "order", "question_id": "Q2124"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f7cbf0cdf5c4af947339b7770dc2bf7da8cd6ea2150db5d57e20f9d8b37ba84", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2128, "doc": {"video_name": "order_201.mp4", "duration": 489.08, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> milking cow --> cooking sausages --> javelin throw\n(B) carving pumpkin --> cooking sausages --> milking cow --> javelin throw\n(C) carving pumpkin --> cooking sausages --> javelin throw --> milking cow\n(D) carving pumpkin --> milking cow --> javelin throw --> cooking sausages\n", "candidates": ["carving pumpkin --> milking cow --> cooking sausages --> javelin throw", "carving pumpkin --> cooking sausages --> milking cow --> javelin throw", "carving pumpkin --> cooking sausages --> javelin throw --> milking cow", "carving pumpkin --> milking cow --> javelin throw --> cooking sausages"], "answer": "C", "task_type": "order", "question_id": "Q2128"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d618fe5d5f2c9934bb061dbcb8207d0e47584b86a5d4d0939e12dd7f9704a8d9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> milking cow --> cooking sausages --> javelin throw\n(B) carving pumpkin --> cooking sausages --> milking cow --> javelin throw\n(C) carving pumpkin --> cooking sausages --> javelin throw --> milking cow\n(D) carving pumpkin --> milking cow --> javelin throw --> cooking sausages\n", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> milking cow --> cooking sausages --> javelin throw\n(B) carving pumpkin --> cooking sausages --> milking cow --> javelin throw\n(C) carving pumpkin --> cooking sausages --> javelin throw --> milking cow\n(D) carving pumpkin --> milking cow --> javelin throw --> cooking sausages\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2132, "doc": {"video_name": "order_120.mp4", "duration": 673.38, "question": "Arrange the following events from the video in the correct chronological order: (1)The interviewer plays with the dogs; (2)A man has dogs on a city street near a car; (3)We see a title screen over the UK flag; (4)We see a banner across the bottom of the screen and the man kneeling playing with his dogs.\n(A) 3->2->1->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 2->1->3->4\n", "candidates": ["3->2->1->4", "1->2->3->4", "4->3->2->1", "2->1->3->4"], "answer": "A", "task_type": "order", "question_id": "Q2132"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4560238a7b4f4e0189d3b992ed7a745cfd3db197426fe0f81c505c580ede5f31", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The interviewer plays with the dogs; (2)A man has dogs on a city street near a car; (3)We see a title screen over the UK flag; (4)We see a banner across the bottom of the screen and the man kneeling playing with his dogs.\n(A) 3->2->1->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The interviewer plays with the dogs; (2)A man has dogs on a city street near a car; (3)We see a title screen over the UK flag; (4)We see a banner across the bottom of the screen and the man kneeling playing with his dogs.\n(A) 3->2->1->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2136, "doc": {"video_name": "order_292.mp4", "duration": 490.03999999999996, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> cooking sausages --> playing trombone --> stomping grapes\n(B) stomping grapes --> carving pumpkin --> cooking sausages --> playing trombone\n(C) playing trombone --> cooking sausages --> stomping grapes --> carving pumpkin\n(D) carving pumpkin --> playing trombone --> cooking sausages --> stomping grapes\n", "candidates": ["carving pumpkin --> cooking sausages --> playing trombone --> stomping grapes", "stomping grapes --> carving pumpkin --> cooking sausages --> playing trombone", "playing trombone --> cooking sausages --> stomping grapes --> carving pumpkin", "carving pumpkin --> playing trombone --> cooking sausages --> stomping grapes"], "answer": "C", "task_type": "order", "question_id": "Q2136"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ea211724b8f65665a6c6d8aadd7a741dc044b2dc132878297e46092b808c76b2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> cooking sausages --> playing trombone --> stomping grapes\n(B) stomping grapes --> carving pumpkin --> cooking sausages --> playing trombone\n(C) playing trombone --> cooking sausages --> stomping grapes --> carving pumpkin\n(D) carving pumpkin --> playing trombone --> cooking sausages --> stomping grapes\n", "task_type": "order", "pred_answer": "D", "answer": "C"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> cooking sausages --> playing trombone --> stomping grapes\n(B) stomping grapes --> carving pumpkin --> cooking sausages --> playing trombone\n(C) playing trombone --> cooking sausages --> stomping grapes --> carving pumpkin\n(D) carving pumpkin --> playing trombone --> cooking sausages --> stomping grapes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2140, "doc": {"video_name": "order_319.mp4", "duration": 728.06, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) milking cow --> riding mule --> paragliding --> cooking sausages\n(B) cooking sausages --> riding mule --> paragliding --> milking cow\n(C) milking cow --> riding mule --> cooking sausages --> paragliding\n(D) cooking sausages --> milking cow --> paragliding --> riding mule\n", "candidates": ["milking cow --> riding mule --> paragliding --> cooking sausages", "cooking sausages --> riding mule --> paragliding --> milking cow", "milking cow --> riding mule --> cooking sausages --> paragliding", "cooking sausages --> milking cow --> paragliding --> riding mule"], "answer": "B", "task_type": "order", "question_id": "Q2140"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "78c3bf0c26f3ae2fe18dc44aa58e15bd05b72e0dc9fc0a2379555c1ede0b08f9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) milking cow --> riding mule --> paragliding --> cooking sausages\n(B) cooking sausages --> riding mule --> paragliding --> milking cow\n(C) milking cow --> riding mule --> cooking sausages --> paragliding\n(D) cooking sausages --> milking cow --> paragliding --> riding mule\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) milking cow --> riding mule --> paragliding --> cooking sausages\n(B) cooking sausages --> riding mule --> paragliding --> milking cow\n(C) milking cow --> riding mule --> cooking sausages --> paragliding\n(D) cooking sausages --> milking cow --> paragliding --> riding mule\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2144, "doc": {"video_name": "order_238.mp4", "duration": 490.03, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) riding mule --> milking cow --> stomping grapes --> pole vault\n(B) stomping grapes --> milking cow --> pole vault --> riding mule\n(C) riding mule --> milking cow --> pole vault --> stomping grapes\n(D) milking cow --> pole vault --> riding mule --> stomping grapes\n", "candidates": ["riding mule --> milking cow --> stomping grapes --> pole vault", "stomping grapes --> milking cow --> pole vault --> riding mule", "riding mule --> milking cow --> pole vault --> stomping grapes", "milking cow --> pole vault --> riding mule --> stomping grapes"], "answer": "B", "task_type": "order", "question_id": "Q2144"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f40890c527f13c76841a16695ae188d94284a03965882b7b6aaa1fcd8d65b75a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) riding mule --> milking cow --> stomping grapes --> pole vault\n(B) stomping grapes --> milking cow --> pole vault --> riding mule\n(C) riding mule --> milking cow --> pole vault --> stomping grapes\n(D) milking cow --> pole vault --> riding mule --> stomping grapes\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) riding mule --> milking cow --> stomping grapes --> pole vault\n(B) stomping grapes --> milking cow --> pole vault --> riding mule\n(C) riding mule --> milking cow --> pole vault --> stomping grapes\n(D) milking cow --> pole vault --> riding mule --> stomping grapes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2148, "doc": {"video_name": "order_308.mp4", "duration": 481.04999999999995, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) pole vault --> making jewelry --> javelin throw --> zumba\n(B) making jewelry --> zumba --> pole vault --> javelin throw\n(C) making jewelry --> javelin throw --> zumba --> pole vault\n(D) making jewelry --> pole vault --> zumba --> javelin throw\n", "candidates": ["pole vault --> making jewelry --> javelin throw --> zumba", "making jewelry --> zumba --> pole vault --> javelin throw", "making jewelry --> javelin throw --> zumba --> pole vault", "making jewelry --> pole vault --> zumba --> javelin throw"], "answer": "B", "task_type": "order", "question_id": "Q2148"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c73fa1b2b8d36d6e1c28bf7f4663aab401b748e621a171bb6dbb9b9e5333f19a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) pole vault --> making jewelry --> javelin throw --> zumba\n(B) making jewelry --> zumba --> pole vault --> javelin throw\n(C) making jewelry --> javelin throw --> zumba --> pole vault\n(D) making jewelry --> pole vault --> zumba --> javelin throw\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) pole vault --> making jewelry --> javelin throw --> zumba\n(B) making jewelry --> zumba --> pole vault --> javelin throw\n(C) making jewelry --> javelin throw --> zumba --> pole vault\n(D) making jewelry --> pole vault --> zumba --> javelin throw\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2152, "doc": {"video_name": "order_237.mp4", "duration": 487.61999999999995, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) stomping grapes --> milking cow --> making jewelry --> water sliding\n(B) water sliding --> stomping grapes --> making jewelry --> milking cow\n(C) milking cow --> making jewelry --> stomping grapes --> water sliding\n(D) milking cow --> water sliding --> making jewelry --> stomping grapes\n", "candidates": ["stomping grapes --> milking cow --> making jewelry --> water sliding", "water sliding --> stomping grapes --> making jewelry --> milking cow", "milking cow --> making jewelry --> stomping grapes --> water sliding", "milking cow --> water sliding --> making jewelry --> stomping grapes"], "answer": "A", "task_type": "order", "question_id": "Q2152"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "40f8a05f7498a6d279946fd465c4acf285010525e9d4a5d1547b044e73b3bda1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) stomping grapes --> milking cow --> making jewelry --> water sliding\n(B) water sliding --> stomping grapes --> making jewelry --> milking cow\n(C) milking cow --> making jewelry --> stomping grapes --> water sliding\n(D) milking cow --> water sliding --> making jewelry --> stomping grapes\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) stomping grapes --> milking cow --> making jewelry --> water sliding\n(B) water sliding --> stomping grapes --> making jewelry --> milking cow\n(C) milking cow --> making jewelry --> stomping grapes --> water sliding\n(D) milking cow --> water sliding --> making jewelry --> stomping grapes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2156, "doc": {"video_name": "order_28.mp4", "duration": 679.71, "question": "Arrange the following events from the video in the correct chronological order: (1) Cheese is sprinkled on the spaghetti; (2) A plate of spaghetti is shown; (3) Vegetables are added to the pot; (4) All of the contents get mixed and cooked.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 2->3->1->4\n", "candidates": ["4->3->2->1", "1->2->3->4", "2->1->3->4", "2->3->1->4"], "answer": "C", "task_type": "order", "question_id": "Q2156"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d64630c95e94d4d8623f8c90ef1bcd4c29568c4d9de14f1ab09d3a2acd9389f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) Cheese is sprinkled on the spaghetti; (2) A plate of spaghetti is shown; (3) Vegetables are added to the pot; (4) All of the contents get mixed and cooked.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 2->3->1->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1) Cheese is sprinkled on the spaghetti; (2) A plate of spaghetti is shown; (3) Vegetables are added to the pot; (4) All of the contents get mixed and cooked.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 2->3->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2160, "doc": {"video_name": "order_25.mp4", "duration": 718.0699999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)Two people are paddling down rapids on a river in canoes; (2)One of them stops at a bank where there is a person in a blue canoe; (3)People are seen in a group large red tube rapids ride; (4)They pass by a building and then fall into the water.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "candidates": ["2->1->3->4", "3->2->1->4", "4->3->2->1", "1->2->3->4"], "answer": "D", "task_type": "order", "question_id": "Q2160"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "188dc5ec85cf7198b22dc17b5eceaa5647023df6edbb9aa1ff57f255dfa2c0a5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Two people are paddling down rapids on a river in canoes; (2)One of them stops at a bank where there is a person in a blue canoe; (3)People are seen in a group large red tube rapids ride; (4)They pass by a building and then fall into the water.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Two people are paddling down rapids on a river in canoes; (2)One of them stops at a bank where there is a person in a blue canoe; (3)People are seen in a group large red tube rapids ride; (4)They pass by a building and then fall into the water.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2164, "doc": {"video_name": "order_314.mp4", "duration": 507.07, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) water sliding --> shredding paper --> pole vault --> milking cow\n(B) milking cow --> pole vault --> shredding paper --> water sliding\n(C) pole vault --> water sliding --> milking cow --> shredding paper\n(D) milking cow --> pole vault --> water sliding --> shredding paper\n", "candidates": ["water sliding --> shredding paper --> pole vault --> milking cow", "milking cow --> pole vault --> shredding paper --> water sliding", "pole vault --> water sliding --> milking cow --> shredding paper", "milking cow --> pole vault --> water sliding --> shredding paper"], "answer": "B", "task_type": "order", "question_id": "Q2164"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6f2573d8d748768e76168e6eee0800aeef16579371cd76cbabc8c5c2b7fe7247", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) water sliding --> shredding paper --> pole vault --> milking cow\n(B) milking cow --> pole vault --> shredding paper --> water sliding\n(C) pole vault --> water sliding --> milking cow --> shredding paper\n(D) milking cow --> pole vault --> water sliding --> shredding paper\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) water sliding --> shredding paper --> pole vault --> milking cow\n(B) milking cow --> pole vault --> shredding paper --> water sliding\n(C) pole vault --> water sliding --> milking cow --> shredding paper\n(D) milking cow --> pole vault --> water sliding --> shredding paper\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2168, "doc": {"video_name": "order_321.mp4", "duration": 284.06999999999994, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) making jewelry --> cleaning toilet --> paragliding --> carving pumpkin\n(B) paragliding --> making jewelry --> cleaning toilet --> carving pumpkin\n(C) carving pumpkin --> cleaning toilet --> making jewelry --> paragliding\n(D) paragliding --> cleaning toilet --> carving pumpkin --> making jewelry\n", "candidates": ["making jewelry --> cleaning toilet --> paragliding --> carving pumpkin", "paragliding --> making jewelry --> cleaning toilet --> carving pumpkin", "carving pumpkin --> cleaning toilet --> making jewelry --> paragliding", "paragliding --> cleaning toilet --> carving pumpkin --> making jewelry"], "answer": "B", "task_type": "order", "question_id": "Q2168"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4efae0c6593f7615a15ae0d4ec2914c2388c43258b17b97ab827d9fbb3b13c0d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) making jewelry --> cleaning toilet --> paragliding --> carving pumpkin\n(B) paragliding --> making jewelry --> cleaning toilet --> carving pumpkin\n(C) carving pumpkin --> cleaning toilet --> making jewelry --> paragliding\n(D) paragliding --> cleaning toilet --> carving pumpkin --> making jewelry\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) making jewelry --> cleaning toilet --> paragliding --> carving pumpkin\n(B) paragliding --> making jewelry --> cleaning toilet --> carving pumpkin\n(C) carving pumpkin --> cleaning toilet --> making jewelry --> paragliding\n(D) paragliding --> cleaning toilet --> carving pumpkin --> making jewelry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2172, "doc": {"video_name": "order_207.mp4", "duration": 681.05, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) stomping grapes --> tossing coin --> carving pumpkin --> shredding paper\n(B) tossing coin --> carving pumpkin --> shredding paper --> stomping grapes\n(C) tossing coin --> shredding paper --> carving pumpkin --> stomping grapes\n(D) shredding paper --> carving pumpkin --> stomping grapes --> tossing coin\n", "candidates": ["stomping grapes --> tossing coin --> carving pumpkin --> shredding paper", "tossing coin --> carving pumpkin --> shredding paper --> stomping grapes", "tossing coin --> shredding paper --> carving pumpkin --> stomping grapes", "shredding paper --> carving pumpkin --> stomping grapes --> tossing coin"], "answer": "A", "task_type": "order", "question_id": "Q2172"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "087e118c860868b5d65540d3ba5cb2a218643136740c3ab97e384c80fdb056a7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) stomping grapes --> tossing coin --> carving pumpkin --> shredding paper\n(B) tossing coin --> carving pumpkin --> shredding paper --> stomping grapes\n(C) tossing coin --> shredding paper --> carving pumpkin --> stomping grapes\n(D) shredding paper --> carving pumpkin --> stomping grapes --> tossing coin\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) stomping grapes --> tossing coin --> carving pumpkin --> shredding paper\n(B) tossing coin --> carving pumpkin --> shredding paper --> stomping grapes\n(C) tossing coin --> shredding paper --> carving pumpkin --> stomping grapes\n(D) shredding paper --> carving pumpkin --> stomping grapes --> tossing coin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1, "doc": {"video_name": "needle_38.mp4", "duration": 1506.62, "question": "What is a popular activity on the beach during the monsoon season?\n(A) Picnicking\n(B) Surfing\n(C) Playing volleyball\n(D) Fishing\n", "candidates": ["Picnicking", "Surfing", "Playing volleyball", "Fishing"], "answer": "D", "task_type": "needle", "question_id": "Q1"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f598e438583c2c50c1a9548d6d31fe4a402e539626969431cc63b60aea15d352", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is a popular activity on the beach during the monsoon season?\n(A) Picnicking\n(B) Surfing\n(C) Playing volleyball\n(D) Fishing\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is a popular activity on the beach during the monsoon season?\n(A) Picnicking\n(B) Surfing\n(C) Playing volleyball\n(D) Fishing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 5, "doc": {"video_name": "needle_126.mp4", "duration": 6008.08, "question": "What color is the water on the tropical beach in the video?\n(A) Dark blue\n(B) Clear\n(C) Green\n(D) Azure blue\n", "candidates": ["Dark blue", "Clear", "Green", "Azure blue"], "answer": "D", "task_type": "needle", "question_id": "Q5"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7639af350939b44ff984b5b8cf51fa210a9640cbb970c6a5dcc6cf6b724f425d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the water on the tropical beach in the video?\n(A) Dark blue\n(B) Clear\n(C) Green\n(D) Azure blue\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What color is the water on the tropical beach in the video?\n(A) Dark blue\n(B) Clear\n(C) Green\n(D) Azure blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 9, "doc": {"video_name": "needle_36.mp4", "duration": 478.7, "question": "What type of view is provided of the tropical beach in the video?\n(A) Close-up view\n(B) Bottom-up view\n(C) Top down aerial view\n(D) Side view\n", "candidates": ["Close-up view", "Bottom-up view", "Top down aerial view", "Side view"], "answer": "C", "task_type": "needle", "question_id": "Q9"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "066a84d0768748daabd243c2e524eba5909fe52e48d6f0dea3050164357a88a8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of view is provided of the tropical beach in the video?\n(A) Close-up view\n(B) Bottom-up view\n(C) Top down aerial view\n(D) Side view\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What type of view is provided of the tropical beach in the video?\n(A) Close-up view\n(B) Bottom-up view\n(C) Top down aerial view\n(D) Side view\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 13, "doc": {"video_name": "needle_138.mp4", "duration": 762.02, "question": "What are the two young women wearing in the street?\n(A) Trendy summer clothes\n(B) Swimwear\n(C) Formal attire\n(D) Winter clothes\n", "candidates": ["Trendy summer clothes", "Swimwear", "Formal attire", "Winter clothes"], "answer": "A", "task_type": "needle", "question_id": "Q13"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68eef52e9ab0472062e9621568b4ee28c048c3929a87d49c1f066e8b77a185bc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What are the two young women wearing in the street?\n(A) Trendy summer clothes\n(B) Swimwear\n(C) Formal attire\n(D) Winter clothes\n", "task_type": "needle", "pred_answer": "A.", "answer": "A"}, "input": "What are the two young women wearing in the street?\n(A) Trendy summer clothes\n(B) Swimwear\n(C) Formal attire\n(D) Winter clothes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 17, "doc": {"video_name": "needle_19.mp4", "duration": 464.0, "question": "What is the setting of the segment where people tourists are walking?\n(A) A quiet forest\n(B) A bustling city\n(C) A beautiful fishing village\n(D) A crowded beach\n", "candidates": ["A quiet forest", "A bustling city", "A beautiful fishing village", "A crowded beach"], "answer": "C", "task_type": "needle", "question_id": "Q17"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5acdcd7a17fc24b4565df90d8e7642f1376703307bd9d4ccfeaddc7383b34690", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the setting of the segment where people tourists are walking?\n(A) A quiet forest\n(B) A bustling city\n(C) A beautiful fishing village\n(D) A crowded beach\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "What is the setting of the segment where people tourists are walking?\n(A) A quiet forest\n(B) A bustling city\n(C) A beautiful fishing village\n(D) A crowded beach\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 21, "doc": {"video_name": "needle_19.mp4", "duration": 464.0, "question": "What activity are the people tourists engaged in at the fishing village?\n(A) Fishing\n(B) Sightseeing\n(C) Swimming\n(D) Walking the embankment\n", "candidates": ["Fishing", "Sightseeing", "Swimming", "Walking the embankment"], "answer": "D", "task_type": "needle", "question_id": "Q21"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8e779bd52c0ba0cffcc2ab46a19af06cd22f768a08147430737a6472dfb7b755", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What activity are the people tourists engaged in at the fishing village?\n(A) Fishing\n(B) Sightseeing\n(C) Swimming\n(D) Walking the embankment\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What activity are the people tourists engaged in at the fishing village?\n(A) Fishing\n(B) Sightseeing\n(C) Swimming\n(D) Walking the embankment\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 25, "doc": {"video_name": "needle_146.mp4", "duration": 399.38, "question": "What tool is being used by the dentist and his assistant while treating the patient's teeth in the video segment?\n(A) Dental drill\n(B) Face bow\n(C) Dental scaler\n(D) Dental mirror\n", "candidates": ["Dental drill", "Face bow", "Dental scaler", "Dental mirror"], "answer": "B", "task_type": "needle", "question_id": "Q25"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "71f55cb160e7079675a42b8fc8993fa112e426d9d5d9f8dd959783fe1397bcd4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What tool is being used by the dentist and his assistant while treating the patient's teeth in the video segment?\n(A) Dental drill\n(B) Face bow\n(C) Dental scaler\n(D) Dental mirror\n", "task_type": "needle", "pred_answer": "A", "answer": "B"}, "input": "What tool is being used by the dentist and his assistant while treating the patient's teeth in the video segment?\n(A) Dental drill\n(B) Face bow\n(C) Dental scaler\n(D) Dental mirror\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 29, "doc": {"video_name": "needle_4.mp4", "duration": 510.0, "question": "What season is it when the man in the black silhouette is on the lake shore?\n(A) Summer\n(B) Winter\n(C) Autumn\n(D) Spring\n", "candidates": ["Summer", "Winter", "Autumn", "Spring"], "answer": "A", "task_type": "needle", "question_id": "Q29"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4e94f050d19f5e78c976270ba4b6ece35ec17560cbe8868a6080d06be074c626", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What season is it when the man in the black silhouette is on the lake shore?\n(A) Summer\n(B) Winter\n(C) Autumn\n(D) Spring\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What season is it when the man in the black silhouette is on the lake shore?\n(A) Summer\n(B) Winter\n(C) Autumn\n(D) Spring\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 33, "doc": {"video_name": "needle_29.mp4", "duration": 461.57, "question": "Who is the little girl sitting on while they read a children's book?\n(A) Her mother\n(B) Her sister\n(C) Her father\n(D) Her grandmother\n", "candidates": ["Her mother", "Her sister", "Her father", "Her grandmother"], "answer": "D", "task_type": "needle", "question_id": "Q33"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "189a48254a42db6ea09a9188a460877988dbb0e949913f7bc7ee61bd43efd766", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Who is the little girl sitting on while they read a children's book?\n(A) Her mother\n(B) Her sister\n(C) Her father\n(D) Her grandmother\n", "task_type": "needle", "pred_answer": "A", "answer": "D"}, "input": "Who is the little girl sitting on while they read a children's book?\n(A) Her mother\n(B) Her sister\n(C) Her father\n(D) Her grandmother\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 37, "doc": {"video_name": "needle_120.mp4", "duration": 341.5, "question": "What is the activity involving the little girl and her grandmother?\n(A) They are painting\n(B) They are gardening\n(C) They are cooking\n(D) They are reading a children's book\n", "candidates": ["They are painting", "They are gardening", "They are cooking", "They are reading a children's book"], "answer": "D", "task_type": "needle", "question_id": "Q37"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c4a54a10fb7e055276cc0c2fe744505f6b78e7338c16e722b5a4c04603e7054c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the activity involving the little girl and her grandmother?\n(A) They are painting\n(B) They are gardening\n(C) They are cooking\n(D) They are reading a children's book\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the activity involving the little girl and her grandmother?\n(A) They are painting\n(B) They are gardening\n(C) They are cooking\n(D) They are reading a children's book\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 41, "doc": {"video_name": "needle_92.mp4", "duration": 458.13, "question": "What is the woman wearing during the summer sunset?\n(A) A winter coat\n(B) A dress and heels\n(C) A swimsuit\n(D) A hat and sunglasses\n", "candidates": ["A winter coat", "A dress and heels", "A swimsuit", "A hat and sunglasses"], "answer": "D", "task_type": "needle", "question_id": "Q41"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D."]], "filtered_resps": ["D."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "42e4c26c37dab66227bcc3b967e2ed3efb02e15a9a96d96805b9d32af4927074", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the woman wearing during the summer sunset?\n(A) A winter coat\n(B) A dress and heels\n(C) A swimsuit\n(D) A hat and sunglasses\n", "task_type": "needle", "pred_answer": "D.", "answer": "D"}, "input": "What is the woman wearing during the summer sunset?\n(A) A winter coat\n(B) A dress and heels\n(C) A swimsuit\n(D) A hat and sunglasses\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 45, "doc": {"video_name": "needle_116.mp4", "duration": 343.07, "question": "What type of vehicle is on the side of the desert highway for green screen or chroma key?\n(A) A motorcycle\n(B) An SUV\n(C) A sedan\n(D) A pickup truck\n", "candidates": ["A motorcycle", "An SUV", "A sedan", "A pickup truck"], "answer": "B", "task_type": "needle", "question_id": "Q45"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ad1fd5a915550a41fc78225a72363b2dc49c1eadb69796b26b2d037a63d10881", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of vehicle is on the side of the desert highway for green screen or chroma key?\n(A) A motorcycle\n(B) An SUV\n(C) A sedan\n(D) A pickup truck\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What type of vehicle is on the side of the desert highway for green screen or chroma key?\n(A) A motorcycle\n(B) An SUV\n(C) A sedan\n(D) A pickup truck\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 49, "doc": {"video_name": "needle_94.mp4", "duration": 857.22, "question": "What is the female potter's position while stirring paint?\n(A) Lying down\n(B) Kneeling\n(C) Sitting\n(D) Standing\n", "candidates": ["Lying down", "Kneeling", "Sitting", "Standing"], "answer": "C", "task_type": "needle", "question_id": "Q49"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a06de62b0010e3b2e67623c070933b594800af546cf43b73b148397aec3fc814", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the female potter's position while stirring paint?\n(A) Lying down\n(B) Kneeling\n(C) Sitting\n(D) Standing\n", "task_type": "needle", "pred_answer": "D", "answer": "C"}, "input": "What is the female potter's position while stirring paint?\n(A) Lying down\n(B) Kneeling\n(C) Sitting\n(D) Standing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 53, "doc": {"video_name": "needle_76.mp4", "duration": 1522.71, "question": "What is the chef doing with the lobster in the dinner preparation?\n(A) Boiling\n(B) Grilling\n(C) Baking\n(D) Cutting\n", "candidates": ["Boiling", "Grilling", "Baking", "Cutting"], "answer": "D", "task_type": "needle", "question_id": "Q53"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "39a1706b744b58c84329de80f93a8047d481a4bc79b7588139362d26ad8356f2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the chef doing with the lobster in the dinner preparation?\n(A) Boiling\n(B) Grilling\n(C) Baking\n(D) Cutting\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the chef doing with the lobster in the dinner preparation?\n(A) Boiling\n(B) Grilling\n(C) Baking\n(D) Cutting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 57, "doc": {"video_name": "needle_42.mp4", "duration": 470.05, "question": "What is the purpose of the SUV being on the side of the desert highway?\n(A) For a car show\n(B) For a road trip\n(C) For a car race\n(D) For green screen or chroma key\n", "candidates": ["For a car show", "For a road trip", "For a car race", "For green screen or chroma key"], "answer": "D", "task_type": "needle", "question_id": "Q57"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3606e64819809cd1cae228744a25adc888b443e6716243a6757b8e41028812ae", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the purpose of the SUV being on the side of the desert highway?\n(A) For a car show\n(B) For a road trip\n(C) For a car race\n(D) For green screen or chroma key\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the purpose of the SUV being on the side of the desert highway?\n(A) For a car show\n(B) For a road trip\n(C) For a car race\n(D) For green screen or chroma key\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 61, "doc": {"video_name": "needle_115.mp4", "duration": 6559.77, "question": "What is the team of scientists doing in the laboratory room?\n(A) Conducting a physics experiment\n(B) Researching sample test with protection equipment and glasses\n(C) Teaching a class\n(D) Having a meeting\n", "candidates": ["Conducting a physics experiment", "Researching sample test with protection equipment and glasses", "Teaching a class", "Having a meeting"], "answer": "B", "task_type": "needle", "question_id": "Q61"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ccc05b86a2761fefc7ad2ede9954b9cc053066c5f4abe6db33d9bb2554e229e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the team of scientists doing in the laboratory room?\n(A) Conducting a physics experiment\n(B) Researching sample test with protection equipment and glasses\n(C) Teaching a class\n(D) Having a meeting\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the team of scientists doing in the laboratory room?\n(A) Conducting a physics experiment\n(B) Researching sample test with protection equipment and glasses\n(C) Teaching a class\n(D) Having a meeting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 65, "doc": {"video_name": "needle_138.mp4", "duration": 762.02, "question": "What accessory are the two young women wearing while sitting on the bench in the street?\n(A) Sunglasses\n(B) Gloves\n(C) Hats\n(D) Scarves\n", "candidates": ["Sunglasses", "Gloves", "Hats", "Scarves"], "answer": "A", "task_type": "needle", "question_id": "Q65"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ca4824226a13a0610311456b74d5d79ec03fe7b3b0c7a87572173f2abc7daf5f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What accessory are the two young women wearing while sitting on the bench in the street?\n(A) Sunglasses\n(B) Gloves\n(C) Hats\n(D) Scarves\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What accessory are the two young women wearing while sitting on the bench in the street?\n(A) Sunglasses\n(B) Gloves\n(C) Hats\n(D) Scarves\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 69, "doc": {"video_name": "needle_40.mp4", "duration": 477.20000000000005, "question": "What is the nature of the den where the American toad is sitting in the video?\n(A) Water cavity\n(B) Wooden cavity\n(C) Stone cavity\n(D) Earthen cavity\n", "candidates": ["Water cavity", "Wooden cavity", "Stone cavity", "Earthen cavity"], "answer": "D", "task_type": "needle", "question_id": "Q69"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "46ecd9fefeb448732e59985571a93a7cd1416d37ffbb78c7f414fa39aaf098a6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the nature of the den where the American toad is sitting in the video?\n(A) Water cavity\n(B) Wooden cavity\n(C) Stone cavity\n(D) Earthen cavity\n", "task_type": "needle", "pred_answer": "A", "answer": "D"}, "input": "What is the nature of the den where the American toad is sitting in the video?\n(A) Water cavity\n(B) Wooden cavity\n(C) Stone cavity\n(D) Earthen cavity\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 73, "doc": {"video_name": "needle_132.mp4", "duration": 558.0699999999999, "question": "What is the young beautiful woman looking at while preparing for the new year?\n(A) She is looking at the presents\n(B) She is looking at the golden christmas toy\n(C) She is looking at the christmas lights\n(D) She is looking at the christmas stockings\n", "candidates": ["She is looking at the presents", "She is looking at the golden christmas toy", "She is looking at the christmas lights", "She is looking at the christmas stockings"], "answer": "B", "task_type": "needle", "question_id": "Q73"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7d57861d80d609f4c7cf12332ab216a2fc5058c36dcc8f7f871d25166a3b66ae", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the young beautiful woman looking at while preparing for the new year?\n(A) She is looking at the presents\n(B) She is looking at the golden christmas toy\n(C) She is looking at the christmas lights\n(D) She is looking at the christmas stockings\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the young beautiful woman looking at while preparing for the new year?\n(A) She is looking at the presents\n(B) She is looking at the golden christmas toy\n(C) She is looking at the christmas lights\n(D) She is looking at the christmas stockings\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 77, "doc": {"video_name": "needle_49.mp4", "duration": 460.01, "question": "What is the child doing at home in the video?\n(A) Playing with toys\n(B) Playing game on mobile phone\n(C) Sleeping\n(D) Eating\n", "candidates": ["Playing with toys", "Playing game on mobile phone", "Sleeping", "Eating"], "answer": "B", "task_type": "needle", "question_id": "Q77"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "70869a658e66f756ab2eeb99f553ce9a461300b1589d870496068e3b33381dce", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the child doing at home in the video?\n(A) Playing with toys\n(B) Playing game on mobile phone\n(C) Sleeping\n(D) Eating\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the child doing at home in the video?\n(A) Playing with toys\n(B) Playing game on mobile phone\n(C) Sleeping\n(D) Eating\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 81, "doc": {"video_name": "needle_17.mp4", "duration": 867.89, "question": "What is the owner wearing while stroking his Jack Russell dog?\n(A) Flip flops\n(B) Sports boots\n(C) Sneakers\n(D) Barefoot\n", "candidates": ["Flip flops", "Sports boots", "Sneakers", "Barefoot"], "answer": "B", "task_type": "needle", "question_id": "Q81"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "028f1035f39de962b8e2cea9c29042b631295298c4cb64a85080c04d0eb65e13", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the owner wearing while stroking his Jack Russell dog?\n(A) Flip flops\n(B) Sports boots\n(C) Sneakers\n(D) Barefoot\n", "task_type": "needle", "pred_answer": "A", "answer": "B"}, "input": "What is the owner wearing while stroking his Jack Russell dog?\n(A) Flip flops\n(B) Sports boots\n(C) Sneakers\n(D) Barefoot\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 85, "doc": {"video_name": "needle_16.mp4", "duration": 467.08, "question": "What does the Jack Russell dog want to do while being stroked by the owner in sports boots?\n(A) Run away\n(B) Play fetch\n(C) Eat\n(D) Sleep\n", "candidates": ["Run away", "Play fetch", "Eat", "Sleep"], "answer": "A", "task_type": "needle", "question_id": "Q85"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1e55e36a87f0a38d53a41ba73527c52e2830a09ef96ec6f2a93f0496cea2b304", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What does the Jack Russell dog want to do while being stroked by the owner in sports boots?\n(A) Run away\n(B) Play fetch\n(C) Eat\n(D) Sleep\n", "task_type": "needle", "pred_answer": "C", "answer": "A"}, "input": "What does the Jack Russell dog want to do while being stroked by the owner in sports boots?\n(A) Run away\n(B) Play fetch\n(C) Eat\n(D) Sleep\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 89, "doc": {"video_name": "needle_86.mp4", "duration": 470.40999999999997, "question": "What is the mood of the boy walking in the water with colorful party balloons?\n(A) Happy\n(B) Sad\n(C) Confused\n(D) Angry\n", "candidates": ["Happy", "Sad", "Confused", "Angry"], "answer": "A", "task_type": "needle", "question_id": "Q89"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "44a6c39cebd2424763ac6dd3b33dacfdb99682ef720eda94b743c5c1ba855a75", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the mood of the boy walking in the water with colorful party balloons?\n(A) Happy\n(B) Sad\n(C) Confused\n(D) Angry\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the mood of the boy walking in the water with colorful party balloons?\n(A) Happy\n(B) Sad\n(C) Confused\n(D) Angry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 93, "doc": {"video_name": "needle_59.mp4", "duration": 460.02000000000004, "question": "What procedure is the female employee of the cosmetology clinic performing on the patient?\n(A) Haircut\n(B) Microblasting\n(C) Manicure\n(D) Facial\n", "candidates": ["Haircut", "Microblasting", "Manicure", "Facial"], "answer": "B", "task_type": "needle", "question_id": "Q93"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0574ddac3d09d7b5d5e2ac795af6b873caa6160b86711788e0768501cc31eb04", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What procedure is the female employee of the cosmetology clinic performing on the patient?\n(A) Haircut\n(B) Microblasting\n(C) Manicure\n(D) Facial\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What procedure is the female employee of the cosmetology clinic performing on the patient?\n(A) Haircut\n(B) Microblasting\n(C) Manicure\n(D) Facial\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 97, "doc": {"video_name": "needle_54.mp4", "duration": 469.72, "question": "What is the older mature businesswoman holding while working on corporate strategy in office?\n(A) A cup of coffee\n(B) A mobile phone\n(C) A laptop\n(D) A marker\n", "candidates": ["A cup of coffee", "A mobile phone", "A laptop", "A marker"], "answer": "D", "task_type": "needle", "question_id": "Q97"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "36430df452dc290ffdc018da962e490607e1bed9b8b6f4cd2fb6ec5970920b46", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the older mature businesswoman holding while working on corporate strategy in office?\n(A) A cup of coffee\n(B) A mobile phone\n(C) A laptop\n(D) A marker\n", "task_type": "needle", "pred_answer": "A", "answer": "D"}, "input": "What is the older mature businesswoman holding while working on corporate strategy in office?\n(A) A cup of coffee\n(B) A mobile phone\n(C) A laptop\n(D) A marker\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 101, "doc": {"video_name": "needle_32.mp4", "duration": 467.98, "question": "What is the woman's appearance?\n(A) She is a middle-aged woman\n(B) She is an elderly woman\n(C) She is a beautiful woman\n(D) She is a young girl\n", "candidates": ["She is a middle-aged woman", "She is an elderly woman", "She is a beautiful woman", "She is a young girl"], "answer": "C", "task_type": "needle", "question_id": "Q101"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "71fed19d76e13f4ec7a9a233496bf5e37e732e2a4da32d418926cd991fffdcc5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the woman's appearance?\n(A) She is a middle-aged woman\n(B) She is an elderly woman\n(C) She is a beautiful woman\n(D) She is a young girl\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "What is the woman's appearance?\n(A) She is a middle-aged woman\n(B) She is an elderly woman\n(C) She is a beautiful woman\n(D) She is a young girl\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 105, "doc": {"video_name": "needle_63.mp4", "duration": 467.04999999999995, "question": "What type of christmas toy is the young beautiful woman hanging on the christmas tree?\n(A) A silver christmas toy\n(B) A golden christmas toy\n(C) A red christmas toy\n(D) A blue christmas toy\n", "candidates": ["A silver christmas toy", "A golden christmas toy", "A red christmas toy", "A blue christmas toy"], "answer": "B", "task_type": "needle", "question_id": "Q105"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5a61b8d91c31bb4b900b59157d0d9cae08ec4e26bec5d887309a8540283f5642", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of christmas toy is the young beautiful woman hanging on the christmas tree?\n(A) A silver christmas toy\n(B) A golden christmas toy\n(C) A red christmas toy\n(D) A blue christmas toy\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What type of christmas toy is the young beautiful woman hanging on the christmas tree?\n(A) A silver christmas toy\n(B) A golden christmas toy\n(C) A red christmas toy\n(D) A blue christmas toy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 109, "doc": {"video_name": "needle_40.mp4", "duration": 477.20000000000005, "question": "What is the American toad's strategy to avoid detection in the video?\n(A) Camouflage\n(B) Running away\n(C) Fighting\n(D) Hiding\n", "candidates": ["Camouflage", "Running away", "Fighting", "Hiding"], "answer": "A", "task_type": "needle", "question_id": "Q109"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "269138569abdebf99f1695467c12dcac04daa11fd4650c449b542ccd4f1dfc73", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the American toad's strategy to avoid detection in the video?\n(A) Camouflage\n(B) Running away\n(C) Fighting\n(D) Hiding\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the American toad's strategy to avoid detection in the video?\n(A) Camouflage\n(B) Running away\n(C) Fighting\n(D) Hiding\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 113, "doc": {"video_name": "needle_25.mp4", "duration": 866.49, "question": "What is the habitat of the blue fin trevally shown in the video?\n(A) Mangrove forest\n(B) Freshwater river\n(C) Open ocean\n(D) Coral reef\n", "candidates": ["Mangrove forest", "Freshwater river", "Open ocean", "Coral reef"], "answer": "D", "task_type": "needle", "question_id": "Q113"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dc4d05ba2c0f3441cfda182a51607192d3c0ec74ab5b0584f6e33f82dcfed84e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the habitat of the blue fin trevally shown in the video?\n(A) Mangrove forest\n(B) Freshwater river\n(C) Open ocean\n(D) Coral reef\n", "task_type": "needle", "pred_answer": "C", "answer": "D"}, "input": "What is the habitat of the blue fin trevally shown in the video?\n(A) Mangrove forest\n(B) Freshwater river\n(C) Open ocean\n(D) Coral reef\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 117, "doc": {"video_name": "needle_25.mp4", "duration": 866.49, "question": "What is the prey of the blue fin trevally in the video?\n(A) Jellyfish\n(B) Small reef fish and anthias\n(C) Plankton\n(D) Seaweed\n", "candidates": ["Jellyfish", "Small reef fish and anthias", "Plankton", "Seaweed"], "answer": "B", "task_type": "needle", "question_id": "Q117"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "194df23bdff9fd2e48e0e16a63626ceeafa1f8a2e52098941a14c265c1b132d1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the prey of the blue fin trevally in the video?\n(A) Jellyfish\n(B) Small reef fish and anthias\n(C) Plankton\n(D) Seaweed\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the prey of the blue fin trevally in the video?\n(A) Jellyfish\n(B) Small reef fish and anthias\n(C) Plankton\n(D) Seaweed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 121, "doc": {"video_name": "needle_60.mp4", "duration": 496.97999999999996, "question": "What is the senior businessman doing by the sea shore?\n(A) Reading a book\n(B) Having a serious conversation on the cell phone\n(C) Swimming in the sea\n(D) Eating lunch\n", "candidates": ["Reading a book", "Having a serious conversation on the cell phone", "Swimming in the sea", "Eating lunch"], "answer": "B", "task_type": "needle", "question_id": "Q121"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "108872f3b69f5a0706711e39b80ca1bb1cc025b288d69450bafcd43f4e47fcff", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the senior businessman doing by the sea shore?\n(A) Reading a book\n(B) Having a serious conversation on the cell phone\n(C) Swimming in the sea\n(D) Eating lunch\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the senior businessman doing by the sea shore?\n(A) Reading a book\n(B) Having a serious conversation on the cell phone\n(C) Swimming in the sea\n(D) Eating lunch\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 125, "doc": {"video_name": "needle_23.mp4", "duration": 487.29, "question": "On what surface is the chef cutting the fresh orange pumpkin?\n(A) Marble counter\n(B) Plastic cutting board\n(C) Wooden table\n(D) Stainless steel table\n", "candidates": ["Marble counter", "Plastic cutting board", "Wooden table", "Stainless steel table"], "answer": "C", "task_type": "needle", "question_id": "Q125"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a903c5547f7268dd89dd434db9f469a65759154f1c0c95fc2275bf77354c4546", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "On what surface is the chef cutting the fresh orange pumpkin?\n(A) Marble counter\n(B) Plastic cutting board\n(C) Wooden table\n(D) Stainless steel table\n", "task_type": "needle", "pred_answer": "A", "answer": "C"}, "input": "On what surface is the chef cutting the fresh orange pumpkin?\n(A) Marble counter\n(B) Plastic cutting board\n(C) Wooden table\n(D) Stainless steel table\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 129, "doc": {"video_name": "needle_34.mp4", "duration": 455.36, "question": "Who are using the face bow in the video segment?\n(A) The assistant alone\n(B) The dentist alone\n(C) The patient\n(D) The dentist and his assistant\n", "candidates": ["The assistant alone", "The dentist alone", "The patient", "The dentist and his assistant"], "answer": "D", "task_type": "needle", "question_id": "Q129"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5e06022889e6eb95ced1f147ccc46883babc5f6f8763ef2779239727add939bd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Who are using the face bow in the video segment?\n(A) The assistant alone\n(B) The dentist alone\n(C) The patient\n(D) The dentist and his assistant\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "Who are using the face bow in the video segment?\n(A) The assistant alone\n(B) The dentist alone\n(C) The patient\n(D) The dentist and his assistant\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 133, "doc": {"video_name": "needle_58.mp4", "duration": 459.98, "question": "What is the profession of the woman performing the microblasting routine in the video?\n(A) Dentist\n(B) Cosmetologist\n(C) Nurse\n(D) Hair Stylist\n", "candidates": ["Dentist", "Cosmetologist", "Nurse", "Hair Stylist"], "answer": "B", "task_type": "needle", "question_id": "Q133"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c10f8e6949599c42f5b6cd5f61a66ec102c6e6d4db2846c01e06200e9d11b8f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the profession of the woman performing the microblasting routine in the video?\n(A) Dentist\n(B) Cosmetologist\n(C) Nurse\n(D) Hair Stylist\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the profession of the woman performing the microblasting routine in the video?\n(A) Dentist\n(B) Cosmetologist\n(C) Nurse\n(D) Hair Stylist\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 137, "doc": {"video_name": "needle_36.mp4", "duration": 478.7, "question": "What color is the water on the tropical beach in the video?\n(A) Azure blue\n(B) Green\n(C) Dark blue\n(D) Clear\n", "candidates": ["Azure blue", "Green", "Dark blue", "Clear"], "answer": "A", "task_type": "needle", "question_id": "Q137"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "380b9eb27c6bf406bd8469285ad7f2ebbae2d2943945727a0bbc52c76ad4eb7f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the water on the tropical beach in the video?\n(A) Azure blue\n(B) Green\n(C) Dark blue\n(D) Clear\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What color is the water on the tropical beach in the video?\n(A) Azure blue\n(B) Green\n(C) Dark blue\n(D) Clear\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 141, "doc": {"video_name": "needle_99.mp4", "duration": 866.3299999999999, "question": "What time of day is it when the young girl in a tracksuit is doing yoga in the park?\n(A) Midday\n(B) Sunset\n(C) Night\n(D) Morning\n", "candidates": ["Midday", "Sunset", "Night", "Morning"], "answer": "B", "task_type": "needle", "question_id": "Q141"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a94dd256f94da3d12b3263841df0959ee8a5b125629c1d930ee2c288b086f3d5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What time of day is it when the young girl in a tracksuit is doing yoga in the park?\n(A) Midday\n(B) Sunset\n(C) Night\n(D) Morning\n", "task_type": "needle", "pred_answer": "D", "answer": "B"}, "input": "What time of day is it when the young girl in a tracksuit is doing yoga in the park?\n(A) Midday\n(B) Sunset\n(C) Night\n(D) Morning\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 145, "doc": {"video_name": "needle_106.mp4", "duration": 744.44, "question": "Where are the young mother and her son decorating the Christmas tree in the video?\n(A) At a shopping mall\n(B) Indoors\n(C) Outdoors\n(D) In a park\n", "candidates": ["At a shopping mall", "Indoors", "Outdoors", "In a park"], "answer": "C", "task_type": "needle", "question_id": "Q145"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dec338cf0080e009cef07f823db9f7c40bfe49b30c17c2d8bebec2138776445a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where are the young mother and her son decorating the Christmas tree in the video?\n(A) At a shopping mall\n(B) Indoors\n(C) Outdoors\n(D) In a park\n", "task_type": "needle", "pred_answer": "B", "answer": "C"}, "input": "Where are the young mother and her son decorating the Christmas tree in the video?\n(A) At a shopping mall\n(B) Indoors\n(C) Outdoors\n(D) In a park\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 149, "doc": {"video_name": "needle_75.mp4", "duration": 505.0, "question": "What are the two young women wearing in the street?\n(A) Trendy summer clothes\n(B) Swimwear\n(C) Formal attire\n(D) Winter clothes\n", "candidates": ["Trendy summer clothes", "Swimwear", "Formal attire", "Winter clothes"], "answer": "A", "task_type": "needle", "question_id": "Q149"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68eef52e9ab0472062e9621568b4ee28c048c3929a87d49c1f066e8b77a185bc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What are the two young women wearing in the street?\n(A) Trendy summer clothes\n(B) Swimwear\n(C) Formal attire\n(D) Winter clothes\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What are the two young women wearing in the street?\n(A) Trendy summer clothes\n(B) Swimwear\n(C) Formal attire\n(D) Winter clothes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 153, "doc": {"video_name": "needle_13.mp4", "duration": 493.15, "question": "What are the volunteers searching for during the simulation drill?\n(A) Hidden treasure\n(B) Rocket attack casualties\n(C) Lost items\n(D) Survivors of a natural disaster\n", "candidates": ["Hidden treasure", "Rocket attack casualties", "Lost items", "Survivors of a natural disaster"], "answer": "B", "task_type": "needle", "question_id": "Q153"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "438637ae482a994c5e466f0891a03f2a71aba5a30cb99260b574bf9a3ae32040", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What are the volunteers searching for during the simulation drill?\n(A) Hidden treasure\n(B) Rocket attack casualties\n(C) Lost items\n(D) Survivors of a natural disaster\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What are the volunteers searching for during the simulation drill?\n(A) Hidden treasure\n(B) Rocket attack casualties\n(C) Lost items\n(D) Survivors of a natural disaster\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 157, "doc": {"video_name": "needle_141.mp4", "duration": 7054.2699999999995, "question": "What season is it when the little Asian girl in a bikini is creating sand piles on the beach at sunset?\n(A) Winter\n(B) Spring\n(C) Autumn\n(D) Summer\n", "candidates": ["Winter", "Spring", "Autumn", "Summer"], "answer": "D", "task_type": "needle", "question_id": "Q157"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b620e43e00c49b21e9e5397b1bb37398e3fbfb957dc1f2ef7bbaa478b2e65307", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What season is it when the little Asian girl in a bikini is creating sand piles on the beach at sunset?\n(A) Winter\n(B) Spring\n(C) Autumn\n(D) Summer\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What season is it when the little Asian girl in a bikini is creating sand piles on the beach at sunset?\n(A) Winter\n(B) Spring\n(C) Autumn\n(D) Summer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 161, "doc": {"video_name": "needle_143.mp4", "duration": 624.8699999999999, "question": "What is the woman's reaction when the hand comes out of the computer?\n(A) She screams\n(B) She laughs\n(C) She is surprised\n(D) She is not surprised\n", "candidates": ["She screams", "She laughs", "She is surprised", "She is not surprised"], "answer": "C", "task_type": "needle", "question_id": "Q161"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C) She is surprised"]], "filtered_resps": ["C) She is surprised"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3adaed4c308986c621e67e284b13cdb15e2b996ed5fede9e5a75eda3865e3abb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the woman's reaction when the hand comes out of the computer?\n(A) She screams\n(B) She laughs\n(C) She is surprised\n(D) She is not surprised\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the woman's reaction when the hand comes out of the computer?\n(A) She screams\n(B) She laughs\n(C) She is surprised\n(D) She is not surprised\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 165, "doc": {"video_name": "needle_26.mp4", "duration": 469.08, "question": "Where are the black cows with yellow tags on their ears eating hay on a sunny day?\n(A) In the barn\n(B) At the farm\n(C) In the forest\n(D) In the field\n", "candidates": ["In the barn", "At the farm", "In the forest", "In the field"], "answer": "B", "task_type": "needle", "question_id": "Q165"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e7b492de1693564117ce83c648f67357ada1158c1b0a22e2fbe7c1bd35b9dec2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where are the black cows with yellow tags on their ears eating hay on a sunny day?\n(A) In the barn\n(B) At the farm\n(C) In the forest\n(D) In the field\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Where are the black cows with yellow tags on their ears eating hay on a sunny day?\n(A) In the barn\n(B) At the farm\n(C) In the forest\n(D) In the field\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 169, "doc": {"video_name": "needle_88.mp4", "duration": 456.0, "question": "Where is the Goldman Sachs Group logo displayed?\n(A) On a billboard\n(B) On a laptop\n(C) On the screen in a meeting room\n(D) On a mobile phone\n", "candidates": ["On a billboard", "On a laptop", "On the screen in a meeting room", "On a mobile phone"], "answer": "C", "task_type": "needle", "question_id": "Q169"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f8bf354e55b983be2309ce1c41619b2b206df2bbc2954acf44cad4b0daf9db69", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the Goldman Sachs Group logo displayed?\n(A) On a billboard\n(B) On a laptop\n(C) On the screen in a meeting room\n(D) On a mobile phone\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "Where is the Goldman Sachs Group logo displayed?\n(A) On a billboard\n(B) On a laptop\n(C) On the screen in a meeting room\n(D) On a mobile phone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 173, "doc": {"video_name": "needle_70.mp4", "duration": 464.76, "question": "Where are the tourists strolling and admiring the fountain?\n(A) Plaza de Espana in Valencia\n(B) Plaza de Espana in Seville\n(C) Plaza de Espana in Barcelona\n(D) Plaza de Espana in Madrid\n", "candidates": ["Plaza de Espana in Valencia", "Plaza de Espana in Seville", "Plaza de Espana in Barcelona", "Plaza de Espana in Madrid"], "answer": "B", "task_type": "needle", "question_id": "Q173"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "975580d8090c1270113bbe829cdc4c9b6c6a17506286633e3c6ff19c73f71c86", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where are the tourists strolling and admiring the fountain?\n(A) Plaza de Espana in Valencia\n(B) Plaza de Espana in Seville\n(C) Plaza de Espana in Barcelona\n(D) Plaza de Espana in Madrid\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Where are the tourists strolling and admiring the fountain?\n(A) Plaza de Espana in Valencia\n(B) Plaza de Espana in Seville\n(C) Plaza de Espana in Barcelona\n(D) Plaza de Espana in Madrid\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 177, "doc": {"video_name": "needle_23.mp4", "duration": 487.29, "question": "What is the chef doing with the fresh orange pumpkin on the wooden table?\n(A) Roasting it\n(B) Peeling it\n(C) Cutting it into slices\n(D) Boiling it\n", "candidates": ["Roasting it", "Peeling it", "Cutting it into slices", "Boiling it"], "answer": "C", "task_type": "needle", "question_id": "Q177"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9e7b7426e8fc040c651088b1ece42ad3a3caec93a6ed49cce137eca6b187226f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the chef doing with the fresh orange pumpkin on the wooden table?\n(A) Roasting it\n(B) Peeling it\n(C) Cutting it into slices\n(D) Boiling it\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the chef doing with the fresh orange pumpkin on the wooden table?\n(A) Roasting it\n(B) Peeling it\n(C) Cutting it into slices\n(D) Boiling it\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 181, "doc": {"video_name": "needle_22.mp4", "duration": 884.64, "question": "What color is the pumpkin that the chef is using to prepare the soup?\n(A) Yellow\n(B) Orange\n(C) White\n(D) Green\n", "candidates": ["Yellow", "Orange", "White", "Green"], "answer": "B", "task_type": "needle", "question_id": "Q181"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85e11b3eae2cb6f23cb9f2d3b5153108a8919086e94a0c91663ecdaf9f628b64", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the pumpkin that the chef is using to prepare the soup?\n(A) Yellow\n(B) Orange\n(C) White\n(D) Green\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What color is the pumpkin that the chef is using to prepare the soup?\n(A) Yellow\n(B) Orange\n(C) White\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 185, "doc": {"video_name": "needle_93.mp4", "duration": 458.01, "question": "What is the status of the woman enjoying the summer sunset?\n(A) She is a student\n(B) She is a retired woman\n(C) She is a child\n(D) She is a working woman\n", "candidates": ["She is a student", "She is a retired woman", "She is a child", "She is a working woman"], "answer": "B", "task_type": "needle", "question_id": "Q185"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cac41b3030fd4dddc1b86a483338332fb1208aa02748d7a08af3675b6380197e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the status of the woman enjoying the summer sunset?\n(A) She is a student\n(B) She is a retired woman\n(C) She is a child\n(D) She is a working woman\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the status of the woman enjoying the summer sunset?\n(A) She is a student\n(B) She is a retired woman\n(C) She is a child\n(D) She is a working woman\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 189, "doc": {"video_name": "needle_21.mp4", "duration": 846.12, "question": "From whose perspective is the scene of the car with a broken windshield riding through the mountain country village viewed?\n(A) Backseat passenger's view\n(B) Pedestrian's view\n(C) Driver's view\n(D) Bird's eye view\n", "candidates": ["Backseat passenger's view", "Pedestrian's view", "Driver's view", "Bird's eye view"], "answer": "A", "task_type": "needle", "question_id": "Q189"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "23654d101dbaafbdb7194180cb10505fc76e337dcd02fc0795f5d7d4a1325243", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "From whose perspective is the scene of the car with a broken windshield riding through the mountain country village viewed?\n(A) Backseat passenger's view\n(B) Pedestrian's view\n(C) Driver's view\n(D) Bird's eye view\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "From whose perspective is the scene of the car with a broken windshield riding through the mountain country village viewed?\n(A) Backseat passenger's view\n(B) Pedestrian's view\n(C) Driver's view\n(D) Bird's eye view\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 193, "doc": {"video_name": "needle_64.mp4", "duration": 470.88, "question": "What is the result of the forest fire on the dry grass in the video segment?\n(A) The grass is producing smoke and flame\n(B) The grass is being covered in snow\n(C) The grass is growing taller\n(D) The grass is turning green\n", "candidates": ["The grass is producing smoke and flame", "The grass is being covered in snow", "The grass is growing taller", "The grass is turning green"], "answer": "A", "task_type": "needle", "question_id": "Q193"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "95e17d14e129569449398290fc6bcf6a5d2a008a42c1417865a7fc50a4961e24", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the result of the forest fire on the dry grass in the video segment?\n(A) The grass is producing smoke and flame\n(B) The grass is being covered in snow\n(C) The grass is growing taller\n(D) The grass is turning green\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the result of the forest fire on the dry grass in the video segment?\n(A) The grass is producing smoke and flame\n(B) The grass is being covered in snow\n(C) The grass is growing taller\n(D) The grass is turning green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 197, "doc": {"video_name": "needle_135.mp4", "duration": 344.4, "question": "What is the background color when the man is laughing and covering his mouth?\n(A) Blue\n(B) Black\n(C) Green\n(D) White\n", "candidates": ["Blue", "Black", "Green", "White"], "answer": "D", "task_type": "needle", "question_id": "Q197"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "605c00c149f0ad1b2126ca02e9411501dee1170645d4011f7df0044de9b4a0fc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the background color when the man is laughing and covering his mouth?\n(A) Blue\n(B) Black\n(C) Green\n(D) White\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "What is the background color when the man is laughing and covering his mouth?\n(A) Blue\n(B) Black\n(C) Green\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 201, "doc": {"video_name": "needle_114.mp4", "duration": 773.72, "question": "What is the concept of the segment where the young girl in a tracksuit is doing yoga in the park?\n(A) Meditation in the fresh air\n(B) Physical fitness\n(C) Outdoor sports\n(D) Leisure and relaxation\n", "candidates": ["Meditation in the fresh air", "Physical fitness", "Outdoor sports", "Leisure and relaxation"], "answer": "A", "task_type": "needle", "question_id": "Q201"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "264a71a39bd34974cc57212b922c0bd4286affec66d012173ef6d2c98dc87387", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the concept of the segment where the young girl in a tracksuit is doing yoga in the park?\n(A) Meditation in the fresh air\n(B) Physical fitness\n(C) Outdoor sports\n(D) Leisure and relaxation\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the concept of the segment where the young girl in a tracksuit is doing yoga in the park?\n(A) Meditation in the fresh air\n(B) Physical fitness\n(C) Outdoor sports\n(D) Leisure and relaxation\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 205, "doc": {"video_name": "needle_62.mp4", "duration": 497.05, "question": "What is the young beautiful woman doing in preparation for the new year?\n(A) She is singing christmas carols\n(B) She is wrapping presents\n(C) She is hanging a golden christmas toy on the christmas tree\n(D) She is baking cookies\n", "candidates": ["She is singing christmas carols", "She is wrapping presents", "She is hanging a golden christmas toy on the christmas tree", "She is baking cookies"], "answer": "C", "task_type": "needle", "question_id": "Q205"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "79b11bc720c432e0e2eed648566e7c81c17c0d5b7cfe4c4a05f401b1c6660030", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the young beautiful woman doing in preparation for the new year?\n(A) She is singing christmas carols\n(B) She is wrapping presents\n(C) She is hanging a golden christmas toy on the christmas tree\n(D) She is baking cookies\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the young beautiful woman doing in preparation for the new year?\n(A) She is singing christmas carols\n(B) She is wrapping presents\n(C) She is hanging a golden christmas toy on the christmas tree\n(D) She is baking cookies\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 209, "doc": {"video_name": "needle_11.mp4", "duration": 454.52, "question": "What is the sound engineer doing in the control room during a recording session?\n(A) Moving levers of a multitrack mixing console\n(B) Editing the video\n(C) Setting up the microphones\n(D) Adjusting the lights\n", "candidates": ["Moving levers of a multitrack mixing console", "Editing the video", "Setting up the microphones", "Adjusting the lights"], "answer": "A", "task_type": "needle", "question_id": "Q209"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8fde8fdf4a529953a4bcb93e24466aaeaf8c5415a7cd5c4419c2aeebe7e87bd5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the sound engineer doing in the control room during a recording session?\n(A) Moving levers of a multitrack mixing console\n(B) Editing the video\n(C) Setting up the microphones\n(D) Adjusting the lights\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the sound engineer doing in the control room during a recording session?\n(A) Moving levers of a multitrack mixing console\n(B) Editing the video\n(C) Setting up the microphones\n(D) Adjusting the lights\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 213, "doc": {"video_name": "needle_56.mp4", "duration": 462.1, "question": "What kind of factory is the woman working in?\n(A) Food factory\n(B) Toy factory\n(C) Garment factory\n(D) Car factory\n", "candidates": ["Food factory", "Toy factory", "Garment factory", "Car factory"], "answer": "C", "task_type": "needle", "question_id": "Q213"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3118f12f9a32f4b83f439b31597c2e5ffb89a1f2ede9b491be3069daac317398", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What kind of factory is the woman working in?\n(A) Food factory\n(B) Toy factory\n(C) Garment factory\n(D) Car factory\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What kind of factory is the woman working in?\n(A) Food factory\n(B) Toy factory\n(C) Garment factory\n(D) Car factory\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 217, "doc": {"video_name": "needle_10.mp4", "duration": 454.52, "question": "What is the sound engineer doing in the control room during a recording session?\n(A) Editing the video\n(B) Moving levers of a multitrack mixing console\n(C) Setting up the microphones\n(D) Adjusting the lights\n", "candidates": ["Editing the video", "Moving levers of a multitrack mixing console", "Setting up the microphones", "Adjusting the lights"], "answer": "B", "task_type": "needle", "question_id": "Q217"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1da15d502bde2f75040b5d72d1897da358f237e20bfb27fd2bd53ad52631121e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the sound engineer doing in the control room during a recording session?\n(A) Editing the video\n(B) Moving levers of a multitrack mixing console\n(C) Setting up the microphones\n(D) Adjusting the lights\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the sound engineer doing in the control room during a recording session?\n(A) Editing the video\n(B) Moving levers of a multitrack mixing console\n(C) Setting up the microphones\n(D) Adjusting the lights\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 221, "doc": {"video_name": "needle_89.mp4", "duration": 456.0, "question": "Where is the Goldman Sachs Group logo displayed?\n(A) On a laptop\n(B) On a billboard\n(C) On the screen in a meeting room\n(D) On a mobile phone\n", "candidates": ["On a laptop", "On a billboard", "On the screen in a meeting room", "On a mobile phone"], "answer": "C", "task_type": "needle", "question_id": "Q221"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "53e5ff06848cb795f043460ccccce7aeaf0f6c79eab09a2f57a6528f29e28790", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the Goldman Sachs Group logo displayed?\n(A) On a laptop\n(B) On a billboard\n(C) On the screen in a meeting room\n(D) On a mobile phone\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "Where is the Goldman Sachs Group logo displayed?\n(A) On a laptop\n(B) On a billboard\n(C) On the screen in a meeting room\n(D) On a mobile phone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 225, "doc": {"video_name": "needle_37.mp4", "duration": 478.78, "question": "What type of beach is shown in the video?\n(A) Sandy beach\n(B) Wild rocky beach\n(C) Man-made beach\n(D) Pebble beach\n", "candidates": ["Sandy beach", "Wild rocky beach", "Man-made beach", "Pebble beach"], "answer": "B", "task_type": "needle", "question_id": "Q225"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d8e6794bc69fa7453f2c9fad29f2dfc57f2b997eda07f95166a068055498861", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of beach is shown in the video?\n(A) Sandy beach\n(B) Wild rocky beach\n(C) Man-made beach\n(D) Pebble beach\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What type of beach is shown in the video?\n(A) Sandy beach\n(B) Wild rocky beach\n(C) Man-made beach\n(D) Pebble beach\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 229, "doc": {"video_name": "needle_56.mp4", "duration": 462.1, "question": "Where is the woman working?\n(A) In a restaurant\n(B) In a garment factory\n(C) In a hospital\n(D) In a school\n", "candidates": ["In a restaurant", "In a garment factory", "In a hospital", "In a school"], "answer": "B", "task_type": "needle", "question_id": "Q229"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5cf1a3bc7bd43e680c050e493c55d07487884a5434a4a84a9affa9f1960cbd16", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where is the woman working?\n(A) In a restaurant\n(B) In a garment factory\n(C) In a hospital\n(D) In a school\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Where is the woman working?\n(A) In a restaurant\n(B) In a garment factory\n(C) In a hospital\n(D) In a school\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 233, "doc": {"video_name": "needle_81.mp4", "duration": 463.1, "question": "From which perspective is the shot of the egg falling on the glass floor taken?\n(A) From below\n(B) From the side\n(C) From a distance\n(D) From above\n", "candidates": ["From below", "From the side", "From a distance", "From above"], "answer": "A", "task_type": "needle", "question_id": "Q233"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1339bd763ef581987634b6404a0eb717436787980051f2e1a770c2e4e2301dc9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "From which perspective is the shot of the egg falling on the glass floor taken?\n(A) From below\n(B) From the side\n(C) From a distance\n(D) From above\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "From which perspective is the shot of the egg falling on the glass floor taken?\n(A) From below\n(B) From the side\n(C) From a distance\n(D) From above\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 237, "doc": {"video_name": "needle_83.mp4", "duration": 464.38, "question": "What is the man doing over the white background?\n(A) Dancing\n(B) Laughing and covering his mouth with hand\n(C) Singing\n(D) Crying\n", "candidates": ["Dancing", "Laughing and covering his mouth with hand", "Singing", "Crying"], "answer": "B", "task_type": "needle", "question_id": "Q237"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5b938126931553dd5333133cca23b27101de2277e0c99d4ff3217825c903f049", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the man doing over the white background?\n(A) Dancing\n(B) Laughing and covering his mouth with hand\n(C) Singing\n(D) Crying\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the man doing over the white background?\n(A) Dancing\n(B) Laughing and covering his mouth with hand\n(C) Singing\n(D) Crying\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 241, "doc": {"video_name": "needle_32.mp4", "duration": 467.98, "question": "What unusual event happens while the woman is using the computer?\n(A) The computer crashes\n(B) The computer starts talking\n(C) The screen goes blank\n(D) A hand comes out of the computer\n", "candidates": ["The computer crashes", "The computer starts talking", "The screen goes blank", "A hand comes out of the computer"], "answer": "D", "task_type": "needle", "question_id": "Q241"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9064166f2f087c44dc84d94af41750985c9bff47884bbcd104f2484f41ee2a19", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What unusual event happens while the woman is using the computer?\n(A) The computer crashes\n(B) The computer starts talking\n(C) The screen goes blank\n(D) A hand comes out of the computer\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What unusual event happens while the woman is using the computer?\n(A) The computer crashes\n(B) The computer starts talking\n(C) The screen goes blank\n(D) A hand comes out of the computer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 245, "doc": {"video_name": "needle_80.mp4", "duration": 463.02, "question": "From which perspective is the shot of the egg falling on the glass floor taken?\n(A) From above\n(B) From below\n(C) From a distance\n(D) From the side\n", "candidates": ["From above", "From below", "From a distance", "From the side"], "answer": "B", "task_type": "needle", "question_id": "Q245"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc7b56eb80b03d7636b84ea301d628d256ba8df143bdc3847db737b970c035fc", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "From which perspective is the shot of the egg falling on the glass floor taken?\n(A) From above\n(B) From below\n(C) From a distance\n(D) From the side\n", "task_type": "needle", "pred_answer": "A", "answer": "B"}, "input": "From which perspective is the shot of the egg falling on the glass floor taken?\n(A) From above\n(B) From below\n(C) From a distance\n(D) From the side\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 249, "doc": {"video_name": "needle_126.mp4", "duration": 6008.08, "question": "What type of view is provided of the tropical beach in the video?\n(A) Close-up view\n(B) Side view\n(C) Top down aerial view\n(D) Bottom-up view\n", "candidates": ["Close-up view", "Side view", "Top down aerial view", "Bottom-up view"], "answer": "C", "task_type": "needle", "question_id": "Q249"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3724a0a0163cf3a66a3484c71b5a743f481d5fb142931b5555d208fd9a375363", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of view is provided of the tropical beach in the video?\n(A) Close-up view\n(B) Side view\n(C) Top down aerial view\n(D) Bottom-up view\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What type of view is provided of the tropical beach in the video?\n(A) Close-up view\n(B) Side view\n(C) Top down aerial view\n(D) Bottom-up view\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 253, "doc": {"video_name": "needle_149.mp4", "duration": 324.02, "question": "Where is the Goldman Sachs Group logo displayed?\n(A) On a mobile phone\n(B) On a billboard\n(C) On the screen in a meeting room\n(D) On a laptop\n", "candidates": ["On a mobile phone", "On a billboard", "On the screen in a meeting room", "On a laptop"], "answer": "C", "task_type": "needle", "question_id": "Q253"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "73a160818733136d76a883621e5f7c6e62fc50d0bdb0002464ed4623ad552b04", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the Goldman Sachs Group logo displayed?\n(A) On a mobile phone\n(B) On a billboard\n(C) On the screen in a meeting room\n(D) On a laptop\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "Where is the Goldman Sachs Group logo displayed?\n(A) On a mobile phone\n(B) On a billboard\n(C) On the screen in a meeting room\n(D) On a laptop\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 257, "doc": {"video_name": "needle_32.mp4", "duration": 467.98, "question": "What is the woman doing on the computer?\n(A) Writing an email\n(B) Watching a movie\n(C) Playing a game\n(D) Shopping online\n", "candidates": ["Writing an email", "Watching a movie", "Playing a game", "Shopping online"], "answer": "D", "task_type": "needle", "question_id": "Q257"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B."]], "filtered_resps": ["B."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d51c96a594f6b0c1dd270b81959a3685d66ed9dd87cfd6f799242ccff043945", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the woman doing on the computer?\n(A) Writing an email\n(B) Watching a movie\n(C) Playing a game\n(D) Shopping online\n", "task_type": "needle", "pred_answer": "B.", "answer": "D"}, "input": "What is the woman doing on the computer?\n(A) Writing an email\n(B) Watching a movie\n(C) Playing a game\n(D) Shopping online\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 261, "doc": {"video_name": "needle_95.mp4", "duration": 464.54, "question": "What is the female potter's position while stirring paint?\n(A) Standing\n(B) Sitting\n(C) Kneeling\n(D) Lying down\n", "candidates": ["Standing", "Sitting", "Kneeling", "Lying down"], "answer": "B", "task_type": "needle", "question_id": "Q261"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "829e5fc72069ed2b09210bd9d4fc28cf1d13cc4d725acd6f202c50f4a32dba9b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the female potter's position while stirring paint?\n(A) Standing\n(B) Sitting\n(C) Kneeling\n(D) Lying down\n", "task_type": "needle", "pred_answer": "A", "answer": "B"}, "input": "What is the female potter's position while stirring paint?\n(A) Standing\n(B) Sitting\n(C) Kneeling\n(D) Lying down\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 265, "doc": {"video_name": "needle_8.mp4", "duration": 461.12, "question": "What device is the female using for the video call?\n(A) Tablet\n(B) Laptop\n(C) Smartphone\n(D) Desktop computer\n", "candidates": ["Tablet", "Laptop", "Smartphone", "Desktop computer"], "answer": "C", "task_type": "needle", "question_id": "Q265"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3b223e64312961f2cac282d745edad6d8d8370a29911562d3a2ea156c2963220", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What device is the female using for the video call?\n(A) Tablet\n(B) Laptop\n(C) Smartphone\n(D) Desktop computer\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What device is the female using for the video call?\n(A) Tablet\n(B) Laptop\n(C) Smartphone\n(D) Desktop computer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 269, "doc": {"video_name": "needle_8.mp4", "duration": 461.12, "question": "From where is the female making the video call?\n(A) From her home living room\n(B) From her office\n(C) From her car\n(D) From a coffee shop\n", "candidates": ["From her home living room", "From her office", "From her car", "From a coffee shop"], "answer": "A", "task_type": "needle", "question_id": "Q269"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "807a9ad5bc5a7ab741469784577285bca8717dc8750be786758950e512770998", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "From where is the female making the video call?\n(A) From her home living room\n(B) From her office\n(C) From her car\n(D) From a coffee shop\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "From where is the female making the video call?\n(A) From her home living room\n(B) From her office\n(C) From her car\n(D) From a coffee shop\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 273, "doc": {"video_name": "needle_142.mp4", "duration": 612.03, "question": "Where is the remembrance war memorial with thousands of Canadian flags located?\n(A) Toronto, Canada\n(B) Vancouver, Canada\n(C) New York, USA\n(D) London, UK\n", "candidates": ["Toronto, Canada", "Vancouver, Canada", "New York, USA", "London, UK"], "answer": "A", "task_type": "needle", "question_id": "Q273"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "14c2cc14dcbb065d188b09a2826a290f62222421505ca13fcda893b559d57053", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where is the remembrance war memorial with thousands of Canadian flags located?\n(A) Toronto, Canada\n(B) Vancouver, Canada\n(C) New York, USA\n(D) London, UK\n", "task_type": "needle", "pred_answer": "B", "answer": "A"}, "input": "Where is the remembrance war memorial with thousands of Canadian flags located?\n(A) Toronto, Canada\n(B) Vancouver, Canada\n(C) New York, USA\n(D) London, UK\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 277, "doc": {"video_name": "needle_7.mp4", "duration": 471.23, "question": "What is the woman wearing while enjoying the summer sun and tanning during her holidays in the swimming pool?\n(A) A one-piece swimsuit\n(B) A sarong\n(C) A sundress\n(D) A bikini\n", "candidates": ["A one-piece swimsuit", "A sarong", "A sundress", "A bikini"], "answer": "D", "task_type": "needle", "question_id": "Q277"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a006052d5b00adbc1661839de121a148d78c2d9f85df91aa5aeaf42c79916cfc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the woman wearing while enjoying the summer sun and tanning during her holidays in the swimming pool?\n(A) A one-piece swimsuit\n(B) A sarong\n(C) A sundress\n(D) A bikini\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the woman wearing while enjoying the summer sun and tanning during her holidays in the swimming pool?\n(A) A one-piece swimsuit\n(B) A sarong\n(C) A sundress\n(D) A bikini\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 281, "doc": {"video_name": "needle_7.mp4", "duration": 471.23, "question": "What is the woman in the bikini doing during her holidays in the swimming pool?\n(A) Enjoying the summer sun and tanning\n(B) Swimming laps\n(C) Playing water polo\n(D) Doing water aerobics\n", "candidates": ["Enjoying the summer sun and tanning", "Swimming laps", "Playing water polo", "Doing water aerobics"], "answer": "A", "task_type": "needle", "question_id": "Q281"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a9a06c0c1a2d5580a07ff739e9f8f7cefbdf263f1695f20fe356b675c7639228", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the woman in the bikini doing during her holidays in the swimming pool?\n(A) Enjoying the summer sun and tanning\n(B) Swimming laps\n(C) Playing water polo\n(D) Doing water aerobics\n", "task_type": "needle", "pred_answer": "A.", "answer": "A"}, "input": "What is the woman in the bikini doing during her holidays in the swimming pool?\n(A) Enjoying the summer sun and tanning\n(B) Swimming laps\n(C) Playing water polo\n(D) Doing water aerobics\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 285, "doc": {"video_name": "needle_98.mp4", "duration": 461.7, "question": "What is the weather like when the young girl in a tracksuit is doing yoga in the park?\n(A) Cloudy\n(B) Rainy\n(C) Snowy\n(D) Sunny\n", "candidates": ["Cloudy", "Rainy", "Snowy", "Sunny"], "answer": "D", "task_type": "needle", "question_id": "Q285"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ea7266282c6991ee562d62bc8422ed8203433839497dfae4b736a7cebf6186c3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the weather like when the young girl in a tracksuit is doing yoga in the park?\n(A) Cloudy\n(B) Rainy\n(C) Snowy\n(D) Sunny\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the weather like when the young girl in a tracksuit is doing yoga in the park?\n(A) Cloudy\n(B) Rainy\n(C) Snowy\n(D) Sunny\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 289, "doc": {"video_name": "needle_1.mp4", "duration": 506.12, "question": "Where is the basketball court located in the video?\n(A) In a gym\n(B) In a park\n(C) On a cruise ship\n(D) In a school\n", "candidates": ["In a gym", "In a park", "On a cruise ship", "In a school"], "answer": "C", "task_type": "needle", "question_id": "Q289"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "57c9b922b12c34f63def2f3fc833fe044adfa731160ffc6b97f9e2cd237a4368", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the basketball court located in the video?\n(A) In a gym\n(B) In a park\n(C) On a cruise ship\n(D) In a school\n", "task_type": "needle", "pred_answer": "B", "answer": "C"}, "input": "Where is the basketball court located in the video?\n(A) In a gym\n(B) In a park\n(C) On a cruise ship\n(D) In a school\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 293, "doc": {"video_name": "needle_77.mp4", "duration": 476.65, "question": "Where is the chef preparing the dinner?\n(A) In a restaurant\n(B) By the ocean shore on an island\n(C) In a kitchen\n(D) In a hotel\n", "candidates": ["In a restaurant", "By the ocean shore on an island", "In a kitchen", "In a hotel"], "answer": "B", "task_type": "needle", "question_id": "Q293"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cd7016fa39bc2832f32708607efe384ed1b52f207e528c5addb6a493f026b2e7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where is the chef preparing the dinner?\n(A) In a restaurant\n(B) By the ocean shore on an island\n(C) In a kitchen\n(D) In a hotel\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Where is the chef preparing the dinner?\n(A) In a restaurant\n(B) By the ocean shore on an island\n(C) In a kitchen\n(D) In a hotel\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 297, "doc": {"video_name": "needle_65.mp4", "duration": 500.88, "question": "What is the condition of the grass in the video segment during the forest fire?\n(A) The grass is old and dry\n(B) The grass is wet\n(C) The grass is green and fresh\n(D) The grass is covered in snow\n", "candidates": ["The grass is old and dry", "The grass is wet", "The grass is green and fresh", "The grass is covered in snow"], "answer": "A", "task_type": "needle", "question_id": "Q297"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "84bf0aa73f71972a9946ed8f49083d1607a4dd61b1d02c56cc4b007d0211a25a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the condition of the grass in the video segment during the forest fire?\n(A) The grass is old and dry\n(B) The grass is wet\n(C) The grass is green and fresh\n(D) The grass is covered in snow\n", "task_type": "needle", "pred_answer": "A.", "answer": "A"}, "input": "What is the condition of the grass in the video segment during the forest fire?\n(A) The grass is old and dry\n(B) The grass is wet\n(C) The grass is green and fresh\n(D) The grass is covered in snow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 301, "doc": {"video_name": "needle_98.mp4", "duration": 461.7, "question": "What time of day is it when the young girl in a tracksuit is doing yoga in the park?\n(A) Night\n(B) Morning\n(C) Sunset\n(D) Midday\n", "candidates": ["Night", "Morning", "Sunset", "Midday"], "answer": "C", "task_type": "needle", "question_id": "Q301"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "658ef2f872b97ad139349ca1d2d813fd870afb826f1b704648db4ac451d973b7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What time of day is it when the young girl in a tracksuit is doing yoga in the park?\n(A) Night\n(B) Morning\n(C) Sunset\n(D) Midday\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What time of day is it when the young girl in a tracksuit is doing yoga in the park?\n(A) Night\n(B) Morning\n(C) Sunset\n(D) Midday\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 305, "doc": {"video_name": "needle_94.mp4", "duration": 857.22, "question": "What is the female potter doing with the brush?\n(A) Drawing on the table\n(B) Stirring paint in a cup\n(C) Painting a picture\n(D) Cleaning the brush\n", "candidates": ["Drawing on the table", "Stirring paint in a cup", "Painting a picture", "Cleaning the brush"], "answer": "B", "task_type": "needle", "question_id": "Q305"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1e4fe81478b3e849a477d4f9ef1815cad56a2f98d2127057e8e24ab747c95dfc", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the female potter doing with the brush?\n(A) Drawing on the table\n(B) Stirring paint in a cup\n(C) Painting a picture\n(D) Cleaning the brush\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the female potter doing with the brush?\n(A) Drawing on the table\n(B) Stirring paint in a cup\n(C) Painting a picture\n(D) Cleaning the brush\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 309, "doc": {"video_name": "needle_68.mp4", "duration": 1367.32, "question": "What time of day is the little Asian girl in a bikini creating sand piles on the beach?\n(A) Sunset\n(B) Afternoon\n(C) Morning\n(D) Midnight\n", "candidates": ["Sunset", "Afternoon", "Morning", "Midnight"], "answer": "A", "task_type": "needle", "question_id": "Q309"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c328054f1bfb6729c2dcd1386ff0ab32eaa3f30ae013128e38ef0f2bcb5a4d1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What time of day is the little Asian girl in a bikini creating sand piles on the beach?\n(A) Sunset\n(B) Afternoon\n(C) Morning\n(D) Midnight\n", "task_type": "needle", "pred_answer": "B", "answer": "A"}, "input": "What time of day is the little Asian girl in a bikini creating sand piles on the beach?\n(A) Sunset\n(B) Afternoon\n(C) Morning\n(D) Midnight\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 313, "doc": {"video_name": "needle_64.mp4", "duration": 470.88, "question": "What is the condition of the grass in the video segment during the forest fire?\n(A) The grass is covered in snow\n(B) The grass is wet\n(C) The grass is old and dry\n(D) The grass is green and fresh\n", "candidates": ["The grass is covered in snow", "The grass is wet", "The grass is old and dry", "The grass is green and fresh"], "answer": "C", "task_type": "needle", "question_id": "Q313"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "75113df6ddbbfa5fff2704579aa96810fa77693677928f947bdc4b03ce55bfca", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the condition of the grass in the video segment during the forest fire?\n(A) The grass is covered in snow\n(B) The grass is wet\n(C) The grass is old and dry\n(D) The grass is green and fresh\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "What is the condition of the grass in the video segment during the forest fire?\n(A) The grass is covered in snow\n(B) The grass is wet\n(C) The grass is old and dry\n(D) The grass is green and fresh\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 317, "doc": {"video_name": "needle_114.mp4", "duration": 773.72, "question": "What time of day is it when the young girl in a tracksuit is doing yoga in the park?\n(A) Sunset\n(B) Night\n(C) Midday\n(D) Morning\n", "candidates": ["Sunset", "Night", "Midday", "Morning"], "answer": "A", "task_type": "needle", "question_id": "Q317"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1b4ae7bf18fa3588e4330f3f0409cfb9fbbdbb592573a88b4ced225af35c639a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What time of day is it when the young girl in a tracksuit is doing yoga in the park?\n(A) Sunset\n(B) Night\n(C) Midday\n(D) Morning\n", "task_type": "needle", "pred_answer": "D", "answer": "A"}, "input": "What time of day is it when the young girl in a tracksuit is doing yoga in the park?\n(A) Sunset\n(B) Night\n(C) Midday\n(D) Morning\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 321, "doc": {"video_name": "needle_57.mp4", "duration": 448.06, "question": "What is the woman doing in the garment factory?\n(A) She is designing clothes\n(B) She is selling clothes\n(C) She is working on the production line\n(D) She is managing the factory\n", "candidates": ["She is designing clothes", "She is selling clothes", "She is working on the production line", "She is managing the factory"], "answer": "C", "task_type": "needle", "question_id": "Q321"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05a1bc99584ef19243f019c0c04c609b1a67ed05585c0fd2d9b7b5d0782b12e1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the woman doing in the garment factory?\n(A) She is designing clothes\n(B) She is selling clothes\n(C) She is working on the production line\n(D) She is managing the factory\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the woman doing in the garment factory?\n(A) She is designing clothes\n(B) She is selling clothes\n(C) She is working on the production line\n(D) She is managing the factory\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 325, "doc": {"video_name": "needle_132.mp4", "duration": 558.0699999999999, "question": "What type of christmas toy is the young beautiful woman hanging on the christmas tree?\n(A) A blue christmas toy\n(B) A silver christmas toy\n(C) A golden christmas toy\n(D) A red christmas toy\n", "candidates": ["A blue christmas toy", "A silver christmas toy", "A golden christmas toy", "A red christmas toy"], "answer": "C", "task_type": "needle", "question_id": "Q325"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f4f44efbbf9e84eab3ff7ab8c63fef476240a24315a80a5938c578ef8d573b0f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of christmas toy is the young beautiful woman hanging on the christmas tree?\n(A) A blue christmas toy\n(B) A silver christmas toy\n(C) A golden christmas toy\n(D) A red christmas toy\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What type of christmas toy is the young beautiful woman hanging on the christmas tree?\n(A) A blue christmas toy\n(B) A silver christmas toy\n(C) A golden christmas toy\n(D) A red christmas toy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 329, "doc": {"video_name": "needle_6.mp4", "duration": 471.23, "question": "What is the woman in the bikini doing during her holidays in the swimming pool?\n(A) Swimming laps\n(B) Enjoying the summer sun and tanning\n(C) Doing water aerobics\n(D) Playing water polo\n", "candidates": ["Swimming laps", "Enjoying the summer sun and tanning", "Doing water aerobics", "Playing water polo"], "answer": "B", "task_type": "needle", "question_id": "Q329"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B."]], "filtered_resps": ["B."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "204d9bd10cb4a747df2f445ed19149ee26d8a7376f8337e3cf6ca51e83702871", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the woman in the bikini doing during her holidays in the swimming pool?\n(A) Swimming laps\n(B) Enjoying the summer sun and tanning\n(C) Doing water aerobics\n(D) Playing water polo\n", "task_type": "needle", "pred_answer": "B.", "answer": "B"}, "input": "What is the woman in the bikini doing during her holidays in the swimming pool?\n(A) Swimming laps\n(B) Enjoying the summer sun and tanning\n(C) Doing water aerobics\n(D) Playing water polo\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 333, "doc": {"video_name": "needle_41.mp4", "duration": 477.07, "question": "What is the nature of the den where the American toad is sitting in the video?\n(A) Wooden cavity\n(B) Stone cavity\n(C) Earthen cavity\n(D) Water cavity\n", "candidates": ["Wooden cavity", "Stone cavity", "Earthen cavity", "Water cavity"], "answer": "C", "task_type": "needle", "question_id": "Q333"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b16a2e05585a66df21be5f7927e70c8e8ff0c6d700e47d84504ec0791ed95aea", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the nature of the den where the American toad is sitting in the video?\n(A) Wooden cavity\n(B) Stone cavity\n(C) Earthen cavity\n(D) Water cavity\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the nature of the den where the American toad is sitting in the video?\n(A) Wooden cavity\n(B) Stone cavity\n(C) Earthen cavity\n(D) Water cavity\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 337, "doc": {"video_name": "needle_65.mp4", "duration": 500.88, "question": "What is the result of the forest fire on the dry grass in the video segment?\n(A) The grass is being covered in snow\n(B) The grass is turning green\n(C) The grass is producing smoke and flame\n(D) The grass is growing taller\n", "candidates": ["The grass is being covered in snow", "The grass is turning green", "The grass is producing smoke and flame", "The grass is growing taller"], "answer": "C", "task_type": "needle", "question_id": "Q337"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5e81ecf970b27319e72736523e81b63b7bf175eef92f4669065345e8f7c5b4ad", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the result of the forest fire on the dry grass in the video segment?\n(A) The grass is being covered in snow\n(B) The grass is turning green\n(C) The grass is producing smoke and flame\n(D) The grass is growing taller\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the result of the forest fire on the dry grass in the video segment?\n(A) The grass is being covered in snow\n(B) The grass is turning green\n(C) The grass is producing smoke and flame\n(D) The grass is growing taller\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 341, "doc": {"video_name": "needle_61.mp4", "duration": 496.97999999999996, "question": "What is the senior businessman doing by the sea shore?\n(A) Having a serious conversation on the cell phone\n(B) Reading a book\n(C) Swimming in the sea\n(D) Eating lunch\n", "candidates": ["Having a serious conversation on the cell phone", "Reading a book", "Swimming in the sea", "Eating lunch"], "answer": "A", "task_type": "needle", "question_id": "Q341"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c2eb9e96a198a0178cc160364608bbd8c746a74898fd84911efb538943cb76a2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the senior businessman doing by the sea shore?\n(A) Having a serious conversation on the cell phone\n(B) Reading a book\n(C) Swimming in the sea\n(D) Eating lunch\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the senior businessman doing by the sea shore?\n(A) Having a serious conversation on the cell phone\n(B) Reading a book\n(C) Swimming in the sea\n(D) Eating lunch\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 345, "doc": {"video_name": "needle_63.mp4", "duration": 467.04999999999995, "question": "What is the young beautiful woman doing in preparation for the new year?\n(A) She is hanging a golden christmas toy on the christmas tree\n(B) She is baking cookies\n(C) She is singing christmas carols\n(D) She is wrapping presents\n", "candidates": ["She is hanging a golden christmas toy on the christmas tree", "She is baking cookies", "She is singing christmas carols", "She is wrapping presents"], "answer": "A", "task_type": "needle", "question_id": "Q345"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aecd2c9900f6243f3f5c553226ebde69efc790f91f9fc71838e5a01b42c50222", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the young beautiful woman doing in preparation for the new year?\n(A) She is hanging a golden christmas toy on the christmas tree\n(B) She is baking cookies\n(C) She is singing christmas carols\n(D) She is wrapping presents\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the young beautiful woman doing in preparation for the new year?\n(A) She is hanging a golden christmas toy on the christmas tree\n(B) She is baking cookies\n(C) She is singing christmas carols\n(D) She is wrapping presents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 349, "doc": {"video_name": "needle_37.mp4", "duration": 478.78, "question": "What type of view is provided of the tropical beach in the video?\n(A) Close-up view\n(B) Top down aerial view\n(C) Bottom-up view\n(D) Side view\n", "candidates": ["Close-up view", "Top down aerial view", "Bottom-up view", "Side view"], "answer": "B", "task_type": "needle", "question_id": "Q349"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B."]], "filtered_resps": ["B."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2454463c3932c95ef054a14a9b82df3d81056d57be40e195b003d22dc0edfee3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of view is provided of the tropical beach in the video?\n(A) Close-up view\n(B) Top down aerial view\n(C) Bottom-up view\n(D) Side view\n", "task_type": "needle", "pred_answer": "B.", "answer": "B"}, "input": "What type of view is provided of the tropical beach in the video?\n(A) Close-up view\n(B) Top down aerial view\n(C) Bottom-up view\n(D) Side view\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 353, "doc": {"video_name": "needle_60.mp4", "duration": 496.97999999999996, "question": "Where is the senior businessman having a serious conversation on the cell phone?\n(A) In a park\n(B) By the sea shore\n(C) At a restaurant\n(D) In his office\n", "candidates": ["In a park", "By the sea shore", "At a restaurant", "In his office"], "answer": "B", "task_type": "needle", "question_id": "Q353"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f6f8cf189f96621250def2631142384dd02cdc73bde49ba56059454e7b4c304d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where is the senior businessman having a serious conversation on the cell phone?\n(A) In a park\n(B) By the sea shore\n(C) At a restaurant\n(D) In his office\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Where is the senior businessman having a serious conversation on the cell phone?\n(A) In a park\n(B) By the sea shore\n(C) At a restaurant\n(D) In his office\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 357, "doc": {"video_name": "surveil_22.mp4", "duration": 195.5, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Fighting\n(C) RoadAccidents\n(D) Shooting\n", "candidates": ["Shooting", "Fighting", "RoadAccidents", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q357"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "373ebf226d65f1e011c48ae1b04960aec0427d8914c449fd3cfaf36d54410cfc", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Fighting\n(C) RoadAccidents\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Fighting\n(C) RoadAccidents\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 361, "doc": {"video_name": "surveil_82.mp4", "duration": 288.02, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Abuse\n(C) RoadAccidents\n(D) Arson\n", "candidates": ["Fighting", "Abuse", "RoadAccidents", "Arson"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q361"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aaf3bf232d06dde0de180b917ea4cec658052899acce9ab4bbe78aa54bfc0cbe", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Abuse\n(C) RoadAccidents\n(D) Arson\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Abuse\n(C) RoadAccidents\n(D) Arson\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 365, "doc": {"video_name": "surveil_165.mp4", "duration": 258.93, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Burglary\n(C) Assault\n(D) Arrest\n", "candidates": ["Arson", "Burglary", "Assault", "Arrest"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q365"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c5ec9047fc9d51a451b83c05d4e9d02b41925a4114e6989b0349163a52d8321", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Burglary\n(C) Assault\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Burglary\n(C) Assault\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 369, "doc": {"video_name": "surveil_35.mp4", "duration": 332.64, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Arson\n(C) Shoplifting\n(D) Burglary\n", "candidates": ["Vandalism", "Arson", "Shoplifting", "Burglary"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q369"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ee3b05834afc84785ee1d13264fe2e678f0fe45f2ad5568afc933b5ba4f55bf3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Arson\n(C) Shoplifting\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Arson\n(C) Shoplifting\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 373, "doc": {"video_name": "surveil_195.mp4", "duration": 239.49, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Stealing\n(C) Assault\n(D) Vandalism\n", "candidates": ["Robbery", "Stealing", "Assault", "Vandalism"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q373"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2dcc9c20154aa33e41819c13e6643538228e6c481ccbbdce03de350fb4196f2c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Stealing\n(C) Assault\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Stealing\n(C) Assault\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 377, "doc": {"video_name": "surveil_179.mp4", "duration": 208.89, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Robbery\n(C) Explosion\n(D) Fighting\n", "candidates": ["Shoplifting", "Robbery", "Explosion", "Fighting"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q377"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1829c53e6087c60f6f498590c766e2a7814f5fcab50bf188671b71192b3873ee", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Robbery\n(C) Explosion\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Robbery\n(C) Explosion\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 381, "doc": {"video_name": "surveil_54.mp4", "duration": 254.88, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Explosion\n(C) Shoplifting\n(D) Normal\n", "candidates": ["Assault", "Explosion", "Shoplifting", "Normal"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q381"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6bbfbeaf0a6200ea9500f5aced1e115e8c5995dea71b3bbdbd8edd16bdea05b2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Explosion\n(C) Shoplifting\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Explosion\n(C) Shoplifting\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 385, "doc": {"video_name": "surveil_188.mp4", "duration": 386.4, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Stealing\n(C) Vandalism\n(D) Shoplifting\n", "candidates": ["Arson", "Stealing", "Vandalism", "Shoplifting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q385"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b03d572da80de4832cadfacaa3c28991284140a01002706471fe040d80480084", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Stealing\n(C) Vandalism\n(D) Shoplifting\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Stealing\n(C) Vandalism\n(D) Shoplifting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 389, "doc": {"video_name": "surveil_30.mp4", "duration": 387.21, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arrest\n(B) Arson\n(C) Robbery\n(D) Stealing\n", "candidates": ["Arrest", "Arson", "Robbery", "Stealing"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q389"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8a60bc1f4929bb57efdfd2077facc052b63af3b8ba481516c3da580032a3d2be", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arrest\n(B) Arson\n(C) Robbery\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arrest\n(B) Arson\n(C) Robbery\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 393, "doc": {"video_name": "surveil_147.mp4", "duration": 251.2, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Normal\n(B) Burglary\n(C) Assault\n(D) Robbery\n", "candidates": ["Normal", "Burglary", "Assault", "Robbery"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q393"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68d81bd955e96972890174618b058901fd57350d5d3155dc8d3763e3619c6ef0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Normal\n(B) Burglary\n(C) Assault\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Normal\n(B) Burglary\n(C) Assault\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 397, "doc": {"video_name": "surveil_124.mp4", "duration": 1054.78, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Arrest\n(C) Robbery\n(D) Fighting\n", "candidates": ["Shooting", "Arrest", "Robbery", "Fighting"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q397"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ba46206bdca37b4b69ba496f4f8fbde458f641c1ec0c0c70ae1aa0d70c290780", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Arrest\n(C) Robbery\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Arrest\n(C) Robbery\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 401, "doc": {"video_name": "surveil_193.mp4", "duration": 205.7, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Normal\n(C) Robbery\n(D) Shooting\n", "candidates": ["Abuse", "Normal", "Robbery", "Shooting"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q401"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0c782a77ed09730f2fda109a08a6d82ee7969d74865ab3a7d2d753f466d956f4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Normal\n(C) Robbery\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Normal\n(C) Robbery\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 405, "doc": {"video_name": "surveil_60.mp4", "duration": 274.1, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Robbery\n(B) Normal\n(C) Shooting\n(D) Burglary\n", "candidates": ["Robbery", "Normal", "Shooting", "Burglary"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q405"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "064c9c92abcac41a4ae1e0ea7915eb4ef6fdd473c1ca732584fd2d507a077dc1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Robbery\n(B) Normal\n(C) Shooting\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Robbery\n(B) Normal\n(C) Shooting\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 409, "doc": {"video_name": "surveil_198.mp4", "duration": 299.7, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Normal\n(B) Robbery\n(C) Arrest\n(D) Fighting\n", "candidates": ["Normal", "Robbery", "Arrest", "Fighting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q409"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d669788a9dcedb5d40a2e8cf9baac631947724f3383dab435895e3f2df1dc16e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Normal\n(B) Robbery\n(C) Arrest\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Normal\n(B) Robbery\n(C) Arrest\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 413, "doc": {"video_name": "surveil_32.mp4", "duration": 371.85, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Vandalism\n(C) Shoplifting\n(D) Stealing\n", "candidates": ["Shooting", "Vandalism", "Shoplifting", "Stealing"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q413"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2fcd1381332fe18408f90aae14ee0f23a38c1f92a6d2faaf9cb6ed7c141d7f2a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Vandalism\n(C) Shoplifting\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Vandalism\n(C) Shoplifting\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 417, "doc": {"video_name": "surveil_81.mp4", "duration": 285.96, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Arson\n(C) Normal\n(D) Vandalism\n", "candidates": ["Assault", "Arson", "Normal", "Vandalism"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q417"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "28729253a0fd161a6ce89c9c16bdba89ebdbead5d0f4cd759ffa04fce41a5527", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Arson\n(C) Normal\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Arson\n(C) Normal\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 421, "doc": {"video_name": "surveil_6.mp4", "duration": 185.6, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Normal\n(C) Arson\n(D) Shooting\n", "candidates": ["Shooting", "Normal", "Arson", "Shooting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q421"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87d43c6911fa37bf673474bd85c58d6c5359693cdf6a07640490c500f612cc60", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Normal\n(C) Arson\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Normal\n(C) Arson\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 425, "doc": {"video_name": "surveil_166.mp4", "duration": 262.53, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Normal\n(C) Abuse\n(D) Shoplifting\n", "candidates": ["Shooting", "Normal", "Abuse", "Shoplifting"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q425"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "66f1e3b984cb428d603f1783cfd02180ea0e20d5398ef76ed9f0f62d2d59097e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Normal\n(C) Abuse\n(D) Shoplifting\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Normal\n(C) Abuse\n(D) Shoplifting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 429, "doc": {"video_name": "surveil_194.mp4", "duration": 1320.17, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Normal\n(C) Robbery\n(D) Arrest\n", "candidates": ["Stealing", "Normal", "Robbery", "Arrest"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q429"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d18cdc3d3da2b7278c63fe1173f8c8f03bdb74aa77d2900e5209ea0e3c7cdc2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Normal\n(C) Robbery\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Normal\n(C) Robbery\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 433, "doc": {"video_name": "surveil_139.mp4", "duration": 239.03, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Vandalism\n(C) Normal\n(D) Robbery\n", "candidates": ["Assault", "Vandalism", "Normal", "Robbery"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q433"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f046cc1832723d93022d60a2b2ddee55f1e78ddd3c2fab239a2f97207150e5a8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Vandalism\n(C) Normal\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Vandalism\n(C) Normal\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 437, "doc": {"video_name": "surveil_152.mp4", "duration": 290.9, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Arrest\n(C) Normal\n(D) Robbery\n", "candidates": ["Abuse", "Arrest", "Normal", "Robbery"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q437"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2559ab50304e09ac7f9171ae644a8792389ff07e3102fb2cad1cc3fd29890e10", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Arrest\n(C) Normal\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Arrest\n(C) Normal\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 441, "doc": {"video_name": "surveil_130.mp4", "duration": 375.26, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Assault\n(C) Burglary\n(D) Robbery\n", "candidates": ["Shoplifting", "Assault", "Burglary", "Robbery"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q441"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4bcc667bf2ede588a7c663e7257837141d7c7ab66dfafcb261b879ae697af374", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Assault\n(C) Burglary\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Assault\n(C) Burglary\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 445, "doc": {"video_name": "surveil_37.mp4", "duration": 2223.27, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Fighting\n(C) Shoplifting\n(D) Explosion\n", "candidates": ["Burglary", "Fighting", "Shoplifting", "Explosion"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q445"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1144695a0d972090895c4e6289a0b984ba5ddc09f4767f1ea1232ae5dfb1e1eb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Fighting\n(C) Shoplifting\n(D) Explosion\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Fighting\n(C) Shoplifting\n(D) Explosion\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 449, "doc": {"video_name": "surveil_186.mp4", "duration": 403.58, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Abuse\n(B) Arson\n(C) Stealing\n(D) Robbery\n", "candidates": ["Abuse", "Arson", "Stealing", "Robbery"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q449"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4344751177c60043b7035ded7dbfafff2c4a98ef83e542ebcd0003a5f7614e8f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Abuse\n(B) Arson\n(C) Stealing\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Abuse\n(B) Arson\n(C) Stealing\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 453, "doc": {"video_name": "surveil_41.mp4", "duration": 781.1, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Burglary\n(B) Shoplifting\n(C) Normal\n(D) Explosion\n", "candidates": ["Burglary", "Shoplifting", "Normal", "Explosion"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q453"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f0da32b0248b69f2e21ae9b0a72bf28d34619f5f74634abf2e7eed93d87257c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Burglary\n(B) Shoplifting\n(C) Normal\n(D) Explosion\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Burglary\n(B) Shoplifting\n(C) Normal\n(D) Explosion\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 457, "doc": {"video_name": "surveil_9.mp4", "duration": 220.07999999999998, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Robbery\n(C) Stealing\n(D) Fighting\n", "candidates": ["Shooting", "Robbery", "Stealing", "Fighting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q457"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fe70c35eebf59d38e2ace278a95947ba89f47e933a2c9bdf5511c9d8be9bb356", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Robbery\n(C) Stealing\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Robbery\n(C) Stealing\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 461, "doc": {"video_name": "surveil_136.mp4", "duration": 195.84, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Arrest\n(C) Stealing\n(D) RoadAccidents\n", "candidates": ["Arson", "Arrest", "Stealing", "RoadAccidents"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q461"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "03663029ef94243d051c64921b309bf01923d9321fe499f1ffabf41e0ceed616", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Arrest\n(C) Stealing\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Arrest\n(C) Stealing\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 465, "doc": {"video_name": "surveil_72.mp4", "duration": 193.73, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Vandalism\n(B) Abuse\n(C) Fighting\n(D) Arson\n", "candidates": ["Vandalism", "Abuse", "Fighting", "Arson"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q465"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a40ff7ffd0218193ef7fa86c7c538347483df93c7a807e6bac00547c8423115a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Vandalism\n(B) Abuse\n(C) Fighting\n(D) Arson\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Vandalism\n(B) Abuse\n(C) Fighting\n(D) Arson\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 469, "doc": {"video_name": "surveil_48.mp4", "duration": 435.26, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Shoplifting\n(C) Arson\n(D) Normal\n", "candidates": ["Stealing", "Shoplifting", "Arson", "Normal"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q469"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c921139374e7e19805c47e0100e82188864c3ff97f9ac060ce7ffa8408ef58c9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Shoplifting\n(C) Arson\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Shoplifting\n(C) Arson\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 473, "doc": {"video_name": "surveil_154.mp4", "duration": 343.6, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) Arson\n(C) Assault\n(D) Vandalism\n", "candidates": ["Stealing", "Arson", "Assault", "Vandalism"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q473"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b270f03eb3871a8e4f888ca78d932696d30c6ffe2cb24fed7f781eb4ebfe3575", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) Arson\n(C) Assault\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) Arson\n(C) Assault\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 477, "doc": {"video_name": "surveil_145.mp4", "duration": 211.07, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Robbery\n(C) Explosion\n(D) Abuse\n", "candidates": ["Arson", "Robbery", "Explosion", "Abuse"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q477"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2feb807db7fe25e54b94e1552ad52456bdfcd528461212a1874932be4f42bd49", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Robbery\n(C) Explosion\n(D) Abuse\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Robbery\n(C) Explosion\n(D) Abuse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 481, "doc": {"video_name": "surveil_105.mp4", "duration": 207.17000000000002, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Burglary\n(C) Arson\n(D) Shoplifting\n", "candidates": ["Robbery", "Burglary", "Arson", "Shoplifting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q481"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "db271f308a319fa2a14222dd5b91a943f0be1d3452900b37a46939567a1b63b7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Burglary\n(C) Arson\n(D) Shoplifting\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "B"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Burglary\n(C) Arson\n(D) Shoplifting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 485, "doc": {"video_name": "surveil_4.mp4", "duration": 192.03, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shooting\n(B) Shooting\n(C) Robbery\n(D) Burglary\n", "candidates": ["Shooting", "Shooting", "Robbery", "Burglary"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q485"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "075c6b6aab9e12451a824bc2f7f8e8e903e959321247943669b5972d76248056", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shooting\n(B) Shooting\n(C) Robbery\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shooting\n(B) Shooting\n(C) Robbery\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 489, "doc": {"video_name": "surveil_67.mp4", "duration": 543.0, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Arson\n(C) Explosion\n(D) RoadAccidents\n", "candidates": ["Vandalism", "Arson", "Explosion", "RoadAccidents"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q489"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "91eed230ada9d19daccf9f46b19d97e042add40276b42b5df9ea4f4bf89a66ac", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Arson\n(C) Explosion\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Arson\n(C) Explosion\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 493, "doc": {"video_name": "surveil_0.mp4", "duration": 875.51, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Shooting\n(B) Fighting\n(C) Assault\n(D) Arson\n", "candidates": ["Shooting", "Fighting", "Assault", "Arson"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q493"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e213725d0ccc272bcfac4bc56710ab042315723fa7a9b20230f463058472c077", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Shooting\n(B) Fighting\n(C) Assault\n(D) Arson\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Shooting\n(B) Fighting\n(C) Assault\n(D) Arson\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 497, "doc": {"video_name": "surveil_52.mp4", "duration": 208.61, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Burglary\n(C) Shooting\n(D) Arson\n", "candidates": ["Shooting", "Burglary", "Shooting", "Arson"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q497"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7f9b02e3d6300ac8584441635665c8176c56d0d69784a5a532541b4b9b8d044a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Burglary\n(C) Shooting\n(D) Arson\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Burglary\n(C) Shooting\n(D) Arson\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 501, "doc": {"video_name": "surveil_77.mp4", "duration": 291.04, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Burglary\n(B) Normal\n(C) Abuse\n(D) Stealing\n", "candidates": ["Burglary", "Normal", "Abuse", "Stealing"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q501"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "225db7bb68e73bbd1e6f786e4c11bd442dbc1c80323be9b8e3c4242d6ea355af", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Burglary\n(B) Normal\n(C) Abuse\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Burglary\n(B) Normal\n(C) Abuse\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 505, "doc": {"video_name": "surveil_38.mp4", "duration": 355.4, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Assault\n(B) RoadAccidents\n(C) Normal\n(D) Stealing\n", "candidates": ["Assault", "RoadAccidents", "Normal", "Stealing"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q505"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "878a317eaeab36311c4d68938efcc55a09279a5095cf9d917d535e1016ff8492", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Assault\n(B) RoadAccidents\n(C) Normal\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Assault\n(B) RoadAccidents\n(C) Normal\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 509, "doc": {"video_name": "surveil_78.mp4", "duration": 294.96, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Arrest\n(C) Normal\n(D) Shooting\n", "candidates": ["RoadAccidents", "Arrest", "Normal", "Shooting"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q509"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "02f084693cd492facfb413d3d0c92d628fadf9174658ab4a3bed76eabaf47549", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Arrest\n(C) Normal\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Arrest\n(C) Normal\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 513, "doc": {"video_name": "surveil_101.mp4", "duration": 419.73, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Vandalism\n(C) Robbery\n(D) Fighting\n", "candidates": ["Stealing", "Vandalism", "Robbery", "Fighting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q513"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1709e7d471ffbce4eb097c7b8c71744ebe2f1e9e0403d49ee745c2f9bd3e964d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Vandalism\n(C) Robbery\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Vandalism\n(C) Robbery\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 517, "doc": {"video_name": "surveil_127.mp4", "duration": 4218.5, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arson\n(B) RoadAccidents\n(C) Abuse\n(D) Assault\n", "candidates": ["Arson", "RoadAccidents", "Abuse", "Assault"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q517"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b7ed5242e7b0aabf73d8a8461c86d5809f028227cc537856c26cb03a136cee", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arson\n(B) RoadAccidents\n(C) Abuse\n(D) Assault\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arson\n(B) RoadAccidents\n(C) Abuse\n(D) Assault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 521, "doc": {"video_name": "surveil_73.mp4", "duration": 212.97, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) Shoplifting\n(C) Arrest\n(D) Robbery\n", "candidates": ["Explosion", "Shoplifting", "Arrest", "Robbery"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q521"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ecabb14f899f42e318779ba7d81160180b721cd89225512e8c3d7a5da4eb67f2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) Shoplifting\n(C) Arrest\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "C"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) Shoplifting\n(C) Arrest\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 525, "doc": {"video_name": "surveil_148.mp4", "duration": 180.33, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Assault\n(B) Burglary\n(C) Vandalism\n(D) Normal\n", "candidates": ["Assault", "Burglary", "Vandalism", "Normal"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q525"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4e52ea13a48b53cd7f86d0e60ff9581f9125662e1e2fe6932449097e0537c757", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Assault\n(B) Burglary\n(C) Vandalism\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Assault\n(B) Burglary\n(C) Vandalism\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 529, "doc": {"video_name": "surveil_175.mp4", "duration": 437.17, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Robbery\n(B) Normal\n(C) Explosion\n(D) Vandalism\n", "candidates": ["Robbery", "Normal", "Explosion", "Vandalism"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q529"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0b4c68a000d2a6498ed32dbea7f87d722662d7abdf8c26e5a32cedf8ce0dc608", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Robbery\n(B) Normal\n(C) Explosion\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Robbery\n(B) Normal\n(C) Explosion\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 533, "doc": {"video_name": "surveil_107.mp4", "duration": 639.59, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Stealing\n(C) RoadAccidents\n(D) Arrest\n", "candidates": ["Fighting", "Stealing", "RoadAccidents", "Arrest"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q533"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "19b22fe91e7c340bf6bee6f71b3d65df5d5533f1bacd797f3b4aead46d3ff16e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Stealing\n(C) RoadAccidents\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Stealing\n(C) RoadAccidents\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 537, "doc": {"video_name": "surveil_44.mp4", "duration": 206.36, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) RoadAccidents\n(C) Abuse\n(D) Arrest\n", "candidates": ["Explosion", "RoadAccidents", "Abuse", "Arrest"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q537"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "607fcb8abe1c1cb315024d98b3eec39b8f1deeb9ab042b005e048b468c03f4f0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) RoadAccidents\n(C) Abuse\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) RoadAccidents\n(C) Abuse\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 541, "doc": {"video_name": "surveil_116.mp4", "duration": 291.18, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shooting\n(B) Arrest\n(C) Stealing\n(D) Robbery\n", "candidates": ["Shooting", "Arrest", "Stealing", "Robbery"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q541"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ce54afa6f7de0d1c7c066af89bdc4f0e8a31621dfb604873cb9ebbc7871456f5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shooting\n(B) Arrest\n(C) Stealing\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shooting\n(B) Arrest\n(C) Stealing\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 545, "doc": {"video_name": "surveil_70.mp4", "duration": 185.27, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Normal\n(C) Explosion\n(D) Vandalism\n", "candidates": ["Shooting", "Normal", "Explosion", "Vandalism"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q545"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ad094083aa0cf99c56cbbaa5daab7625c320835bff3c8e7fd18cb4ccf349efec", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Normal\n(C) Explosion\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Normal\n(C) Explosion\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 549, "doc": {"video_name": "surveil_125.mp4", "duration": 190.32, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Robbery\n(C) RoadAccidents\n(D) Burglary\n", "candidates": ["Fighting", "Robbery", "RoadAccidents", "Burglary"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q549"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6bc83023bc2c16fa88afeaebb6b29f698b99270c77a32b412deddcedaebedd9b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Robbery\n(C) RoadAccidents\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Robbery\n(C) RoadAccidents\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 553, "doc": {"video_name": "surveil_66.mp4", "duration": 319.72, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Explosion\n(C) Normal\n(D) Burglary\n", "candidates": ["Assault", "Explosion", "Normal", "Burglary"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q553"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d4016755d35b1a1c44d603561a7854181e61d04a5dfe807d0e18ab6196ab82a2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Explosion\n(C) Normal\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Explosion\n(C) Normal\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 557, "doc": {"video_name": "count_108.mp4", "duration": 500.06, "question": "In this video, how many instances are there of the 'carving pumpkin' action scene in total?\n(A) 5\n(B) 3\n(C) 1\n(D) 0\n", "candidates": ["5", "3", "1", "0"], "answer": "A", "task_type": "count", "question_id": "Q557"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7b7d82040763ea9647cbd7a82b185d73cd063da3ca7fcd70b145fc26d35acbb4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'carving pumpkin' action scene in total?\n(A) 5\n(B) 3\n(C) 1\n(D) 0\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "In this video, how many instances are there of the 'carving pumpkin' action scene in total?\n(A) 5\n(B) 3\n(C) 1\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 561, "doc": {"video_name": "count_203.mp4", "duration": 805.06, "question": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 6\n(B) 4\n(C) 5\n(D) 1\n", "candidates": ["6", "4", "5", "1"], "answer": "D", "task_type": "count", "question_id": "Q561"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "abcbeb672893dc2a780b3ddbd66faa02030c38eb32bc4421b8f4b69b477d7394", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 6\n(B) 4\n(C) 5\n(D) 1\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 6\n(B) 4\n(C) 5\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 565, "doc": {"video_name": "count_75.mp4", "duration": 460.01, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cleaning toilet' action\n(A) 6\n(B) 5\n(C) 0\n(D) 1\n", "candidates": ["6", "5", "0", "1"], "answer": "D", "task_type": "count", "question_id": "Q565"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "555101a0438a8a1846427682c74b71c4c4b74ac60c72a5ec85e66439a6f4c382", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cleaning toilet' action\n(A) 6\n(B) 5\n(C) 0\n(D) 1\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cleaning toilet' action\n(A) 6\n(B) 5\n(C) 0\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 569, "doc": {"video_name": "count_132.mp4", "duration": 480.02, "question": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 1\n(B) 0\n(C) 3\n(D) 5\n", "candidates": ["1", "0", "3", "5"], "answer": "C", "task_type": "count", "question_id": "Q569"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ed8060989c198ffa59fdf5bb2320505fef0b3b0c5c5220cf765a36d468bdcb42", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 1\n(B) 0\n(C) 3\n(D) 5\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 1\n(B) 0\n(C) 3\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 573, "doc": {"video_name": "count_18.mp4", "duration": 871.75, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'tossing coin' action\n(A) 3\n(B) 1\n(C) 2\n(D) 0\n", "candidates": ["3", "1", "2", "0"], "answer": "A", "task_type": "count", "question_id": "Q573"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9c1240d8d70cf72a3c0e788748798f1cab77513dc35a886b443479746657a29e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'tossing coin' action\n(A) 3\n(B) 1\n(C) 2\n(D) 0\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'tossing coin' action\n(A) 3\n(B) 1\n(C) 2\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 577, "doc": {"video_name": "count_15.mp4", "duration": 490.05999999999995, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'tossing coin' action\n(A) 5\n(B) 4\n(C) 6\n(D) 3\n", "candidates": ["5", "4", "6", "3"], "answer": "B", "task_type": "count", "question_id": "Q577"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3edc2aaee7835bdb16aaa0829a424adaa5d3e18a747a815c6a5f8f8716f93060", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'tossing coin' action\n(A) 5\n(B) 4\n(C) 6\n(D) 3\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'tossing coin' action\n(A) 5\n(B) 4\n(C) 6\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 581, "doc": {"video_name": "count_121.mp4", "duration": 500.07, "question": "In this video, how many times does the scene of the 'playing trombone' action appear in total?\n(A) 6\n(B) 0\n(C) 5\n(D) 2\n", "candidates": ["6", "0", "5", "2"], "answer": "C", "task_type": "count", "question_id": "Q581"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "269476c34b92c2fa284b26435a760fbc7b71f57d032b77d64adad4ad9266d1f2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'playing trombone' action appear in total?\n(A) 6\n(B) 0\n(C) 5\n(D) 2\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "In this video, how many times does the scene of the 'playing trombone' action appear in total?\n(A) 6\n(B) 0\n(C) 5\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 585, "doc": {"video_name": "count_103.mp4", "duration": 510.03, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 4\n(B) 5\n(C) 3\n(D) 1\n", "candidates": ["4", "5", "3", "1"], "answer": "C", "task_type": "count", "question_id": "Q585"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "731469b862920ec49daf7ed0cbcc2e1c773a27ec30700606194c6ee41f29467f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 4\n(B) 5\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 4\n(B) 5\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 589, "doc": {"video_name": "count_102.mp4", "duration": 1395.69, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 2\n(B) 4\n(C) 0\n(D) 6\n", "candidates": ["2", "4", "0", "6"], "answer": "B", "task_type": "count", "question_id": "Q589"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f6f3c59a78bc9a9e076791b372880d1e160ffc82b9658416682ad091dbb1d67", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 2\n(B) 4\n(C) 0\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 2\n(B) 4\n(C) 0\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 593, "doc": {"video_name": "count_165.mp4", "duration": 490.01, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 1\n(B) 3\n(C) 0\n(D) 4\n", "candidates": ["1", "3", "0", "4"], "answer": "A", "task_type": "count", "question_id": "Q593"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "72e7f576a07eb8d4671a9dbcdea7ecceb6aa99db9d3815298a31a7b651039be5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 1\n(B) 3\n(C) 0\n(D) 4\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 1\n(B) 3\n(C) 0\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 597, "doc": {"video_name": "count_68.mp4", "duration": 864.3100000000001, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'pole vault' action\n(A) 3\n(B) 6\n(C) 5\n(D) 2\n", "candidates": ["3", "6", "5", "2"], "answer": "D", "task_type": "count", "question_id": "Q597"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "446af4de49f3325dec7c5a1e6d62c6b89726ca62050b3a28612429dcd5b17cc0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'pole vault' action\n(A) 3\n(B) 6\n(C) 5\n(D) 2\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'pole vault' action\n(A) 3\n(B) 6\n(C) 5\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 601, "doc": {"video_name": "count_235.mp4", "duration": 7379.0, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 0\n(B) 4\n(C) 3\n(D) 1\n", "candidates": ["0", "4", "3", "1"], "answer": "C", "task_type": "count", "question_id": "Q601"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b8436ffded7a826b34659f489f69ef2e23ad70836b714743abeb2c5259851958", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 0\n(B) 4\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 0\n(B) 4\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 605, "doc": {"video_name": "count_62.mp4", "duration": 495.68, "question": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 0\n(B) 5\n(C) 1\n(D) 3\n", "candidates": ["0", "5", "1", "3"], "answer": "B", "task_type": "count", "question_id": "Q605"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "45a8d363287fa765a71c38b595887ac8a1f3a2d9756ee8962ec3d5f682f0984c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 0\n(B) 5\n(C) 1\n(D) 3\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 0\n(B) 5\n(C) 1\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 609, "doc": {"video_name": "count_164.mp4", "duration": 470.13, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 6\n(B) 2\n(C) 3\n(D) 1\n", "candidates": ["6", "2", "3", "1"], "answer": "B", "task_type": "count", "question_id": "Q609"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3ea290f73572e3faa1278bd316de39c7cd0238284aba4f057a89087db018421f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 6\n(B) 2\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 6\n(B) 2\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 613, "doc": {"video_name": "count_171.mp4", "duration": 480.03, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 3\n(B) 6\n(C) 1\n(D) 4\n", "candidates": ["3", "6", "1", "4"], "answer": "A", "task_type": "count", "question_id": "Q613"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5efb72a800f7f09065cc0025cf8b0b846a04b90ddcb8ef29f962c2a0dd5d913a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 3\n(B) 6\n(C) 1\n(D) 4\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 3\n(B) 6\n(C) 1\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 617, "doc": {"video_name": "count_135.mp4", "duration": 475.97999999999996, "question": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 0\n(B) 4\n(C) 5\n(D) 3\n", "candidates": ["0", "4", "5", "3"], "answer": "B", "task_type": "count", "question_id": "Q617"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8db9607a3ab9ee4745e6ededc80aac12abc99268e120734eba37647f97ab4c05", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 0\n(B) 4\n(C) 5\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "B"}, "input": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 0\n(B) 4\n(C) 5\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 621, "doc": {"video_name": "count_98.mp4", "duration": 480.01, "question": "In this video, how many times does the scene of the 'playing harp' action appear in total?\n(A) 0\n(B) 5\n(C) 2\n(D) 3\n", "candidates": ["0", "5", "2", "3"], "answer": "D", "task_type": "count", "question_id": "Q621"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "718091595a8f0f4f6b00de0ce0753be28057a8f136ba09834d1965a18dd10cbd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'playing harp' action appear in total?\n(A) 0\n(B) 5\n(C) 2\n(D) 3\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many times does the scene of the 'playing harp' action appear in total?\n(A) 0\n(B) 5\n(C) 2\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 625, "doc": {"video_name": "count_94.mp4", "duration": 520.04, "question": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 4\n(B) 2\n(C) 3\n(D) 1\n", "candidates": ["4", "2", "3", "1"], "answer": "A", "task_type": "count", "question_id": "Q625"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3aef10c1c60e199338c02c64513050a642b5d5a343b72731740c13c306b7e90d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 4\n(B) 2\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 4\n(B) 2\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 629, "doc": {"video_name": "count_109.mp4", "duration": 520.05, "question": "In this video, how many times does the scene of the 'carving pumpkin' action appear in total?\n(A) 4\n(B) 0\n(C) 1\n(D) 3\n", "candidates": ["4", "0", "1", "3"], "answer": "A", "task_type": "count", "question_id": "Q629"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5c5b14839be860af4d4480064bebfd3eef65aabb093061618b018819d885afd5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'carving pumpkin' action appear in total?\n(A) 4\n(B) 0\n(C) 1\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many times does the scene of the 'carving pumpkin' action appear in total?\n(A) 4\n(B) 0\n(C) 1\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 633, "doc": {"video_name": "count_123.mp4", "duration": 496.97, "question": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 0\n(B) 5\n(C) 4\n(D) 2\n", "candidates": ["0", "5", "4", "2"], "answer": "B", "task_type": "count", "question_id": "Q633"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f9efb113ce361efab6538c645c1a8fa2e8a099b2a9b8c8a273399195cc205bf9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 0\n(B) 5\n(C) 4\n(D) 2\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 0\n(B) 5\n(C) 4\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 637, "doc": {"video_name": "count_56.mp4", "duration": 862.99, "question": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 3\n(B) 1\n(C) 4\n(D) 2\n", "candidates": ["3", "1", "4", "2"], "answer": "B", "task_type": "count", "question_id": "Q637"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f740bd1f081e1662f9dd5e719c67a12f3bed5dd3ff2a74fa7bcaeed4470b3212", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 3\n(B) 1\n(C) 4\n(D) 2\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 3\n(B) 1\n(C) 4\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 641, "doc": {"video_name": "count_130.mp4", "duration": 470.14, "question": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 2\n", "candidates": ["1", "6", "3", "2"], "answer": "D", "task_type": "count", "question_id": "Q641"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ac62cd7bc38b4845ff96d5f6dc2a7cfc84bf97257f27c4763e67d2e3fd114fae", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 2\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 645, "doc": {"video_name": "count_189.mp4", "duration": 490.0, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 2\n(B) 5\n(C) 6\n(D) 1\n", "candidates": ["2", "5", "6", "1"], "answer": "D", "task_type": "count", "question_id": "Q645"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9f98daff12b7adee31d4ce2adeda563f3b6b0a7387867ae3b86782486153d10e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 2\n(B) 5\n(C) 6\n(D) 1\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 2\n(B) 5\n(C) 6\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 649, "doc": {"video_name": "count_196.mp4", "duration": 480.15, "question": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 3\n(B) 4\n(C) 1\n(D) 0\n", "candidates": ["3", "4", "1", "0"], "answer": "A", "task_type": "count", "question_id": "Q649"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0bdd5c9f19b7127b8c9fe20005cc632b028fcb4e03ec9f08b3cd60b2748a7b05", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 3\n(B) 4\n(C) 1\n(D) 0\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 3\n(B) 4\n(C) 1\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 653, "doc": {"video_name": "count_0.mp4", "duration": 469.54999999999995, "question": "In this video, how many instances are there of the 'abseiling' action scene in total?\n(A) 1\n(B) 6\n(C) 2\n(D) 4\n", "candidates": ["1", "6", "2", "4"], "answer": "C", "task_type": "count", "question_id": "Q653"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5a6b7bffdfb5341ba5c5c6a42770bd285300ae78a4a1a2e7d00249a892a9ea95", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'abseiling' action scene in total?\n(A) 1\n(B) 6\n(C) 2\n(D) 4\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many instances are there of the 'abseiling' action scene in total?\n(A) 1\n(B) 6\n(C) 2\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 657, "doc": {"video_name": "count_87.mp4", "duration": 530.05, "question": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 5\n(B) 4\n(C) 1\n(D) 3\n", "candidates": ["5", "4", "1", "3"], "answer": "A", "task_type": "count", "question_id": "Q657"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c67717cba08d7682dbb6292f18119cd4f65391b1e29d86fb048a9e7dc94a7457", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 5\n(B) 4\n(C) 1\n(D) 3\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 5\n(B) 4\n(C) 1\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 661, "doc": {"video_name": "count_100.mp4", "duration": 460.01, "question": "In this video, how many instances are there of the 'carving pumpkin' action scene in total?\n(A) 3\n(B) 4\n(C) 1\n(D) 5\n", "candidates": ["3", "4", "1", "5"], "answer": "C", "task_type": "count", "question_id": "Q661"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "687bac70035c292137e67a0d85e55aa07e23a55fb923f7bd5b2b9f7f73ae524a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'carving pumpkin' action scene in total?\n(A) 3\n(B) 4\n(C) 1\n(D) 5\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "In this video, how many instances are there of the 'carving pumpkin' action scene in total?\n(A) 3\n(B) 4\n(C) 1\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 665, "doc": {"video_name": "count_159.mp4", "duration": 470.01, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'shredding paper' action\n(A) 6\n(B) 5\n(C) 0\n(D) 2\n", "candidates": ["6", "5", "0", "2"], "answer": "D", "task_type": "count", "question_id": "Q665"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8fc05ad079b6e83512ea68d96454c6287b03f362657c1e7c5b5a6cfbaf1a69fd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'shredding paper' action\n(A) 6\n(B) 5\n(C) 0\n(D) 2\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'shredding paper' action\n(A) 6\n(B) 5\n(C) 0\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 669, "doc": {"video_name": "count_166.mp4", "duration": 490.04, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 1\n(B) 0\n(C) 4\n(D) 3\n", "candidates": ["1", "0", "4", "3"], "answer": "C", "task_type": "count", "question_id": "Q669"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "97e05bdc3d6c93df17becc0c34edb248374076f12a1f3c03995780f4773e371e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 1\n(B) 0\n(C) 4\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 1\n(B) 0\n(C) 4\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 673, "doc": {"video_name": "count_6.mp4", "duration": 480.03, "question": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 4\n(B) 5\n(C) 3\n(D) 1\n", "candidates": ["4", "5", "3", "1"], "answer": "C", "task_type": "count", "question_id": "Q673"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5cf0836d3615978c002bfd72aa8863e837db2d845a17d3f5ab013308363d3e0d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 4\n(B) 5\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 4\n(B) 5\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 677, "doc": {"video_name": "count_1.mp4", "duration": 906.9000000000001, "question": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 3\n(B) 2\n(C) 5\n(D) 1\n", "candidates": ["3", "2", "5", "1"], "answer": "A", "task_type": "count", "question_id": "Q677"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "698914a37cb16e8b5dd48fae7cc9665ec3f69d94bf5c3b6f1dbc6879ac32bcd6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 3\n(B) 2\n(C) 5\n(D) 1\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 3\n(B) 2\n(C) 5\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 681, "doc": {"video_name": "count_46.mp4", "duration": 500.02000000000004, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 6\n(B) 1\n(C) 2\n(D) 5\n", "candidates": ["6", "1", "2", "5"], "answer": "C", "task_type": "count", "question_id": "Q681"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d6c547e248c74e5771538c933ddac04e06c67e54841292ff1afdfa474a37839a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 6\n(B) 1\n(C) 2\n(D) 5\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 6\n(B) 1\n(C) 2\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 685, "doc": {"video_name": "count_44.mp4", "duration": 526.8, "question": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 1\n(B) 2\n(C) 5\n(D) 6\n", "candidates": ["1", "2", "5", "6"], "answer": "C", "task_type": "count", "question_id": "Q685"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "84a991d555adfa926ef029ab6cf7abb861f3b215282953e3d4a999969fa03b57", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 1\n(B) 2\n(C) 5\n(D) 6\n", "task_type": "count", "pred_answer": "B", "answer": "C"}, "input": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 1\n(B) 2\n(C) 5\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 689, "doc": {"video_name": "count_40.mp4", "duration": 605.9599999999999, "question": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 3\n(B) 1\n(C) 4\n(D) 2\n", "candidates": ["3", "1", "4", "2"], "answer": "C", "task_type": "count", "question_id": "Q689"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "189c11f6219c2b62ea0a73bcd8db5c9f0f78995c5737c39f601e9dcfef819569", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 3\n(B) 1\n(C) 4\n(D) 2\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 3\n(B) 1\n(C) 4\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 693, "doc": {"video_name": "count_154.mp4", "duration": 490.03999999999996, "question": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 5\n(B) 4\n(C) 0\n(D) 2\n", "candidates": ["5", "4", "0", "2"], "answer": "B", "task_type": "count", "question_id": "Q693"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "15a8c6aee2e01ce88a627a437a11b2705366b97b344a96fb198effdca01885b8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 5\n(B) 4\n(C) 0\n(D) 2\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 5\n(B) 4\n(C) 0\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 697, "doc": {"video_name": "count_224.mp4", "duration": 631.04, "question": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 1\n(B) 5\n(C) 2\n(D) 4\n", "candidates": ["1", "5", "2", "4"], "answer": "C", "task_type": "count", "question_id": "Q697"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d98fb05e7db82722f21e175f112d86ae9c30532c7f6cd77ca5eb859652cf4fc0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 1\n(B) 5\n(C) 2\n(D) 4\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 1\n(B) 5\n(C) 2\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 701, "doc": {"video_name": "count_150.mp4", "duration": 470.0, "question": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 4\n(B) 3\n(C) 2\n(D) 0\n", "candidates": ["4", "3", "2", "0"], "answer": "C", "task_type": "count", "question_id": "Q701"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "edb7706dcfa69d8e96b694d3978565b37d9fc713ea85a6b49d86591244fcb577", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 4\n(B) 3\n(C) 2\n(D) 0\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 4\n(B) 3\n(C) 2\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 705, "doc": {"video_name": "count_8.mp4", "duration": 525.3199999999999, "question": "In this video, how many instances are there of the 'abseiling' action scene in total?\n(A) 2\n(B) 3\n(C) 4\n(D) 5\n", "candidates": ["2", "3", "4", "5"], "answer": "D", "task_type": "count", "question_id": "Q705"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "71f982c7af6a0e011d224bd856d60e6f831751829238e10959c4ce79e5171f1f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'abseiling' action scene in total?\n(A) 2\n(B) 3\n(C) 4\n(D) 5\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many instances are there of the 'abseiling' action scene in total?\n(A) 2\n(B) 3\n(C) 4\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 709, "doc": {"video_name": "count_38.mp4", "duration": 477.58, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 3\n(B) 6\n(C) 5\n(D) 4\n", "candidates": ["3", "6", "5", "4"], "answer": "A", "task_type": "count", "question_id": "Q709"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eaccd325e9291c9545ccc6b10d19430fb62c0e1b1f103ec23fd30357f44eeb90", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 3\n(B) 6\n(C) 5\n(D) 4\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 3\n(B) 6\n(C) 5\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 713, "doc": {"video_name": "count_34.mp4", "duration": 482.26, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 1\n(B) 4\n(C) 6\n(D) 0\n", "candidates": ["1", "4", "6", "0"], "answer": "B", "task_type": "count", "question_id": "Q713"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d94be069327ea2486418ac7b4775722e47e4a53bccc149d49cdbe3afbfb2d641", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 1\n(B) 4\n(C) 6\n(D) 0\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 1\n(B) 4\n(C) 6\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 717, "doc": {"video_name": "count_31.mp4", "duration": 495.40999999999997, "question": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 2\n(B) 5\n(C) 1\n(D) 0\n", "candidates": ["2", "5", "1", "0"], "answer": "A", "task_type": "count", "question_id": "Q717"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "52d90cbd4e7648f4b8c96783e2d76a74165def2ddacac5bf27af67691b2be28d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 2\n(B) 5\n(C) 1\n(D) 0\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 2\n(B) 5\n(C) 1\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 721, "doc": {"video_name": "count_120.mp4", "duration": 460.14, "question": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 2\n(B) 4\n(C) 3\n(D) 1\n", "candidates": ["2", "4", "3", "1"], "answer": "D", "task_type": "count", "question_id": "Q721"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2cf370caf42702986ed79317cd14871a0ab2158f02bf90dce6395c98d71d07bf", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 2\n(B) 4\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 2\n(B) 4\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 725, "doc": {"video_name": "count_112.mp4", "duration": 480.02, "question": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 3\n(B) 0\n(C) 1\n(D) 5\n", "candidates": ["3", "0", "1", "5"], "answer": "A", "task_type": "count", "question_id": "Q725"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b9a7947bf9fd31f17c37ad6cfa7fab922cbaf25a126ef6e73f9b1176af902a56", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 3\n(B) 0\n(C) 1\n(D) 5\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 3\n(B) 0\n(C) 1\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 729, "doc": {"video_name": "count_101.mp4", "duration": 490.01, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 4\n(B) 6\n(C) 5\n(D) 1\n", "candidates": ["4", "6", "5", "1"], "answer": "D", "task_type": "count", "question_id": "Q729"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7db310dde58fa26ae9a602d843fe0b08025614107a6ba834fec704fc9350da04", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 4\n(B) 6\n(C) 5\n(D) 1\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 4\n(B) 6\n(C) 5\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 733, "doc": {"video_name": "count_5.mp4", "duration": 479.6, "question": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 3\n(B) 0\n(C) 4\n(D) 2\n", "candidates": ["3", "0", "4", "2"], "answer": "A", "task_type": "count", "question_id": "Q733"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f6afd0ec25360696e2f6c7767d56459df18b218599c6c28161ae0d7352c6b57", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 3\n(B) 0\n(C) 4\n(D) 2\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 3\n(B) 0\n(C) 4\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 737, "doc": {"video_name": "count_156.mp4", "duration": 480.15999999999997, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'shredding paper' action\n(A) 5\n(B) 4\n(C) 2\n(D) 3\n", "candidates": ["5", "4", "2", "3"], "answer": "D", "task_type": "count", "question_id": "Q737"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "82d1d7c89efc702ec7c2fd0a0faec8c1f38bb5a66b691c66cf1985b91e07446f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'shredding paper' action\n(A) 5\n(B) 4\n(C) 2\n(D) 3\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'shredding paper' action\n(A) 5\n(B) 4\n(C) 2\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 741, "doc": {"video_name": "count_181.mp4", "duration": 500.04999999999995, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 4\n(B) 5\n(C) 0\n(D) 2\n", "candidates": ["4", "5", "0", "2"], "answer": "B", "task_type": "count", "question_id": "Q741"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5f7227d142d783705dccd700d6a489fc04e0eae2facac6479f4660d7b744e817", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 4\n(B) 5\n(C) 0\n(D) 2\n", "task_type": "count", "pred_answer": "D", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 4\n(B) 5\n(C) 0\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 745, "doc": {"video_name": "count_25.mp4", "duration": 476.72999999999996, "question": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 4\n", "candidates": ["1", "6", "3", "4"], "answer": "C", "task_type": "count", "question_id": "Q745"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "23584fc3f47812cebc2254c2f4b50e2b5333e67ac898f58122442e8ea6565155", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 4\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 749, "doc": {"video_name": "count_23.mp4", "duration": 886.09, "question": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 4\n(B) 2\n(C) 6\n(D) 3\n", "candidates": ["4", "2", "6", "3"], "answer": "D", "task_type": "count", "question_id": "Q749"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6bdff8cdd4c70944d540c9c796556db3a314ad7f7f4a44acdc08e1aecadc8bf2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 4\n(B) 2\n(C) 6\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 4\n(B) 2\n(C) 6\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 753, "doc": {"video_name": "count_208.mp4", "duration": 234.03, "question": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 2\n(B) 3\n(C) 4\n(D) 0\n", "candidates": ["2", "3", "4", "0"], "answer": "A", "task_type": "count", "question_id": "Q753"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c11d9b95593cc7f4385257b9ffc30563e9d1fd5b39c4153e7feb0b7a72cc6792", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 2\n(B) 3\n(C) 4\n(D) 0\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 2\n(B) 3\n(C) 4\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 757, "doc": {"video_name": "count_137.mp4", "duration": 500.02, "question": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 1\n(B) 2\n(C) 6\n(D) 3\n", "candidates": ["1", "2", "6", "3"], "answer": "B", "task_type": "count", "question_id": "Q757"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "841a5fca31992b9c505d79e142b711cfe0b4a24f2d1ce036313c3440865484de", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 1\n(B) 2\n(C) 6\n(D) 3\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 1\n(B) 2\n(C) 6\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 761, "doc": {"video_name": "ego_35.mp4", "duration": 408.63333328750014, "question": "What did I put in the orange trashcan \n(A) a lemon green sponge\n(B) a blue pen\n(C) a red apple\n(D) a yellow banana\n", "candidates": ["a lemon green sponge", "a blue pen", "a red apple", "a yellow banana"], "answer": "A", "task_type": "ego", "question_id": "Q761"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9356edf90dc6733fd8dc455e70e010ceba3690590779e467ae7f342dc5f4aaca", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I put in the orange trashcan \n(A) a lemon green sponge\n(B) a blue pen\n(C) a red apple\n(D) a yellow banana\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What did I put in the orange trashcan \n(A) a lemon green sponge\n(B) a blue pen\n(C) a red apple\n(D) a yellow banana\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 765, "doc": {"video_name": "ego_29.mp4", "duration": 428.0666666666666, "question": "How many bolts did I screw on the circuit board of the first radio?\n(A) 3\n(B) 1\n(C) 7\n(D) 5\n", "candidates": ["3", "1", "7", "5"], "answer": "A", "task_type": "ego", "question_id": "Q765"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c2ee47f9d7ea184585e4dbb725f8fe13b3f87502d3a2ed5f4399231485cd2c33", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many bolts did I screw on the circuit board of the first radio?\n(A) 3\n(B) 1\n(C) 7\n(D) 5\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "How many bolts did I screw on the circuit board of the first radio?\n(A) 3\n(B) 1\n(C) 7\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 769, "doc": {"video_name": "ego_72.mp4", "duration": 479.9999999999998, "question": "What did i put the plastic mug?\n\n(A) water\n(B) coffee\n(C) orange juice\n(D) milk\n", "candidates": ["water", "coffee", "orange juice", "milk"], "answer": "C", "task_type": "ego", "question_id": "Q769"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37cb72a780e58775a1e09bce0e6e4b914834c1bc33c33d085e17f273c83ffe14", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did i put the plastic mug?\n\n(A) water\n(B) coffee\n(C) orange juice\n(D) milk\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "What did i put the plastic mug?\n\n(A) water\n(B) coffee\n(C) orange juice\n(D) milk\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 773, "doc": {"video_name": "ego_50.mp4", "duration": 480.0333333333333, "question": "Where did I throw the trash from the plate?\n(A) in the dustbin\n(B) in the refrigerator\n(C) in the sink\n(D) on the floor\n", "candidates": ["in the dustbin", "in the refrigerator", "in the sink", "on the floor"], "answer": "A", "task_type": "ego", "question_id": "Q773"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ee260facbff9d17f36ec6d7602a66f44afaa8bacabd15fd2446da061fb164a6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I throw the trash from the plate?\n(A) in the dustbin\n(B) in the refrigerator\n(C) in the sink\n(D) on the floor\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I throw the trash from the plate?\n(A) in the dustbin\n(B) in the refrigerator\n(C) in the sink\n(D) on the floor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 777, "doc": {"video_name": "ego_22.mp4", "duration": 480.0, "question": "Who did I talk to in the living room before I first sat down\n(A) two women and one man\n(B) one woman and two men\n(C) three women\n(D) two men\n", "candidates": ["two women and one man", "one woman and two men", "three women", "two men"], "answer": "A", "task_type": "ego", "question_id": "Q777"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "818daa35d0596b6000e87147cd3e9ac721c23e957e0aa6c1d0f820b2815a1659", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Who did I talk to in the living room before I first sat down\n(A) two women and one man\n(B) one woman and two men\n(C) three women\n(D) two men\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Who did I talk to in the living room before I first sat down\n(A) two women and one man\n(B) one woman and two men\n(C) three women\n(D) two men\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 781, "doc": {"video_name": "ego_29.mp4", "duration": 428.0666666666666, "question": "Where did I place the circuit board of the first radio on?\n(A) the back case of the first radio\n(B) underneath the power button\n(C) inside the battery compartment\n(D) on top of the speaker\n", "candidates": ["the back case of the first radio", "underneath the power button", "inside the battery compartment", "on top of the speaker"], "answer": "A", "task_type": "ego", "question_id": "Q781"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8389951b7ae321452a1ed08a0cace02fff6ad43f14f7b10a7a967ed33b1b634a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I place the circuit board of the first radio on?\n(A) the back case of the first radio\n(B) underneath the power button\n(C) inside the battery compartment\n(D) on top of the speaker\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I place the circuit board of the first radio on?\n(A) the back case of the first radio\n(B) underneath the power button\n(C) inside the battery compartment\n(D) on top of the speaker\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 785, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Who did I interact with when I skated\n(A) the dog running alongside\n(B) the woman on black sports tight\n(C) the man in red sneakers\n(D) the child with a skateboard\n", "candidates": ["the dog running alongside", "the woman on black sports tight", "the man in red sneakers", "the child with a skateboard"], "answer": "B", "task_type": "ego", "question_id": "Q785"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e3a6f50f6d65e9a7d1765e4418b604d2dee37a80e8b317e2cee2dc259fb20f7f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who did I interact with when I skated\n(A) the dog running alongside\n(B) the woman on black sports tight\n(C) the man in red sneakers\n(D) the child with a skateboard\n", "task_type": "ego", "pred_answer": "A", "answer": "B"}, "input": "Who did I interact with when I skated\n(A) the dog running alongside\n(B) the woman on black sports tight\n(C) the man in red sneakers\n(D) the child with a skateboard\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 789, "doc": {"video_name": "ego_5.mp4", "duration": 480.0, "question": "Did I pick up the green herdez jar?\n(A) I don't know.\n(B) No.\n(C) Yes.\n(D) I picked up the red herdez jar.\n", "candidates": ["I don't know.", "No.", "Yes.", "I picked up the red herdez jar."], "answer": "C", "task_type": "ego", "question_id": "Q789"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7e99f562dfab73dd5c23a5ad7f222e9bc6c2ac6d85d1ebf08d0aaaa895e57bed", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Did I pick up the green herdez jar?\n(A) I don't know.\n(B) No.\n(C) Yes.\n(D) I picked up the red herdez jar.\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "Did I pick up the green herdez jar?\n(A) I don't know.\n(B) No.\n(C) Yes.\n(D) I picked up the red herdez jar.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 793, "doc": {"video_name": "ego_30.mp4", "duration": 480.0, "question": "Where did I put the money?\n(A) Inside the draw.\n(B) In the kitchen cabinet\n(C) Under the bed\n(D) In my pocket\n", "candidates": ["Inside the draw.", "In the kitchen cabinet", "Under the bed", "In my pocket"], "answer": "A", "task_type": "ego", "question_id": "Q793"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e6dcecb83314a61b024188a1ced099e15111f0d78328d1b1333297e052611376", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I put the money?\n(A) Inside the draw.\n(B) In the kitchen cabinet\n(C) Under the bed\n(D) In my pocket\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I put the money?\n(A) Inside the draw.\n(B) In the kitchen cabinet\n(C) Under the bed\n(D) In my pocket\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 797, "doc": {"video_name": "ego_50.mp4", "duration": 480.0333333333333, "question": "Where did I put the white cloth?\n(A) under the bed\n(B) on the shelf\n(C) in the closet\n(D) in the drawer\n", "candidates": ["under the bed", "on the shelf", "in the closet", "in the drawer"], "answer": "D", "task_type": "ego", "question_id": "Q797"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "152bd672cfc2090d8428e9cf9431c327f1e5bcd6445683a2715cee35bd04fabd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where did I put the white cloth?\n(A) under the bed\n(B) on the shelf\n(C) in the closet\n(D) in the drawer\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where did I put the white cloth?\n(A) under the bed\n(B) on the shelf\n(C) in the closet\n(D) in the drawer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 801, "doc": {"video_name": "ego_3.mp4", "duration": 418.0333332874998, "question": "Where was the yellow shoe before I picked it up?\n(A) In the closet\n(B) In the car\n(C) On the table\n(D) On the floor\n", "candidates": ["In the closet", "In the car", "On the table", "On the floor"], "answer": "D", "task_type": "ego", "question_id": "Q801"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2fe861958ff7cbde27bfa5d4c270bb89677adb5da4dacd70cee2086fbd810b1c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the yellow shoe before I picked it up?\n(A) In the closet\n(B) In the car\n(C) On the table\n(D) On the floor\n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "Where was the yellow shoe before I picked it up?\n(A) In the closet\n(B) In the car\n(C) On the table\n(D) On the floor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 805, "doc": {"video_name": "ego_48.mp4", "duration": 480.0, "question": "How many bar soaps were on the washing  machine?\n(A) 5\n(B) 9\n(C) 3\n(D) 7\n", "candidates": ["5", "9", "3", "7"], "answer": "D", "task_type": "ego", "question_id": "Q805"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cfc0fbf2588dfa2745f8e3de5d37eb45fc7e5782182a93e736c2d0560302f49a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many bar soaps were on the washing  machine?\n(A) 5\n(B) 9\n(C) 3\n(D) 7\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "How many bar soaps were on the washing  machine?\n(A) 5\n(B) 9\n(C) 3\n(D) 7\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 809, "doc": {"video_name": "ego_81.mp4", "duration": 1200.0333333333333, "question": "What did I put in the egg yolk?\n(A) milk\n(B) cheese\n(C) bread\n(D) butter\n", "candidates": ["milk", "cheese", "bread", "butter"], "answer": "C", "task_type": "ego", "question_id": "Q809"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c5639600fa88ec6f3212f005c4b5f1f27cd792448329c280dcd08ba80ec6b081", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did I put in the egg yolk?\n(A) milk\n(B) cheese\n(C) bread\n(D) butter\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What did I put in the egg yolk?\n(A) milk\n(B) cheese\n(C) bread\n(D) butter\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 813, "doc": {"video_name": "ego_46.mp4", "duration": 480.0, "question": "Where was the 'wire plier'?\n(A) hanging on the wall\n(B) on the white boxes\n(C) in the toolbox\n(D) on the black boxes\n", "candidates": ["hanging on the wall", "on the white boxes", "in the toolbox", "on the black boxes"], "answer": "B", "task_type": "ego", "question_id": "Q813"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c28377bfa1ed14b9f382210b75907bb8ecba1f83c5a49ac9ba071ae95952153a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the 'wire plier'?\n(A) hanging on the wall\n(B) on the white boxes\n(C) in the toolbox\n(D) on the black boxes\n", "task_type": "ego", "pred_answer": "C", "answer": "B"}, "input": "Where was the 'wire plier'?\n(A) hanging on the wall\n(B) on the white boxes\n(C) in the toolbox\n(D) on the black boxes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 817, "doc": {"video_name": "ego_41.mp4", "duration": 480.0333333333333, "question": "What did I put on the shelf?\n(A) Book\n(B) Cup\n(C) Pencil\n(D) Spray\n", "candidates": ["Book", "Cup", "Pencil", "Spray"], "answer": "D", "task_type": "ego", "question_id": "Q817"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1cc61f593448ab4a9b8d0f8c0684f8ee060a26143c57ae351a676c329c8717e7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I put on the shelf?\n(A) Book\n(B) Cup\n(C) Pencil\n(D) Spray\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "What did I put on the shelf?\n(A) Book\n(B) Cup\n(C) Pencil\n(D) Spray\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 821, "doc": {"video_name": "ego_81.mp4", "duration": 1200.0333333333333, "question": "What did i put in the pan?\n(A) Salt\n(B) Sugar\n(C) Butter\n(D) Oil\n", "candidates": ["Salt", "Sugar", "Butter", "Oil"], "answer": "C", "task_type": "ego", "question_id": "Q821"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "62a36280385300acea1fa1a75cc25cf6ece6ba21cb0aa2058251f5e5b28de513", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did i put in the pan?\n(A) Salt\n(B) Sugar\n(C) Butter\n(D) Oil\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What did i put in the pan?\n(A) Salt\n(B) Sugar\n(C) Butter\n(D) Oil\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 825, "doc": {"video_name": "ego_64.mp4", "duration": 480.0, "question": "What did I take out of my left pocket ?\n(A) pocket folding knife\n(B) keys\n(C) phone\n(D) wallet\n", "candidates": ["pocket folding knife", "keys", "phone", "wallet"], "answer": "A", "task_type": "ego", "question_id": "Q825"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9017347f2571ec26fb7a6f20c809c279efa214d2ebc537a27ba8118da74c77ed", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I take out of my left pocket ?\n(A) pocket folding knife\n(B) keys\n(C) phone\n(D) wallet\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What did I take out of my left pocket ?\n(A) pocket folding knife\n(B) keys\n(C) phone\n(D) wallet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 829, "doc": {"video_name": "ego_45.mp4", "duration": 428.83333328749984, "question": "Where did I kept the brown packet?\n(A) In the cupboard\n(B) In the fridge\n(C) On the fllor\n(D) On the table\n", "candidates": ["In the cupboard", "In the fridge", "On the fllor", "On the table"], "answer": "C", "task_type": "ego", "question_id": "Q829"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5131f170437be89b39b881f312351873eb337d21d4f41e8e31ae50ecd05eecf0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I kept the brown packet?\n(A) In the cupboard\n(B) In the fridge\n(C) On the fllor\n(D) On the table\n", "task_type": "ego", "pred_answer": "D", "answer": "C"}, "input": "Where did I kept the brown packet?\n(A) In the cupboard\n(B) In the fridge\n(C) On the fllor\n(D) On the table\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 833, "doc": {"video_name": "ego_80.mp4", "duration": 480.03333333333336, "question": "When the woman did dropped the paper?\n(A) Without swipe\n(B) During swipe\n(C) After swipe\n(D) Before swipe\n", "candidates": ["Without swipe", "During swipe", "After swipe", "Before swipe"], "answer": "C", "task_type": "ego", "question_id": "Q833"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a673d3c8908cd9b16c847d812df935d81c74c548b92452b9bf7cc62a2ab84374", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "When the woman did dropped the paper?\n(A) Without swipe\n(B) During swipe\n(C) After swipe\n(D) Before swipe\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "When the woman did dropped the paper?\n(A) Without swipe\n(B) During swipe\n(C) After swipe\n(D) Before swipe\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 837, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Did I throw the papers in the trash bin?\n(A) No\n(B) Maybe\n(C) I don't know\n(D) Yes\n", "candidates": ["No", "Maybe", "I don't know", "Yes"], "answer": "D", "task_type": "ego", "question_id": "Q837"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "46a39198791fcaa0d1daec5bbc7429d57030cf79b6fe8d28ab928c641d540dbc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Did I throw the papers in the trash bin?\n(A) No\n(B) Maybe\n(C) I don't know\n(D) Yes\n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "Did I throw the papers in the trash bin?\n(A) No\n(B) Maybe\n(C) I don't know\n(D) Yes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 841, "doc": {"video_name": "ego_19.mp4", "duration": 480.0, "question": "What did I put in the pocket?\n(A) a piece of candy\n(B) bunch of nails\n(C) a wallet\n(D) a pen\n", "candidates": ["a piece of candy", "bunch of nails", "a wallet", "a pen"], "answer": "B", "task_type": "ego", "question_id": "Q841"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f6c59e644324460d5a7d0e9d9336608595812afe94417ea5fc2cf8f0f4fa0d7e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I put in the pocket?\n(A) a piece of candy\n(B) bunch of nails\n(C) a wallet\n(D) a pen\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What did I put in the pocket?\n(A) a piece of candy\n(B) bunch of nails\n(C) a wallet\n(D) a pen\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 845, "doc": {"video_name": "ego_58.mp4", "duration": 414.5333333333333, "question": "How many dogs did I touch first?\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n", "candidates": ["1", "2", "3", "4"], "answer": "B", "task_type": "ego", "question_id": "Q845"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5f29d54684059a3548fc0b05c0dc44a27d7765196a690e44710b4af5cefb68ad", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many dogs did I touch first?\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n", "task_type": "ego", "pred_answer": "A", "answer": "B"}, "input": "How many dogs did I touch first?\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 849, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Did I leave the storeroom door open?\n(A) yes\n(B) I don't know\n(C) no\n(D) maybe\n", "candidates": ["yes", "I don't know", "no", "maybe"], "answer": "C", "task_type": "ego", "question_id": "Q849"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "67212355ac50235d64a34176502b83d5c4043ee9004f0e17fa30f2392d002728", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Did I leave the storeroom door open?\n(A) yes\n(B) I don't know\n(C) no\n(D) maybe\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Did I leave the storeroom door open?\n(A) yes\n(B) I don't know\n(C) no\n(D) maybe\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 853, "doc": {"video_name": "ego_18.mp4", "duration": 460.7333333333331, "question": "Where did I keep the wooden cutter?\n(A) in the drawer\n(B) in the toolbox\n(C) near wooden block\n(D) on the shelf\n", "candidates": ["in the drawer", "in the toolbox", "near wooden block", "on the shelf"], "answer": "C", "task_type": "ego", "question_id": "Q853"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f74ec1682631b77c75506692f04abdf54a2df1ab1e9808743e29e5f6e3458c8e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I keep the wooden cutter?\n(A) in the drawer\n(B) in the toolbox\n(C) near wooden block\n(D) on the shelf\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where did I keep the wooden cutter?\n(A) in the drawer\n(B) in the toolbox\n(C) near wooden block\n(D) on the shelf\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 857, "doc": {"video_name": "ego_54.mp4", "duration": 480.0, "question": "What did I see in the vehicle?\n(A) tools\n(B) clothing\n(C) pieces of pipes.\n(D) food wrappers\n", "candidates": ["tools", "clothing", "pieces of pipes.", "food wrappers"], "answer": "C", "task_type": "ego", "question_id": "Q857"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "035b1770b0076448fd07c0a9dc3e47191b975044ee4f255fa14a8ee0ab92eb85", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did I see in the vehicle?\n(A) tools\n(B) clothing\n(C) pieces of pipes.\n(D) food wrappers\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "What did I see in the vehicle?\n(A) tools\n(B) clothing\n(C) pieces of pipes.\n(D) food wrappers\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 861, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Where did I put my lemon green wristwatch\n(A) in the car's glove compartment\n(B) in the bedroom closet\n(C) on the white table\n(D) in the kitchen drawer\n", "candidates": ["in the car's glove compartment", "in the bedroom closet", "on the white table", "in the kitchen drawer"], "answer": "C", "task_type": "ego", "question_id": "Q861"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d783c287469308d88b69af0a2b7d96db260341a30ca4d745deb43fb160b7b6c3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I put my lemon green wristwatch\n(A) in the car's glove compartment\n(B) in the bedroom closet\n(C) on the white table\n(D) in the kitchen drawer\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where did I put my lemon green wristwatch\n(A) in the car's glove compartment\n(B) in the bedroom closet\n(C) on the white table\n(D) in the kitchen drawer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 865, "doc": {"video_name": "ego_35.mp4", "duration": 408.63333328750014, "question": "Where did I put the leftover pack of dog food\n(A) in the garage\n(B) in the fridge\n(C) in the pantry\n(D) in the car\n", "candidates": ["in the garage", "in the fridge", "in the pantry", "in the car"], "answer": "B", "task_type": "ego", "question_id": "Q865"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3e7d1836a2f98ad9da94bf161f945c1aec9c41dc761799dce7cdb3a48642542b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where did I put the leftover pack of dog food\n(A) in the garage\n(B) in the fridge\n(C) in the pantry\n(D) in the car\n", "task_type": "ego", "pred_answer": "C", "answer": "B"}, "input": "Where did I put the leftover pack of dog food\n(A) in the garage\n(B) in the fridge\n(C) in the pantry\n(D) in the car\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 869, "doc": {"video_name": "ego_60.mp4", "duration": 480.0333333333334, "question": "Where was the juice packet before I opened the refrigerator?\n(A) On the kitchen counter\n(B) Inside the refrigerator.\n(C) In the dishwasher\n(D) In the pantry\n", "candidates": ["On the kitchen counter", "Inside the refrigerator.", "In the dishwasher", "In the pantry"], "answer": "B", "task_type": "ego", "question_id": "Q869"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f922d2d1a32f6e9679395a05e0ff70cf097d748155121bc1f982e0f99c3686d4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the juice packet before I opened the refrigerator?\n(A) On the kitchen counter\n(B) Inside the refrigerator.\n(C) In the dishwasher\n(D) In the pantry\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the juice packet before I opened the refrigerator?\n(A) On the kitchen counter\n(B) Inside the refrigerator.\n(C) In the dishwasher\n(D) In the pantry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 873, "doc": {"video_name": "ego_52.mp4", "duration": 480.0, "question": "What word was written on the banner ?\n(A) Dream big\n(B) \"We can do it\" was written on the banner.\n(C) Believe in yourself\n(D) Never give up\n", "candidates": ["Dream big", "\"We can do it\" was written on the banner.", "Believe in yourself", "Never give up"], "answer": "B", "task_type": "ego", "question_id": "Q873"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6af1ee3d97a816399f57d3b2006b12452802a09c346cfe96c15cf8dcbf623115", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What word was written on the banner ?\n(A) Dream big\n(B) \"We can do it\" was written on the banner.\n(C) Believe in yourself\n(D) Never give up\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What word was written on the banner ?\n(A) Dream big\n(B) \"We can do it\" was written on the banner.\n(C) Believe in yourself\n(D) Never give up\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 877, "doc": {"video_name": "ego_26.mp4", "duration": 375.3666666666667, "question": "What part of the bicycle did I spray with the lube?\n(A) the pedals\n(B) the bicycle chain area\n(C) the handlebars\n(D) the seat\n", "candidates": ["the pedals", "the bicycle chain area", "the handlebars", "the seat"], "answer": "B", "task_type": "ego", "question_id": "Q877"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3df172d52bb4d184a7c92c48bda27d7388791a1f62ad5c38d93d6f276e9188a6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What part of the bicycle did I spray with the lube?\n(A) the pedals\n(B) the bicycle chain area\n(C) the handlebars\n(D) the seat\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What part of the bicycle did I spray with the lube?\n(A) the pedals\n(B) the bicycle chain area\n(C) the handlebars\n(D) the seat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 881, "doc": {"video_name": "ego_35.mp4", "duration": 408.63333328750014, "question": "Where did I put the blue helmet\n(A) on the wall hanger\n(B) on the kitchen counter\n(C) under the bed\n(D) in the closet\n", "candidates": ["on the wall hanger", "on the kitchen counter", "under the bed", "in the closet"], "answer": "A", "task_type": "ego", "question_id": "Q881"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e9d3d041f784f3729f28636377bfaff2853759edf3bda4b36993ec1746b348bc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I put the blue helmet\n(A) on the wall hanger\n(B) on the kitchen counter\n(C) under the bed\n(D) in the closet\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I put the blue helmet\n(A) on the wall hanger\n(B) on the kitchen counter\n(C) under the bed\n(D) in the closet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 885, "doc": {"video_name": "ego_25.mp4", "duration": 480.0, "question": "Where is the package in a plastic bag?\n(A) inside the mailbox at the end of the street\n(B) under the flower vase at the entrance of the house\n(C) on top of the refrigerator in the kitchen\n(D) hidden inside the shoe rack near the door\n", "candidates": ["inside the mailbox at the end of the street", "under the flower vase at the entrance of the house", "on top of the refrigerator in the kitchen", "hidden inside the shoe rack near the door"], "answer": "B", "task_type": "ego", "question_id": "Q885"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "df646bbc65103cece98b3d3f4597b86a2ba7e67ecb8d296557f14d1e053ffee3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where is the package in a plastic bag?\n(A) inside the mailbox at the end of the street\n(B) under the flower vase at the entrance of the house\n(C) on top of the refrigerator in the kitchen\n(D) hidden inside the shoe rack near the door\n", "task_type": "ego", "pred_answer": "A", "answer": "B"}, "input": "Where is the package in a plastic bag?\n(A) inside the mailbox at the end of the street\n(B) under the flower vase at the entrance of the house\n(C) on top of the refrigerator in the kitchen\n(D) hidden inside the shoe rack near the door\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 889, "doc": {"video_name": "ego_24.mp4", "duration": 480.0, "question": "What did I put on the table?\n(A) Hammer\n(B) Drill machine\n(C) Screwdriver\n(D) Wrench\n", "candidates": ["Hammer", "Drill machine", "Screwdriver", "Wrench"], "answer": "B", "task_type": "ego", "question_id": "Q889"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c3da7356b9d78784a761d55522bea57a069658a02966b7f22c2491ab651714d8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I put on the table?\n(A) Hammer\n(B) Drill machine\n(C) Screwdriver\n(D) Wrench\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What did I put on the table?\n(A) Hammer\n(B) Drill machine\n(C) Screwdriver\n(D) Wrench\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 893, "doc": {"video_name": "ego_72.mp4", "duration": 479.9999999999998, "question": "Where did i put the juice mug?\n(A) in the dishwasher\n(B) on the kitchen counter\n(C) in the microwave\n(D) inside fridge\n", "candidates": ["in the dishwasher", "on the kitchen counter", "in the microwave", "inside fridge"], "answer": "D", "task_type": "ego", "question_id": "Q893"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e709843470a1cc15e6c53b5c60e84e2acabcd105ac9021da216b2d79ddc09e73", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where did i put the juice mug?\n(A) in the dishwasher\n(B) on the kitchen counter\n(C) in the microwave\n(D) inside fridge\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where did i put the juice mug?\n(A) in the dishwasher\n(B) on the kitchen counter\n(C) in the microwave\n(D) inside fridge\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 897, "doc": {"video_name": "ego_69.mp4", "duration": 480.0, "question": "Who did I interact with when I entered the clothing store?\n(A) Yellow dress girl\n(B) Grey t-shirt man\n(C) Red hat woman\n(D) Blue jeans boy\n", "candidates": ["Yellow dress girl", "Grey t-shirt man", "Red hat woman", "Blue jeans boy"], "answer": "B", "task_type": "ego", "question_id": "Q897"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "02fa199204099b6e3eaaaf126ba0f3ae80fedf6b5f248709cdcbcef68c59f656", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who did I interact with when I entered the clothing store?\n(A) Yellow dress girl\n(B) Grey t-shirt man\n(C) Red hat woman\n(D) Blue jeans boy\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Who did I interact with when I entered the clothing store?\n(A) Yellow dress girl\n(B) Grey t-shirt man\n(C) Red hat woman\n(D) Blue jeans boy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 901, "doc": {"video_name": "ego_16.mp4", "duration": 480.0, "question": "Did I leave the door of the second bedroom open?\n(A) Yes\n(B) No\n(C) Maybe\n(D) I don't know\n", "candidates": ["Yes", "No", "Maybe", "I don't know"], "answer": "A", "task_type": "ego", "question_id": "Q901"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "befb611146206a6958d86534cb5f6a99e73d90b466a9bc449d48a1db34b5faab", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Did I leave the door of the second bedroom open?\n(A) Yes\n(B) No\n(C) Maybe\n(D) I don't know\n", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "Did I leave the door of the second bedroom open?\n(A) Yes\n(B) No\n(C) Maybe\n(D) I don't know\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 905, "doc": {"video_name": "ego_81.mp4", "duration": 1200.0333333333333, "question": "Where did I put the plate?\n(A) In the dishwasher\n(B) In the fridge\n(C) On the counter\n(D) In the sink\n", "candidates": ["In the dishwasher", "In the fridge", "On the counter", "In the sink"], "answer": "D", "task_type": "ego", "question_id": "Q905"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bd7a62a0b13d7ca3f527d6834156a5287aed5c246ff8954d31abcaf8c568fa87", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where did I put the plate?\n(A) In the dishwasher\n(B) In the fridge\n(C) On the counter\n(D) In the sink\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "Where did I put the plate?\n(A) In the dishwasher\n(B) In the fridge\n(C) On the counter\n(D) In the sink\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 909, "doc": {"video_name": "ego_35.mp4", "duration": 408.63333328750014, "question": "Where was the cat after I put food in the red pet plate\n(A) on the back terrace of the house\n(B) in the kitchen\n(C) under the bed\n(D) in the living room\n", "candidates": ["on the back terrace of the house", "in the kitchen", "under the bed", "in the living room"], "answer": "A", "task_type": "ego", "question_id": "Q909"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2a3ebe54894509ef803da578cbc8d56ab8fc48b29e51b0d18206bfa2841bb3cd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the cat after I put food in the red pet plate\n(A) on the back terrace of the house\n(B) in the kitchen\n(C) under the bed\n(D) in the living room\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where was the cat after I put food in the red pet plate\n(A) on the back terrace of the house\n(B) in the kitchen\n(C) under the bed\n(D) in the living room\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 913, "doc": {"video_name": "ego_47.mp4", "duration": 1200.0333333333333, "question": "How many nails did I first drive into the wood? \n(A) 1\n(B) 3\n(C) 2\n(D) 0\n", "candidates": ["1", "3", "2", "0"], "answer": "A", "task_type": "ego", "question_id": "Q913"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a0a51b6101811b9c262995aeb2c1a626c9eee6d794b5f1bae07d4c0bd2668ac5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many nails did I first drive into the wood? \n(A) 1\n(B) 3\n(C) 2\n(D) 0\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "How many nails did I first drive into the wood? \n(A) 1\n(B) 3\n(C) 2\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 917, "doc": {"video_name": "ego_33.mp4", "duration": 480.0, "question": "In what location did I see a leaf blower?\n(A) Tool Storage Container\n(B) Garden shed\n(C) Kitchen pantry\n(D) Bathroom cabinet\n", "candidates": ["Tool Storage Container", "Garden shed", "Kitchen pantry", "Bathroom cabinet"], "answer": "A", "task_type": "ego", "question_id": "Q917"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "325e5572a7d66f2c58ba45722e9763302b1e6f45f54e695e9522b1e5989adeee", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what location did I see a leaf blower?\n(A) Tool Storage Container\n(B) Garden shed\n(C) Kitchen pantry\n(D) Bathroom cabinet\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "In what location did I see a leaf blower?\n(A) Tool Storage Container\n(B) Garden shed\n(C) Kitchen pantry\n(D) Bathroom cabinet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 921, "doc": {"video_name": "ego_81.mp4", "duration": 1200.0333333333333, "question": "How many mugs did i see on the counter top?\n(A) 2\n(B) 3\n(C) 4\n(D) 1\n", "candidates": ["2", "3", "4", "1"], "answer": "B", "task_type": "ego", "question_id": "Q921"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5081310a20b8c582f236c7caab04b3429765d1b10c3fc12648b9e0d0633a3322", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many mugs did i see on the counter top?\n(A) 2\n(B) 3\n(C) 4\n(D) 1\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "How many mugs did i see on the counter top?\n(A) 2\n(B) 3\n(C) 4\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 925, "doc": {"video_name": "ego_80.mp4", "duration": 480.03333333333336, "question": "Where was the red bag ?\n(A) under the bed\n(B) in the car\n(C) in the closet\n(D) on the shelve \n", "candidates": ["under the bed", "in the car", "in the closet", "on the shelve "], "answer": "D", "task_type": "ego", "question_id": "Q925"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5288230f88c4911f6cb476fba360912bfd802423833252f5c43e9b24664247f1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the red bag ?\n(A) under the bed\n(B) in the car\n(C) in the closet\n(D) on the shelve \n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "Where was the red bag ?\n(A) under the bed\n(B) in the car\n(C) in the closet\n(D) on the shelve \n\nOnly give the best option.\nBest option: ("}
{"doc_id": 929, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Where did I put the wooden rack?\n(A) inside store room\n(B) on the balcony\n(C) in the kitchen\n(D) under the bed\n", "candidates": ["inside store room", "on the balcony", "in the kitchen", "under the bed"], "answer": "A", "task_type": "ego", "question_id": "Q929"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "81e339186675f65dc20bbe6917c4fec1e986a728ad340c9b63a0b821ddc8ce7c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I put the wooden rack?\n(A) inside store room\n(B) on the balcony\n(C) in the kitchen\n(D) under the bed\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I put the wooden rack?\n(A) inside store room\n(B) on the balcony\n(C) in the kitchen\n(D) under the bed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 933, "doc": {"video_name": "ego_24.mp4", "duration": 480.0, "question": "What trolley was the man pulling?\n(A) plastic trolley\n(B) wooden trolley\n(C) cement trolley.\n(D) metal trolley\n", "candidates": ["plastic trolley", "wooden trolley", "cement trolley.", "metal trolley"], "answer": "C", "task_type": "ego", "question_id": "Q933"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d85bc9f09cd245cdd1d07facdfa77c540d8d844d2e8542efd505c4225f72db78", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What trolley was the man pulling?\n(A) plastic trolley\n(B) wooden trolley\n(C) cement trolley.\n(D) metal trolley\n", "task_type": "ego", "pred_answer": "D", "answer": "C"}, "input": "What trolley was the man pulling?\n(A) plastic trolley\n(B) wooden trolley\n(C) cement trolley.\n(D) metal trolley\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 937, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Did I throw the papers in the trash bin?\n(A) Maybe\n(B) Yes\n(C) No\n(D) I don't know\n", "candidates": ["Maybe", "Yes", "No", "I don't know"], "answer": "B", "task_type": "ego", "question_id": "Q937"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e941bc1f5de9be7a53b71602c13d652fda52895cd1df6179b5c08b0cb7ff8635", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did I throw the papers in the trash bin?\n(A) Maybe\n(B) Yes\n(C) No\n(D) I don't know\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Did I throw the papers in the trash bin?\n(A) Maybe\n(B) Yes\n(C) No\n(D) I don't know\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 941, "doc": {"video_name": "ego_23.mp4", "duration": 479.9999999999998, "question": "Where was the white vase?\n(A) beside the extension board\n(B) in the bathroom\n(C) under the bed\n(D) on top of the refrigerator\n", "candidates": ["beside the extension board", "in the bathroom", "under the bed", "on top of the refrigerator"], "answer": "A", "task_type": "ego", "question_id": "Q941"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "179685fb0b7e666c21865e3ae5244888f2f8675fe314c2c6130040d69f4ed1f0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the white vase?\n(A) beside the extension board\n(B) in the bathroom\n(C) under the bed\n(D) on top of the refrigerator\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where was the white vase?\n(A) beside the extension board\n(B) in the bathroom\n(C) under the bed\n(D) on top of the refrigerator\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 945, "doc": {"video_name": "ego_22.mp4", "duration": 480.0, "question": "Where were the eyeglasses\n(A) on the window sill\n(B) on the kitchen counter\n(C) in the drawer\n(D) under the bed\n", "candidates": ["on the window sill", "on the kitchen counter", "in the drawer", "under the bed"], "answer": "A", "task_type": "ego", "question_id": "Q945"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "49a4a137fb7bacd930f6cbc21ab3f190480b273ebc128466ae88606d16e00282", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where were the eyeglasses\n(A) on the window sill\n(B) on the kitchen counter\n(C) in the drawer\n(D) under the bed\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where were the eyeglasses\n(A) on the window sill\n(B) on the kitchen counter\n(C) in the drawer\n(D) under the bed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 949, "doc": {"video_name": "ego_22.mp4", "duration": 480.0, "question": "Where was the guitar before I held it in my hand\n(A) in the backyard\n(B) underneath the bed\n(C) beside the white cabinet\n(D) on the kitchen counter\n", "candidates": ["in the backyard", "underneath the bed", "beside the white cabinet", "on the kitchen counter"], "answer": "C", "task_type": "ego", "question_id": "Q949"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "51b52bc718d1b88a314e3671ecf87335096b5fdd84a13cb528f53124938dd3bc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the guitar before I held it in my hand\n(A) in the backyard\n(B) underneath the bed\n(C) beside the white cabinet\n(D) on the kitchen counter\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was the guitar before I held it in my hand\n(A) in the backyard\n(B) underneath the bed\n(C) beside the white cabinet\n(D) on the kitchen counter\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 953, "doc": {"video_name": "ego_68.mp4", "duration": 480.03333333333336, "question": "What symbol did I see on my ID ?\n(A) Instagram symbol\n(B) Facebook symbol\n(C) Snapchat symbol\n(D) Twitter symbol\n", "candidates": ["Instagram symbol", "Facebook symbol", "Snapchat symbol", "Twitter symbol"], "answer": "B", "task_type": "ego", "question_id": "Q953"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c7f74338db4967bfd553101bdf73a41b19cd0b52a8ad18f80a927db9fbe8b15", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What symbol did I see on my ID ?\n(A) Instagram symbol\n(B) Facebook symbol\n(C) Snapchat symbol\n(D) Twitter symbol\n", "task_type": "ego", "pred_answer": "C", "answer": "B"}, "input": "What symbol did I see on my ID ?\n(A) Instagram symbol\n(B) Facebook symbol\n(C) Snapchat symbol\n(D) Twitter symbol\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 957, "doc": {"video_name": "ego_47.mp4", "duration": 1200.0333333333333, "question": "What word was written on the tape measure? \n(A) bosch\n(B) stanley\n(C) dewalt\n(D) makita\n", "candidates": ["bosch", "stanley", "dewalt", "makita"], "answer": "C", "task_type": "ego", "question_id": "Q957"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ce0529e2e30e3a03dd55e07c5abab9272b6ee085e516abb88b27792374e6f01", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What word was written on the tape measure? \n(A) bosch\n(B) stanley\n(C) dewalt\n(D) makita\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What word was written on the tape measure? \n(A) bosch\n(B) stanley\n(C) dewalt\n(D) makita\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 961, "doc": {"video_name": "ego_2.mp4", "duration": 480.0000000000001, "question": "How many ties are on the display table? \n(A) 8\n(B) 6\n(C) 4\n(D) 2\n", "candidates": ["8", "6", "4", "2"], "answer": "C", "task_type": "ego", "question_id": "Q961"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dce72ba870256d23626f67395cb3257af7fccb48884e91d62ea3fb3ca6243387", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many ties are on the display table? \n(A) 8\n(B) 6\n(C) 4\n(D) 2\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "How many ties are on the display table? \n(A) 8\n(B) 6\n(C) 4\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 965, "doc": {"video_name": "ego_15.mp4", "duration": 480.0, "question": "What did I put in the refrigerator?\n(A) water bottle\n(B) phone charger\n(C) socks\n(D) lunch box\n", "candidates": ["water bottle", "phone charger", "socks", "lunch box"], "answer": "D", "task_type": "ego", "question_id": "Q965"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e2e33cd457f530bf87ce11db4f7d4806d160cc17c1c52f9e1698a4f9effcf64d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I put in the refrigerator?\n(A) water bottle\n(B) phone charger\n(C) socks\n(D) lunch box\n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "What did I put in the refrigerator?\n(A) water bottle\n(B) phone charger\n(C) socks\n(D) lunch box\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 969, "doc": {"video_name": "ego_14.mp4", "duration": 480.0333333333334, "question": "Did I leave the bathroom door open?\n(A) No\n(B) Maybe\n(C) Yes\n(D) I don't know\n", "candidates": ["No", "Maybe", "Yes", "I don't know"], "answer": "C", "task_type": "ego", "question_id": "Q969"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8953c64237df8c14010c8403d7cb5062b2324f7acb209e66c4a25cc0f50e4427", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Did I leave the bathroom door open?\n(A) No\n(B) Maybe\n(C) Yes\n(D) I don't know\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "Did I leave the bathroom door open?\n(A) No\n(B) Maybe\n(C) Yes\n(D) I don't know\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 973, "doc": {"video_name": "ego_52.mp4", "duration": 480.0, "question": "Did i see a Flower drawing?\n(A) Maybe\n(B) No\n(C) Yes\n(D) I'm not sure\n", "candidates": ["Maybe", "No", "Yes", "I'm not sure"], "answer": "C", "task_type": "ego", "question_id": "Q973"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "93c8db72405e8e75d0dace78ec7dbcdebd837b531b51d14de16af49244d7b649", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Did i see a Flower drawing?\n(A) Maybe\n(B) No\n(C) Yes\n(D) I'm not sure\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Did i see a Flower drawing?\n(A) Maybe\n(B) No\n(C) Yes\n(D) I'm not sure\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 977, "doc": {"video_name": "ego_60.mp4", "duration": 480.0333333333334, "question": "What did I take from the fridge?\n(A) yogurt\n(B) milk \n(C) juice\n(D) cheese\n", "candidates": ["yogurt", "milk ", "juice", "cheese"], "answer": "B", "task_type": "ego", "question_id": "Q977"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1ff56c0d9043c7724898a325b1749a03ad1e891d74cffe342c7e0dd5b3c66be1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I take from the fridge?\n(A) yogurt\n(B) milk \n(C) juice\n(D) cheese\n", "task_type": "ego", "pred_answer": "C", "answer": "B"}, "input": "What did I take from the fridge?\n(A) yogurt\n(B) milk \n(C) juice\n(D) cheese\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 981, "doc": {"video_name": "ego_13.mp4", "duration": 480.0, "question": "How many green cups were on the table in the kitchen area?\n(A) 3\n(B) 1\n(C) 2\n(D) 0\n", "candidates": ["3", "1", "2", "0"], "answer": "C", "task_type": "ego", "question_id": "Q981"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4ebccc1e222b524f4cca395d76e5ae25f78b84f53d9a145533ecd9da7a6195f2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many green cups were on the table in the kitchen area?\n(A) 3\n(B) 1\n(C) 2\n(D) 0\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "How many green cups were on the table in the kitchen area?\n(A) 3\n(B) 1\n(C) 2\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 985, "doc": {"video_name": "ego_52.mp4", "duration": 480.0, "question": "How many paintings were there in the bedroom?\n(A) There were two paintings in the bedroom.\n(B) There were five paintings in the bedroom.\n(C) There were three paintings in the bedroom.\n(D) There were eight paintings in the bedroom.\n", "candidates": ["There were two paintings in the bedroom.", "There were five paintings in the bedroom.", "There were three paintings in the bedroom.", "There were eight paintings in the bedroom."], "answer": "C", "task_type": "ego", "question_id": "Q985"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ecd85889ca692ed12cbcf4801bb52f048ff09901c3a33eb939df5ad9684734d6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many paintings were there in the bedroom?\n(A) There were two paintings in the bedroom.\n(B) There were five paintings in the bedroom.\n(C) There were three paintings in the bedroom.\n(D) There were eight paintings in the bedroom.\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "How many paintings were there in the bedroom?\n(A) There were two paintings in the bedroom.\n(B) There were five paintings in the bedroom.\n(C) There were three paintings in the bedroom.\n(D) There were eight paintings in the bedroom.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 989, "doc": {"video_name": "ego_35.mp4", "duration": 408.63333328750014, "question": "What colour was the bottle I pressed\n(A) yellow\n(B) green\n(C) orange\n(D) blue\n", "candidates": ["yellow", "green", "orange", "blue"], "answer": "C", "task_type": "ego", "question_id": "Q989"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1216918dcbd71d88dabb45e292362abc8c7fe9a05b3e1cd84c969a57158dac88", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What colour was the bottle I pressed\n(A) yellow\n(B) green\n(C) orange\n(D) blue\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What colour was the bottle I pressed\n(A) yellow\n(B) green\n(C) orange\n(D) blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 993, "doc": {"video_name": "ego_2.mp4", "duration": 480.0000000000001, "question": "Who did I talk to in the boutique? \n(A) my friend\n(B) the security guard\n(C) a customer\n(D) the shop attendant\n", "candidates": ["my friend", "the security guard", "a customer", "the shop attendant"], "answer": "D", "task_type": "ego", "question_id": "Q993"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "13e1098ce9d2cbc805d7d798aacac7bf54e5b0004cd87b5c7a4fcd73e6d1c9cf", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Who did I talk to in the boutique? \n(A) my friend\n(B) the security guard\n(C) a customer\n(D) the shop attendant\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Who did I talk to in the boutique? \n(A) my friend\n(B) the security guard\n(C) a customer\n(D) the shop attendant\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 997, "doc": {"video_name": "ego_1.mp4", "duration": 480.0, "question": "Where is the metre tape before putting on the wood table saw?\n(A) The metre tape was on the shelf before I put on the wood table saw.\n(B) The metre tape was on the floor before I put on the wood table saw.\n(C) The metre tape was in the toolbox before I put on the wood table saw.\n(D) The metre tape was in my pocket before I put on the wood table saw.\n", "candidates": ["The metre tape was on the shelf before I put on the wood table saw.", "The metre tape was on the floor before I put on the wood table saw.", "The metre tape was in the toolbox before I put on the wood table saw.", "The metre tape was in my pocket before I put on the wood table saw."], "answer": "D", "task_type": "ego", "question_id": "Q997"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5af5ce0b30602aa6188a1e512ec60101763a0956272e0e946e3abf1c2f7ad85f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where is the metre tape before putting on the wood table saw?\n(A) The metre tape was on the shelf before I put on the wood table saw.\n(B) The metre tape was on the floor before I put on the wood table saw.\n(C) The metre tape was in the toolbox before I put on the wood table saw.\n(D) The metre tape was in my pocket before I put on the wood table saw.\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "Where is the metre tape before putting on the wood table saw?\n(A) The metre tape was on the shelf before I put on the wood table saw.\n(B) The metre tape was on the floor before I put on the wood table saw.\n(C) The metre tape was in the toolbox before I put on the wood table saw.\n(D) The metre tape was in my pocket before I put on the wood table saw.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1001, "doc": {"video_name": "ego_31.mp4", "duration": 480.0, "question": "What did I put in the table?\n(A) book\n(B) pen\n(C) gum\n(D) phone\n", "candidates": ["book", "pen", "gum", "phone"], "answer": "C", "task_type": "ego", "question_id": "Q1001"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "19c23b93ef1b1d17c690e7704945f06c92cc446b748396df51465af54d9b06d6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did I put in the table?\n(A) book\n(B) pen\n(C) gum\n(D) phone\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "What did I put in the table?\n(A) book\n(B) pen\n(C) gum\n(D) phone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1005, "doc": {"video_name": "ego_57.mp4", "duration": 480.0, "question": "Where was the violin?\n(A) behind the black chair \n(B) on top of the bookshelf\n(C) in the kitchen drawer\n(D) under the table\n", "candidates": ["behind the black chair ", "on top of the bookshelf", "in the kitchen drawer", "under the table"], "answer": "A", "task_type": "ego", "question_id": "Q1005"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "290f114b3c759d81d5f2e06a7144389abd8909eb9889e42fee7a1df9623bfcef", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the violin?\n(A) behind the black chair \n(B) on top of the bookshelf\n(C) in the kitchen drawer\n(D) under the table\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where was the violin?\n(A) behind the black chair \n(B) on top of the bookshelf\n(C) in the kitchen drawer\n(D) under the table\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1009, "doc": {"video_name": "ego_73.mp4", "duration": 480.0, "question": "What did I put in the fridge?\n(A) Soda cans\n(B) Water bottles\n(C) Milk cartons\n(D) Orange juice\n", "candidates": ["Soda cans", "Water bottles", "Milk cartons", "Orange juice"], "answer": "B", "task_type": "ego", "question_id": "Q1009"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7f7120d5e5d98bc03a276780276b3db8644e42934b75d9878b6d922036c62436", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I put in the fridge?\n(A) Soda cans\n(B) Water bottles\n(C) Milk cartons\n(D) Orange juice\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What did I put in the fridge?\n(A) Soda cans\n(B) Water bottles\n(C) Milk cartons\n(D) Orange juice\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1013, "doc": {"video_name": "ego_2.mp4", "duration": 480.0000000000001, "question": "What word was written on the poster on the wall? \n(A) get a discount on your first purchase\n(B) sign up Extra 20% off\n(C) register now for 10% off\n(D) join our loyalty program for exclusive deals\n", "candidates": ["get a discount on your first purchase", "sign up Extra 20% off", "register now for 10% off", "join our loyalty program for exclusive deals"], "answer": "B", "task_type": "ego", "question_id": "Q1013"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dab400bed597ab53165db5e5c44b8a1bb33288c84fa1f0bf9d08dfcb1a90a14d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What word was written on the poster on the wall? \n(A) get a discount on your first purchase\n(B) sign up Extra 20% off\n(C) register now for 10% off\n(D) join our loyalty program for exclusive deals\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What word was written on the poster on the wall? \n(A) get a discount on your first purchase\n(B) sign up Extra 20% off\n(C) register now for 10% off\n(D) join our loyalty program for exclusive deals\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1017, "doc": {"video_name": "ego_34.mp4", "duration": 442.29999999999995, "question": "Whom did I talk to in the backyard of the house? \n(A) person wearing red hat\n(B) person wearing black hat.\n(C) person wearing white hat\n(D) person wearing blue hat\n", "candidates": ["person wearing red hat", "person wearing black hat.", "person wearing white hat", "person wearing blue hat"], "answer": "B", "task_type": "ego", "question_id": "Q1017"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ebdd0a287cf21b4a445fbccc32f4f24c5d52ac75a2c30822c05b10f1e915f091", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Whom did I talk to in the backyard of the house? \n(A) person wearing red hat\n(B) person wearing black hat.\n(C) person wearing white hat\n(D) person wearing blue hat\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Whom did I talk to in the backyard of the house? \n(A) person wearing red hat\n(B) person wearing black hat.\n(C) person wearing white hat\n(D) person wearing blue hat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1021, "doc": {"video_name": "ego_82.mp4", "duration": 480.0, "question": "Did I turn off the stove?\n(A) maybe\n(B) no\n(C) yes\n(D) I don't know\n", "candidates": ["maybe", "no", "yes", "I don't know"], "answer": "C", "task_type": "ego", "question_id": "Q1021"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dac8d2b18227fd4c4aa82b5025b6d890a9ebd8109b91a4ad8f04ad4378928807", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Did I turn off the stove?\n(A) maybe\n(B) no\n(C) yes\n(D) I don't know\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Did I turn off the stove?\n(A) maybe\n(B) no\n(C) yes\n(D) I don't know\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1025, "doc": {"video_name": "ego_35.mp4", "duration": 408.63333328750014, "question": "Did I leave the front door open\n(A) yes\n(B) no\n(C) I don't remember\n(D) I'm not sure\n", "candidates": ["yes", "no", "I don't remember", "I'm not sure"], "answer": "A", "task_type": "ego", "question_id": "Q1025"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "139f7a9a00de006add1cec9bdca78b5e396fad369218e46437822bf765d80fc8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Did I leave the front door open\n(A) yes\n(B) no\n(C) I don't remember\n(D) I'm not sure\n", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "Did I leave the front door open\n(A) yes\n(B) no\n(C) I don't remember\n(D) I'm not sure\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1029, "doc": {"video_name": "ego_11.mp4", "duration": 480.0, "question": "Where are the books?\n(A) in the drawer\n(B) hanging from the ceiling\n(C) on the shelf\n(D) on the floor\n", "candidates": ["in the drawer", "hanging from the ceiling", "on the shelf", "on the floor"], "answer": "D", "task_type": "ego", "question_id": "Q1029"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "765b704bbf0784652d1b03f8ee75feb7f3358f731b53bba04158465ec2c1a475", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where are the books?\n(A) in the drawer\n(B) hanging from the ceiling\n(C) on the shelf\n(D) on the floor\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "Where are the books?\n(A) in the drawer\n(B) hanging from the ceiling\n(C) on the shelf\n(D) on the floor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1033, "doc": {"video_name": "ego_10.mp4", "duration": 420.5666666666666, "question": "What colour is the stool I sat on? \n(A) black\n(B) red\n(C) white\n(D) blue\n", "candidates": ["black", "red", "white", "blue"], "answer": "A", "task_type": "ego", "question_id": "Q1033"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "faa26274a66924053f0f2f06e947d3f5ff08221b2bf22d5df1448d89d05b7e81", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What colour is the stool I sat on? \n(A) black\n(B) red\n(C) white\n(D) blue\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What colour is the stool I sat on? \n(A) black\n(B) red\n(C) white\n(D) blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1037, "doc": {"video_name": "ego_69.mp4", "duration": 480.0, "question": "Where was the red ketchup bottle?\n(A) In the refrigerator\n(B) In the pantry\n(C) Next to the bakery counter\n(D) On the top shelf\n", "candidates": ["In the refrigerator", "In the pantry", "Next to the bakery counter", "On the top shelf"], "answer": "C", "task_type": "ego", "question_id": "Q1037"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5346a9f169b493d150c46bece10d9b0df4687faa7a04aa91ff31f10fbfedfc28", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the red ketchup bottle?\n(A) In the refrigerator\n(B) In the pantry\n(C) Next to the bakery counter\n(D) On the top shelf\n", "task_type": "ego", "pred_answer": "D", "answer": "C"}, "input": "Where was the red ketchup bottle?\n(A) In the refrigerator\n(B) In the pantry\n(C) Next to the bakery counter\n(D) On the top shelf\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1041, "doc": {"video_name": "ego_32.mp4", "duration": 480.03333333333376, "question": "Where did I put the red cap bottle?\n(A) On the table\n(B) In the shelf.\n(C) Under the bed\n(D) In the fridge\n", "candidates": ["On the table", "In the shelf.", "Under the bed", "In the fridge"], "answer": "B", "task_type": "ego", "question_id": "Q1041"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "094eb29a6a5670ca3a47959c3af26e06bf982c3bd3cfcb29436d355154a462d5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where did I put the red cap bottle?\n(A) On the table\n(B) In the shelf.\n(C) Under the bed\n(D) In the fridge\n", "task_type": "ego", "pred_answer": "A", "answer": "B"}, "input": "Where did I put the red cap bottle?\n(A) On the table\n(B) In the shelf.\n(C) Under the bed\n(D) In the fridge\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1045, "doc": {"video_name": "ego_66.mp4", "duration": 480.03333333333336, "question": "Where was the can ?\n(A) under the bed\n(B) in the fridge\n(C) on the table\n(D) in the hand \n", "candidates": ["under the bed", "in the fridge", "on the table", "in the hand "], "answer": "D", "task_type": "ego", "question_id": "Q1045"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1f5828484c35c16dae859c87717fc8e9e35b2635038816caa4e2594061380b97", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the can ?\n(A) under the bed\n(B) in the fridge\n(C) on the table\n(D) in the hand \n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the can ?\n(A) under the bed\n(B) in the fridge\n(C) on the table\n(D) in the hand \n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1049, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "What did i put in the trash bin?\n(A) empty soda cans\n(B) banana peels\n(C) plastic bottles\n(D) scrap paper manuals\n", "candidates": ["empty soda cans", "banana peels", "plastic bottles", "scrap paper manuals"], "answer": "D", "task_type": "ego", "question_id": "Q1049"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f68b65f6f3db288c009de1cfdf165a7122ef698804b51ecd1110cdc1b8db12e6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did i put in the trash bin?\n(A) empty soda cans\n(B) banana peels\n(C) plastic bottles\n(D) scrap paper manuals\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What did i put in the trash bin?\n(A) empty soda cans\n(B) banana peels\n(C) plastic bottles\n(D) scrap paper manuals\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1053, "doc": {"video_name": "ego_47.mp4", "duration": 1200.0333333333333, "question": "How many tool boxes are on the floor? \n(A) 3\n(B) 4\n(C) 1\n(D) 2\n", "candidates": ["3", "4", "1", "2"], "answer": "D", "task_type": "ego", "question_id": "Q1053"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "47489318a512e6ae44cf45abbf7e423b504def70b25791ca98321ae771d8dc8a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many tool boxes are on the floor? \n(A) 3\n(B) 4\n(C) 1\n(D) 2\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "How many tool boxes are on the floor? \n(A) 3\n(B) 4\n(C) 1\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1057, "doc": {"video_name": "ego_81.mp4", "duration": 1200.0333333333333, "question": "How many knives did I see in the cabinet?\n(A) 3\n(B) 2\n(C) 4\n(D) 1\n", "candidates": ["3", "2", "4", "1"], "answer": "C", "task_type": "ego", "question_id": "Q1057"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4df157a8f434d244c1009dd3bd74ae12ce30230e85029e308a50d58293fdf992", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many knives did I see in the cabinet?\n(A) 3\n(B) 2\n(C) 4\n(D) 1\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "How many knives did I see in the cabinet?\n(A) 3\n(B) 2\n(C) 4\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1061, "doc": {"video_name": "ego_64.mp4", "duration": 480.0, "question": "Where was the wood cutter before I keep it on the stand ?\n(A) in the car\n(B) in the house\n(C) in the shed\n(D) on the ground\n", "candidates": ["in the car", "in the house", "in the shed", "on the ground"], "answer": "D", "task_type": "ego", "question_id": "Q1061"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b31454a38c0449e21f0465b4aed36a5c0670a141f261fcc44b2c2e6425b36ecd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the wood cutter before I keep it on the stand ?\n(A) in the car\n(B) in the house\n(C) in the shed\n(D) on the ground\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the wood cutter before I keep it on the stand ?\n(A) in the car\n(B) in the house\n(C) in the shed\n(D) on the ground\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1065, "doc": {"video_name": "ego_2.mp4", "duration": 480.0000000000001, "question": "How many menu holders are on the display table? \n(A) 3\n(B) 0\n(C) 1\n(D) 2\n", "candidates": ["3", "0", "1", "2"], "answer": "D", "task_type": "ego", "question_id": "Q1065"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ea5a3035545a6197603d43eeafd812a6f0739ec00b178fd12a21030f1f55ad2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many menu holders are on the display table? \n(A) 3\n(B) 0\n(C) 1\n(D) 2\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "How many menu holders are on the display table? \n(A) 3\n(B) 0\n(C) 1\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1069, "doc": {"video_name": "ego_67.mp4", "duration": 480.0000000000001, "question": "Where was the connect4 game before we played it?\n(A) The Connect4 game was on the kitchen counter before we played it.\n(B) The Connect4 game was in the bathroom before we played it.\n(C) The Connect4 game was in the backyard before we played it.\n(D) The Connect4 game was on the TV stand before we played it.\n", "candidates": ["The Connect4 game was on the kitchen counter before we played it.", "The Connect4 game was in the bathroom before we played it.", "The Connect4 game was in the backyard before we played it.", "The Connect4 game was on the TV stand before we played it."], "answer": "D", "task_type": "ego", "question_id": "Q1069"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1d09c8f685a544ec6299997956333c829473e0b0d1ff6c1326924a66edc5150a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the connect4 game before we played it?\n(A) The Connect4 game was on the kitchen counter before we played it.\n(B) The Connect4 game was in the bathroom before we played it.\n(C) The Connect4 game was in the backyard before we played it.\n(D) The Connect4 game was on the TV stand before we played it.\n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "Where was the connect4 game before we played it?\n(A) The Connect4 game was on the kitchen counter before we played it.\n(B) The Connect4 game was in the bathroom before we played it.\n(C) The Connect4 game was in the backyard before we played it.\n(D) The Connect4 game was on the TV stand before we played it.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1073, "doc": {"video_name": "ego_58.mp4", "duration": 414.5333333333333, "question": "Where did I put the pot lid?\n(A) In the refrigerator\n(B) In the dishwasher\n(C) In the pantry\n(D) On the cooking pot\n", "candidates": ["In the refrigerator", "In the dishwasher", "In the pantry", "On the cooking pot"], "answer": "D", "task_type": "ego", "question_id": "Q1073"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f04aa835eae46ec76d1bad80750a365c96b04269501f7844b9fe1481f3afbb54", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where did I put the pot lid?\n(A) In the refrigerator\n(B) In the dishwasher\n(C) In the pantry\n(D) On the cooking pot\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where did I put the pot lid?\n(A) In the refrigerator\n(B) In the dishwasher\n(C) In the pantry\n(D) On the cooking pot\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1077, "doc": {"video_name": "ego_39.mp4", "duration": 480.0333333333333, "question": "Did I leave the car bonnet open?\n(A) Yes\n(B) Maybe\n(C) I don't know\n(D) No\n", "candidates": ["Yes", "Maybe", "I don't know", "No"], "answer": "A", "task_type": "ego", "question_id": "Q1077"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d4220d3a06aad77e907366758292f4a880602a1a9766d96b2b8549e98bdc2214", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Did I leave the car bonnet open?\n(A) Yes\n(B) Maybe\n(C) I don't know\n(D) No\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Did I leave the car bonnet open?\n(A) Yes\n(B) Maybe\n(C) I don't know\n(D) No\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1081, "doc": {"video_name": "ego_45.mp4", "duration": 428.83333328749984, "question": "How many dustbins was  their?\n(A) 1 dustbin\n(B) 5 dustbins\n(C) 3 dustbins\n(D) 10 dustbins\n", "candidates": ["1 dustbin", "5 dustbins", "3 dustbins", "10 dustbins"], "answer": "C", "task_type": "ego", "question_id": "Q1081"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4a2e005990495b53777b4908bf4a64122fa3dbd15e5da2fe9633e284e1acb611", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many dustbins was  their?\n(A) 1 dustbin\n(B) 5 dustbins\n(C) 3 dustbins\n(D) 10 dustbins\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "How many dustbins was  their?\n(A) 1 dustbin\n(B) 5 dustbins\n(C) 3 dustbins\n(D) 10 dustbins\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1085, "doc": {"video_name": "ego_7.mp4", "duration": 480.03333333333336, "question": "How many purses did I see on the lower desk?\n(A) one\n(B) five\n(C) three\n(D) ten\n", "candidates": ["one", "five", "three", "ten"], "answer": "C", "task_type": "ego", "question_id": "Q1085"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fb7b73a6fb7b36a925bb5255ed61f1146d6addee774b89aa3876d322a98a4baf", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many purses did I see on the lower desk?\n(A) one\n(B) five\n(C) three\n(D) ten\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "How many purses did I see on the lower desk?\n(A) one\n(B) five\n(C) three\n(D) ten\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1089, "doc": {"video_name": "ego_7.mp4", "duration": 480.03333333333336, "question": "How many dresses were hanged close to the XL section?\n(A) two\n(B) fifteen\n(C) five\n(D) ten\n", "candidates": ["two", "fifteen", "five", "ten"], "answer": "C", "task_type": "ego", "question_id": "Q1089"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d1a91678f3a18ff77e3e748b8e71ffc712bb87f389bbfec41b78ca6d8a38025", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many dresses were hanged close to the XL section?\n(A) two\n(B) fifteen\n(C) five\n(D) ten\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "How many dresses were hanged close to the XL section?\n(A) two\n(B) fifteen\n(C) five\n(D) ten\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1093, "doc": {"video_name": "ego_6.mp4", "duration": 385.1333332875, "question": "What word was written on the glass door's white paper?\n(A) Open\n(B) Closed\n(C) Exit\n(D) Light Bar\n", "candidates": ["Open", "Closed", "Exit", "Light Bar"], "answer": "D", "task_type": "ego", "question_id": "Q1093"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "84dbc39393b154bd8a1428b0a47ad55118d915ef3101367d66d2e5d3e18eb7fe", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What word was written on the glass door's white paper?\n(A) Open\n(B) Closed\n(C) Exit\n(D) Light Bar\n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "What word was written on the glass door's white paper?\n(A) Open\n(B) Closed\n(C) Exit\n(D) Light Bar\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1097, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Where was the yellow bag\n(A) in the red drawer\n(B) on the black cabinet top\n(C) under the white chair\n(D) next to the blue vase\n", "candidates": ["in the red drawer", "on the black cabinet top", "under the white chair", "next to the blue vase"], "answer": "B", "task_type": "ego", "question_id": "Q1097"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "11929a5872ddcdff672b7f86672f5ceadd3191dd45464e48747b04d741f405d3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the yellow bag\n(A) in the red drawer\n(B) on the black cabinet top\n(C) under the white chair\n(D) next to the blue vase\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the yellow bag\n(A) in the red drawer\n(B) on the black cabinet top\n(C) under the white chair\n(D) next to the blue vase\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1101, "doc": {"video_name": "ego_77.mp4", "duration": 468.23333328749993, "question": "Did my friend pay the bill in cash?\n(A) yes\n(B) he paid with a check\n(C) no\n(D) he paid with a credit card\n", "candidates": ["yes", "he paid with a check", "no", "he paid with a credit card"], "answer": "A", "task_type": "ego", "question_id": "Q1101"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c65c1ece21e22f316034fa7fd9668d28e8db74dd2539838c7aa6beed08ff7891", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Did my friend pay the bill in cash?\n(A) yes\n(B) he paid with a check\n(C) no\n(D) he paid with a credit card\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Did my friend pay the bill in cash?\n(A) yes\n(B) he paid with a check\n(C) no\n(D) he paid with a credit card\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1105, "doc": {"video_name": "ego_59.mp4", "duration": 480.0, "question": "What name was written on the door?\n(A) Fuego.\n(B) Aqua\n(C) Ventus\n(D) Ignis\n", "candidates": ["Fuego.", "Aqua", "Ventus", "Ignis"], "answer": "A", "task_type": "ego", "question_id": "Q1105"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "26b7dd50b18bd0df0e30b20b5c9485c53d45052a8fc344f2a193c6f2c09e1eec", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What name was written on the door?\n(A) Fuego.\n(B) Aqua\n(C) Ventus\n(D) Ignis\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What name was written on the door?\n(A) Fuego.\n(B) Aqua\n(C) Ventus\n(D) Ignis\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1109, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Did I leave the extension box on\n(A) no\n(B) I don't know\n(C) yes\n(D) maybe\n", "candidates": ["no", "I don't know", "yes", "maybe"], "answer": "C", "task_type": "ego", "question_id": "Q1109"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4d50b9e2e2ec18581585885add39d02bd4d986ca5e5245952adc97adf782e174", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Did I leave the extension box on\n(A) no\n(B) I don't know\n(C) yes\n(D) maybe\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "Did I leave the extension box on\n(A) no\n(B) I don't know\n(C) yes\n(D) maybe\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1113, "doc": {"video_name": "movie101_66.mp4", "duration": 246.0, "question": "What color is the main male character in the video?\n(A) Yellow\n(B) Red\n(C) Green\n(D) Blue\n", "candidates": ["Yellow", "Red", "Green", "Blue"], "answer": "A", "task_type": "plotQA", "question_id": "Q1113"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f4a5cbff8af1e25d72b13e70fbb23384997900008dd66e0c3e7066c096caf307", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the main male character in the video?\n(A) Yellow\n(B) Red\n(C) Green\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What color is the main male character in the video?\n(A) Yellow\n(B) Red\n(C) Green\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1117, "doc": {"video_name": "movie101_53.mp4", "duration": 328.0, "question": "At the beginning of the video, why does the woman in red change from long hair to short hair?\n(A) The woman takes off her own wig\n(B) The woman cuts her own hair\n(C) The man in the blue jacket takes off the woman's wig\n(D) The man in the blue jacket cuts the woman's hair short\n", "candidates": ["The woman takes off her own wig", "The woman cuts her own hair", "The man in the blue jacket takes off the woman's wig", "The man in the blue jacket cuts the woman's hair short"], "answer": "C", "task_type": "plotQA", "question_id": "Q1117"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4f2733737ab70678627bf2fe0d885205cfc87a55bdc831633b1ed5127541837e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "At the beginning of the video, why does the woman in red change from long hair to short hair?\n(A) The woman takes off her own wig\n(B) The woman cuts her own hair\n(C) The man in the blue jacket takes off the woman's wig\n(D) The man in the blue jacket cuts the woman's hair short\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "At the beginning of the video, why does the woman in red change from long hair to short hair?\n(A) The woman takes off her own wig\n(B) The woman cuts her own hair\n(C) The man in the blue jacket takes off the woman's wig\n(D) The man in the blue jacket cuts the woman's hair short\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1121, "doc": {"video_name": "tomjerry_7.mp4", "duration": 306.0, "question": "Why does the cartoon mouse hide in the water?\n(A) To learn swimming\n(B) To avoid the bee\n(C) To hide from the cartoon dog\n(D) For fun\n", "candidates": ["To learn swimming", "To avoid the bee", "To hide from the cartoon dog", "For fun"], "answer": "B", "task_type": "plotQA", "question_id": "Q1121"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d794276988b5efbee3d640155c0ebd0b3aab240b3dfd9803503daba656e403fa", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why does the cartoon mouse hide in the water?\n(A) To learn swimming\n(B) To avoid the bee\n(C) To hide from the cartoon dog\n(D) For fun\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "Why does the cartoon mouse hide in the water?\n(A) To learn swimming\n(B) To avoid the bee\n(C) To hide from the cartoon dog\n(D) For fun\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1125, "doc": {"video_name": "en_tv_1.mp4", "duration": 514.0, "question": "At the beginning of the video, a woman in a red coat and a man with a hat are talking in the car. What does the man do after he opens the car door and leaves?\n(A) Sleep\n(B) Eat\n(C) Make a phone call\n(D) Play games\n", "candidates": ["Sleep", "Eat", "Make a phone call", "Play games"], "answer": "C", "task_type": "plotQA", "question_id": "Q1125"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "580acd61d9779fb25855e8e02ca251ec482d288d38f12436979059b0c16eaad1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "At the beginning of the video, a woman in a red coat and a man with a hat are talking in the car. What does the man do after he opens the car door and leaves?\n(A) Sleep\n(B) Eat\n(C) Make a phone call\n(D) Play games\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "At the beginning of the video, a woman in a red coat and a man with a hat are talking in the car. What does the man do after he opens the car door and leaves?\n(A) Sleep\n(B) Eat\n(C) Make a phone call\n(D) Play games\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1129, "doc": {"video_name": "movie101_74.mp4", "duration": 323.0, "question": "What did the police officer throw into the crowd in the video?\n(A) Knife\n(B) Bomb\n(C) Smoke grenade\n(D) Bat\n", "candidates": ["Knife", "Bomb", "Smoke grenade", "Bat"], "answer": "C", "task_type": "plotQA", "question_id": "Q1129"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3207a54ffbc568dd0d5bf1bbc20ef1b43faff5a7eacb355e365cfbc18141943f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did the police officer throw into the crowd in the video?\n(A) Knife\n(B) Bomb\n(C) Smoke grenade\n(D) Bat\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What did the police officer throw into the crowd in the video?\n(A) Knife\n(B) Bomb\n(C) Smoke grenade\n(D) Bat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1133, "doc": {"video_name": "movie101_5.mp4", "duration": 795.0, "question": "What is the weather during the outdoor fight scene with the soldiers standing?\n(A) Cloudy\n(B) Rainy\n(C) Sunny\n(D) Snowstorm\n", "candidates": ["Cloudy", "Rainy", "Sunny", "Snowstorm"], "answer": "B", "task_type": "plotQA", "question_id": "Q1133"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87ac0b8f837b8e762cd7153f7490d52ddac3d139c8badd3de6f2e4466785d955", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the weather during the outdoor fight scene with the soldiers standing?\n(A) Cloudy\n(B) Rainy\n(C) Sunny\n(D) Snowstorm\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the weather during the outdoor fight scene with the soldiers standing?\n(A) Cloudy\n(B) Rainy\n(C) Sunny\n(D) Snowstorm\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1137, "doc": {"video_name": "movie101_111.mp4", "duration": 720.0, "question": "Why did the man in the black coat leave after handing something to the speaker?\n(A) There was a case he needed to handle\n(B) He was preparing for the next speech\n(C) He was in a hurry to go to the bathroom\n(D) He went to get another item\n", "candidates": ["There was a case he needed to handle", "He was preparing for the next speech", "He was in a hurry to go to the bathroom", "He went to get another item"], "answer": "A", "task_type": "plotQA", "question_id": "Q1137"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aab51fdc0f083f29d14308321b0c1819a8f14ea126374ca68442c7e04c685e22", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why did the man in the black coat leave after handing something to the speaker?\n(A) There was a case he needed to handle\n(B) He was preparing for the next speech\n(C) He was in a hurry to go to the bathroom\n(D) He went to get another item\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "Why did the man in the black coat leave after handing something to the speaker?\n(A) There was a case he needed to handle\n(B) He was preparing for the next speech\n(C) He was in a hurry to go to the bathroom\n(D) He went to get another item\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1141, "doc": {"video_name": "en_tv_25.mp4", "duration": 478.0, "question": "What color is the clothing of the person holding a gun in the fight scene?\n(A) Black\n(B) White\n(C) Green\n(D) Yellow\n", "candidates": ["Black", "White", "Green", "Yellow"], "answer": "A", "task_type": "plotQA", "question_id": "Q1141"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1250d4013e2e31fe7ead3564c5fa467541bf59db52fbb5d67f6664228694c714", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the clothing of the person holding a gun in the fight scene?\n(A) Black\n(B) White\n(C) Green\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the clothing of the person holding a gun in the fight scene?\n(A) Black\n(B) White\n(C) Green\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1145, "doc": {"video_name": "en_tv_31.mp4", "duration": 577.0, "question": "What is the woman in the blue dress holding in the video?\n(A) Food\n(B) Mirror\n(C) Flower\n(D) Water Cup\n", "candidates": ["Food", "Mirror", "Flower", "Water Cup"], "answer": "C", "task_type": "plotQA", "question_id": "Q1145"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0afd7f119e65d1a594e105f84b35ae0b16272272531803f43a78f648d512f9ac", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the woman in the blue dress holding in the video?\n(A) Food\n(B) Mirror\n(C) Flower\n(D) Water Cup\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What is the woman in the blue dress holding in the video?\n(A) Food\n(B) Mirror\n(C) Flower\n(D) Water Cup\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1149, "doc": {"video_name": "movie101_71.mp4", "duration": 227.0, "question": "What color is the dress of the woman writing on the wall in the video?\n(A) Black\n(B) White\n(C) Blue\n(D) Red\n", "candidates": ["Black", "White", "Blue", "Red"], "answer": "B", "task_type": "plotQA", "question_id": "Q1149"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f2942aee17d41de491fb83072531f3c52a49fbc64a49f4535145d84f613ebd8b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the dress of the woman writing on the wall in the video?\n(A) Black\n(B) White\n(C) Blue\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the dress of the woman writing on the wall in the video?\n(A) Black\n(B) White\n(C) Blue\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1153, "doc": {"video_name": "xiaoliyu_9.mp4", "duration": 302.0, "question": "What did the cartoon snake do to the cartoon carp?\n(A) Attacked\n(B) Played\n(C) Invited for a meal\n(D) Invited for a drink\n", "candidates": ["Attacked", "Played", "Invited for a meal", "Invited for a drink"], "answer": "A", "task_type": "plotQA", "question_id": "Q1153"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cb6cc9ffb79c9222e14c0a1b389b39a3b8ac8b31b9661495d01e039e4c9eb562", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did the cartoon snake do to the cartoon carp?\n(A) Attacked\n(B) Played\n(C) Invited for a meal\n(D) Invited for a drink\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What did the cartoon snake do to the cartoon carp?\n(A) Attacked\n(B) Played\n(C) Invited for a meal\n(D) Invited for a drink\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1157, "doc": {"video_name": "1.mp4", "duration": 401.23, "question": "Why does the man suddenly stand up from the sailboat and walk to the other end of the boat at the beginning of the video?\n(A) A wall blocks his way\n(B) He has to complete some important tasks\n(C) He wants to talk to other people on the boat\n(D) He wants to check the weather or sea conditions to ensure the safety of navigation.\n", "candidates": ["A wall blocks his way", "He has to complete some important tasks", "He wants to talk to other people on the boat", "He wants to check the weather or sea conditions to ensure the safety of navigation."], "answer": "A", "task_type": "plotQA", "question_id": "Q1157"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5255171380abd7c8203e52a56926886438e9f61d0e7173b15724eb8557ec0b36", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why does the man suddenly stand up from the sailboat and walk to the other end of the boat at the beginning of the video?\n(A) A wall blocks his way\n(B) He has to complete some important tasks\n(C) He wants to talk to other people on the boat\n(D) He wants to check the weather or sea conditions to ensure the safety of navigation.\n", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "Why does the man suddenly stand up from the sailboat and walk to the other end of the boat at the beginning of the video?\n(A) A wall blocks his way\n(B) He has to complete some important tasks\n(C) He wants to talk to other people on the boat\n(D) He wants to check the weather or sea conditions to ensure the safety of navigation.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1161, "doc": {"video_name": "tomjerry_2.mp4", "duration": 303.0, "question": "What kind of bed is the cartoon mouse lying on?\n(A) Stone bed\n(B) A bed made of cheese\n(C) Wooden bed\n(D) Iron bed\n", "candidates": ["Stone bed", "A bed made of cheese", "Wooden bed", "Iron bed"], "answer": "B", "task_type": "plotQA", "question_id": "Q1161"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ec51106545b8b8861475735abfeeba793175778ee1bcc0c9d57e832c6a161d73", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What kind of bed is the cartoon mouse lying on?\n(A) Stone bed\n(B) A bed made of cheese\n(C) Wooden bed\n(D) Iron bed\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What kind of bed is the cartoon mouse lying on?\n(A) Stone bed\n(B) A bed made of cheese\n(C) Wooden bed\n(D) Iron bed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1165, "doc": {"video_name": "en_tv_2.mp4", "duration": 509.0, "question": "What happens to the woman with yellow hair on the street?\n(A) She is shot down by someone\n(B) She is hit by an object\n(C) She is hit by a car\n(D) She is pulled away by someone\n", "candidates": ["She is shot down by someone", "She is hit by an object", "She is hit by a car", "She is pulled away by someone"], "answer": "A", "task_type": "plotQA", "question_id": "Q1165"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3e1da62177302b2e0df69d3b09f3bf5a6882a1a4a7e1da1d170979f372e4c445", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What happens to the woman with yellow hair on the street?\n(A) She is shot down by someone\n(B) She is hit by an object\n(C) She is hit by a car\n(D) She is pulled away by someone\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What happens to the woman with yellow hair on the street?\n(A) She is shot down by someone\n(B) She is hit by an object\n(C) She is hit by a car\n(D) She is pulled away by someone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1169, "doc": {"video_name": "movie101_30.mp4", "duration": 726.0, "question": "What color is the clown's hair in the video?\n(A) White\n(B) Red\n(C) Blue\n(D) Green\n", "candidates": ["White", "Red", "Blue", "Green"], "answer": "B", "task_type": "plotQA", "question_id": "Q1169"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0dc83d2f9da7c01b1fe906b5a561ec710b7159550f8a4c0608277047159d6aac", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the clown's hair in the video?\n(A) White\n(B) Red\n(C) Blue\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the clown's hair in the video?\n(A) White\n(B) Red\n(C) Blue\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1173, "doc": {"video_name": "en_tv_18.mp4", "duration": 568.0, "question": "What color is the scarf of the woman making a phone call in the video?\n(A) Blue\n(B) Purple\n(C) Green\n(D) Orange\n", "candidates": ["Blue", "Purple", "Green", "Orange"], "answer": "B", "task_type": "plotQA", "question_id": "Q1173"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "23112a9fa28c30a56065ba5790f35f646e35115a54ee806a9e1230089f889674", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the scarf of the woman making a phone call in the video?\n(A) Blue\n(B) Purple\n(C) Green\n(D) Orange\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the scarf of the woman making a phone call in the video?\n(A) Blue\n(B) Purple\n(C) Green\n(D) Orange\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1177, "doc": {"video_name": "haimian_6.mp4", "duration": 324.0, "question": "What are the cartoon octopus and cartoon sponge doing?\n(A) Eating\n(B) Arguing\n(C) Running a restaurant\n(D) Fighting\n", "candidates": ["Eating", "Arguing", "Running a restaurant", "Fighting"], "answer": "C", "task_type": "plotQA", "question_id": "Q1177"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ff9fb9e25644452bbc75be184c9746b4e6790a21cc3c65dfd3426b4c4e2cef2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the cartoon octopus and cartoon sponge doing?\n(A) Eating\n(B) Arguing\n(C) Running a restaurant\n(D) Fighting\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What are the cartoon octopus and cartoon sponge doing?\n(A) Eating\n(B) Arguing\n(C) Running a restaurant\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1181, "doc": {"video_name": "haimian_6.mp4", "duration": 324.0, "question": "What does the red cartoon character in red clothes give to the cartoon character in yellow clothes?\n(A) A ring\n(B) Necklace\n(C) Earrings\n(D) Mobile phone\n", "candidates": ["A ring", "Necklace", "Earrings", "Mobile phone"], "answer": "A", "task_type": "plotQA", "question_id": "Q1181"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8bca07cd97e9842254f62aca94fb3c608daa821ed0575880a4c46d487ffe1cdb", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What does the red cartoon character in red clothes give to the cartoon character in yellow clothes?\n(A) A ring\n(B) Necklace\n(C) Earrings\n(D) Mobile phone\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What does the red cartoon character in red clothes give to the cartoon character in yellow clothes?\n(A) A ring\n(B) Necklace\n(C) Earrings\n(D) Mobile phone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1185, "doc": {"video_name": "tomjerry_5.mp4", "duration": 303.0, "question": "What did the cartoon mouse place at the door after opening it?\n(A) Wooden stick\n(B) Towel\n(C) Banana peel\n(D) Blanket\n", "candidates": ["Wooden stick", "Towel", "Banana peel", "Blanket"], "answer": "C", "task_type": "plotQA", "question_id": "Q1185"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bcd3a14888b2bbc29d33f8b2442f916f81932eba68db8cf227e9a510190ade48", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did the cartoon mouse place at the door after opening it?\n(A) Wooden stick\n(B) Towel\n(C) Banana peel\n(D) Blanket\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What did the cartoon mouse place at the door after opening it?\n(A) Wooden stick\n(B) Towel\n(C) Banana peel\n(D) Blanket\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1189, "doc": {"video_name": "tomjerry_10.mp4", "duration": 302.0, "question": "Why does the cartoon little mouse want to eat the cheese on the clip?\n(A) Because the cartoon little mouse is angry\n(B) Because the cartoon little mouse wants to grow\n(C) Because the cartoon little mouse is hungry\n(D) Because the cartoon little mouse is happy\n", "candidates": ["Because the cartoon little mouse is angry", "Because the cartoon little mouse wants to grow", "Because the cartoon little mouse is hungry", "Because the cartoon little mouse is happy"], "answer": "C", "task_type": "plotQA", "question_id": "Q1189"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e8046a6c2a44176964cb79e35b8915ed8601fa6142cb237bebe6dd3327cb8437", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why does the cartoon little mouse want to eat the cheese on the clip?\n(A) Because the cartoon little mouse is angry\n(B) Because the cartoon little mouse wants to grow\n(C) Because the cartoon little mouse is hungry\n(D) Because the cartoon little mouse is happy\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Why does the cartoon little mouse want to eat the cheese on the clip?\n(A) Because the cartoon little mouse is angry\n(B) Because the cartoon little mouse wants to grow\n(C) Because the cartoon little mouse is hungry\n(D) Because the cartoon little mouse is happy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1193, "doc": {"video_name": "movie101_14.mp4", "duration": 465.0, "question": "What color is the suit worn by the man holding a concert in the video?\n(A) Blue\n(B) Black\n(C) Red\n(D) Green\n", "candidates": ["Blue", "Black", "Red", "Green"], "answer": "B", "task_type": "plotQA", "question_id": "Q1193"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6d52f642263ddd259dcb8429933ee29c329e98c5ce49bdbcc94df4c645bb54bf", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the suit worn by the man holding a concert in the video?\n(A) Blue\n(B) Black\n(C) Red\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the suit worn by the man holding a concert in the video?\n(A) Blue\n(B) Black\n(C) Red\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1197, "doc": {"video_name": "tomjerry_9.mp4", "duration": 328.0, "question": "What does the cartoon mouse hide in the food?\n(A) Tennis Ball\n(B) Iron\n(C) Guitar\n(D) Baseball\n", "candidates": ["Tennis Ball", "Iron", "Guitar", "Baseball"], "answer": "B", "task_type": "plotQA", "question_id": "Q1197"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c33d1372b9cda18eda36a16492ae20199df4995b8687f205d824d12af462602a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What does the cartoon mouse hide in the food?\n(A) Tennis Ball\n(B) Iron\n(C) Guitar\n(D) Baseball\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What does the cartoon mouse hide in the food?\n(A) Tennis Ball\n(B) Iron\n(C) Guitar\n(D) Baseball\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1201, "doc": {"video_name": "movie101_82.mp4", "duration": 467.0, "question": "What color is the car driven by the two men?\n(A) Green\n(B) Red\n(C) Black\n(D) White\n", "candidates": ["Green", "Red", "Black", "White"], "answer": "A", "task_type": "plotQA", "question_id": "Q1201"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "419887e8aae29d299e0512735204125072dfce5abdc7c0fee1703e3492f99f0c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the car driven by the two men?\n(A) Green\n(B) Red\n(C) Black\n(D) White\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What color is the car driven by the two men?\n(A) Green\n(B) Red\n(C) Black\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1205, "doc": {"video_name": "haimian_2.mp4", "duration": 356.0, "question": "What did the cartoon bug try to take away from the restaurant?\n(A) A hamburger\n(B) Fries\n(C) Table\n(D) Drawing board\n", "candidates": ["A hamburger", "Fries", "Table", "Drawing board"], "answer": "A", "task_type": "plotQA", "question_id": "Q1205"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2583e6c1a191e4b7552b108bbb06791947b20d4d9b99a6ec0f38b3ad6e3dde6c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did the cartoon bug try to take away from the restaurant?\n(A) A hamburger\n(B) Fries\n(C) Table\n(D) Drawing board\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What did the cartoon bug try to take away from the restaurant?\n(A) A hamburger\n(B) Fries\n(C) Table\n(D) Drawing board\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1209, "doc": {"video_name": "movie101_108.mp4", "duration": 330.0, "question": "What happened when the two little girls came down from the mountain road?\n(A) They rested\n(B) They fell\n(C) They drank water\n(D) They were taken away\n", "candidates": ["They rested", "They fell", "They drank water", "They were taken away"], "answer": "D", "task_type": "plotQA", "question_id": "Q1209"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c93a3a7b7e4c845dd30796acf6cb5afd0a7de9a6d972f084c2bd3f5f350456d0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What happened when the two little girls came down from the mountain road?\n(A) They rested\n(B) They fell\n(C) They drank water\n(D) They were taken away\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What happened when the two little girls came down from the mountain road?\n(A) They rested\n(B) They fell\n(C) They drank water\n(D) They were taken away\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1213, "doc": {"video_name": "movie101_75.mp4", "duration": 669.0, "question": "What happens to the dog in the video?\n(A) Gets killed\n(B) Gets stunned\n(C) Gets let go\n(D) Gets tied up\n", "candidates": ["Gets killed", "Gets stunned", "Gets let go", "Gets tied up"], "answer": "A", "task_type": "plotQA", "question_id": "Q1213"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7ad71dfb9b1604e8b845ba57a6e7f696f53a1cbed7599821de01b72f8464afcd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What happens to the dog in the video?\n(A) Gets killed\n(B) Gets stunned\n(C) Gets let go\n(D) Gets tied up\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What happens to the dog in the video?\n(A) Gets killed\n(B) Gets stunned\n(C) Gets let go\n(D) Gets tied up\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1217, "doc": {"video_name": "en_tv_36.mp4", "duration": 574.0, "question": "What color is the sweater worn by the girl sitting by the bed?\n(A) Purple\n(B) Blue\n(C) White\n(D) Black\n", "candidates": ["Purple", "Blue", "White", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1217"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c0c7fb7878eb9b0b9cd46cd28c58a650fa801fcd8fc62a0669a103633c1ef99f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the sweater worn by the girl sitting by the bed?\n(A) Purple\n(B) Blue\n(C) White\n(D) Black\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the sweater worn by the girl sitting by the bed?\n(A) Purple\n(B) Blue\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1221, "doc": {"video_name": "movie101_105.mp4", "duration": 744.0, "question": "Why did the man at the beginning of the video rush into the house at the end?\n(A) Because he wanted to turn off the TV\n(B) Because he wanted to save his child\n(C) Because he wanted to get his phone\n(D) Because he wanted to take an umbrella\n", "candidates": ["Because he wanted to turn off the TV", "Because he wanted to save his child", "Because he wanted to get his phone", "Because he wanted to take an umbrella"], "answer": "B", "task_type": "plotQA", "question_id": "Q1221"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c27f52776ce9effad5b08b5d8a6180ff212e047054f77294cc92e8562a94729e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why did the man at the beginning of the video rush into the house at the end?\n(A) Because he wanted to turn off the TV\n(B) Because he wanted to save his child\n(C) Because he wanted to get his phone\n(D) Because he wanted to take an umbrella\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Why did the man at the beginning of the video rush into the house at the end?\n(A) Because he wanted to turn off the TV\n(B) Because he wanted to save his child\n(C) Because he wanted to get his phone\n(D) Because he wanted to take an umbrella\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1225, "doc": {"video_name": "haimian_4.mp4", "duration": 323.0, "question": "What is the cartoon sponge's reaction when the cartoon octopus finds him in the seaweed pile?\n(A) The cartoon sponge laughed\n(B) The cartoon sponge cried\n(C) The cartoon sponge was excited\n(D) The cartoon sponge was scared\n", "candidates": ["The cartoon sponge laughed", "The cartoon sponge cried", "The cartoon sponge was excited", "The cartoon sponge was scared"], "answer": "D", "task_type": "plotQA", "question_id": "Q1225"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b11141d14f7d27d67ec1bf1d5919d5abf5884c35f6b21895bc52ea2de2769cf3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the cartoon sponge's reaction when the cartoon octopus finds him in the seaweed pile?\n(A) The cartoon sponge laughed\n(B) The cartoon sponge cried\n(C) The cartoon sponge was excited\n(D) The cartoon sponge was scared\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What is the cartoon sponge's reaction when the cartoon octopus finds him in the seaweed pile?\n(A) The cartoon sponge laughed\n(B) The cartoon sponge cried\n(C) The cartoon sponge was excited\n(D) The cartoon sponge was scared\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1229, "doc": {"video_name": "movie101_21.mp4", "duration": 289.0, "question": "What color is the sweater worn by the boy chatting with the girl in the video?\n(A) Yellow\n(B) Blue\n(C) Black\n(D) White\n", "candidates": ["Yellow", "Blue", "Black", "White"], "answer": "D", "task_type": "plotQA", "question_id": "Q1229"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fbd0b7a486d651df957550d489b7d44b7af7d7c7f5768c7ad6f95cd6a28443cb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the sweater worn by the boy chatting with the girl in the video?\n(A) Yellow\n(B) Blue\n(C) Black\n(D) White\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the sweater worn by the boy chatting with the girl in the video?\n(A) Yellow\n(B) Blue\n(C) Black\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1233, "doc": {"video_name": "en_tv_8.mp4", "duration": 460.0, "question": "Where does the woman in black leather jacket go after talking alone with the man with a scar on his face?\n(A) Office\n(B) Football field\n(C) Swimming pool\n(D) Gymnasium\n", "candidates": ["Office", "Football field", "Swimming pool", "Gymnasium"], "answer": "C", "task_type": "plotQA", "question_id": "Q1233"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ce442292a1981f6a5455674910c2ad8ce44f07bb706e610bc92ddc801f6e0c8a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where does the woman in black leather jacket go after talking alone with the man with a scar on his face?\n(A) Office\n(B) Football field\n(C) Swimming pool\n(D) Gymnasium\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Where does the woman in black leather jacket go after talking alone with the man with a scar on his face?\n(A) Office\n(B) Football field\n(C) Swimming pool\n(D) Gymnasium\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1237, "doc": {"video_name": "movie101_67.mp4", "duration": 530.0, "question": "What color is the skirt of the woman interviewing the man?\n(A) Blue\n(B) Red\n(C) Black\n(D) Green\n", "candidates": ["Blue", "Red", "Black", "Green"], "answer": "B", "task_type": "plotQA", "question_id": "Q1237"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "de3193f936ad2cc64f3ecaa51d182a67462532bc24af863a4a44595fa690f3bd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the skirt of the woman interviewing the man?\n(A) Blue\n(B) Red\n(C) Black\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the skirt of the woman interviewing the man?\n(A) Blue\n(B) Red\n(C) Black\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1241, "doc": {"video_name": "movie101_67.mp4", "duration": 530.0, "question": "What is the emotion of the blonde woman at the beginning?\n(A) Neutral\n(B) Joy\n(C) Grievance\n(D) Crying\n", "candidates": ["Neutral", "Joy", "Grievance", "Crying"], "answer": "D", "task_type": "plotQA", "question_id": "Q1241"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c536255c5b0f5a416a73a6df12c68c71050d100c80ee86e1fc59b931a37ef624", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the emotion of the blonde woman at the beginning?\n(A) Neutral\n(B) Joy\n(C) Grievance\n(D) Crying\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What is the emotion of the blonde woman at the beginning?\n(A) Neutral\n(B) Joy\n(C) Grievance\n(D) Crying\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1245, "doc": {"video_name": "movie101_64.mp4", "duration": 626.0, "question": "What is the girl's emotion in the elevator?\n(A) Joy\n(B) Crying\n(C) Anger\n(D) Neutral\n", "candidates": ["Joy", "Crying", "Anger", "Neutral"], "answer": "B", "task_type": "plotQA", "question_id": "Q1245"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3e17bc58d48a6e3372e2646b03d2bd46ac1e294f65812ba9c7f42683f575bc38", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the girl's emotion in the elevator?\n(A) Joy\n(B) Crying\n(C) Anger\n(D) Neutral\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the girl's emotion in the elevator?\n(A) Joy\n(B) Crying\n(C) Anger\n(D) Neutral\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1249, "doc": {"video_name": "movie101_64.mp4", "duration": 626.0, "question": "What kind of food does the girl bring at the beginning of the movie?\n(A) Porridge\n(B) Rice\n(C) Cake\n(D) Steamed bun\n", "candidates": ["Porridge", "Rice", "Cake", "Steamed bun"], "answer": "D", "task_type": "plotQA", "question_id": "Q1249"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c7464a1e5e3c9cdeaf92f0461e271c19eaaeeaed3c6f77ae8ccc2b2a7706ae5f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What kind of food does the girl bring at the beginning of the movie?\n(A) Porridge\n(B) Rice\n(C) Cake\n(D) Steamed bun\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What kind of food does the girl bring at the beginning of the movie?\n(A) Porridge\n(B) Rice\n(C) Cake\n(D) Steamed bun\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1253, "doc": {"video_name": "movie101_83.mp4", "duration": 640.0, "question": "What scratched the man in yellow clothes in the video?\n(A) Cactus\n(B) Knife\n(C) Paper\n(D) Scissors\n", "candidates": ["Cactus", "Knife", "Paper", "Scissors"], "answer": "A", "task_type": "plotQA", "question_id": "Q1253"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ea0968163a205b676e0c1d0880822a19ac4214a8f2c7a453af5a0fc5fd7242c5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What scratched the man in yellow clothes in the video?\n(A) Cactus\n(B) Knife\n(C) Paper\n(D) Scissors\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What scratched the man in yellow clothes in the video?\n(A) Cactus\n(B) Knife\n(C) Paper\n(D) Scissors\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1257, "doc": {"video_name": "movie101_21.mp4", "duration": 289.0, "question": "What color are the shoes worn by the boy and girl chatting in the video?\n(A) Yellow\n(B) Blue\n(C) White\n(D) Black\n", "candidates": ["Yellow", "Blue", "White", "Black"], "answer": "C", "task_type": "plotQA", "question_id": "Q1257"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1bc10a8b934a2732e76d996c4bc89f95e3dbcc449c0c26201fc81b9a831df5dc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color are the shoes worn by the boy and girl chatting in the video?\n(A) Yellow\n(B) Blue\n(C) White\n(D) Black\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color are the shoes worn by the boy and girl chatting in the video?\n(A) Yellow\n(B) Blue\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1261, "doc": {"video_name": "movie101_52.mp4", "duration": 338.0, "question": "What preparation did the soldiers in the camp make to fight against the enemy's tanks?\n(A) They ran away quickly\n(B) They lay down to hide\n(C) They rolled out a cannon to fight\n(D) They set up a machine gun\n", "candidates": ["They ran away quickly", "They lay down to hide", "They rolled out a cannon to fight", "They set up a machine gun"], "answer": "C", "task_type": "plotQA", "question_id": "Q1261"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "88f8d4d4fbe5b070375b7e39b60c3d61783cf49eddb5c89919dc2de145de0eab", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What preparation did the soldiers in the camp make to fight against the enemy's tanks?\n(A) They ran away quickly\n(B) They lay down to hide\n(C) They rolled out a cannon to fight\n(D) They set up a machine gun\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What preparation did the soldiers in the camp make to fight against the enemy's tanks?\n(A) They ran away quickly\n(B) They lay down to hide\n(C) They rolled out a cannon to fight\n(D) They set up a machine gun\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1265, "doc": {"video_name": "movie101_81.mp4", "duration": 252.0, "question": "What color is the woman's dress at the beginning of the video?\n(A) Black\n(B) Blue\n(C) Pink\n(D) White\n", "candidates": ["Black", "Blue", "Pink", "White"], "answer": "C", "task_type": "plotQA", "question_id": "Q1265"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6d932aa0983ea8a2d1dfa43dcd2a0402eb37258903de41f9305136b493d50d84", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the woman's dress at the beginning of the video?\n(A) Black\n(B) Blue\n(C) Pink\n(D) White\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the woman's dress at the beginning of the video?\n(A) Black\n(B) Blue\n(C) Pink\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1269, "doc": {"video_name": "movie101_85.mp4", "duration": 560.0, "question": "What is the woman doing by the river at the beginning of the video?\n(A) Grooming\n(B) Drinking water\n(C) Weaving\n(D) Playing\n", "candidates": ["Grooming", "Drinking water", "Weaving", "Playing"], "answer": "C", "task_type": "plotQA", "question_id": "Q1269"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "67fd95a59869750c65ab425c7e88a3e0b68d3423ef72b243c8867b2d8dc0edf4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the woman doing by the river at the beginning of the video?\n(A) Grooming\n(B) Drinking water\n(C) Weaving\n(D) Playing\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What is the woman doing by the river at the beginning of the video?\n(A) Grooming\n(B) Drinking water\n(C) Weaving\n(D) Playing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1273, "doc": {"video_name": "en_tv_2.mp4", "duration": 509.0, "question": "What happens when the man with glasses interacts with the woman with yellow hair?\n(A) The woman ties a tie for the man\n(B) The woman slaps the man\n(C) The woman knocks the man down\n(D) The woman hands water to the man\n", "candidates": ["The woman ties a tie for the man", "The woman slaps the man", "The woman knocks the man down", "The woman hands water to the man"], "answer": "C", "task_type": "plotQA", "question_id": "Q1273"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6dff967538d0817b53fab05e09c0fd0eb5ecf3cc4f19ae8e4e2b781bf52d6bf4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What happens when the man with glasses interacts with the woman with yellow hair?\n(A) The woman ties a tie for the man\n(B) The woman slaps the man\n(C) The woman knocks the man down\n(D) The woman hands water to the man\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "What happens when the man with glasses interacts with the woman with yellow hair?\n(A) The woman ties a tie for the man\n(B) The woman slaps the man\n(C) The woman knocks the man down\n(D) The woman hands water to the man\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1277, "doc": {"video_name": "tomjerry_7.mp4", "duration": 306.0, "question": "Whose pink handkerchief fell?\n(A) The cartoon dog's\n(B) The white cartoon female cat's\n(C) The cartoon turtle's\n(D) The cartoon mouse's\n", "candidates": ["The cartoon dog's", "The white cartoon female cat's", "The cartoon turtle's", "The cartoon mouse's"], "answer": "B", "task_type": "plotQA", "question_id": "Q1277"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d11538cc94ebe4ac541823a7ed5744ea37be11b2e7a319df89914901c0914e9d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Whose pink handkerchief fell?\n(A) The cartoon dog's\n(B) The white cartoon female cat's\n(C) The cartoon turtle's\n(D) The cartoon mouse's\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Whose pink handkerchief fell?\n(A) The cartoon dog's\n(B) The white cartoon female cat's\n(C) The cartoon turtle's\n(D) The cartoon mouse's\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1281, "doc": {"video_name": "en_tv_1.mp4", "duration": 514.0, "question": "At the end of the video, what does the man in the black coat force the woman in the red coat to do?\n(A) Leave\n(B) Withdraw money\n(C) Sleep\n(D) Go to see a man in white clothes\n", "candidates": ["Leave", "Withdraw money", "Sleep", "Go to see a man in white clothes"], "answer": "D", "task_type": "plotQA", "question_id": "Q1281"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4db096f53c7763dfb78e8a28969fa05452765f1fffdd1337f6cd4cff226fd63d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "At the end of the video, what does the man in the black coat force the woman in the red coat to do?\n(A) Leave\n(B) Withdraw money\n(C) Sleep\n(D) Go to see a man in white clothes\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "At the end of the video, what does the man in the black coat force the woman in the red coat to do?\n(A) Leave\n(B) Withdraw money\n(C) Sleep\n(D) Go to see a man in white clothes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1285, "doc": {"video_name": "en_tv_39.mp4", "duration": 476.0, "question": "What is the emotion of the woman who is making a phone call?\n(A) Sad\n(B) Angry\n(C) Neutral\n(D) Happy\n", "candidates": ["Sad", "Angry", "Neutral", "Happy"], "answer": "B", "task_type": "plotQA", "question_id": "Q1285"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "da891068bca3755795766f56d925f6bc46317db3535fdce0c93f844f24aa8713", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the emotion of the woman who is making a phone call?\n(A) Sad\n(B) Angry\n(C) Neutral\n(D) Happy\n", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "What is the emotion of the woman who is making a phone call?\n(A) Sad\n(B) Angry\n(C) Neutral\n(D) Happy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1289, "doc": {"video_name": "movie101_72.mp4", "duration": 289.0, "question": "What color is the clothes of the man standing on the pole?\n(A) Black\n(B) Yellow\n(C) White\n(D) Blue\n", "candidates": ["Black", "Yellow", "White", "Blue"], "answer": "C", "task_type": "plotQA", "question_id": "Q1289"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7251c7c25b2b5b4c5b27f5306ec132c318c9dcd60ac9e0158ba08ddab7e86a90", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the clothes of the man standing on the pole?\n(A) Black\n(B) Yellow\n(C) White\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the clothes of the man standing on the pole?\n(A) Black\n(B) Yellow\n(C) White\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1293, "doc": {"video_name": "movie101_72.mp4", "duration": 289.0, "question": "What color is the woman's dress at the beginning of the video?\n(A) Red\n(B) White\n(C) Blue\n(D) Black\n", "candidates": ["Red", "White", "Blue", "Black"], "answer": "C", "task_type": "plotQA", "question_id": "Q1293"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d283b6485ff202d59e72f5532e8b57a1a02bd008ff62b380d8ee07d7e1a2630d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the woman's dress at the beginning of the video?\n(A) Red\n(B) White\n(C) Blue\n(D) Black\n", "task_type": "plotQA", "pred_answer": "C.", "answer": "C"}, "input": "What color is the woman's dress at the beginning of the video?\n(A) Red\n(B) White\n(C) Blue\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1297, "doc": {"video_name": "xiaoliyu_3.mp4", "duration": 405.0, "question": "What did the cartoon dragon turn into after flying into the volcano?\n(A) Apple\n(B) Peach\n(C) Orange\n(D) Colorful gemstones\n", "candidates": ["Apple", "Peach", "Orange", "Colorful gemstones"], "answer": "D", "task_type": "plotQA", "question_id": "Q1297"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b3696bb433ef741442b855a1cd1c6b54336bb2d0ccd1c22c02e935bcecf7edf", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did the cartoon dragon turn into after flying into the volcano?\n(A) Apple\n(B) Peach\n(C) Orange\n(D) Colorful gemstones\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What did the cartoon dragon turn into after flying into the volcano?\n(A) Apple\n(B) Peach\n(C) Orange\n(D) Colorful gemstones\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1301, "doc": {"video_name": "movie101_37.mp4", "duration": 457.0, "question": "What color is the clothes of the person wearing a plastic bag on their head at the staircase at the beginning of the video?\n(A) White\n(B) Black\n(C) Red\n(D) Green\n", "candidates": ["White", "Black", "Red", "Green"], "answer": "C", "task_type": "plotQA", "question_id": "Q1301"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b96e491ee904d6718eadc4375a322f4e5a42ae4936cd4d94cc288bb1e1d22af4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the clothes of the person wearing a plastic bag on their head at the staircase at the beginning of the video?\n(A) White\n(B) Black\n(C) Red\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the clothes of the person wearing a plastic bag on their head at the staircase at the beginning of the video?\n(A) White\n(B) Black\n(C) Red\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1305, "doc": {"video_name": "xiaoliyu_9.mp4", "duration": 302.0, "question": "What did the cartoon lobster do to the unconscious cartoon turtle?\n(A) Treated\n(B) Kicked away\n(C) Helped up\n(D) Attacked\n", "candidates": ["Treated", "Kicked away", "Helped up", "Attacked"], "answer": "B", "task_type": "plotQA", "question_id": "Q1305"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "66967a9be17d805d20ca005d996b358c27a171c57c1bf0035eb7c171c40daba3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did the cartoon lobster do to the unconscious cartoon turtle?\n(A) Treated\n(B) Kicked away\n(C) Helped up\n(D) Attacked\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What did the cartoon lobster do to the unconscious cartoon turtle?\n(A) Treated\n(B) Kicked away\n(C) Helped up\n(D) Attacked\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1309, "doc": {"video_name": "en_tv_1.mp4", "duration": 514.0, "question": "After the woman in the red coat enters the car, what happens to her?\n(A) Car accident\n(B) She is hit\n(C) She is scolded\n(D) She is threatened with a gun\n", "candidates": ["Car accident", "She is hit", "She is scolded", "She is threatened with a gun"], "answer": "D", "task_type": "plotQA", "question_id": "Q1309"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "77292f0868b61885090507cd0996a97a60b03561fe41b7ccd795139bbbefdcd3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "After the woman in the red coat enters the car, what happens to her?\n(A) Car accident\n(B) She is hit\n(C) She is scolded\n(D) She is threatened with a gun\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "After the woman in the red coat enters the car, what happens to her?\n(A) Car accident\n(B) She is hit\n(C) She is scolded\n(D) She is threatened with a gun\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1313, "doc": {"video_name": "8.mp4", "duration": 507.29, "question": "What color are the shoes that the main cartoon character in the video is wearing?\n(A) Green\n(B) Red\n(C) White\n(D) Purple\n", "candidates": ["Green", "Red", "White", "Purple"], "answer": "D", "task_type": "plotQA", "question_id": "Q1313"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b0cd467414c2cae3ab163b98593c83b13a61e1e5781f5122fc8497c385d96c0c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color are the shoes that the main cartoon character in the video is wearing?\n(A) Green\n(B) Red\n(C) White\n(D) Purple\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What color are the shoes that the main cartoon character in the video is wearing?\n(A) Green\n(B) Red\n(C) White\n(D) Purple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1317, "doc": {"video_name": "movie101_28.mp4", "duration": 234.0, "question": "What color is the monster with wings in the forest in the movie?\n(A) Green\n(B) Black\n(C) Red\n(D) Yellow\n", "candidates": ["Green", "Black", "Red", "Yellow"], "answer": "C", "task_type": "plotQA", "question_id": "Q1317"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2adc7e1a17c0b13927a4cc5035aeeca664127252fddda7f634560b3cc8ddf5a2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the monster with wings in the forest in the movie?\n(A) Green\n(B) Black\n(C) Red\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the monster with wings in the forest in the movie?\n(A) Green\n(B) Black\n(C) Red\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1321, "doc": {"video_name": "en_tv_20.mp4", "duration": 426.0, "question": "What color is the chair in the room at the beginning of the video?\n(A) Red\n(B) Green\n(C) Yellow\n(D) White\n", "candidates": ["Red", "Green", "Yellow", "White"], "answer": "A", "task_type": "plotQA", "question_id": "Q1321"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fa6eb62489d6fa90f21cdf9939dc58d22c493c7f405911f7be58f81508732fc8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the chair in the room at the beginning of the video?\n(A) Red\n(B) Green\n(C) Yellow\n(D) White\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the chair in the room at the beginning of the video?\n(A) Red\n(B) Green\n(C) Yellow\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1325, "doc": {"video_name": "movie101_6.mp4", "duration": 693.0, "question": "What color pants does the woman wear when walking the dog?\n(A) Red\n(B) Yellow\n(C) White\n(D) Green\n", "candidates": ["Red", "Yellow", "White", "Green"], "answer": "A", "task_type": "plotQA", "question_id": "Q1325"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4964c00bb79ae0c125bcaf852e2767dab43dc169f327a57dd0c255402cd8b47f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color pants does the woman wear when walking the dog?\n(A) Red\n(B) Yellow\n(C) White\n(D) Green\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color pants does the woman wear when walking the dog?\n(A) Red\n(B) Yellow\n(C) White\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1329, "doc": {"video_name": "movie101_102.mp4", "duration": 607.0, "question": "What are the monks doing on the battlefield at the start of the video?\n(A) Chanting\n(B) Blowing the horn\n(C) Treating the wounded\n(D) Fighting\n", "candidates": ["Chanting", "Blowing the horn", "Treating the wounded", "Fighting"], "answer": "C", "task_type": "plotQA", "question_id": "Q1329"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6d3a998528c3dc562e01c755d1a9ee24083f603114cf606e2a34837253d30fa9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the monks doing on the battlefield at the start of the video?\n(A) Chanting\n(B) Blowing the horn\n(C) Treating the wounded\n(D) Fighting\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What are the monks doing on the battlefield at the start of the video?\n(A) Chanting\n(B) Blowing the horn\n(C) Treating the wounded\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1333, "doc": {"video_name": "en_tv_35.mp4", "duration": 505.0, "question": "What does the woman bring to knock on the door in the video?\n(A) Food\n(B) Coffee\n(C) Wine\n(D) Fruit\n", "candidates": ["Food", "Coffee", "Wine", "Fruit"], "answer": "B", "task_type": "plotQA", "question_id": "Q1333"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5b290141995c61ddfaef49759593a0f21e4762309e86bccac7ebc455ea149fa0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What does the woman bring to knock on the door in the video?\n(A) Food\n(B) Coffee\n(C) Wine\n(D) Fruit\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What does the woman bring to knock on the door in the video?\n(A) Food\n(B) Coffee\n(C) Wine\n(D) Fruit\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1337, "doc": {"video_name": "en_tv_16.mp4", "duration": 552.0, "question": "What color is the clothes of the man who is locked up in the video?\n(A) White\n(B) Yellow\n(C) Black\n(D) Green\n", "candidates": ["White", "Yellow", "Black", "Green"], "answer": "C", "task_type": "plotQA", "question_id": "Q1337"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1958d4dacff6b480cb394509190de9084ecb61522e0225886425abdfbf3c443e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the clothes of the man who is locked up in the video?\n(A) White\n(B) Yellow\n(C) Black\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the clothes of the man who is locked up in the video?\n(A) White\n(B) Yellow\n(C) Black\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1341, "doc": {"video_name": "movie101_65.mp4", "duration": 611.0, "question": "What did the riverside sorcerer conjure up at the start of the video?\n(A) Flame\n(B) Flower\n(C) Wine\n(D) Food\n", "candidates": ["Flame", "Flower", "Wine", "Food"], "answer": "A", "task_type": "plotQA", "question_id": "Q1341"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd233541c45dee3f15433ccff1a639350cd56b6f0fe9955e0415317fae8330b5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did the riverside sorcerer conjure up at the start of the video?\n(A) Flame\n(B) Flower\n(C) Wine\n(D) Food\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What did the riverside sorcerer conjure up at the start of the video?\n(A) Flame\n(B) Flower\n(C) Wine\n(D) Food\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1345, "doc": {"video_name": "tomjerry_9.mp4", "duration": 328.0, "question": "What does the cartoon mouse use to hit the cartoon dog?\n(A) A ball\n(B) A vase\n(C) A yellow plank\n(D) An iron rod\n", "candidates": ["A ball", "A vase", "A yellow plank", "An iron rod"], "answer": "C", "task_type": "plotQA", "question_id": "Q1345"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "03a7b9720c39003952d7d5d93faf90298f14b85696c2219803e20ed5bbbe0d16", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What does the cartoon mouse use to hit the cartoon dog?\n(A) A ball\n(B) A vase\n(C) A yellow plank\n(D) An iron rod\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What does the cartoon mouse use to hit the cartoon dog?\n(A) A ball\n(B) A vase\n(C) A yellow plank\n(D) An iron rod\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1349, "doc": {"video_name": "haimian_5.mp4", "duration": 335.0, "question": "What does the cartoon lobster lift from the sea?\n(A) Paper money\n(B) Carp\n(C) Black fish\n(D) Octopus\n", "candidates": ["Paper money", "Carp", "Black fish", "Octopus"], "answer": "A", "task_type": "plotQA", "question_id": "Q1349"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ddef39a4001cdb06746fe0db84190de540973214a41ece975c48d667a4645d8c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What does the cartoon lobster lift from the sea?\n(A) Paper money\n(B) Carp\n(C) Black fish\n(D) Octopus\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What does the cartoon lobster lift from the sea?\n(A) Paper money\n(B) Carp\n(C) Black fish\n(D) Octopus\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1353, "doc": {"video_name": "movie101_4.mp4", "duration": 560.0, "question": "What was the mood of the person in camouflage clothing while talking to a lady on the street?\n(A) Sad\n(B) Lost\n(C) Excited\n(D) Depressed\n", "candidates": ["Sad", "Lost", "Excited", "Depressed"], "answer": "C", "task_type": "plotQA", "question_id": "Q1353"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0c1c184d9afb6000fc0d20a2b74a2482d2a4dfa10c577c866b747af822e22a9f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What was the mood of the person in camouflage clothing while talking to a lady on the street?\n(A) Sad\n(B) Lost\n(C) Excited\n(D) Depressed\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What was the mood of the person in camouflage clothing while talking to a lady on the street?\n(A) Sad\n(B) Lost\n(C) Excited\n(D) Depressed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1357, "doc": {"video_name": "movie101_34.mp4", "duration": 748.0, "question": "What color is the girl's hair in the video?\n(A) Red\n(B) Blue\n(C) White\n(D) Black\n", "candidates": ["Red", "Blue", "White", "Black"], "answer": "B", "task_type": "plotQA", "question_id": "Q1357"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "46f35fe022f4d863acb1422ca4860b390b9ed70c870cf3eca10044fcc49f5828", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the girl's hair in the video?\n(A) Red\n(B) Blue\n(C) White\n(D) Black\n", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "What color is the girl's hair in the video?\n(A) Red\n(B) Blue\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1361, "doc": {"video_name": "movie101_17.mp4", "duration": 613.0, "question": "What color is the hat of the person interacting with the man in white in the video?\n(A) Black\n(B) Blue\n(C) Red\n(D) Green\n", "candidates": ["Black", "Blue", "Red", "Green"], "answer": "A", "task_type": "plotQA", "question_id": "Q1361"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7b744dd80414b7ab4d40cacefd9b15c236c83b1be6586e2bbf3b60b887319d61", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the hat of the person interacting with the man in white in the video?\n(A) Black\n(B) Blue\n(C) Red\n(D) Green\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the hat of the person interacting with the man in white in the video?\n(A) Black\n(B) Blue\n(C) Red\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1365, "doc": {"video_name": "movie101_14.mp4", "duration": 465.0, "question": "What color is the long skirt worn by the girl in the video?\n(A) Green\n(B) White\n(C) Blue\n(D) Red\n", "candidates": ["Green", "White", "Blue", "Red"], "answer": "B", "task_type": "plotQA", "question_id": "Q1365"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "78495d105aae54821586305d30f78ea2ba26bf04ac3f66605329321c84eedf40", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the long skirt worn by the girl in the video?\n(A) Green\n(B) White\n(C) Blue\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the long skirt worn by the girl in the video?\n(A) Green\n(B) White\n(C) Blue\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1369, "doc": {"video_name": "movie101_37.mp4", "duration": 457.0, "question": "What color is the car parked on the road in the video?\n(A) Orange\n(B) Blue\n(C) Green\n(D) Purple\n", "candidates": ["Orange", "Blue", "Green", "Purple"], "answer": "A", "task_type": "plotQA", "question_id": "Q1369"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "adf6227564bd83052c4f9f08a5233798cb0dc0c21172ad68a48a63f6054692d5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the car parked on the road in the video?\n(A) Orange\n(B) Blue\n(C) Green\n(D) Purple\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the car parked on the road in the video?\n(A) Orange\n(B) Blue\n(C) Green\n(D) Purple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1373, "doc": {"video_name": "9.mp4", "duration": 471.67, "question": "Why did the person in light-colored clothes release the person in dark-colored clothes at the end of the video?\n(A) The person in dark-colored clothes defeated the person in light-colored clothes\n(B) The person in dark-colored clothes sneaked away when the person in light-colored clothes was not paying attention\n(C) The person in light-colored clothes was blocked by armed personnel in a yard\n(D) The person in light-colored clothes was suddenly moved\n", "candidates": ["The person in dark-colored clothes defeated the person in light-colored clothes", "The person in dark-colored clothes sneaked away when the person in light-colored clothes was not paying attention", "The person in light-colored clothes was blocked by armed personnel in a yard", "The person in light-colored clothes was suddenly moved"], "answer": "C", "task_type": "plotQA", "question_id": "Q1373"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0497807c0894db4f520823e25382a405948df445aceacf5eeb7fdf24e2713de9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why did the person in light-colored clothes release the person in dark-colored clothes at the end of the video?\n(A) The person in dark-colored clothes defeated the person in light-colored clothes\n(B) The person in dark-colored clothes sneaked away when the person in light-colored clothes was not paying attention\n(C) The person in light-colored clothes was blocked by armed personnel in a yard\n(D) The person in light-colored clothes was suddenly moved\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Why did the person in light-colored clothes release the person in dark-colored clothes at the end of the video?\n(A) The person in dark-colored clothes defeated the person in light-colored clothes\n(B) The person in dark-colored clothes sneaked away when the person in light-colored clothes was not paying attention\n(C) The person in light-colored clothes was blocked by armed personnel in a yard\n(D) The person in light-colored clothes was suddenly moved\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1377, "doc": {"video_name": "movie101_4.mp4", "duration": 560.0, "question": "What happened when a person in black clothing and a person wearing glasses were driving a race car?\n(A) Earthquake\n(B) Fire\n(C) Explosion\n(D) Had a car accident\n", "candidates": ["Earthquake", "Fire", "Explosion", "Had a car accident"], "answer": "D", "task_type": "plotQA", "question_id": "Q1377"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d8332f69e0e11e51f44d56ed84932ec0464ac1b33cd4320ed68ed4d19ecc331", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What happened when a person in black clothing and a person wearing glasses were driving a race car?\n(A) Earthquake\n(B) Fire\n(C) Explosion\n(D) Had a car accident\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What happened when a person in black clothing and a person wearing glasses were driving a race car?\n(A) Earthquake\n(B) Fire\n(C) Explosion\n(D) Had a car accident\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1381, "doc": {"video_name": "movie101_33.mp4", "duration": 240.0, "question": "What color is the boat on the sea in the movie?\n(A) Black\n(B) White\n(C) Blue\n(D) Green\n", "candidates": ["Black", "White", "Blue", "Green"], "answer": "B", "task_type": "plotQA", "question_id": "Q1381"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d2800852afbd428be72ab1a4789142c04841d4cb3e798450d654520f6ae8f31f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the boat on the sea in the movie?\n(A) Black\n(B) White\n(C) Blue\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What color is the boat on the sea in the movie?\n(A) Black\n(B) White\n(C) Blue\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1385, "doc": {"video_name": "movie101_41.mp4", "duration": 319.0, "question": "What color is the pottery on the ground in the video?\n(A) Black\n(B) Red\n(C) Blue\n(D) Green\n", "candidates": ["Black", "Red", "Blue", "Green"], "answer": "B", "task_type": "plotQA", "question_id": "Q1385"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f897d8bccf29b0132c177487f5c03de18418d041941f1380a4abe1b6fa27b60d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the pottery on the ground in the video?\n(A) Black\n(B) Red\n(C) Blue\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the pottery on the ground in the video?\n(A) Black\n(B) Red\n(C) Blue\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1389, "doc": {"video_name": "movie101_33.mp4", "duration": 240.0, "question": "How many of the five people sitting on the ground in the movie are wearing sunglasses?\n(A) Three\n(B) One\n(C) Four\n(D) Two\n", "candidates": ["Three", "One", "Four", "Two"], "answer": "B", "task_type": "plotQA", "question_id": "Q1389"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bfc3999fc07c9405a084c120a27cefd3b7d293f39607d427faa9b15e8618a6b5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many of the five people sitting on the ground in the movie are wearing sunglasses?\n(A) Three\n(B) One\n(C) Four\n(D) Two\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "How many of the five people sitting on the ground in the movie are wearing sunglasses?\n(A) Three\n(B) One\n(C) Four\n(D) Two\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1393, "doc": {"video_name": "movie101_38.mp4", "duration": 539.0, "question": "What color is the taxi the policewoman gets on in the video?\n(A) Red\n(B) Blue\n(C) Black\n(D) Green\n", "candidates": ["Red", "Blue", "Black", "Green"], "answer": "A", "task_type": "plotQA", "question_id": "Q1393"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6ff8918e7a5d9dd60b3a38ac15fdc0901abe0744bdb769851a25502bda23e63f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the taxi the policewoman gets on in the video?\n(A) Red\n(B) Blue\n(C) Black\n(D) Green\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the taxi the policewoman gets on in the video?\n(A) Red\n(B) Blue\n(C) Black\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1397, "doc": {"video_name": "movie101_59.mp4", "duration": 458.0, "question": "Why are the two men in the video looking for a cabinet shop?\n(A) To get change\n(B) To ask a question\n(C) Robbery\n(D) To buy something\n", "candidates": ["To get change", "To ask a question", "Robbery", "To buy something"], "answer": "C", "task_type": "plotQA", "question_id": "Q1397"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e4095e39ddcaceddcca1d9aea06c35a064017fe1af94d9398ffc2385fd9c816e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why are the two men in the video looking for a cabinet shop?\n(A) To get change\n(B) To ask a question\n(C) Robbery\n(D) To buy something\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Why are the two men in the video looking for a cabinet shop?\n(A) To get change\n(B) To ask a question\n(C) Robbery\n(D) To buy something\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1401, "doc": {"video_name": "movie101_20.mp4", "duration": 476.0, "question": "What is the target that the two people are practicing gun shooting at in the video?\n(A) Pear\n(B) Sweet Potato\n(C) Apple\n(D) Persimmon\n", "candidates": ["Pear", "Sweet Potato", "Apple", "Persimmon"], "answer": "D", "task_type": "plotQA", "question_id": "Q1401"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5e56ebb464b1ce88cc924c7bd9811f44fceda27747b8011a726d00f11f16978f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the target that the two people are practicing gun shooting at in the video?\n(A) Pear\n(B) Sweet Potato\n(C) Apple\n(D) Persimmon\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What is the target that the two people are practicing gun shooting at in the video?\n(A) Pear\n(B) Sweet Potato\n(C) Apple\n(D) Persimmon\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1405, "doc": {"video_name": "en_tv_38.mp4", "duration": 410.0, "question": "What color of clothes is the elderly person on the sickbed wearing in the video?\n(A) Black\n(B) Red\n(C) Blue\n(D) Yellow\n", "candidates": ["Black", "Red", "Blue", "Yellow"], "answer": "C", "task_type": "plotQA", "question_id": "Q1405"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "17ffff22cb86ca6f5bbbc9d11ea7b179471d78b70bf938d23304fe1ff9cb2e46", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color of clothes is the elderly person on the sickbed wearing in the video?\n(A) Black\n(B) Red\n(C) Blue\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What color of clothes is the elderly person on the sickbed wearing in the video?\n(A) Black\n(B) Red\n(C) Blue\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1409, "doc": {"video_name": "en_tv_21.mp4", "duration": 538.0, "question": "In the scene where two people are chatting on the street, what color are the numbers on the wall?\n(A) Green\n(B) White\n(C) Orange\n(D) Blue\n", "candidates": ["Green", "White", "Orange", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1409"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3abde53bf1ad4605d79ded7b48591939f26030b855798ac17036e2c9c2074c9f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the scene where two people are chatting on the street, what color are the numbers on the wall?\n(A) Green\n(B) White\n(C) Orange\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "In the scene where two people are chatting on the street, what color are the numbers on the wall?\n(A) Green\n(B) White\n(C) Orange\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1413, "doc": {"video_name": "movie101_41.mp4", "duration": 319.0, "question": "What is floating on the river's surface?\n(A) Leaves\n(B) Ducks\n(C) Feathers\n(D) Animal Skin\n", "candidates": ["Leaves", "Ducks", "Feathers", "Animal Skin"], "answer": "D", "task_type": "plotQA", "question_id": "Q1413"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a74d089790beb5014bc43729f4c2843c55abacee189bddb142ef86e6ec695b8b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is floating on the river's surface?\n(A) Leaves\n(B) Ducks\n(C) Feathers\n(D) Animal Skin\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What is floating on the river's surface?\n(A) Leaves\n(B) Ducks\n(C) Feathers\n(D) Animal Skin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1417, "doc": {"video_name": "movie101_7.mp4", "duration": 279.0, "question": "What was the weather during the outdoor car accident scene?\n(A) Sunny\n(B) Foggy\n(C) Snowy\n(D) Rainy\n", "candidates": ["Sunny", "Foggy", "Snowy", "Rainy"], "answer": "A", "task_type": "plotQA", "question_id": "Q1417"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bb504e38e37039cfdf62eca4855e0f193ab1e9549fbca3282a1c6b3dae80f41e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What was the weather during the outdoor car accident scene?\n(A) Sunny\n(B) Foggy\n(C) Snowy\n(D) Rainy\n", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "What was the weather during the outdoor car accident scene?\n(A) Sunny\n(B) Foggy\n(C) Snowy\n(D) Rainy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1421, "doc": {"video_name": "movie101_19.mp4", "duration": 477.0, "question": "What color is the car in the video?\n(A) White\n(B) Blue\n(C) Red\n(D) Black\n", "candidates": ["White", "Blue", "Red", "Black"], "answer": "D", "task_type": "plotQA", "question_id": "Q1421"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a8c7822f037729ebdc0599782a2a751a842f8c021e23a188abd30fbc7bff6acc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the car in the video?\n(A) White\n(B) Blue\n(C) Red\n(D) Black\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What color is the car in the video?\n(A) White\n(B) Blue\n(C) Red\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1425, "doc": {"video_name": "xiaoliyu_8.mp4", "duration": 347.0, "question": "What is put on the head of the fish with fins like butterfly wings?\n(A) Hat\n(B) Ribbon\n(C) Crown\n(D) Headscarf\n", "candidates": ["Hat", "Ribbon", "Crown", "Headscarf"], "answer": "C", "task_type": "plotQA", "question_id": "Q1425"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1c65a5c487996ae52df56ce8e6ef90019ea75c39b03aa30e767eb0eea760d252", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is put on the head of the fish with fins like butterfly wings?\n(A) Hat\n(B) Ribbon\n(C) Crown\n(D) Headscarf\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What is put on the head of the fish with fins like butterfly wings?\n(A) Hat\n(B) Ribbon\n(C) Crown\n(D) Headscarf\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1429, "doc": {"video_name": "en_tv_29.mp4", "duration": 451.0, "question": "What piece of clothing appears for the old man after the scan?\n(A) Sweater\n(B) Skirt\n(C) Pants\n(D) T-shirt\n", "candidates": ["Sweater", "Skirt", "Pants", "T-shirt"], "answer": "B", "task_type": "plotQA", "question_id": "Q1429"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "890eaf66efa24d5deb5c39bd07a00ebc2f8216acdc1ff825ad2ec06f87f2c28c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What piece of clothing appears for the old man after the scan?\n(A) Sweater\n(B) Skirt\n(C) Pants\n(D) T-shirt\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What piece of clothing appears for the old man after the scan?\n(A) Sweater\n(B) Skirt\n(C) Pants\n(D) T-shirt\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1433, "doc": {"video_name": "movie101_103.mp4", "duration": 294.0, "question": "Why does the man need to close the bedroom window?\n(A) It starts to rain outside\n(B) Windy\n(C) Going to talk\n(D) Cold\n", "candidates": ["It starts to rain outside", "Windy", "Going to talk", "Cold"], "answer": "A", "task_type": "plotQA", "question_id": "Q1433"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "673b1e0e33c4c845d913009bd6701492ca05bf4dd1283ebe46f4d3e61825743c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why does the man need to close the bedroom window?\n(A) It starts to rain outside\n(B) Windy\n(C) Going to talk\n(D) Cold\n", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "Why does the man need to close the bedroom window?\n(A) It starts to rain outside\n(B) Windy\n(C) Going to talk\n(D) Cold\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1437, "doc": {"video_name": "en_tv_5.mp4", "duration": 491.0, "question": "What happened to the woman who was washed away by the water?\n(A) Bitten by a dog\n(B) Found and rescued by a man\n(C) Shot by a gun\n(D) Hit by an object\n", "candidates": ["Bitten by a dog", "Found and rescued by a man", "Shot by a gun", "Hit by an object"], "answer": "B", "task_type": "plotQA", "question_id": "Q1437"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c263402f8df97ec9797e452fdcbccd44812a7a202cbdf442f390954ba05387a3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What happened to the woman who was washed away by the water?\n(A) Bitten by a dog\n(B) Found and rescued by a man\n(C) Shot by a gun\n(D) Hit by an object\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What happened to the woman who was washed away by the water?\n(A) Bitten by a dog\n(B) Found and rescued by a man\n(C) Shot by a gun\n(D) Hit by an object\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1441, "doc": {"video_name": "xiaoliyu_5.mp4", "duration": 415.0, "question": "Why did the cartoon seahorse stop attacking?\n(A) Because the target has left its attack range\n(B) Because the cartoon snake used the cartoon turtle to block it\n(C) Because it has no strength\n(D) Because it doesn't want to continue hurting the cartoon snake\n", "candidates": ["Because the target has left its attack range", "Because the cartoon snake used the cartoon turtle to block it", "Because it has no strength", "Because it doesn't want to continue hurting the cartoon snake"], "answer": "B", "task_type": "plotQA", "question_id": "Q1441"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "41babc0acdb27e4af8a83b0140679d41961e141cb037ccab71df2833678005b5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why did the cartoon seahorse stop attacking?\n(A) Because the target has left its attack range\n(B) Because the cartoon snake used the cartoon turtle to block it\n(C) Because it has no strength\n(D) Because it doesn't want to continue hurting the cartoon snake\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Why did the cartoon seahorse stop attacking?\n(A) Because the target has left its attack range\n(B) Because the cartoon snake used the cartoon turtle to block it\n(C) Because it has no strength\n(D) Because it doesn't want to continue hurting the cartoon snake\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1445, "doc": {"video_name": "movie101_62.mp4", "duration": 601.0, "question": "What is the blonde woman doing when the bald man comes to find her?\n(A) Eating\n(B) Taking a bath\n(C) Sleeping\n(D) Doing makeup\n", "candidates": ["Eating", "Taking a bath", "Sleeping", "Doing makeup"], "answer": "B", "task_type": "plotQA", "question_id": "Q1445"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "33bef75fd065247a30685fe1ace4697db322f1e26bad42ecca0d94ab69b26ee3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the blonde woman doing when the bald man comes to find her?\n(A) Eating\n(B) Taking a bath\n(C) Sleeping\n(D) Doing makeup\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the blonde woman doing when the bald man comes to find her?\n(A) Eating\n(B) Taking a bath\n(C) Sleeping\n(D) Doing makeup\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1449, "doc": {"video_name": "movie101_22.mp4", "duration": 335.0, "question": "What color is the background of the mural in the video?\n(A) Yellow\n(B) Black\n(C) Gold\n(D) Green\n", "candidates": ["Yellow", "Black", "Gold", "Green"], "answer": "C", "task_type": "plotQA", "question_id": "Q1449"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a09d8395e97f6104ecb180c8be80ab79f6ed395ca2a44ef4453f648a7c087a89", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the background of the mural in the video?\n(A) Yellow\n(B) Black\n(C) Gold\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the background of the mural in the video?\n(A) Yellow\n(B) Black\n(C) Gold\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1453, "doc": {"video_name": "movie101_110.mp4", "duration": 423.0, "question": "When a man holding a black object and a woman are talking at the red door, what emotion does the man show?\n(A) Happy\n(B) Excited\n(C) Surprised\n(D) Neutral\n", "candidates": ["Happy", "Excited", "Surprised", "Neutral"], "answer": "B", "task_type": "plotQA", "question_id": "Q1453"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aea7f5a9d8ca037bbeae4e4f045c1f2b5083ef45f43397dc800fc047c4fc754c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "When a man holding a black object and a woman are talking at the red door, what emotion does the man show?\n(A) Happy\n(B) Excited\n(C) Surprised\n(D) Neutral\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "When a man holding a black object and a woman are talking at the red door, what emotion does the man show?\n(A) Happy\n(B) Excited\n(C) Surprised\n(D) Neutral\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1457, "doc": {"video_name": "en_tv_24.mp4", "duration": 482.0, "question": "What color is the taxi at the beginning of the clip?\n(A) Yellow\n(B) Blue\n(C) Black\n(D) Red\n", "candidates": ["Yellow", "Blue", "Black", "Red"], "answer": "A", "task_type": "plotQA", "question_id": "Q1457"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1251fa73b76ec0962e155dbfdcb2c372ce47bc051e054498fa5cdf3c16fb24d1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the taxi at the beginning of the clip?\n(A) Yellow\n(B) Blue\n(C) Black\n(D) Red\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the taxi at the beginning of the clip?\n(A) Yellow\n(B) Blue\n(C) Black\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1461, "doc": {"video_name": "movie101_106.mp4", "duration": 746.0, "question": "What happens when the woman is taking a bath in the video?\n(A) She falls\n(B) A man barges in\n(C) A fire breaks out\n(D) An earthquake occurs\n", "candidates": ["She falls", "A man barges in", "A fire breaks out", "An earthquake occurs"], "answer": "B", "task_type": "plotQA", "question_id": "Q1461"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7e26e7fd1030bdcc1e833dd4cc4f17be91a5fe18503696115c267f2b7edd32f0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What happens when the woman is taking a bath in the video?\n(A) She falls\n(B) A man barges in\n(C) A fire breaks out\n(D) An earthquake occurs\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What happens when the woman is taking a bath in the video?\n(A) She falls\n(B) A man barges in\n(C) A fire breaks out\n(D) An earthquake occurs\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1465, "doc": {"video_name": "movie101_16.mp4", "duration": 740.0, "question": "What color is the water bottle on the table in the video?\n(A) White\n(B) Yellow\n(C) Blue\n(D) Green\n", "candidates": ["White", "Yellow", "Blue", "Green"], "answer": "D", "task_type": "plotQA", "question_id": "Q1465"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e21e529227a1d6f8d75fc78f3be62831fd94803bd025a03289348d4023179b0c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the water bottle on the table in the video?\n(A) White\n(B) Yellow\n(C) Blue\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What color is the water bottle on the table in the video?\n(A) White\n(B) Yellow\n(C) Blue\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1469, "doc": {"video_name": "movie101_56.mp4", "duration": 338.0, "question": "What is the man's emotion at the beginning?\n(A) Angry\n(B) Excited\n(C) Sad\n(D) Neutral\n", "candidates": ["Angry", "Excited", "Sad", "Neutral"], "answer": "A", "task_type": "plotQA", "question_id": "Q1469"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3af0648f2dc9295324b3d09ca355de84ccc32010671175a963abfb2617f8a309", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the man's emotion at the beginning?\n(A) Angry\n(B) Excited\n(C) Sad\n(D) Neutral\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What is the man's emotion at the beginning?\n(A) Angry\n(B) Excited\n(C) Sad\n(D) Neutral\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1473, "doc": {"video_name": "movie101_84.mp4", "duration": 387.0, "question": "What is on the man's head at the beginning of the video?\n(A) Saw\n(B) Knife\n(C) Gun\n(D) Axe\n", "candidates": ["Saw", "Knife", "Gun", "Axe"], "answer": "D", "task_type": "plotQA", "question_id": "Q1473"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8b67a376ae6eefce8aca2be3e28cbbfbe48d66cd7d111bfd8a84de8a284d8a46", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is on the man's head at the beginning of the video?\n(A) Saw\n(B) Knife\n(C) Gun\n(D) Axe\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "What is on the man's head at the beginning of the video?\n(A) Saw\n(B) Knife\n(C) Gun\n(D) Axe\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1477, "doc": {"video_name": "en_tv_14.mp4", "duration": 459.0, "question": "What color is the man's shirt when two people are chatting in the room?\n(A) White\n(B) Black\n(C) Red\n(D) Yellow\n", "candidates": ["White", "Black", "Red", "Yellow"], "answer": "B", "task_type": "plotQA", "question_id": "Q1477"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05ecd498fea54f823db9461e80c693cc1b87f8b7ccf22269ec9053bd72bd92a3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the man's shirt when two people are chatting in the room?\n(A) White\n(B) Black\n(C) Red\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the man's shirt when two people are chatting in the room?\n(A) White\n(B) Black\n(C) Red\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1481, "doc": {"video_name": "movie101_66.mp4", "duration": 246.0, "question": "What color is the ball the man throws out at the beginning of the video?\n(A) White\n(B) Black\n(C) Red\n(D) Green\n", "candidates": ["White", "Black", "Red", "Green"], "answer": "D", "task_type": "plotQA", "question_id": "Q1481"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b8e6edabaf344900ab3358d3eb105d3d6a12b31c8cde57016119c9698fd88a77", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the ball the man throws out at the beginning of the video?\n(A) White\n(B) Black\n(C) Red\n(D) Green\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "What color is the ball the man throws out at the beginning of the video?\n(A) White\n(B) Black\n(C) Red\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1485, "doc": {"video_name": "movie101_102.mp4", "duration": 607.0, "question": "What is the final fate of the wounded?\n(A) Escaped\n(B) Recovered\n(C) Shot dead by the soldier\n(D) Died\n", "candidates": ["Escaped", "Recovered", "Shot dead by the soldier", "Died"], "answer": "C", "task_type": "plotQA", "question_id": "Q1485"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "86368b61c75b2e6f433d322e002058b3c4f1533d31fa75b52f8bcdcd66321bf0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the final fate of the wounded?\n(A) Escaped\n(B) Recovered\n(C) Shot dead by the soldier\n(D) Died\n", "task_type": "plotQA", "pred_answer": "D", "answer": "C"}, "input": "What is the final fate of the wounded?\n(A) Escaped\n(B) Recovered\n(C) Shot dead by the soldier\n(D) Died\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1489, "doc": {"video_name": "movie101_65.mp4", "duration": 611.0, "question": "What fell off the table at the end of the video?\n(A) Food\n(B) Knife\n(C) Painting\n(D) Wine glass\n", "candidates": ["Food", "Knife", "Painting", "Wine glass"], "answer": "C", "task_type": "plotQA", "question_id": "Q1489"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ace2d0330dd782848205b914fa7ba45cb51f3365dc8caf06b9d72335d8e2329", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What fell off the table at the end of the video?\n(A) Food\n(B) Knife\n(C) Painting\n(D) Wine glass\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What fell off the table at the end of the video?\n(A) Food\n(B) Knife\n(C) Painting\n(D) Wine glass\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1493, "doc": {"video_name": "en_tv_24.mp4", "duration": 482.0, "question": "What color is the top worn by the woman among the three people chatting at the end of the clip?\n(A) Green\n(B) Red\n(C) Blue\n(D) Black\n", "candidates": ["Green", "Red", "Blue", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1493"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dbec6b7dbfc0f7bc09b2b0731efd9315cc9791b878a6da61aa6e3d317e6d8755", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the top worn by the woman among the three people chatting at the end of the clip?\n(A) Green\n(B) Red\n(C) Blue\n(D) Black\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the top worn by the woman among the three people chatting at the end of the clip?\n(A) Green\n(B) Red\n(C) Blue\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1497, "doc": {"video_name": "movie101_12.mp4", "duration": 346.0, "question": "What color of hat does the person chatting with the seated man in the video wear?\n(A) White\n(B) Blue\n(C) Yellow\n(D) Black\n", "candidates": ["White", "Blue", "Yellow", "Black"], "answer": "D", "task_type": "plotQA", "question_id": "Q1497"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ddbdb5a8ccee92e4cc8cdafaee36faf44f0ec55aa2dc3cd7b7654e3b3be5b0f1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color of hat does the person chatting with the seated man in the video wear?\n(A) White\n(B) Blue\n(C) Yellow\n(D) Black\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What color of hat does the person chatting with the seated man in the video wear?\n(A) White\n(B) Blue\n(C) Yellow\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1501, "doc": {"video_name": "movie101_1.mp4", "duration": 214.0, "question": "At the beginning of the video, what behavior of the person in the black shirt is the person in the blue pants trying to prevent?\n(A) Lighting a cigarette\n(B) Bowing down\n(C) Raising hand\n(D) Squatting down\n", "candidates": ["Lighting a cigarette", "Bowing down", "Raising hand", "Squatting down"], "answer": "A", "task_type": "plotQA", "question_id": "Q1501"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f1043c91b863f66d62c26ce24fecde6a670d2b465886093755064eec2bc13ecf", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "At the beginning of the video, what behavior of the person in the black shirt is the person in the blue pants trying to prevent?\n(A) Lighting a cigarette\n(B) Bowing down\n(C) Raising hand\n(D) Squatting down\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "At the beginning of the video, what behavior of the person in the black shirt is the person in the blue pants trying to prevent?\n(A) Lighting a cigarette\n(B) Bowing down\n(C) Raising hand\n(D) Squatting down\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1505, "doc": {"video_name": "movie101_58.mp4", "duration": 605.0, "question": "What is the little boy doing at the beginning of the video?\n(A) Drawing\n(B) Writing\n(C) Playing piano\n(D) Reading\n", "candidates": ["Drawing", "Writing", "Playing piano", "Reading"], "answer": "A", "task_type": "plotQA", "question_id": "Q1505"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9781f038dadf9c853b5c980c09762be2733bbf53da1f189ced14f8d8a6157764", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the little boy doing at the beginning of the video?\n(A) Drawing\n(B) Writing\n(C) Playing piano\n(D) Reading\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What is the little boy doing at the beginning of the video?\n(A) Drawing\n(B) Writing\n(C) Playing piano\n(D) Reading\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1509, "doc": {"video_name": "en_tv_38.mp4", "duration": 410.0, "question": "What is the man in orange clothes holding in the video?\n(A) Flower\n(B) Stick\n(C) Computer\n(D) Water cup\n", "candidates": ["Flower", "Stick", "Computer", "Water cup"], "answer": "A", "task_type": "plotQA", "question_id": "Q1509"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fb7b3266a5f8fa37c0714787049f9a49be5da1bb6caf0a35c48d9387ddf073e0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the man in orange clothes holding in the video?\n(A) Flower\n(B) Stick\n(C) Computer\n(D) Water cup\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What is the man in orange clothes holding in the video?\n(A) Flower\n(B) Stick\n(C) Computer\n(D) Water cup\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1513, "doc": {"video_name": "movie101_14.mp4", "duration": 465.0, "question": "What color is the bag on the table in the video?\n(A) Red\n(B) White\n(C) Blue\n(D) Green\n", "candidates": ["Red", "White", "Blue", "Green"], "answer": "B", "task_type": "plotQA", "question_id": "Q1513"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e871e5364a4774ec9f72fefa2b890fa23ad2b52deb6dec45ebe14b154729b2ed", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the bag on the table in the video?\n(A) Red\n(B) White\n(C) Blue\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the bag on the table in the video?\n(A) Red\n(B) White\n(C) Blue\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1517, "doc": {"video_name": "en_tv_16.mp4", "duration": 552.0, "question": "What color is the door of the room where the person is locked?\n(A) Blue\n(B) Yellow\n(C) Green\n(D) White\n", "candidates": ["Blue", "Yellow", "Green", "White"], "answer": "C", "task_type": "plotQA", "question_id": "Q1517"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1fe552ef0e3702dbe7213b05fd3ac6e1117e02c86342c291eb70b52504a5995a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the door of the room where the person is locked?\n(A) Blue\n(B) Yellow\n(C) Green\n(D) White\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the door of the room where the person is locked?\n(A) Blue\n(B) Yellow\n(C) Green\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1521, "doc": {"video_name": "en_tv_7.mp4", "duration": 533.0, "question": "Who does the woman in the black leather jacket call after picking up the phone from the ground?\n(A) A woman in a white coat.\n(B) A young man.\n(C) A woman in a black coat.\n(D) An old man with pale hair.\n", "candidates": ["A woman in a white coat.", "A young man.", "A woman in a black coat.", "An old man with pale hair."], "answer": "A", "task_type": "plotQA", "question_id": "Q1521"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D."]], "filtered_resps": ["D."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2edf160462e5ba438293b2df3205a75da298f28cc2a7f5817df48da7140c235c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Who does the woman in the black leather jacket call after picking up the phone from the ground?\n(A) A woman in a white coat.\n(B) A young man.\n(C) A woman in a black coat.\n(D) An old man with pale hair.\n", "task_type": "plotQA", "pred_answer": "D.", "answer": "A"}, "input": "Who does the woman in the black leather jacket call after picking up the phone from the ground?\n(A) A woman in a white coat.\n(B) A young man.\n(C) A woman in a black coat.\n(D) An old man with pale hair.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1525, "doc": {"video_name": "movie101_74.mp4", "duration": 323.0, "question": "What is the mood of the man being hit in the video?\n(A) Sad\n(B) Joyful\n(C) Angry\n(D) Neutral\n", "candidates": ["Sad", "Joyful", "Angry", "Neutral"], "answer": "C", "task_type": "plotQA", "question_id": "Q1525"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4f9b835f0d785022e50c43e4de6c1370c054c7bb541aee7a8ea1891e02f49645", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the mood of the man being hit in the video?\n(A) Sad\n(B) Joyful\n(C) Angry\n(D) Neutral\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What is the mood of the man being hit in the video?\n(A) Sad\n(B) Joyful\n(C) Angry\n(D) Neutral\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1529, "doc": {"video_name": "haimian_7.mp4", "duration": 426.0, "question": "What does the Cartoon Sponge do with the Cartoon Shark?\n(A) Sleep\n(B) Dance\n(C) Sing\n(D) Play games\n", "candidates": ["Sleep", "Dance", "Sing", "Play games"], "answer": "B", "task_type": "plotQA", "question_id": "Q1529"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5c9df48e3d370706e0dda4bb3558781d940d192a78f58bc7e8e998b505270970", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What does the Cartoon Sponge do with the Cartoon Shark?\n(A) Sleep\n(B) Dance\n(C) Sing\n(D) Play games\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What does the Cartoon Sponge do with the Cartoon Shark?\n(A) Sleep\n(B) Dance\n(C) Sing\n(D) Play games\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1533, "doc": {"video_name": "movie101_73.mp4", "duration": 421.0, "question": "What color is the dress of the first woman to appear in the video?\n(A) White\n(B) Green\n(C) Black\n(D) Yellow\n", "candidates": ["White", "Green", "Black", "Yellow"], "answer": "A", "task_type": "plotQA", "question_id": "Q1533"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "48b7ec1d1f767a07cf56f8dd06420da06a5fd949e6a2f2e369fb2402ec77274c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the dress of the first woman to appear in the video?\n(A) White\n(B) Green\n(C) Black\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the dress of the first woman to appear in the video?\n(A) White\n(B) Green\n(C) Black\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1537, "doc": {"video_name": "movie101_70.mp4", "duration": 361.0, "question": "What color is the woman's dress in the video?\n(A) Green\n(B) White\n(C) Red\n(D) Black\n", "candidates": ["Green", "White", "Red", "Black"], "answer": "D", "task_type": "plotQA", "question_id": "Q1537"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85b0668e0935d0396e3b28d3eeb6a1b4a786a7771bce3104932b7948c28c4fa2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the woman's dress in the video?\n(A) Green\n(B) White\n(C) Red\n(D) Black\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What color is the woman's dress in the video?\n(A) Green\n(B) White\n(C) Red\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1541, "doc": {"video_name": "movie101_25.mp4", "duration": 737.0, "question": "What color is the neck brace worn by the driver in the movie?\n(A) Yellow\n(B) White\n(C) Green\n(D) Blue\n", "candidates": ["Yellow", "White", "Green", "Blue"], "answer": "D", "task_type": "plotQA", "question_id": "Q1541"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "729615e973132d1b94642202bf14ca62f89f611c2fb9d02487c0764645b4a874", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the neck brace worn by the driver in the movie?\n(A) Yellow\n(B) White\n(C) Green\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What color is the neck brace worn by the driver in the movie?\n(A) Yellow\n(B) White\n(C) Green\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1545, "doc": {"video_name": "movie101_31.mp4", "duration": 403.0, "question": "How many cars are there by the small river?\n(A) One\n(B) Six\n(C) Two\n(D) Four\n", "candidates": ["One", "Six", "Two", "Four"], "answer": "C", "task_type": "plotQA", "question_id": "Q1545"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f15a09c5fe2e45f5ae4df7dd3025ca7a1800c91ee04955b8412d8d52e1c53e97", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many cars are there by the small river?\n(A) One\n(B) Six\n(C) Two\n(D) Four\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "How many cars are there by the small river?\n(A) One\n(B) Six\n(C) Two\n(D) Four\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1549, "doc": {"video_name": "xiaoliyu_1.mp4", "duration": 381.0, "question": "What is the cartoon carp holding?\n(A) Coral\n(B) Starfish\n(C) Small fish\n(D) Seaweed\n", "candidates": ["Coral", "Starfish", "Small fish", "Seaweed"], "answer": "D", "task_type": "plotQA", "question_id": "Q1549"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1027313538dcd4b10157f0053afd1b2b6e464f1065549eadb417192b87d4e179", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the cartoon carp holding?\n(A) Coral\n(B) Starfish\n(C) Small fish\n(D) Seaweed\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What is the cartoon carp holding?\n(A) Coral\n(B) Starfish\n(C) Small fish\n(D) Seaweed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1553, "doc": {"video_name": "movie101_18.mp4", "duration": 318.0, "question": "Under what environmental conditions does the scene in the video take place?\n(A) Ocean\n(B) Desert\n(C) City\n(D) Forest\n", "candidates": ["Ocean", "Desert", "City", "Forest"], "answer": "D", "task_type": "plotQA", "question_id": "Q1553"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "434988b1b5bfbd5c3cd70f58ebe40407f55cf7faef822a00a8b83d3608737eb3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Under what environmental conditions does the scene in the video take place?\n(A) Ocean\n(B) Desert\n(C) City\n(D) Forest\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Under what environmental conditions does the scene in the video take place?\n(A) Ocean\n(B) Desert\n(C) City\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1557, "doc": {"video_name": "tomjerry_11.mp4", "duration": 322.0, "question": "Why does the cartoon cat exercise?\n(A) To defeat the cartoon mouse in yellow clothes\n(B) To defeat their kind\n(C) To attract the opposite sex\n(D) To attract the attention of their owner\n", "candidates": ["To defeat the cartoon mouse in yellow clothes", "To defeat their kind", "To attract the opposite sex", "To attract the attention of their owner"], "answer": "A", "task_type": "plotQA", "question_id": "Q1557"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1a424a2f7781ac8c28545c1e3322213b5838ccfdb23e954414bf9f3345c117bc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why does the cartoon cat exercise?\n(A) To defeat the cartoon mouse in yellow clothes\n(B) To defeat their kind\n(C) To attract the opposite sex\n(D) To attract the attention of their owner\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "Why does the cartoon cat exercise?\n(A) To defeat the cartoon mouse in yellow clothes\n(B) To defeat their kind\n(C) To attract the opposite sex\n(D) To attract the attention of their owner\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1561, "doc": {"video_name": "en_tv_27.mp4", "duration": 486.0, "question": "What is the woman's emotion at the beginning of the video?\n(A) Scared\n(B) Neutral\n(C) Sad\n(D) Joyful\n", "candidates": ["Scared", "Neutral", "Sad", "Joyful"], "answer": "C", "task_type": "plotQA", "question_id": "Q1561"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "307626a9cc9639135b5d35902f0a758e93abe067251d33c8086d202da9d57e43", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the woman's emotion at the beginning of the video?\n(A) Scared\n(B) Neutral\n(C) Sad\n(D) Joyful\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What is the woman's emotion at the beginning of the video?\n(A) Scared\n(B) Neutral\n(C) Sad\n(D) Joyful\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1565, "doc": {"video_name": "movie101_61.mp4", "duration": 627.0, "question": "What is the profession of the female protagonist in the video?\n(A) Photographer\n(B) Teacher\n(C) Doctor\n(D) Nanny\n", "candidates": ["Photographer", "Teacher", "Doctor", "Nanny"], "answer": "A", "task_type": "plotQA", "question_id": "Q1565"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2b6b72145dc17de6a693b0cf86b22ae7db6dd13cf223b40cd338afa318baf8a9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the profession of the female protagonist in the video?\n(A) Photographer\n(B) Teacher\n(C) Doctor\n(D) Nanny\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What is the profession of the female protagonist in the video?\n(A) Photographer\n(B) Teacher\n(C) Doctor\n(D) Nanny\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1569, "doc": {"video_name": "movie101_12.mp4", "duration": 346.0, "question": "What color of skirt does the woman who is doing the makeup in the video wear?\n(A) Pink\n(B) White\n(C) Blue\n(D) Yellow\n", "candidates": ["Pink", "White", "Blue", "Yellow"], "answer": "A", "task_type": "plotQA", "question_id": "Q1569"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b30a4640762aad07c7aa801623debbe64827a2d1aae94ecfd889c698f63f8ece", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color of skirt does the woman who is doing the makeup in the video wear?\n(A) Pink\n(B) White\n(C) Blue\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What color of skirt does the woman who is doing the makeup in the video wear?\n(A) Pink\n(B) White\n(C) Blue\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1573, "doc": {"video_name": "movie101_59.mp4", "duration": 458.0, "question": "Why does one of the men close the door after entering the cabinet shop in the video?\n(A) He wants to inflict violence on the people in the cabinet shop\n(B) He noticed a group of people chasing him\n(C) He found a dangerous beast\n(D) He realized there was a sudden fire in the cabinet shop\n", "candidates": ["He wants to inflict violence on the people in the cabinet shop", "He noticed a group of people chasing him", "He found a dangerous beast", "He realized there was a sudden fire in the cabinet shop"], "answer": "A", "task_type": "plotQA", "question_id": "Q1573"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d6605b36142e0347e2caea50adcd55207eb4c4b603a1c9568ca2cdbb8fcfdf82", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why does one of the men close the door after entering the cabinet shop in the video?\n(A) He wants to inflict violence on the people in the cabinet shop\n(B) He noticed a group of people chasing him\n(C) He found a dangerous beast\n(D) He realized there was a sudden fire in the cabinet shop\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "Why does one of the men close the door after entering the cabinet shop in the video?\n(A) He wants to inflict violence on the people in the cabinet shop\n(B) He noticed a group of people chasing him\n(C) He found a dangerous beast\n(D) He realized there was a sudden fire in the cabinet shop\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1577, "doc": {"video_name": "movie101_9.mp4", "duration": 713.0, "question": "What does the man use the fire for in the video?\n(A) Lighting\n(B) Barbecue\n(C) Making a bonfire\n(D) Building a wall\n", "candidates": ["Lighting", "Barbecue", "Making a bonfire", "Building a wall"], "answer": "D", "task_type": "plotQA", "question_id": "Q1577"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3e23a9b018f91a8dcc2542095bef31845c8589678752536df858986ac09b06b9", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What does the man use the fire for in the video?\n(A) Lighting\n(B) Barbecue\n(C) Making a bonfire\n(D) Building a wall\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What does the man use the fire for in the video?\n(A) Lighting\n(B) Barbecue\n(C) Making a bonfire\n(D) Building a wall\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1581, "doc": {"video_name": "en_tv_13.mp4", "duration": 585.0, "question": "Where does the family of three finally stop after speeding on the road in their car?\n(A) Park\n(B) Parking lot\n(C) Courtyard\n(D) Square\n", "candidates": ["Park", "Parking lot", "Courtyard", "Square"], "answer": "B", "task_type": "plotQA", "question_id": "Q1581"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0682cf0e1b6e13d31cdef02e4318c72c3de19394b02e17974e9718f9b6b3f0e6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where does the family of three finally stop after speeding on the road in their car?\n(A) Park\n(B) Parking lot\n(C) Courtyard\n(D) Square\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Where does the family of three finally stop after speeding on the road in their car?\n(A) Park\n(B) Parking lot\n(C) Courtyard\n(D) Square\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1585, "doc": {"video_name": "tomjerry_1.mp4", "duration": 311.0, "question": "Where does the grey cartoon cat hide the cartoon mouse?\n(A) On the sofa\n(B) Under the grey cartoon cat's bottom\n(C) On the table\n(D) On the refrigerator\n", "candidates": ["On the sofa", "Under the grey cartoon cat's bottom", "On the table", "On the refrigerator"], "answer": "B", "task_type": "plotQA", "question_id": "Q1585"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "932158e1138bdb44cb4369a4ae65234d61aedabe82a877a4257a806fa53c1e07", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where does the grey cartoon cat hide the cartoon mouse?\n(A) On the sofa\n(B) Under the grey cartoon cat's bottom\n(C) On the table\n(D) On the refrigerator\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Where does the grey cartoon cat hide the cartoon mouse?\n(A) On the sofa\n(B) Under the grey cartoon cat's bottom\n(C) On the table\n(D) On the refrigerator\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1589, "doc": {"video_name": "movie101_29.mp4", "duration": 482.0, "question": "How many men are being treated for injuries in the video?\n(A) Three\n(B) One\n(C) Four\n(D) Two\n", "candidates": ["Three", "One", "Four", "Two"], "answer": "D", "task_type": "plotQA", "question_id": "Q1589"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "efb2f53f51ae69e453c6c90d05010986651ab5cf5511093a0af1f3b03514f668", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many men are being treated for injuries in the video?\n(A) Three\n(B) One\n(C) Four\n(D) Two\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "How many men are being treated for injuries in the video?\n(A) Three\n(B) One\n(C) Four\n(D) Two\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1593, "doc": {"video_name": "movie101_11.mp4", "duration": 206.0, "question": "What color is the coat of the leading adult in the video?\n(A) White\n(B) Red\n(C) Green\n(D) Blue\n", "candidates": ["White", "Red", "Green", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1593"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a2657cb94a117e8d22fb1bbe24d0167a39a5ceb91aa9f8f8a25b1245a3ad6b4a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the coat of the leading adult in the video?\n(A) White\n(B) Red\n(C) Green\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the coat of the leading adult in the video?\n(A) White\n(B) Red\n(C) Green\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1597, "doc": {"video_name": "movie101_9.mp4", "duration": 713.0, "question": "What color is the child's hair in the video?\n(A) Yellow\n(B) Red\n(C) White\n(D) Black\n", "candidates": ["Yellow", "Red", "White", "Black"], "answer": "D", "task_type": "plotQA", "question_id": "Q1597"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3090b946dc959c480733517f4eb3781ffa5592488a46985f5edc59693b60dfb6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the child's hair in the video?\n(A) Yellow\n(B) Red\n(C) White\n(D) Black\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What color is the child's hair in the video?\n(A) Yellow\n(B) Red\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1601, "doc": {"video_name": "movie101_10.mp4", "duration": 716.0, "question": "What color is the hat worn by the character in the video?\n(A) White\n(B) Green\n(C) Red\n(D) Black\n", "candidates": ["White", "Green", "Red", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1601"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3851f7df431f932e0e7c3ebd18d4c36956a1e833b11ea0dd44aee6689f290433", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the hat worn by the character in the video?\n(A) White\n(B) Green\n(C) Red\n(D) Black\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the hat worn by the character in the video?\n(A) White\n(B) Green\n(C) Red\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1605, "doc": {"video_name": "movie101_35.mp4", "duration": 481.0, "question": "What color is the car that is detonated in the video?\n(A) Purple\n(B) Green\n(C) Blue\n(D) Silver\n", "candidates": ["Purple", "Green", "Blue", "Silver"], "answer": "D", "task_type": "plotQA", "question_id": "Q1605"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9bae7125d426008f06835001bc00dec46ebb398f07eb554188054c92f9d52a15", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the car that is detonated in the video?\n(A) Purple\n(B) Green\n(C) Blue\n(D) Silver\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the car that is detonated in the video?\n(A) Purple\n(B) Green\n(C) Blue\n(D) Silver\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1609, "doc": {"video_name": "movie101_60.mp4", "duration": 402.0, "question": "What color is the car the woman used to pick up the child?\n(A) White\n(B) Black\n(C) Blue\n(D) Red\n", "candidates": ["White", "Black", "Blue", "Red"], "answer": "A", "task_type": "plotQA", "question_id": "Q1609"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "db30469d813eaa67022e56cfde188f03e95e7085c2b243c891576f37da2af908", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the car the woman used to pick up the child?\n(A) White\n(B) Black\n(C) Blue\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What color is the car the woman used to pick up the child?\n(A) White\n(B) Black\n(C) Blue\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1613, "doc": {"video_name": "en_tv_15.mp4", "duration": 541.0, "question": "What color is the chair the man sits on in the scene where two men are chatting in the office?\n(A) White\n(B) Green\n(C) Purple\n(D) Black\n", "candidates": ["White", "Green", "Purple", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1613"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d857454d3018742d87a27336a16b062544c9af46116753194262195cca994f4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the chair the man sits on in the scene where two men are chatting in the office?\n(A) White\n(B) Green\n(C) Purple\n(D) Black\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the chair the man sits on in the scene where two men are chatting in the office?\n(A) White\n(B) Green\n(C) Purple\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1617, "doc": {"video_name": "en_tv_15.mp4", "duration": 541.0, "question": "What color is the train that passes through the city?\n(A) Blue\n(B) Silver\n(C) Green\n(D) Red\n", "candidates": ["Blue", "Silver", "Green", "Red"], "answer": "B", "task_type": "plotQA", "question_id": "Q1617"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e388b320edb5b61a061b9b9c108db011098ce0b6cf0ca0a776df61748f2a9171", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the train that passes through the city?\n(A) Blue\n(B) Silver\n(C) Green\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the train that passes through the city?\n(A) Blue\n(B) Silver\n(C) Green\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1621, "doc": {"video_name": "en_tv_15.mp4", "duration": 541.0, "question": "What color is the woman's clothes in the chat at the beginning of the video?\n(A) Black\n(B) Red\n(C) Green\n(D) White\n", "candidates": ["Black", "Red", "Green", "White"], "answer": "B", "task_type": "plotQA", "question_id": "Q1621"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4c8dfdcdc6e7a66d855ee27a5916df6add5f8dab7e5b9ba374dff5045947908d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the woman's clothes in the chat at the beginning of the video?\n(A) Black\n(B) Red\n(C) Green\n(D) White\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the woman's clothes in the chat at the beginning of the video?\n(A) Black\n(B) Red\n(C) Green\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1625, "doc": {"video_name": "movie101_80.mp4", "duration": 467.0, "question": "What is the mood of the man who surrenders and then shoots someone at the end of the video?\n(A) Joyful\n(B) Disappointed\n(C) Angry\n(D) Neutral\n", "candidates": ["Joyful", "Disappointed", "Angry", "Neutral"], "answer": "C", "task_type": "plotQA", "question_id": "Q1625"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f9ec1c7410f916b1a8b0a9ccc8333d7f1d87fbcc4b637a8e516243cb909216e7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the mood of the man who surrenders and then shoots someone at the end of the video?\n(A) Joyful\n(B) Disappointed\n(C) Angry\n(D) Neutral\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What is the mood of the man who surrenders and then shoots someone at the end of the video?\n(A) Joyful\n(B) Disappointed\n(C) Angry\n(D) Neutral\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1629, "doc": {"video_name": "en_tv_14.mp4", "duration": 459.0, "question": "What is the expression of the girl when she leaves the prison?\n(A) Joyful\n(B) Neutral\n(C) Angry\n(D) Sad\n", "candidates": ["Joyful", "Neutral", "Angry", "Sad"], "answer": "C", "task_type": "plotQA", "question_id": "Q1629"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "482e6d724a45e5068d0f3d796ca83e5f8b68923c81416a6a156c9c8142cc4bae", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the expression of the girl when she leaves the prison?\n(A) Joyful\n(B) Neutral\n(C) Angry\n(D) Sad\n", "task_type": "plotQA", "pred_answer": "D", "answer": "C"}, "input": "What is the expression of the girl when she leaves the prison?\n(A) Joyful\n(B) Neutral\n(C) Angry\n(D) Sad\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1633, "doc": {"video_name": "xiaoliyu_6.mp4", "duration": 400.0, "question": "What stopped the cartoon octopus from attacking?\n(A) The green light emitted by the cartoon carp\n(B) The golden light emitted by the cartoon carp\n(C) The blue light emitted by the cartoon carp\n(D) The red light emitted by the cartoon carp\n", "candidates": ["The green light emitted by the cartoon carp", "The golden light emitted by the cartoon carp", "The blue light emitted by the cartoon carp", "The red light emitted by the cartoon carp"], "answer": "B", "task_type": "plotQA", "question_id": "Q1633"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "844aacb270a0f96bd661bb9530374d3e3284f0edda96fa2f3f0664313b7c8fd3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What stopped the cartoon octopus from attacking?\n(A) The green light emitted by the cartoon carp\n(B) The golden light emitted by the cartoon carp\n(C) The blue light emitted by the cartoon carp\n(D) The red light emitted by the cartoon carp\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What stopped the cartoon octopus from attacking?\n(A) The green light emitted by the cartoon carp\n(B) The golden light emitted by the cartoon carp\n(C) The blue light emitted by the cartoon carp\n(D) The red light emitted by the cartoon carp\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1637, "doc": {"video_name": "movie101_68.mp4", "duration": 445.0, "question": "What animal approached the woman at the end of the video?\n(A) Cat\n(B) Horse\n(C) Dog\n(D) Cow\n", "candidates": ["Cat", "Horse", "Dog", "Cow"], "answer": "C", "task_type": "plotQA", "question_id": "Q1637"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "19b1713d23aa97119d338cf6766756af34a31b06f11c71291e8e06e7d836740b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What animal approached the woman at the end of the video?\n(A) Cat\n(B) Horse\n(C) Dog\n(D) Cow\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What animal approached the woman at the end of the video?\n(A) Cat\n(B) Horse\n(C) Dog\n(D) Cow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1641, "doc": {"video_name": "movie101_56.mp4", "duration": 338.0, "question": "What color is the man's clothes when he arrives at the woman's house?\n(A) Red\n(B) Yellow\n(C) White\n(D) Black\n", "candidates": ["Red", "Yellow", "White", "Black"], "answer": "D", "task_type": "plotQA", "question_id": "Q1641"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b7f2e24108d9f2055e4277bf7b80f5623c6979585c0977224f18f254c9ead923", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the man's clothes when he arrives at the woman's house?\n(A) Red\n(B) Yellow\n(C) White\n(D) Black\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the man's clothes when he arrives at the woman's house?\n(A) Red\n(B) Yellow\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1645, "doc": {"video_name": "movie101_40.mp4", "duration": 450.0, "question": "What color is the clothing of the person filling the water bottle inside the Buddha statue?\n(A) Blue\n(B) Black\n(C) Green\n(D) Red\n", "candidates": ["Blue", "Black", "Green", "Red"], "answer": "A", "task_type": "plotQA", "question_id": "Q1645"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "052731d6f6b7a99721bfab04c2563f8ecd34aae75d1193aaac7334f3df7094a2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the clothing of the person filling the water bottle inside the Buddha statue?\n(A) Blue\n(B) Black\n(C) Green\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What color is the clothing of the person filling the water bottle inside the Buddha statue?\n(A) Blue\n(B) Black\n(C) Green\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1649, "doc": {"video_name": "haimian_9.mp4", "duration": 444.0, "question": "Where did the cartoon jellyfish attach itself on the cartoon sponge?\n(A) Thigh\n(B) Back of the head\n(C) Top of the head\n(D) Arm\n", "candidates": ["Thigh", "Back of the head", "Top of the head", "Arm"], "answer": "B", "task_type": "plotQA", "question_id": "Q1649"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8478d53bebebf679c49d85a7468c28fc17ec1a59bed250234221d6dccfd2452f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where did the cartoon jellyfish attach itself on the cartoon sponge?\n(A) Thigh\n(B) Back of the head\n(C) Top of the head\n(D) Arm\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "Where did the cartoon jellyfish attach itself on the cartoon sponge?\n(A) Thigh\n(B) Back of the head\n(C) Top of the head\n(D) Arm\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1653, "doc": {"video_name": "movie101_10.mp4", "duration": 716.0, "question": "What color is the scarf worn by the woman in the video?\n(A) Red\n(B) Blue\n(C) White\n(D) Pink\n", "candidates": ["Red", "Blue", "White", "Pink"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1653"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "35d0e0d829fb99c9914d92344bdaf51a9734902c395c0846dbd7ed5d8394f767", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the scarf worn by the woman in the video?\n(A) Red\n(B) Blue\n(C) White\n(D) Pink\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "B"}, "input": "What color is the scarf worn by the woman in the video?\n(A) Red\n(B) Blue\n(C) White\n(D) Pink\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1657, "doc": {"video_name": "ego_13.mp4", "duration": 480.02, "question": "What is the first-person character doing in this video?\n(A) Making coffee\n(B) Making milk\n(C) Making a cake\n(D) Baking cookies\n", "candidates": ["Making coffee", "Making milk", "Making a cake", "Baking cookies"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1657"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1acb89f27f1a52164d7d851911df3d10e8f0d3765db3c3f59e2b0e2ceeb61b2c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the first-person character doing in this video?\n(A) Making coffee\n(B) Making milk\n(C) Making a cake\n(D) Baking cookies\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the first-person character doing in this video?\n(A) Making coffee\n(B) Making milk\n(C) Making a cake\n(D) Baking cookies\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1661, "doc": {"video_name": "movie101_106.mp4", "duration": 746.0, "question": "What is the main setting of the video?\n(A) Desert\n(B) Ocean\n(C) City\n(D) Palace\n", "candidates": ["Desert", "Ocean", "City", "Palace"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1661"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d113c78a138d723e411cd0bdafba9d1dbb4a278a247f44fbb180643785ea463", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the main setting of the video?\n(A) Desert\n(B) Ocean\n(C) City\n(D) Palace\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the main setting of the video?\n(A) Desert\n(B) Ocean\n(C) City\n(D) Palace\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1665, "doc": {"video_name": "ego_17.mp4", "duration": 480.03, "question": "What is the weather like in the video?\n(A) Blizzard\n(B) Overcast\n(C) Sunny\n(D) Hail\n", "candidates": ["Blizzard", "Overcast", "Sunny", "Hail"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1665"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ffb3c75ee9f32cb0276c7e6195a24054e49c65ba8f919c8cbb742034a190565b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the weather like in the video?\n(A) Blizzard\n(B) Overcast\n(C) Sunny\n(D) Hail\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "B"}, "input": "What is the weather like in the video?\n(A) Blizzard\n(B) Overcast\n(C) Sunny\n(D) Hail\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1669, "doc": {"video_name": "en_tv_27.mp4", "duration": 486.0, "question": "What type of film is this?\n(A) Comedy\n(B) Romance\n(C) History\n(D) Action\n", "candidates": ["Comedy", "Romance", "History", "Action"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1669"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "360fc90dfab4dc6cad21965f70c3aaf5bcfa79ef4ff3c36a9276ee01a5d3e72f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Comedy\n(B) Romance\n(C) History\n(D) Action\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "D"}, "input": "What type of film is this?\n(A) Comedy\n(B) Romance\n(C) History\n(D) Action\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1673, "doc": {"video_name": "AWC-3.mp4", "duration": 450.0, "question": "What is the most frequent scene in the video?\n(A) Cliff\n(B) Forest\n(C) Desert\n(D) Ocean\n", "candidates": ["Cliff", "Forest", "Desert", "Ocean"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1673"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b47ddfeb662ab4ee612fa10bc5a988b1f87920d63e84ab57a99d66e83b049577", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the most frequent scene in the video?\n(A) Cliff\n(B) Forest\n(C) Desert\n(D) Ocean\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the most frequent scene in the video?\n(A) Cliff\n(B) Forest\n(C) Desert\n(D) Ocean\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1677, "doc": {"video_name": "xiaoliyu_2.mp4", "duration": 364.0, "question": "What is the background of the video?\n(A) Under the sea\n(B) Beach\n(C) Desert\n(D) Forest\n", "candidates": ["Under the sea", "Beach", "Desert", "Forest"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1677"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "938335173b706723b250d11267fcb836b829bb533e6d2b943aadc9919efc0816", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the background of the video?\n(A) Under the sea\n(B) Beach\n(C) Desert\n(D) Forest\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the background of the video?\n(A) Under the sea\n(B) Beach\n(C) Desert\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1681, "doc": {"video_name": "movie101_8.mp4", "duration": 750.0, "question": "What is the weather in the video scene?\n(A) Rainy\n(B) Sunny\n(C) Foggy\n(D) Snowy\n", "candidates": ["Rainy", "Sunny", "Foggy", "Snowy"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1681"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2e3be2cdae577b95964238c32ff3f714608e0f5eb92d4347807dca777686c3f4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the weather in the video scene?\n(A) Rainy\n(B) Sunny\n(C) Foggy\n(D) Snowy\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the weather in the video scene?\n(A) Rainy\n(B) Sunny\n(C) Foggy\n(D) Snowy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1685, "doc": {"video_name": "en_tv_16.mp4", "duration": 552.0, "question": "In what scenario does the scene in the video take place?\n(A) Snow mountain\n(B) Forest\n(C) Island\n(D) City\n", "candidates": ["Snow mountain", "Forest", "Island", "City"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1685"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ef474198796027a572a218e26e22548b7f1babaadc9eda0be042325271defdc3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what scenario does the scene in the video take place?\n(A) Snow mountain\n(B) Forest\n(C) Island\n(D) City\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "D"}, "input": "In what scenario does the scene in the video take place?\n(A) Snow mountain\n(B) Forest\n(C) Island\n(D) City\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1689, "doc": {"video_name": "239.mp4", "duration": 480.0, "question": "What type of video is this?\n(A) Holiday\n(B) Nature\n(C) Food\n(D) Lifestyle\n", "candidates": ["Holiday", "Nature", "Food", "Lifestyle"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1689"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "58adb9d561c0dc9fdcd8bab1fcfcb1fc86d5caaba756833dbb544a4d9a61652d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Holiday\n(B) Nature\n(C) Food\n(D) Lifestyle\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Holiday\n(B) Nature\n(C) Food\n(D) Lifestyle\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1693, "doc": {"video_name": "movie101_70.mp4", "duration": 361.0, "question": "What is the genre of this film?\n(A) Sci-Fi\n(B) Romance\n(C) Action\n(D) Mystery\n", "candidates": ["Sci-Fi", "Romance", "Action", "Mystery"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1693"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "606f1c77ee9bc6f0be6bb4e19cf85307e3017222753ea63bebea56f9cbc262c3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the genre of this film?\n(A) Sci-Fi\n(B) Romance\n(C) Action\n(D) Mystery\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "C"}, "input": "What is the genre of this film?\n(A) Sci-Fi\n(B) Romance\n(C) Action\n(D) Mystery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1697, "doc": {"video_name": "en_tv_28.mp4", "duration": 575.0, "question": "Where is the setting of the video story?\n(A) City\n(B) Seaside\n(C) Desert\n(D) Countryside\n", "candidates": ["City", "Seaside", "Desert", "Countryside"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1697"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43047c1df07df1a45440a43c8966089716955f7176ae202397ee78567a869d9c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where is the setting of the video story?\n(A) City\n(B) Seaside\n(C) Desert\n(D) Countryside\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "A"}, "input": "Where is the setting of the video story?\n(A) City\n(B) Seaside\n(C) Desert\n(D) Countryside\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1701, "doc": {"video_name": "AWA-4.mp4", "duration": 450.0, "question": "What is the main background of the video?\n(A) Grassland\n(B) Lake\n(C) Ocean\n(D) Desert\n", "candidates": ["Grassland", "Lake", "Ocean", "Desert"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1701"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "51338ffd3d543f35e5bb6fcb3671b462fd96f83df383235dfe2a4aaf3ae88ec2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main background of the video?\n(A) Grassland\n(B) Lake\n(C) Ocean\n(D) Desert\n", "task_type": "topic_reasoning", "pred_answer": "A.", "answer": "A"}, "input": "What is the main background of the video?\n(A) Grassland\n(B) Lake\n(C) Ocean\n(D) Desert\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1705, "doc": {"video_name": "AWB-12.mp4", "duration": 450.0, "question": "What is the primary environment in the video?\n(A) Forest\n(B) Gobi\n(C) Desert\n(D) Grassland\n", "candidates": ["Forest", "Gobi", "Desert", "Grassland"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1705"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "afeb154e9ac2b4ed658547d0a4a8decd70e462b91c8fe1bb1d438ecc367273f1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the primary environment in the video?\n(A) Forest\n(B) Gobi\n(C) Desert\n(D) Grassland\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the primary environment in the video?\n(A) Forest\n(B) Gobi\n(C) Desert\n(D) Grassland\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1709, "doc": {"video_name": "ego_11.mp4", "duration": 480.02, "question": "What is the first-person character doing in this first-person video?\n(A) Posting sticky notes\n(B) Hanging wallpaper\n(C) Posting posters\n(D) Posting Spring Festival couplets\n", "candidates": ["Posting sticky notes", "Hanging wallpaper", "Posting posters", "Posting Spring Festival couplets"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1709"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "74709a1c03309b362b4864d4404b888778b10a9e519049c4585be6f7741c9d42", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the first-person character doing in this first-person video?\n(A) Posting sticky notes\n(B) Hanging wallpaper\n(C) Posting posters\n(D) Posting Spring Festival couplets\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the first-person character doing in this first-person video?\n(A) Posting sticky notes\n(B) Hanging wallpaper\n(C) Posting posters\n(D) Posting Spring Festival couplets\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1713, "doc": {"video_name": "AWC-8.mp4", "duration": 450.0, "question": "What is the content of this video about?\n(A) Birds\n(B) Dinosaurs\n(C) Whales\n(D) Sea Turtles\n", "candidates": ["Birds", "Dinosaurs", "Whales", "Sea Turtles"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1713"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "77d82bd7a5acabd0387c16c8176563e39bbbce48f4f94e5d168be217c57b75dd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the content of this video about?\n(A) Birds\n(B) Dinosaurs\n(C) Whales\n(D) Sea Turtles\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the content of this video about?\n(A) Birds\n(B) Dinosaurs\n(C) Whales\n(D) Sea Turtles\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1717, "doc": {"video_name": "203.mp4", "duration": 480.0, "question": "What is the main scene in the video?\n(A) Sky\n(B) Barren land\n(C) Ocean\n(D) Wetland\n", "candidates": ["Sky", "Barren land", "Ocean", "Wetland"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1717"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "79cfd3b8f08a65458225238090fa57e21e2a20880415c48e7a08825aa6f3cf4a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main scene in the video?\n(A) Sky\n(B) Barren land\n(C) Ocean\n(D) Wetland\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the main scene in the video?\n(A) Sky\n(B) Barren land\n(C) Ocean\n(D) Wetland\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1721, "doc": {"video_name": "xiaoliyu_2.mp4", "duration": 364.0, "question": "What is the protagonist in the video?\n(A) Marine animal\n(B) Bird\n(C) Bear\n(D) Dinosaur\n", "candidates": ["Marine animal", "Bird", "Bear", "Dinosaur"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1721"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "50ee4fec8799c35e41011949d5fd48bae1f13686e8a2769b366d87697556f8e5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the protagonist in the video?\n(A) Marine animal\n(B) Bird\n(C) Bear\n(D) Dinosaur\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the protagonist in the video?\n(A) Marine animal\n(B) Bird\n(C) Bear\n(D) Dinosaur\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1725, "doc": {"video_name": "AWB-1.mp4", "duration": 450.04, "question": "What is the main background of the video?\n(A) Grassland\n(B) Ocean\n(C) Desert\n(D) Lake\n", "candidates": ["Grassland", "Ocean", "Desert", "Lake"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1725"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c9bf485aad75ce15733709cb4f80d6995efb0b93d878c84bd08461a0aa12cce5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main background of the video?\n(A) Grassland\n(B) Ocean\n(C) Desert\n(D) Lake\n", "task_type": "topic_reasoning", "pred_answer": "A.", "answer": "A"}, "input": "What is the main background of the video?\n(A) Grassland\n(B) Ocean\n(C) Desert\n(D) Lake\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1729, "doc": {"video_name": "movie101_33.mp4", "duration": 240.0, "question": "In which scene does the footage of being chased by bees in the video take place?\n(A) Forest\n(B) City\n(C) Snowy Mountain\n(D) Grassland\n", "candidates": ["Forest", "City", "Snowy Mountain", "Grassland"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1729"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "815ce68a34b67c23ff6b6e06c39cfb46c37ae6de3ffc533d5350e95439bbbf3f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In which scene does the footage of being chased by bees in the video take place?\n(A) Forest\n(B) City\n(C) Snowy Mountain\n(D) Grassland\n", "task_type": "topic_reasoning", "pred_answer": "A.", "answer": "A"}, "input": "In which scene does the footage of being chased by bees in the video take place?\n(A) Forest\n(B) City\n(C) Snowy Mountain\n(D) Grassland\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1733, "doc": {"video_name": "217.mp4", "duration": 480.0, "question": "What is the main scene shown in the video?\n(A) Sky\n(B) Ocean\n(C) Desert\n(D) Grassland\n", "candidates": ["Sky", "Ocean", "Desert", "Grassland"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1733"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "91a70f9fbcac97dd8182d6bad678e0cce25da8dd9a4b925e84e07923098f6782", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main scene shown in the video?\n(A) Sky\n(B) Ocean\n(C) Desert\n(D) Grassland\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the main scene shown in the video?\n(A) Sky\n(B) Ocean\n(C) Desert\n(D) Grassland\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1737, "doc": {"video_name": "208.mp4", "duration": 480.0, "question": "What type of video is this?\n(A) Comedy\n(B) Animals\n(C) Science Fiction\n(D) Action\n", "candidates": ["Comedy", "Animals", "Science Fiction", "Action"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1737"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "179a23b4fb506189c97816aea077364da20cc8bdd3af703eeb3677e62427a74b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Comedy\n(B) Animals\n(C) Science Fiction\n(D) Action\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Comedy\n(B) Animals\n(C) Science Fiction\n(D) Action\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1741, "doc": {"video_name": "211.mp4", "duration": 480.0, "question": "What type of video is this?\n(A) Comedy\n(B) Animals\n(C) Science Fiction\n(D) Romance\n", "candidates": ["Comedy", "Animals", "Science Fiction", "Romance"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1741"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "df81bad062e530c472de5cda2123db298d98af29ac621864384e5a5264f4da88", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Comedy\n(B) Animals\n(C) Science Fiction\n(D) Romance\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Comedy\n(B) Animals\n(C) Science Fiction\n(D) Romance\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1745, "doc": {"video_name": "movie101_15.mp4", "duration": 441.0, "question": "What type of movie is the scene in the video from?\n(A) Comedy\n(B) Action\n(C) Horror\n(D) Modern film\n", "candidates": ["Comedy", "Action", "Horror", "Modern film"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1745"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e00f24f0dc67f39cabad6438d44853bc0ea4d85802554d1041fe86101088e81d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of movie is the scene in the video from?\n(A) Comedy\n(B) Action\n(C) Horror\n(D) Modern film\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of movie is the scene in the video from?\n(A) Comedy\n(B) Action\n(C) Horror\n(D) Modern film\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1749, "doc": {"video_name": "movie101_10.mp4", "duration": 716.0, "question": "What color is the hat worn by the person who appeared in the market?\n(A) Red\n(B) Blue\n(C) Black\n(D) Green\n", "candidates": ["Red", "Blue", "Black", "Green"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1749"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e58fc8aba2843ea7f9a3a7650711dbdf55f6ccdb4c4d0ae96891010bfabfed88", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the hat worn by the person who appeared in the market?\n(A) Red\n(B) Blue\n(C) Black\n(D) Green\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What color is the hat worn by the person who appeared in the market?\n(A) Red\n(B) Blue\n(C) Black\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1753, "doc": {"video_name": "en_tv_13.mp4", "duration": 585.0, "question": "What type of video is this?\n(A) Cartoon\n(B) Daily life documentary\n(C) Advertisement video\n(D) Music video\n", "candidates": ["Cartoon", "Daily life documentary", "Advertisement video", "Music video"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1753"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3e4147009b057c3c43a8dd3373b957240285c40ba083ad47697020e21f1f5c83", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Cartoon\n(B) Daily life documentary\n(C) Advertisement video\n(D) Music video\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Cartoon\n(B) Daily life documentary\n(C) Advertisement video\n(D) Music video\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1757, "doc": {"video_name": "movie101_69.mp4", "duration": 385.0, "question": "What is the genre of this film?\n(A) Police and criminals\n(B) Romance\n(C) Science fiction\n(D) Mystery\n", "candidates": ["Police and criminals", "Romance", "Science fiction", "Mystery"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1757"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "df709280ab2f24fc38f5942d25e3d70aa7fb3edb867c5639cce4b46a9627b72c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the genre of this film?\n(A) Police and criminals\n(B) Romance\n(C) Science fiction\n(D) Mystery\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the genre of this film?\n(A) Police and criminals\n(B) Romance\n(C) Science fiction\n(D) Mystery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1761, "doc": {"video_name": "movie101_81.mp4", "duration": 252.0, "question": "What type of film is this?\n(A) Action\n(B) Romance\n(C) Thriller\n(D) Mystery\n", "candidates": ["Action", "Romance", "Thriller", "Mystery"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1761"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "136040af48345e399987ba17576c5d47d7e5204adfcd6cf218ee71286ee12507", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Action\n(B) Romance\n(C) Thriller\n(D) Mystery\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of film is this?\n(A) Action\n(B) Romance\n(C) Thriller\n(D) Mystery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1765, "doc": {"video_name": "tomjerry_10.mp4", "duration": 302.0, "question": "Where is the scene of the video?\n(A) Street\n(B) Park\n(C) Outdoors\n(D) Inside the house\n", "candidates": ["Street", "Park", "Outdoors", "Inside the house"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1765"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "50d3507bea638f848981cc59535918d5ddaee98cc8cefb25dbd71db6bcf9a242", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where is the scene of the video?\n(A) Street\n(B) Park\n(C) Outdoors\n(D) Inside the house\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "Where is the scene of the video?\n(A) Street\n(B) Park\n(C) Outdoors\n(D) Inside the house\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1769, "doc": {"video_name": "AWB-15.mp4", "duration": 450.0, "question": "What is the main content related to in the video?\n(A) Animals\n(B) Humans\n(C) Food\n(D) Plants\n", "candidates": ["Animals", "Humans", "Food", "Plants"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1769"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8ce7510b78e20c210474af35c2cc33b60600d641083f85c1354af01f33aa7434", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main content related to in the video?\n(A) Animals\n(B) Humans\n(C) Food\n(D) Plants\n", "task_type": "topic_reasoning", "pred_answer": "A.", "answer": "A"}, "input": "What is the main content related to in the video?\n(A) Animals\n(B) Humans\n(C) Food\n(D) Plants\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1773, "doc": {"video_name": "movie101_14.mp4", "duration": 465.0, "question": "In what setting does the clip in the video take place?\n(A) Grassland\n(B) Forest\n(C) City\n(D) Snow Mountain\n", "candidates": ["Grassland", "Forest", "City", "Snow Mountain"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1773"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bf65fd774dd0a8b8e44e3de4dd5b74d0a8fcc94fcd762d139a0e041fc9e5da52", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what setting does the clip in the video take place?\n(A) Grassland\n(B) Forest\n(C) City\n(D) Snow Mountain\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "In what setting does the clip in the video take place?\n(A) Grassland\n(B) Forest\n(C) City\n(D) Snow Mountain\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1777, "doc": {"video_name": "movie101_34.mp4", "duration": 748.0, "question": "What is the genre of the movie clip?\n(A) Horror\n(B) War\n(C) Modern\n(D) Comedy\n", "candidates": ["Horror", "War", "Modern", "Comedy"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1777"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "04a7699df053d8eeabacce579dcfe6fce1149b146113125b52445422112e8d11", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the genre of the movie clip?\n(A) Horror\n(B) War\n(C) Modern\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the genre of the movie clip?\n(A) Horror\n(B) War\n(C) Modern\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1781, "doc": {"video_name": "9.mp4", "duration": 471.67, "question": "What is the main setting of the video?\n(A) Library\n(B) Construction site\n(C) Stadium\n(D) School\n", "candidates": ["Library", "Construction site", "Stadium", "School"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1781"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a6dfc8be7a3ff9296d7c41d7f034c5485bdf271b695c8576408cb93b67718327", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main setting of the video?\n(A) Library\n(B) Construction site\n(C) Stadium\n(D) School\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the main setting of the video?\n(A) Library\n(B) Construction site\n(C) Stadium\n(D) School\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1785, "doc": {"video_name": "haimian_7.mp4", "duration": 426.0, "question": "Who is the protagonist of the video?\n(A) Cartoon Sponge\n(B) Cartoon Fish\n(C) Cartoon Shark\n(D) Cartoon Jellyfish\n", "candidates": ["Cartoon Sponge", "Cartoon Fish", "Cartoon Shark", "Cartoon Jellyfish"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1785"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7581585691c8f317bae208847729197e14877d9792e8aa5274774d8f3907d4fa", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Who is the protagonist of the video?\n(A) Cartoon Sponge\n(B) Cartoon Fish\n(C) Cartoon Shark\n(D) Cartoon Jellyfish\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "Who is the protagonist of the video?\n(A) Cartoon Sponge\n(B) Cartoon Fish\n(C) Cartoon Shark\n(D) Cartoon Jellyfish\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1789, "doc": {"video_name": "movie101_71.mp4", "duration": 227.0, "question": "What is the genre of this film?\n(A) Romance\n(B) Comedy\n(C) Mystery\n(D) Science Fiction\n", "candidates": ["Romance", "Comedy", "Mystery", "Science Fiction"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1789"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "beb3f61e8da0466b4395687d631b74ed86a7dd781d7983f76f7a2a4c2a41fad5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the genre of this film?\n(A) Romance\n(B) Comedy\n(C) Mystery\n(D) Science Fiction\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "B"}, "input": "What is the genre of this film?\n(A) Romance\n(B) Comedy\n(C) Mystery\n(D) Science Fiction\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1793, "doc": {"video_name": "216.mp4", "duration": 480.0, "question": "What is the main scene shown in the video?\n(A) Ocean\n(B) Forest\n(C) Grassland\n(D) Desert\n", "candidates": ["Ocean", "Forest", "Grassland", "Desert"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1793"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D."]], "filtered_resps": ["D."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e89b9a893ed2c52f40196c69a300afce0434db0ae8c4d0307c7e24b00643baf3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the main scene shown in the video?\n(A) Ocean\n(B) Forest\n(C) Grassland\n(D) Desert\n", "task_type": "topic_reasoning", "pred_answer": "D.", "answer": "D"}, "input": "What is the main scene shown in the video?\n(A) Ocean\n(B) Forest\n(C) Grassland\n(D) Desert\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1797, "doc": {"video_name": "movie101_41.mp4", "duration": 319.0, "question": "In what setting does the scene in the video take place?\n(A) Forest\n(B) Snowy Mountain\n(C) City\n(D) Island\n", "candidates": ["Forest", "Snowy Mountain", "City", "Island"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1797"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8a5300f825ffb22de63f84ce663c1920702d7ab9fb1694d79856bcf201219d71", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what setting does the scene in the video take place?\n(A) Forest\n(B) Snowy Mountain\n(C) City\n(D) Island\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "In what setting does the scene in the video take place?\n(A) Forest\n(B) Snowy Mountain\n(C) City\n(D) Island\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1801, "doc": {"video_name": "AWD-7.mp4", "duration": 450.13, "question": "What is the main content related to in the video?\n(A) Animals\n(B) Weather\n(C) Plants\n(D) Food\n", "candidates": ["Animals", "Weather", "Plants", "Food"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1801"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dfa2385c001b887c7203ba5a2e8234451111236dfdf348548d6dc90cfb404f90", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main content related to in the video?\n(A) Animals\n(B) Weather\n(C) Plants\n(D) Food\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the main content related to in the video?\n(A) Animals\n(B) Weather\n(C) Plants\n(D) Food\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1805, "doc": {"video_name": "xiaoliyu_4.mp4", "duration": 399.0, "question": "What type of video is this?\n(A) Mystery\n(B) Thriller\n(C) Science Fiction\n(D) Cartoon animation\n", "candidates": ["Mystery", "Thriller", "Science Fiction", "Cartoon animation"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1805"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8509ca85d6e750022bf89a2f12968ede0a28d6bdf88fdd9ee77d0780d0d21c99", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Mystery\n(B) Thriller\n(C) Science Fiction\n(D) Cartoon animation\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of video is this?\n(A) Mystery\n(B) Thriller\n(C) Science Fiction\n(D) Cartoon animation\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1809, "doc": {"video_name": "en_tv_2.mp4", "duration": 509.0, "question": "What time is the overall video set in?\n(A) Early Morning\n(B) Noon\n(C) Afternoon\n(D) Evening\n", "candidates": ["Early Morning", "Noon", "Afternoon", "Evening"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1809"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ff9348ace88a062ae2cfa706ac2bd6b564bf62ee97d426d778b5c8dd3f566d13", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What time is the overall video set in?\n(A) Early Morning\n(B) Noon\n(C) Afternoon\n(D) Evening\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "D"}, "input": "What time is the overall video set in?\n(A) Early Morning\n(B) Noon\n(C) Afternoon\n(D) Evening\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1813, "doc": {"video_name": "movie101_83.mp4", "duration": 640.0, "question": "What is the main setting shown in the video?\n(A) City\n(B) Forest\n(C) Ocean\n(D) Desert\n", "candidates": ["City", "Forest", "Ocean", "Desert"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1813"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "302b9625d556fd005e3646f0956f99fe1975bde64e3c0b75b5b46a4f6e5da7d2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main setting shown in the video?\n(A) City\n(B) Forest\n(C) Ocean\n(D) Desert\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the main setting shown in the video?\n(A) City\n(B) Forest\n(C) Ocean\n(D) Desert\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1817, "doc": {"video_name": "en_tv_20.mp4", "duration": 426.0, "question": "What genre of movie is the clip in the video from?\n(A) War\n(B) Documentary\n(C) Action\n(D) Horror\n", "candidates": ["War", "Documentary", "Action", "Horror"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1817"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1d95ea41f64881d81e01e4c65ffdd83d016e8c0766a7152abfe64c7bca5d3bb0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What genre of movie is the clip in the video from?\n(A) War\n(B) Documentary\n(C) Action\n(D) Horror\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What genre of movie is the clip in the video from?\n(A) War\n(B) Documentary\n(C) Action\n(D) Horror\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1821, "doc": {"video_name": "movie101_73.mp4", "duration": 421.0, "question": "What type of film is this?\n(A) Sci-fi\n(B) Comedy\n(C) Mystery\n(D) Romance\n", "candidates": ["Sci-fi", "Comedy", "Mystery", "Romance"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1821"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "93ebac6af4a166923727304815525a13edef3050125b3b4dd269fd468a6be8aa", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Sci-fi\n(B) Comedy\n(C) Mystery\n(D) Romance\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of film is this?\n(A) Sci-fi\n(B) Comedy\n(C) Mystery\n(D) Romance\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1825, "doc": {"video_name": "en_tv_39.mp4", "duration": 476.0, "question": "What type of film is this?\n(A) Thriller\n(B) Romance\n(C) Comedy\n(D) Science Fiction\n", "candidates": ["Thriller", "Romance", "Comedy", "Science Fiction"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1825"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "12809207955b278c23c96b092e791d73fe00918a15bb2986e9ca7bac2e2f811e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Thriller\n(B) Romance\n(C) Comedy\n(D) Science Fiction\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "C"}, "input": "What type of film is this?\n(A) Thriller\n(B) Romance\n(C) Comedy\n(D) Science Fiction\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1829, "doc": {"video_name": "en_tv_4.mp4", "duration": 421.0, "question": "When does the event in the video take place?\n(A) Noon\n(B) Evening\n(C) Early morning\n(D) Afternoon\n", "candidates": ["Noon", "Evening", "Early morning", "Afternoon"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1829"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c4b8af0e37b2a09fdba3b77826ccd3d520d971a8ed8f067dd31d69859739acd7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "When does the event in the video take place?\n(A) Noon\n(B) Evening\n(C) Early morning\n(D) Afternoon\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "B"}, "input": "When does the event in the video take place?\n(A) Noon\n(B) Evening\n(C) Early morning\n(D) Afternoon\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1833, "doc": {"video_name": "movie101_105.mp4", "duration": 744.0, "question": "What scene is primarily depicted in the video?\n(A) City\n(B) Desert\n(C) Grassland\n(D) Forest\n", "candidates": ["City", "Desert", "Grassland", "Forest"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1833"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "88563ba71fb565c41f0700daab054896354d5812953748678f5cc4b419950988", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What scene is primarily depicted in the video?\n(A) City\n(B) Desert\n(C) Grassland\n(D) Forest\n", "task_type": "topic_reasoning", "pred_answer": "A.", "answer": "A"}, "input": "What scene is primarily depicted in the video?\n(A) City\n(B) Desert\n(C) Grassland\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1837, "doc": {"video_name": "tomjerry_8.mp4", "duration": 308.0, "question": "What is the protagonist of the video?\n(A) Two cartoon cats\n(B) Two cartoon cats and two cartoon mice\n(C) Two cartoon cats and a cartoon mouse\n(D) A cartoon cat and a cartoon mouse\n", "candidates": ["Two cartoon cats", "Two cartoon cats and two cartoon mice", "Two cartoon cats and a cartoon mouse", "A cartoon cat and a cartoon mouse"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1837"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd968e6576db12be17a37906bf31a0b84a91f44cc0e6d4276afca321b6701f79", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the protagonist of the video?\n(A) Two cartoon cats\n(B) Two cartoon cats and two cartoon mice\n(C) Two cartoon cats and a cartoon mouse\n(D) A cartoon cat and a cartoon mouse\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the protagonist of the video?\n(A) Two cartoon cats\n(B) Two cartoon cats and two cartoon mice\n(C) Two cartoon cats and a cartoon mouse\n(D) A cartoon cat and a cartoon mouse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1841, "doc": {"video_name": "AWC-7.mp4", "duration": 450.0, "question": "What is the content of the video about?\n(A) Dinosaurs\n(B) Birds\n(C) Sea Turtles\n(D) Whales\n", "candidates": ["Dinosaurs", "Birds", "Sea Turtles", "Whales"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1841"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "96f53ba28fe8404f19278667abf6fd9fee3b5df12a7215f265a7afe05240be86", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the content of the video about?\n(A) Dinosaurs\n(B) Birds\n(C) Sea Turtles\n(D) Whales\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the content of the video about?\n(A) Dinosaurs\n(B) Birds\n(C) Sea Turtles\n(D) Whales\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1845, "doc": {"video_name": "movie101_87.mp4", "duration": 599.0, "question": "What type of film is this?\n(A) Action\n(B) Mystery\n(C) Comedy\n(D) Romance\n", "candidates": ["Action", "Mystery", "Comedy", "Romance"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1845"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d19010d1c2128504412a8c70b6687e4f6780c19fb0234465e52d97bf00e8fd2f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Action\n(B) Mystery\n(C) Comedy\n(D) Romance\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of film is this?\n(A) Action\n(B) Mystery\n(C) Comedy\n(D) Romance\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1849, "doc": {"video_name": "en_tv_34.mp4", "duration": 492.0, "question": "Where is the setting of the video story?\n(A) Desert\n(B) Seaside\n(C) Countryside\n(D) City\n", "candidates": ["Desert", "Seaside", "Countryside", "City"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1849"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a8b42813fd24a9738a27dca562968cf189c0c5134c06ab44ded221be52994fb5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where is the setting of the video story?\n(A) Desert\n(B) Seaside\n(C) Countryside\n(D) City\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "Where is the setting of the video story?\n(A) Desert\n(B) Seaside\n(C) Countryside\n(D) City\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1853, "doc": {"video_name": "en_tv_39.mp4", "duration": 476.0, "question": "Where does the story of the video take place?\n(A) Countryside\n(B) Desert\n(C) Seaside\n(D) City\n", "candidates": ["Countryside", "Desert", "Seaside", "City"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1853"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "056529c998c5dfa67f9d043a973f8b6d7a9c3e4083ed088c362539cb646d6539", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where does the story of the video take place?\n(A) Countryside\n(B) Desert\n(C) Seaside\n(D) City\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "Where does the story of the video take place?\n(A) Countryside\n(B) Desert\n(C) Seaside\n(D) City\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1857, "doc": {"video_name": "movie101_24.mp4", "duration": 311.0, "question": "What genre of movie is the animation in the video?\n(A) War\n(B) Comedy\n(C) Science Fiction\n(D) Horror\n", "candidates": ["War", "Comedy", "Science Fiction", "Horror"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1857"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f9afc6948b6580635100aba9c6e022a0149d4f22c2c7c1aa6412b9c52a0097c1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What genre of movie is the animation in the video?\n(A) War\n(B) Comedy\n(C) Science Fiction\n(D) Horror\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "C"}, "input": "What genre of movie is the animation in the video?\n(A) War\n(B) Comedy\n(C) Science Fiction\n(D) Horror\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1861, "doc": {"video_name": "en_tv_18.mp4", "duration": 568.0, "question": "In what setting does the scene in the video take place?\n(A) City\n(B) Island\n(C) Forest\n(D) Snowy mountain\n", "candidates": ["City", "Island", "Forest", "Snowy mountain"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1861"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e3c1cbc1ea218cff561d4927e29f7d6b43432a4560bf6b88beff36eeea2fce7c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what setting does the scene in the video take place?\n(A) City\n(B) Island\n(C) Forest\n(D) Snowy mountain\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "In what setting does the scene in the video take place?\n(A) City\n(B) Island\n(C) Forest\n(D) Snowy mountain\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1865, "doc": {"video_name": "movie101_62.mp4", "duration": 601.0, "question": "What type of film is this?\n(A) Action\n(B) Thriller\n(C) Mystery\n(D) Comedy\n", "candidates": ["Action", "Thriller", "Mystery", "Comedy"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1865"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "997e7d3ab9cdda5cafb6700b16b902a2a1fc440ee4b1aa45d4ca4dade03f6d59", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Action\n(B) Thriller\n(C) Mystery\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of film is this?\n(A) Action\n(B) Thriller\n(C) Mystery\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1869, "doc": {"video_name": "tomjerry_2.mp4", "duration": 303.0, "question": "What is the main character of the video?\n(A) A cartoon cat and a cartoon mouse\n(B) Three cats\n(C) One cat and two mice\n(D) Three mice\n", "candidates": ["A cartoon cat and a cartoon mouse", "Three cats", "One cat and two mice", "Three mice"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1869"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6951420e4ded34602eae3f41a7be30615d1ef9a3cd3f73af96a8c9dc4b9a875e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main character of the video?\n(A) A cartoon cat and a cartoon mouse\n(B) Three cats\n(C) One cat and two mice\n(D) Three mice\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the main character of the video?\n(A) A cartoon cat and a cartoon mouse\n(B) Three cats\n(C) One cat and two mice\n(D) Three mice\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1873, "doc": {"video_name": "game_21.mp4", "duration": 275.07, "question": "What is the type of this video?\n(A) Movie trailer\n(B) Documentary\n(C) Tutorial\n(D) Video game\n", "candidates": ["Movie trailer", "Documentary", "Tutorial", "Video game"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1873"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9fdc7210622e91d3310cba19d7e73a4f08e7ed769e06a51e620a24cfd4bc1930", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the type of this video?\n(A) Movie trailer\n(B) Documentary\n(C) Tutorial\n(D) Video game\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the type of this video?\n(A) Movie trailer\n(B) Documentary\n(C) Tutorial\n(D) Video game\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1877, "doc": {"video_name": "game_6.mp4", "duration": 195.81, "question": "What is the object built by the main character in the video?\n(A) Castle\n(B) fort\n(C) Tent\n(D) fireworks\n", "candidates": ["Castle", "fort", "Tent", "fireworks"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1877"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "beff7153e31c982f90eb7ce2c6caf53ee347fca87942bd0c1d8589ae2024b5e5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the object built by the main character in the video?\n(A) Castle\n(B) fort\n(C) Tent\n(D) fireworks\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the object built by the main character in the video?\n(A) Castle\n(B) fort\n(C) Tent\n(D) fireworks\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1881, "doc": {"video_name": "game_20.mp4", "duration": 299.26, "question": "What color is the building that appears in this gameplay video?\n(A) Dark blue\n(B) Silver-gray\n(C) Red\n(D) Brown\n", "candidates": ["Dark blue", "Silver-gray", "Red", "Brown"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1881"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ab5712ea8f3ccb7ee51131c48c201dd6abf301e2b885ae0331be1f5bf145c30f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the building that appears in this gameplay video?\n(A) Dark blue\n(B) Silver-gray\n(C) Red\n(D) Brown\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What color is the building that appears in this gameplay video?\n(A) Dark blue\n(B) Silver-gray\n(C) Red\n(D) Brown\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1885, "doc": {"video_name": "game_30.mp4", "duration": 200.85, "question": "What is the main outdoor setting where the game protagonist is located?\n(A) Desert\n(B) Forest\n(C) Mountain\n(D) City\n", "candidates": ["Desert", "Forest", "Mountain", "City"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1885"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b3743e10025666dc5d909f5a0b72aea96250a11ccea02c0b1a9ce07335a3f75", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main outdoor setting where the game protagonist is located?\n(A) Desert\n(B) Forest\n(C) Mountain\n(D) City\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the main outdoor setting where the game protagonist is located?\n(A) Desert\n(B) Forest\n(C) Mountain\n(D) City\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1889, "doc": {"video_name": "game_32.mp4", "duration": 278.83, "question": "What is the theme of this video?\n(A) A person singing\n(B) A cartoon animation\n(C) A person demonstrating how they play a game\n(D) A person live-streaming a sale\n", "candidates": ["A person singing", "A cartoon animation", "A person demonstrating how they play a game", "A person live-streaming a sale"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1889"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ef56d3d84b110710b636f0a7a3fe66a9b5f1a76215093d74e166099930b1b87f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the theme of this video?\n(A) A person singing\n(B) A cartoon animation\n(C) A person demonstrating how they play a game\n(D) A person live-streaming a sale\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the theme of this video?\n(A) A person singing\n(B) A cartoon animation\n(C) A person demonstrating how they play a game\n(D) A person live-streaming a sale\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1893, "doc": {"video_name": "game_26.mp4", "duration": 438.53, "question": "What is the indoor scene where the game protagonist is located?\n(A) A high school classroom\n(B) A luxury penthouse\n(C) A local library\n(D) A McDonald's fast food restaurant\n", "candidates": ["A high school classroom", "A luxury penthouse", "A local library", "A McDonald's fast food restaurant"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1893"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bd214a28cfb66630dbea046c360eccabde14f1c9ba97a3e764a04c51dbffbc88", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the indoor scene where the game protagonist is located?\n(A) A high school classroom\n(B) A luxury penthouse\n(C) A local library\n(D) A McDonald's fast food restaurant\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the indoor scene where the game protagonist is located?\n(A) A high school classroom\n(B) A luxury penthouse\n(C) A local library\n(D) A McDonald's fast food restaurant\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1897, "doc": {"video_name": "game_9.mp4", "duration": 240.91, "question": "What is the character always holding in their hand in the game?\n(A) Pickaxe\n(B) Gun\n(C) Map\n(D) Torch\n", "candidates": ["Pickaxe", "Gun", "Map", "Torch"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1897"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5bb4c5774c617cf7c66536b76c740be5a02a47494308a0d0c44ec9b86bcbde5e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the character always holding in their hand in the game?\n(A) Pickaxe\n(B) Gun\n(C) Map\n(D) Torch\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the character always holding in their hand in the game?\n(A) Pickaxe\n(B) Gun\n(C) Map\n(D) Torch\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1901, "doc": {"video_name": "game_10.mp4", "duration": 291.27, "question": "What is the character building in the game?\n(A) Pool\n(B) House\n(C) Bridge\n(D) Garden\n", "candidates": ["Pool", "House", "Bridge", "Garden"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1901"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a32c6e5b93e295efd355d76dc6a538018493944836b80ce1d0c7ff215bef5344", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the character building in the game?\n(A) Pool\n(B) House\n(C) Bridge\n(D) Garden\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the character building in the game?\n(A) Pool\n(B) House\n(C) Bridge\n(D) Garden\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1905, "doc": {"video_name": "game_12.mp4", "duration": 224.47, "question": "What is the main action performed by the character in this video?\n(A) Mining\n(B) Farming\n(C) Building\n(D) Exploring\n", "candidates": ["Mining", "Farming", "Building", "Exploring"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1905"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "735bff670c4ecc2d9a458bb81e5b3aad4586c4d1c1dd62cd38d799723e32e7ce", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the main action performed by the character in this video?\n(A) Mining\n(B) Farming\n(C) Building\n(D) Exploring\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "C"}, "input": "What is the main action performed by the character in this video?\n(A) Mining\n(B) Farming\n(C) Building\n(D) Exploring\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1909, "doc": {"video_name": "game_14.mp4", "duration": 260.09, "question": "What is the object being built in this video?\n(A) Farm\n(B) Bridge\n(C) Tower\n(D) Pool\n", "candidates": ["Farm", "Bridge", "Tower", "Pool"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1909"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e2204f202645617c714c3c77be25d973808485acdd1a7ad4c079c9c24018d238", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the object being built in this video?\n(A) Farm\n(B) Bridge\n(C) Tower\n(D) Pool\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the object being built in this video?\n(A) Farm\n(B) Bridge\n(C) Tower\n(D) Pool\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1913, "doc": {"video_name": "game_3.mp4", "duration": 316.99, "question": "What is my main action in the video?\n(A) sitting\n(B) fishing\n(C) running\n(D) climbing\n", "candidates": ["sitting", "fishing", "running", "climbing"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1913"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3fc514b4af68e6c6a0fbbfb71a8a7de619d23c63a9fa7fa7d35507041250ebcb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is my main action in the video?\n(A) sitting\n(B) fishing\n(C) running\n(D) climbing\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "C"}, "input": "What is my main action in the video?\n(A) sitting\n(B) fishing\n(C) running\n(D) climbing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1917, "doc": {"video_name": "order_214.mp4", "duration": 485.77, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) abseiling --> making jewelry --> milking cow --> cleaning toilet\n(B) making jewelry --> abseiling --> cleaning toilet --> milking cow\n(C) milking cow --> cleaning toilet --> abseiling --> making jewelry\n(D) cleaning toilet --> milking cow --> abseiling --> making jewelry\n", "candidates": ["abseiling --> making jewelry --> milking cow --> cleaning toilet", "making jewelry --> abseiling --> cleaning toilet --> milking cow", "milking cow --> cleaning toilet --> abseiling --> making jewelry", "cleaning toilet --> milking cow --> abseiling --> making jewelry"], "answer": "A", "task_type": "order", "question_id": "Q1917"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ac7bd4e439216d42aff67bf921e2c9178e5d117b5728777d493797e73f2d9d02", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) abseiling --> making jewelry --> milking cow --> cleaning toilet\n(B) making jewelry --> abseiling --> cleaning toilet --> milking cow\n(C) milking cow --> cleaning toilet --> abseiling --> making jewelry\n(D) cleaning toilet --> milking cow --> abseiling --> making jewelry\n", "task_type": "order", "pred_answer": "D", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) abseiling --> making jewelry --> milking cow --> cleaning toilet\n(B) making jewelry --> abseiling --> cleaning toilet --> milking cow\n(C) milking cow --> cleaning toilet --> abseiling --> making jewelry\n(D) cleaning toilet --> milking cow --> abseiling --> making jewelry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1921, "doc": {"video_name": "order_133.mp4", "duration": 1734.55, "question": "Arrange the following events from the video in the correct chronological order: (1)The man starts to demonstrate playing the bongos in a lesson; (2)LP and Giovanni Logo appear on the black screen opening; (3)The lesson continues, alternating between color and black and white footage; (4)A man sits behind a set of bongo drums.\n(A) 2->4->1->3\n(B) 4->2->1->3\n(C) 2->1->4->3\n(D) 1->2->3->4\n", "candidates": ["2->4->1->3", "4->2->1->3", "2->1->4->3", "1->2->3->4"], "answer": "A", "task_type": "order", "question_id": "Q1921"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ffc24367c209c5cc307bee2ce396f35443f6f1532a1eb7f2649cfe2a33611a47", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man starts to demonstrate playing the bongos in a lesson; (2)LP and Giovanni Logo appear on the black screen opening; (3)The lesson continues, alternating between color and black and white footage; (4)A man sits behind a set of bongo drums.\n(A) 2->4->1->3\n(B) 4->2->1->3\n(C) 2->1->4->3\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man starts to demonstrate playing the bongos in a lesson; (2)LP and Giovanni Logo appear on the black screen opening; (3)The lesson continues, alternating between color and black and white footage; (4)A man sits behind a set of bongo drums.\n(A) 2->4->1->3\n(B) 4->2->1->3\n(C) 2->1->4->3\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1925, "doc": {"video_name": "order_140.mp4", "duration": 6191.46, "question": "Arrange the following events from the video in the correct chronological order: (1)People crash into the bottom of a bridge; (2)People are sitting on a raft going down a river; (3)People are walking across the water and down a trail; (4)People are carrying their raft and get into a van.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 2->3->1->4\n(D) 1->3->2->4\n", "candidates": ["1->2->3->4", "2->1->3->4", "2->3->1->4", "1->3->2->4"], "answer": "B", "task_type": "order", "question_id": "Q1925"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "160d833041afed7e763c9c969452dfd8dcff24d49872c88919aacb1746a1c1bf", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)People crash into the bottom of a bridge; (2)People are sitting on a raft going down a river; (3)People are walking across the water and down a trail; (4)People are carrying their raft and get into a van.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 2->3->1->4\n(D) 1->3->2->4\n", "task_type": "order", "pred_answer": "A", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)People crash into the bottom of a bridge; (2)People are sitting on a raft going down a river; (3)People are walking across the water and down a trail; (4)People are carrying their raft and get into a van.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 2->3->1->4\n(D) 1->3->2->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1929, "doc": {"video_name": "order_77.mp4", "duration": 688.89, "question": "Arrange the following events from the video in the correct chronological order: (1)Woman begins ripping the wrapping paper with her hands; (2)Woman sets the box on top of the wrapping paper and begins wrapping the box; (3)Woman lifts up a box and sets it on the table; (4)Woman grabs a pair of scissors and tape.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 3->4->1->2\n", "candidates": ["1->2->3->4", "4->3->2->1", "2->1->3->4", "3->4->1->2"], "answer": "D", "task_type": "order", "question_id": "Q1929"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "47f872e705dd02ed81ee1a5c44634d86dcfb29b6d0ee34434ad672d6c261ebbf", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Woman begins ripping the wrapping paper with her hands; (2)Woman sets the box on top of the wrapping paper and begins wrapping the box; (3)Woman lifts up a box and sets it on the table; (4)Woman grabs a pair of scissors and tape.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 3->4->1->2\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Woman begins ripping the wrapping paper with her hands; (2)Woman sets the box on top of the wrapping paper and begins wrapping the box; (3)Woman lifts up a box and sets it on the table; (4)Woman grabs a pair of scissors and tape.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 3->4->1->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1933, "doc": {"video_name": "order_287.mp4", "duration": 490.03999999999996, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> javelin throw --> riding mule --> water sliding\n(B) riding mule --> zumba --> javelin throw --> water sliding\n(C) riding mule --> javelin throw --> water sliding --> zumba\n(D) water sliding --> javelin throw --> zumba --> riding mule\n", "candidates": ["zumba --> javelin throw --> riding mule --> water sliding", "riding mule --> zumba --> javelin throw --> water sliding", "riding mule --> javelin throw --> water sliding --> zumba", "water sliding --> javelin throw --> zumba --> riding mule"], "answer": "A", "task_type": "order", "question_id": "Q1933"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "33aeee420ee31045615251b59347684b475f308f863c3cbd2225e2ad60d78675", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> javelin throw --> riding mule --> water sliding\n(B) riding mule --> zumba --> javelin throw --> water sliding\n(C) riding mule --> javelin throw --> water sliding --> zumba\n(D) water sliding --> javelin throw --> zumba --> riding mule\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> javelin throw --> riding mule --> water sliding\n(B) riding mule --> zumba --> javelin throw --> water sliding\n(C) riding mule --> javelin throw --> water sliding --> zumba\n(D) water sliding --> javelin throw --> zumba --> riding mule\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1937, "doc": {"video_name": "order_72.mp4", "duration": 684.1500000000001, "question": "Arrange the following events from the video in the correct chronological order: (1)A shirtless man lifts a ball onto one shoulder; (2)A series of tug of war matches are shown; (3)A third man flips a heavy tire; (4)Individuals are shown exercising with weights, kegs, or tires in a parking lot.\n(A) 1->3->4->2\n(B) 3->1->2->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "candidates": ["1->3->4->2", "3->1->2->4", "1->2->3->4", "2->1->3->4"], "answer": "A", "task_type": "order", "question_id": "Q1937"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "271e90fd31276e1cee8b83ecfed9dfff40963ef982718fa9e5f45773f137271a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A shirtless man lifts a ball onto one shoulder; (2)A series of tug of war matches are shown; (3)A third man flips a heavy tire; (4)Individuals are shown exercising with weights, kegs, or tires in a parking lot.\n(A) 1->3->4->2\n(B) 3->1->2->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A shirtless man lifts a ball onto one shoulder; (2)A series of tug of war matches are shown; (3)A third man flips a heavy tire; (4)Individuals are shown exercising with weights, kegs, or tires in a parking lot.\n(A) 1->3->4->2\n(B) 3->1->2->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1941, "doc": {"video_name": "order_70.mp4", "duration": 695.25, "question": "Arrange the following events from the video in the correct chronological order: (1) Woman measures and cuts the wallpaper; (2) Woman grabs wallpaper paste and materials; (3) Woman hangs the wallpaper and flattens it; (4) Woman pastes the wallpaper with a brush and soaks it.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->4->3\n", "candidates": ["4->3->2->1", "1->2->3->4", "3->2->1->4", "2->1->4->3"], "answer": "D", "task_type": "order", "question_id": "Q1941"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9fb6063ed7fd80e92e08da9ffde86a792aca95de84b7ad46261db0b37b213c8b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) Woman measures and cuts the wallpaper; (2) Woman grabs wallpaper paste and materials; (3) Woman hangs the wallpaper and flattens it; (4) Woman pastes the wallpaper with a brush and soaks it.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->4->3\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1) Woman measures and cuts the wallpaper; (2) Woman grabs wallpaper paste and materials; (3) Woman hangs the wallpaper and flattens it; (4) Woman pastes the wallpaper with a brush and soaks it.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1945, "doc": {"video_name": "order_17.mp4", "duration": 681.28, "question": "Arrange the following events from the video in the correct chronological order: (1)People crash into the bottom of a bridge; (2)People are sitting on a raft going down a river; (3)People are walking across the water and down a trail; (4)People are carrying their raft and get into a van.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 1->3->2->4\n(D) 2->3->1->4\n", "candidates": ["2->1->3->4", "1->2->3->4", "1->3->2->4", "2->3->1->4"], "answer": "A", "task_type": "order", "question_id": "Q1945"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e1855b2d2f2563c3dbe7d3cbdf1ded54e7d30dd661452591a438ed31288b54d7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)People crash into the bottom of a bridge; (2)People are sitting on a raft going down a river; (3)People are walking across the water and down a trail; (4)People are carrying their raft and get into a van.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 1->3->2->4\n(D) 2->3->1->4\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)People crash into the bottom of a bridge; (2)People are sitting on a raft going down a river; (3)People are walking across the water and down a trail; (4)People are carrying their raft and get into a van.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 1->3->2->4\n(D) 2->3->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1949, "doc": {"video_name": "order_202.mp4", "duration": 490.04999999999995, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) abseiling --> carving pumpkin --> javelin throw --> riding mule\n(B) riding mule --> carving pumpkin --> javelin throw --> abseiling\n(C) javelin throw --> abseiling --> carving pumpkin --> riding mule\n(D) abseiling --> riding mule --> javelin throw --> carving pumpkin\n", "candidates": ["abseiling --> carving pumpkin --> javelin throw --> riding mule", "riding mule --> carving pumpkin --> javelin throw --> abseiling", "javelin throw --> abseiling --> carving pumpkin --> riding mule", "abseiling --> riding mule --> javelin throw --> carving pumpkin"], "answer": "A", "task_type": "order", "question_id": "Q1949"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "33eaf858686cd100db6e00f53439868ee1bb6c82726b63ebca758f484b840c16", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) abseiling --> carving pumpkin --> javelin throw --> riding mule\n(B) riding mule --> carving pumpkin --> javelin throw --> abseiling\n(C) javelin throw --> abseiling --> carving pumpkin --> riding mule\n(D) abseiling --> riding mule --> javelin throw --> carving pumpkin\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) abseiling --> carving pumpkin --> javelin throw --> riding mule\n(B) riding mule --> carving pumpkin --> javelin throw --> abseiling\n(C) javelin throw --> abseiling --> carving pumpkin --> riding mule\n(D) abseiling --> riding mule --> javelin throw --> carving pumpkin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1953, "doc": {"video_name": "order_13.mp4", "duration": 673.2, "question": "Arrange the following events from the video in the correct chronological order: (1)The batter is poured into bowls and dye is added; (2)Ingredients are shown on a counter; (3)The cake is frosted with blue frosting and sprinkles are added; (4)The pans are greased and the different colored batter is poured into them.\n(A) 2->1->4->3\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 3->4->1->2\n", "candidates": ["2->1->4->3", "4->3->2->1", "1->2->3->4", "3->4->1->2"], "answer": "A", "task_type": "order", "question_id": "Q1953"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8ea17194108ab21de9564e33c87eba45aafaa715e4389203c3161f7cb63e8b47", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The batter is poured into bowls and dye is added; (2)Ingredients are shown on a counter; (3)The cake is frosted with blue frosting and sprinkles are added; (4)The pans are greased and the different colored batter is poured into them.\n(A) 2->1->4->3\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 3->4->1->2\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The batter is poured into bowls and dye is added; (2)Ingredients are shown on a counter; (3)The cake is frosted with blue frosting and sprinkles are added; (4)The pans are greased and the different colored batter is poured into them.\n(A) 2->1->4->3\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 3->4->1->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1957, "doc": {"video_name": "order_258.mp4", "duration": 605.05, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> stomping grapes --> tossing coin --> cooking sausages\n(B) tossing coin --> stomping grapes --> javelin throw --> cooking sausages\n(C) cooking sausages --> javelin throw --> stomping grapes --> tossing coin\n(D) javelin throw --> stomping grapes --> cooking sausages --> tossing coin\n", "candidates": ["javelin throw --> stomping grapes --> tossing coin --> cooking sausages", "tossing coin --> stomping grapes --> javelin throw --> cooking sausages", "cooking sausages --> javelin throw --> stomping grapes --> tossing coin", "javelin throw --> stomping grapes --> cooking sausages --> tossing coin"], "answer": "A", "task_type": "order", "question_id": "Q1957"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e25c4e0c0403b13596c1f066d710c71c0f8324c7f3e6a51f7a322b772803f74b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> stomping grapes --> tossing coin --> cooking sausages\n(B) tossing coin --> stomping grapes --> javelin throw --> cooking sausages\n(C) cooking sausages --> javelin throw --> stomping grapes --> tossing coin\n(D) javelin throw --> stomping grapes --> cooking sausages --> tossing coin\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> stomping grapes --> tossing coin --> cooking sausages\n(B) tossing coin --> stomping grapes --> javelin throw --> cooking sausages\n(C) cooking sausages --> javelin throw --> stomping grapes --> tossing coin\n(D) javelin throw --> stomping grapes --> cooking sausages --> tossing coin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1961, "doc": {"video_name": "order_103.mp4", "duration": 717.89, "question": "Arrange the following events from the video in the correct chronological order: (1)A lady in blue talks about the Extreme Dog Grooming company; (2)A poodle is groomed and dyed with different colors; (3)A dog painted to resemble a zebra is shown; (4)A dog with spots resembling a giraffe is brought to an elementary school for kids to play with.\n(A) 4->3->2->1\n(B) 3->4->1->2\n(C) 1->2->4->3\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "3->4->1->2", "1->2->4->3", "2->1->3->4"], "answer": "C", "task_type": "order", "question_id": "Q1961"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2fc7e09a3790c90b7329235967024038c9bb21e684bb117ad6e9af4e0d91819f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A lady in blue talks about the Extreme Dog Grooming company; (2)A poodle is groomed and dyed with different colors; (3)A dog painted to resemble a zebra is shown; (4)A dog with spots resembling a giraffe is brought to an elementary school for kids to play with.\n(A) 4->3->2->1\n(B) 3->4->1->2\n(C) 1->2->4->3\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "D", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A lady in blue talks about the Extreme Dog Grooming company; (2)A poodle is groomed and dyed with different colors; (3)A dog painted to resemble a zebra is shown; (4)A dog with spots resembling a giraffe is brought to an elementary school for kids to play with.\n(A) 4->3->2->1\n(B) 3->4->1->2\n(C) 1->2->4->3\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1965, "doc": {"video_name": "order_110.mp4", "duration": 667.64, "question": "Arrange the following events from the video in the correct chronological order: (1)Water is added to the cup and more limes are squeezed in by hand; (2)The small bowls of salt are arranged and limes are sliced in halves; (3)The cup is stirred with more water and a set of cups filled with the refreshment are seen; (4)The limes are juiced into a cup using a hand held press.\n(A) 2->1->4->3\n(B) 1->2->4->3\n(C) 2->4->1->3\n(D) 4->2->1->3\n", "candidates": ["2->1->4->3", "1->2->4->3", "2->4->1->3", "4->2->1->3"], "answer": "C", "task_type": "order", "question_id": "Q1965"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e930061566c4f875ae2bdc98b50fb7d603ca8967b71dd18f89c74a8a92c56896", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Water is added to the cup and more limes are squeezed in by hand; (2)The small bowls of salt are arranged and limes are sliced in halves; (3)The cup is stirred with more water and a set of cups filled with the refreshment are seen; (4)The limes are juiced into a cup using a hand held press.\n(A) 2->1->4->3\n(B) 1->2->4->3\n(C) 2->4->1->3\n(D) 4->2->1->3\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Water is added to the cup and more limes are squeezed in by hand; (2)The small bowls of salt are arranged and limes are sliced in halves; (3)The cup is stirred with more water and a set of cups filled with the refreshment are seen; (4)The limes are juiced into a cup using a hand held press.\n(A) 2->1->4->3\n(B) 1->2->4->3\n(C) 2->4->1->3\n(D) 4->2->1->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1969, "doc": {"video_name": "order_345.mp4", "duration": 488.32, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) pole vault --> playing harp --> paragliding --> playing trombone\n(B) paragliding --> playing harp --> playing trombone --> pole vault\n(C) pole vault --> paragliding --> playing harp --> playing trombone\n(D) playing harp --> paragliding --> pole vault --> playing trombone\n", "candidates": ["pole vault --> playing harp --> paragliding --> playing trombone", "paragliding --> playing harp --> playing trombone --> pole vault", "pole vault --> paragliding --> playing harp --> playing trombone", "playing harp --> paragliding --> pole vault --> playing trombone"], "answer": "C", "task_type": "order", "question_id": "Q1969"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c7c8d98701d157f2b8de69f04b282b12db402467cad2f013509af9d4e65b0200", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) pole vault --> playing harp --> paragliding --> playing trombone\n(B) paragliding --> playing harp --> playing trombone --> pole vault\n(C) pole vault --> paragliding --> playing harp --> playing trombone\n(D) playing harp --> paragliding --> pole vault --> playing trombone\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) pole vault --> playing harp --> paragliding --> playing trombone\n(B) paragliding --> playing harp --> playing trombone --> pole vault\n(C) pole vault --> paragliding --> playing harp --> playing trombone\n(D) playing harp --> paragliding --> pole vault --> playing trombone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1973, "doc": {"video_name": "order_264.mp4", "duration": 274.05, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> tossing coin --> milking cow --> shredding paper\n(B) tossing coin --> shredding paper --> milking cow --> carving pumpkin\n(C) milking cow --> shredding paper --> carving pumpkin --> tossing coin\n(D) carving pumpkin --> milking cow --> tossing coin --> shredding paper\n", "candidates": ["carving pumpkin --> tossing coin --> milking cow --> shredding paper", "tossing coin --> shredding paper --> milking cow --> carving pumpkin", "milking cow --> shredding paper --> carving pumpkin --> tossing coin", "carving pumpkin --> milking cow --> tossing coin --> shredding paper"], "answer": "C", "task_type": "order", "question_id": "Q1973"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "872ff64531d723148ee288e12ef8d9b511e6ceb5713a0e04dfe97420955b9752", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> tossing coin --> milking cow --> shredding paper\n(B) tossing coin --> shredding paper --> milking cow --> carving pumpkin\n(C) milking cow --> shredding paper --> carving pumpkin --> tossing coin\n(D) carving pumpkin --> milking cow --> tossing coin --> shredding paper\n", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> tossing coin --> milking cow --> shredding paper\n(B) tossing coin --> shredding paper --> milking cow --> carving pumpkin\n(C) milking cow --> shredding paper --> carving pumpkin --> tossing coin\n(D) carving pumpkin --> milking cow --> tossing coin --> shredding paper\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1977, "doc": {"video_name": "order_309.mp4", "duration": 535.1099999999999, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) zumba --> clean and jerk --> milking cow --> playing trombone\n(B) clean and jerk --> zumba --> milking cow --> playing trombone\n(C) playing trombone --> milking cow --> clean and jerk --> zumba\n(D) milking cow --> zumba --> clean and jerk --> playing trombone\n", "candidates": ["zumba --> clean and jerk --> milking cow --> playing trombone", "clean and jerk --> zumba --> milking cow --> playing trombone", "playing trombone --> milking cow --> clean and jerk --> zumba", "milking cow --> zumba --> clean and jerk --> playing trombone"], "answer": "D", "task_type": "order", "question_id": "Q1977"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "15e2f02fd91610a95505709f8c8578b1fad26664817e4d344958ead418ae304c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) zumba --> clean and jerk --> milking cow --> playing trombone\n(B) clean and jerk --> zumba --> milking cow --> playing trombone\n(C) playing trombone --> milking cow --> clean and jerk --> zumba\n(D) milking cow --> zumba --> clean and jerk --> playing trombone\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) zumba --> clean and jerk --> milking cow --> playing trombone\n(B) clean and jerk --> zumba --> milking cow --> playing trombone\n(C) playing trombone --> milking cow --> clean and jerk --> zumba\n(D) milking cow --> zumba --> clean and jerk --> playing trombone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1981, "doc": {"video_name": "order_228.mp4", "duration": 7200.040000000001, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) javelin throw --> cooking sausages --> riding mule --> cleaning toilet\n(B) cleaning toilet --> cooking sausages --> javelin throw --> riding mule\n(C) cooking sausages --> cleaning toilet --> riding mule --> javelin throw\n(D) javelin throw --> cooking sausages --> cleaning toilet --> riding mule\n", "candidates": ["javelin throw --> cooking sausages --> riding mule --> cleaning toilet", "cleaning toilet --> cooking sausages --> javelin throw --> riding mule", "cooking sausages --> cleaning toilet --> riding mule --> javelin throw", "javelin throw --> cooking sausages --> cleaning toilet --> riding mule"], "answer": "D", "task_type": "order", "question_id": "Q1981"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c659c93cf42a3f9a6d4eba998292120d8d12d012aad7b44f3b6e40f8afd150a8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) javelin throw --> cooking sausages --> riding mule --> cleaning toilet\n(B) cleaning toilet --> cooking sausages --> javelin throw --> riding mule\n(C) cooking sausages --> cleaning toilet --> riding mule --> javelin throw\n(D) javelin throw --> cooking sausages --> cleaning toilet --> riding mule\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) javelin throw --> cooking sausages --> riding mule --> cleaning toilet\n(B) cleaning toilet --> cooking sausages --> javelin throw --> riding mule\n(C) cooking sausages --> cleaning toilet --> riding mule --> javelin throw\n(D) javelin throw --> cooking sausages --> cleaning toilet --> riding mule\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1985, "doc": {"video_name": "order_68.mp4", "duration": 686.23, "question": "Arrange the following events from the video in the correct chronological order: (1)The mix is poured into cupcake liners; (2)A cake with a Hershey shape is placed on a white plate; (3)Eggs, flour, and other ingredients are mixed in a bowl; (4)The cake is cut into a piece and served on a white plate.\n(A) 3->1->4->2\n(B) 3->1->2->4\n(C) 2->3->1->4\n(D) 1->2->3->4\n", "candidates": ["3->1->4->2", "3->1->2->4", "2->3->1->4", "1->2->3->4"], "answer": "B", "task_type": "order", "question_id": "Q1985"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "58c1e912405a405fcc134993333461c936f806a5e67fd02d85a1fba161ddf6a2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The mix is poured into cupcake liners; (2)A cake with a Hershey shape is placed on a white plate; (3)Eggs, flour, and other ingredients are mixed in a bowl; (4)The cake is cut into a piece and served on a white plate.\n(A) 3->1->4->2\n(B) 3->1->2->4\n(C) 2->3->1->4\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The mix is poured into cupcake liners; (2)A cake with a Hershey shape is placed on a white plate; (3)Eggs, flour, and other ingredients are mixed in a bowl; (4)The cake is cut into a piece and served on a white plate.\n(A) 3->1->4->2\n(B) 3->1->2->4\n(C) 2->3->1->4\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1989, "doc": {"video_name": "order_66.mp4", "duration": 743.44, "question": "Arrange the following events from the video in the correct chronological order: (1)The mix is poured into cupcake liners; (2)A cake with a Hershey shape is placed on a white plate; (3)Eggs, flour, and other ingredients are mixed in a bowl; (4)The cake is cut into a piece and served on a white plate.\n(A) 3->1->4->2\n(B) 3->1->2->4\n(C) 1->2->3->4\n(D) 2->3->1->4\n", "candidates": ["3->1->4->2", "3->1->2->4", "1->2->3->4", "2->3->1->4"], "answer": "B", "task_type": "order", "question_id": "Q1989"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9765ddf144b6c1ed1b4e2f22a3fda6d2fe285b44247d352b6e16e83e5b382948", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The mix is poured into cupcake liners; (2)A cake with a Hershey shape is placed on a white plate; (3)Eggs, flour, and other ingredients are mixed in a bowl; (4)The cake is cut into a piece and served on a white plate.\n(A) 3->1->4->2\n(B) 3->1->2->4\n(C) 1->2->3->4\n(D) 2->3->1->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The mix is poured into cupcake liners; (2)A cake with a Hershey shape is placed on a white plate; (3)Eggs, flour, and other ingredients are mixed in a bowl; (4)The cake is cut into a piece and served on a white plate.\n(A) 3->1->4->2\n(B) 3->1->2->4\n(C) 1->2->3->4\n(D) 2->3->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1993, "doc": {"video_name": "order_157.mp4", "duration": 835.17, "question": "Arrange the following events from the video in the correct chronological order: (1)The mix is poured into cupcake liners; (2)A cake with a Hershey shape is placed on a white plate; (3)Eggs, flour, and other ingredients are mixed in a bowl; (4)The cake is cut into a piece and served on a white plate.\n(A) 3->1->4->2\n(B) 1->2->3->4\n(C) 3->1->2->4\n(D) 2->3->1->4\n", "candidates": ["3->1->4->2", "1->2->3->4", "3->1->2->4", "2->3->1->4"], "answer": "C", "task_type": "order", "question_id": "Q1993"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b1f260a02ab826134319fc174728d68694331275cd7124c18a4f6899f13ef72", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The mix is poured into cupcake liners; (2)A cake with a Hershey shape is placed on a white plate; (3)Eggs, flour, and other ingredients are mixed in a bowl; (4)The cake is cut into a piece and served on a white plate.\n(A) 3->1->4->2\n(B) 1->2->3->4\n(C) 3->1->2->4\n(D) 2->3->1->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The mix is poured into cupcake liners; (2)A cake with a Hershey shape is placed on a white plate; (3)Eggs, flour, and other ingredients are mixed in a bowl; (4)The cake is cut into a piece and served on a white plate.\n(A) 3->1->4->2\n(B) 1->2->3->4\n(C) 3->1->2->4\n(D) 2->3->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1997, "doc": {"video_name": "order_62.mp4", "duration": 1102.12, "question": "Arrange the following events from the video in the correct chronological order: (1)The chef pours soy sauce into the cup; (2)The chef shows off shredded garlic before throwing it into the cup; (3)The chef grabs a bowl of salad and shows it off; (4)The chef grabs a cup of nuts and throws it on top of a salad.\n(A) 3->4->1->2\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "candidates": ["3->4->1->2", "4->3->2->1", "1->2->3->4", "2->1->3->4"], "answer": "A", "task_type": "order", "question_id": "Q1997"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "628396cb7f8ddb2409d225cfe6f3773e534cb6d005d77cd4d241737a6d00d353", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The chef pours soy sauce into the cup; (2)The chef shows off shredded garlic before throwing it into the cup; (3)The chef grabs a bowl of salad and shows it off; (4)The chef grabs a cup of nuts and throws it on top of a salad.\n(A) 3->4->1->2\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The chef pours soy sauce into the cup; (2)The chef shows off shredded garlic before throwing it into the cup; (3)The chef grabs a bowl of salad and shows it off; (4)The chef grabs a cup of nuts and throws it on top of a salad.\n(A) 3->4->1->2\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2001, "doc": {"video_name": "order_164.mp4", "duration": 5936.1900000000005, "question": "Arrange the following events from the video in the correct chronological order: (1) The slack line athletes finish competing and shake hands at the finish; (2) A bald headed man performs tricks on a yellow trick line; (3) Two men meet to compete in a slack line competition; (4) The camera pans back to the bald man performing trick on the slack line and back to the man wearing a baseball cap performing tricks on the traditional line.\n(A) 4->3->2->1\n(B) 2->3->1->4\n(C) 3->2->4->1\n(D) 1->2->3->4\n", "candidates": ["4->3->2->1", "2->3->1->4", "3->2->4->1", "1->2->3->4"], "answer": "C", "task_type": "order", "question_id": "Q2001"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "61c2c0b14d8495e51942646aff98bd2d67c3088355fc84155ad4cdc9ef623e1e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) The slack line athletes finish competing and shake hands at the finish; (2) A bald headed man performs tricks on a yellow trick line; (3) Two men meet to compete in a slack line competition; (4) The camera pans back to the bald man performing trick on the slack line and back to the man wearing a baseball cap performing tricks on the traditional line.\n(A) 4->3->2->1\n(B) 2->3->1->4\n(C) 3->2->4->1\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1) The slack line athletes finish competing and shake hands at the finish; (2) A bald headed man performs tricks on a yellow trick line; (3) Two men meet to compete in a slack line competition; (4) The camera pans back to the bald man performing trick on the slack line and back to the man wearing a baseball cap performing tricks on the traditional line.\n(A) 4->3->2->1\n(B) 2->3->1->4\n(C) 3->2->4->1\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2005, "doc": {"video_name": "order_315.mp4", "duration": 6428.970000000001, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> zumba --> abseiling --> javelin throw\n(B) playing trombone --> abseiling --> zumba --> javelin throw\n(C) abseiling --> javelin throw --> zumba --> playing trombone\n(D) playing trombone --> javelin throw --> abseiling --> zumba\n", "candidates": ["playing trombone --> zumba --> abseiling --> javelin throw", "playing trombone --> abseiling --> zumba --> javelin throw", "abseiling --> javelin throw --> zumba --> playing trombone", "playing trombone --> javelin throw --> abseiling --> zumba"], "answer": "C", "task_type": "order", "question_id": "Q2005"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "81c4e0a647053b6c7630b701317c1961fbd9cab87fce530e1e82a8b025cb3c6b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> zumba --> abseiling --> javelin throw\n(B) playing trombone --> abseiling --> zumba --> javelin throw\n(C) abseiling --> javelin throw --> zumba --> playing trombone\n(D) playing trombone --> javelin throw --> abseiling --> zumba\n", "task_type": "order", "pred_answer": "D", "answer": "C"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> zumba --> abseiling --> javelin throw\n(B) playing trombone --> abseiling --> zumba --> javelin throw\n(C) abseiling --> javelin throw --> zumba --> playing trombone\n(D) playing trombone --> javelin throw --> abseiling --> zumba\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2009, "doc": {"video_name": "order_234.mp4", "duration": 520.02, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) stomping grapes --> javelin throw --> clean and jerk --> milking cow\n(B) clean and jerk --> milking cow --> javelin throw --> stomping grapes\n(C) milking cow --> javelin throw --> clean and jerk --> stomping grapes\n(D) clean and jerk --> javelin throw --> milking cow --> stomping grapes\n", "candidates": ["stomping grapes --> javelin throw --> clean and jerk --> milking cow", "clean and jerk --> milking cow --> javelin throw --> stomping grapes", "milking cow --> javelin throw --> clean and jerk --> stomping grapes", "clean and jerk --> javelin throw --> milking cow --> stomping grapes"], "answer": "A", "task_type": "order", "question_id": "Q2009"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d68f4915df11882e86f463e97c76db5fd05cc5169d2574811a314fc58ed0d723", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) stomping grapes --> javelin throw --> clean and jerk --> milking cow\n(B) clean and jerk --> milking cow --> javelin throw --> stomping grapes\n(C) milking cow --> javelin throw --> clean and jerk --> stomping grapes\n(D) clean and jerk --> javelin throw --> milking cow --> stomping grapes\n", "task_type": "order", "pred_answer": "D", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) stomping grapes --> javelin throw --> clean and jerk --> milking cow\n(B) clean and jerk --> milking cow --> javelin throw --> stomping grapes\n(C) milking cow --> javelin throw --> clean and jerk --> stomping grapes\n(D) clean and jerk --> javelin throw --> milking cow --> stomping grapes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2013, "doc": {"video_name": "order_216.mp4", "duration": 490.03, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> baking cookies --> pole vault --> jetskiing\n(B) baking cookies --> jetskiing --> pole vault --> riding mule\n(C) jetskiing --> baking cookies --> pole vault --> riding mule\n(D) riding mule --> pole vault --> baking cookies --> jetskiing\n", "candidates": ["riding mule --> baking cookies --> pole vault --> jetskiing", "baking cookies --> jetskiing --> pole vault --> riding mule", "jetskiing --> baking cookies --> pole vault --> riding mule", "riding mule --> pole vault --> baking cookies --> jetskiing"], "answer": "C", "task_type": "order", "question_id": "Q2013"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d2017878b25bf8bacd3b030cafc8462365779d0156c5dcbc9416b699e5e0cb2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> baking cookies --> pole vault --> jetskiing\n(B) baking cookies --> jetskiing --> pole vault --> riding mule\n(C) jetskiing --> baking cookies --> pole vault --> riding mule\n(D) riding mule --> pole vault --> baking cookies --> jetskiing\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> baking cookies --> pole vault --> jetskiing\n(B) baking cookies --> jetskiing --> pole vault --> riding mule\n(C) jetskiing --> baking cookies --> pole vault --> riding mule\n(D) riding mule --> pole vault --> baking cookies --> jetskiing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2017, "doc": {"video_name": "order_160.mp4", "duration": 8198.619999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)Woman begins ripping the wrapping paper with her hands; (2)Woman sets the box on top of the wrapping paper and begins wrapping the box; (3)Woman lifts up a box and sets it on the table; (4)Woman grabs a pair of scissors and tape.\n(A) 3->4->1->2\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "candidates": ["3->4->1->2", "4->3->2->1", "1->2->3->4", "2->1->3->4"], "answer": "A", "task_type": "order", "question_id": "Q2017"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d6fb07e2498f7c3d9f316e987acb9f3406c583cb6676fc77f240f34d79fc556d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Woman begins ripping the wrapping paper with her hands; (2)Woman sets the box on top of the wrapping paper and begins wrapping the box; (3)Woman lifts up a box and sets it on the table; (4)Woman grabs a pair of scissors and tape.\n(A) 3->4->1->2\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Woman begins ripping the wrapping paper with her hands; (2)Woman sets the box on top of the wrapping paper and begins wrapping the box; (3)Woman lifts up a box and sets it on the table; (4)Woman grabs a pair of scissors and tape.\n(A) 3->4->1->2\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2021, "doc": {"video_name": "order_311.mp4", "duration": 516.06, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> javelin throw --> cleaning toilet --> carving pumpkin\n(B) cleaning toilet --> cooking sausages --> javelin throw --> carving pumpkin\n(C) cooking sausages --> cleaning toilet --> javelin throw --> carving pumpkin\n(D) cleaning toilet --> javelin throw --> cooking sausages --> carving pumpkin\n", "candidates": ["cooking sausages --> javelin throw --> cleaning toilet --> carving pumpkin", "cleaning toilet --> cooking sausages --> javelin throw --> carving pumpkin", "cooking sausages --> cleaning toilet --> javelin throw --> carving pumpkin", "cleaning toilet --> javelin throw --> cooking sausages --> carving pumpkin"], "answer": "D", "task_type": "order", "question_id": "Q2021"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "329ff887a68d58d342b043710ffc4cabb2c9daddbbe85dce4a76011b6a5763e9", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> javelin throw --> cleaning toilet --> carving pumpkin\n(B) cleaning toilet --> cooking sausages --> javelin throw --> carving pumpkin\n(C) cooking sausages --> cleaning toilet --> javelin throw --> carving pumpkin\n(D) cleaning toilet --> javelin throw --> cooking sausages --> carving pumpkin\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> javelin throw --> cleaning toilet --> carving pumpkin\n(B) cleaning toilet --> cooking sausages --> javelin throw --> carving pumpkin\n(C) cooking sausages --> cleaning toilet --> javelin throw --> carving pumpkin\n(D) cleaning toilet --> javelin throw --> cooking sausages --> carving pumpkin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2025, "doc": {"video_name": "order_95.mp4", "duration": 677.6, "question": "Arrange the following events from the video in the correct chronological order: (1) The marching band aligns in the street with their instruments; (2) A man passes in front of the marching band holding a camera; (3) The marching band performs in a field and in a gym, moving around while playing; (4) The marching band performs in front of a building and other places.\n(A) 2->4->3->1\n(B) 4->2->1->3\n(C) 1->3->2->4\n(D) 3->1->4->2\n", "candidates": ["2->4->3->1", "4->2->1->3", "1->3->2->4", "3->1->4->2"], "answer": "D", "task_type": "order", "question_id": "Q2025"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "18c1caaf5735e7ac7c932b76bb631264d2edc6ca88fb24f696af68321c7f8b07", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) The marching band aligns in the street with their instruments; (2) A man passes in front of the marching band holding a camera; (3) The marching band performs in a field and in a gym, moving around while playing; (4) The marching band performs in front of a building and other places.\n(A) 2->4->3->1\n(B) 4->2->1->3\n(C) 1->3->2->4\n(D) 3->1->4->2\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1) The marching band aligns in the street with their instruments; (2) A man passes in front of the marching band holding a camera; (3) The marching band performs in a field and in a gym, moving around while playing; (4) The marching band performs in front of a building and other places.\n(A) 2->4->3->1\n(B) 4->2->1->3\n(C) 1->3->2->4\n(D) 3->1->4->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2029, "doc": {"video_name": "order_93.mp4", "duration": 1723.73, "question": "Arrange the following events from the video in the correct chronological order: (1) The marching band aligns in the street with their instruments; (2) A man passes in front of the marching band holding a camera; (3) The marching band performs in a field and in a gym, moving around while playing; (4) The marching band performs in front of a building and other places.\n(A) 4->2->1->3\n(B) 2->4->3->1\n(C) 1->3->2->4\n(D) 3->1->4->2\n", "candidates": ["4->2->1->3", "2->4->3->1", "1->3->2->4", "3->1->4->2"], "answer": "D", "task_type": "order", "question_id": "Q2029"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "08680bbd878691147d095ab16f296375a1349f2d26d4d7ac4bb3339a63fbff99", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) The marching band aligns in the street with their instruments; (2) A man passes in front of the marching band holding a camera; (3) The marching band performs in a field and in a gym, moving around while playing; (4) The marching band performs in front of a building and other places.\n(A) 4->2->1->3\n(B) 2->4->3->1\n(C) 1->3->2->4\n(D) 3->1->4->2\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1) The marching band aligns in the street with their instruments; (2) A man passes in front of the marching band holding a camera; (3) The marching band performs in a field and in a gym, moving around while playing; (4) The marching band performs in front of a building and other places.\n(A) 4->2->1->3\n(B) 2->4->3->1\n(C) 1->3->2->4\n(D) 3->1->4->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2033, "doc": {"video_name": "order_134.mp4", "duration": 688.49, "question": "Arrange the following events from the video in the correct chronological order: (1)The man starts to demonstrate playing the bongos in a lesson; (2)LP and Giovanni Logo appear on the black screen opening; (3)The lesson continues, alternating between color and black and white footage; (4)A man sits behind a set of bongo drums.\n(A) 4->2->1->3\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 2->4->1->3\n", "candidates": ["4->2->1->3", "2->1->4->3", "1->2->3->4", "2->4->1->3"], "answer": "D", "task_type": "order", "question_id": "Q2033"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b6376d2b59ab8d6fcef7284c2290255df15f2c9179d03c5f42b3c9d5d15b4103", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man starts to demonstrate playing the bongos in a lesson; (2)LP and Giovanni Logo appear on the black screen opening; (3)The lesson continues, alternating between color and black and white footage; (4)A man sits behind a set of bongo drums.\n(A) 4->2->1->3\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 2->4->1->3\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man starts to demonstrate playing the bongos in a lesson; (2)LP and Giovanni Logo appear on the black screen opening; (3)The lesson continues, alternating between color and black and white footage; (4)A man sits behind a set of bongo drums.\n(A) 4->2->1->3\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 2->4->1->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2037, "doc": {"video_name": "order_204.mp4", "duration": 497.04999999999995, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) abseiling --> cleaning toilet --> jetskiing --> clean and jerk\n(B) jetskiing --> cleaning toilet --> clean and jerk --> abseiling\n(C) clean and jerk --> jetskiing --> abseiling --> cleaning toilet\n(D) cleaning toilet --> jetskiing --> abseiling --> clean and jerk\n", "candidates": ["abseiling --> cleaning toilet --> jetskiing --> clean and jerk", "jetskiing --> cleaning toilet --> clean and jerk --> abseiling", "clean and jerk --> jetskiing --> abseiling --> cleaning toilet", "cleaning toilet --> jetskiing --> abseiling --> clean and jerk"], "answer": "D", "task_type": "order", "question_id": "Q2037"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b519827cd09bf2c7b575c7dd987cd01b7270d57af5d478795356de8edbb7fe68", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) abseiling --> cleaning toilet --> jetskiing --> clean and jerk\n(B) jetskiing --> cleaning toilet --> clean and jerk --> abseiling\n(C) clean and jerk --> jetskiing --> abseiling --> cleaning toilet\n(D) cleaning toilet --> jetskiing --> abseiling --> clean and jerk\n", "task_type": "order", "pred_answer": "A", "answer": "D"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) abseiling --> cleaning toilet --> jetskiing --> clean and jerk\n(B) jetskiing --> cleaning toilet --> clean and jerk --> abseiling\n(C) clean and jerk --> jetskiing --> abseiling --> cleaning toilet\n(D) cleaning toilet --> jetskiing --> abseiling --> clean and jerk\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2041, "doc": {"video_name": "order_285.mp4", "duration": 455.04999999999995, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) shredding paper --> paragliding --> making jewelry --> carving pumpkin\n(B) paragliding --> making jewelry --> shredding paper --> carving pumpkin\n(C) shredding paper --> carving pumpkin --> making jewelry --> paragliding\n(D) making jewelry --> paragliding --> carving pumpkin --> shredding paper\n", "candidates": ["shredding paper --> paragliding --> making jewelry --> carving pumpkin", "paragliding --> making jewelry --> shredding paper --> carving pumpkin", "shredding paper --> carving pumpkin --> making jewelry --> paragliding", "making jewelry --> paragliding --> carving pumpkin --> shredding paper"], "answer": "A", "task_type": "order", "question_id": "Q2041"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a82885b69a9bc096bbf001ac92c44a669a3aa6461b914593aa63dfd5d30007f6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) shredding paper --> paragliding --> making jewelry --> carving pumpkin\n(B) paragliding --> making jewelry --> shredding paper --> carving pumpkin\n(C) shredding paper --> carving pumpkin --> making jewelry --> paragliding\n(D) making jewelry --> paragliding --> carving pumpkin --> shredding paper\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) shredding paper --> paragliding --> making jewelry --> carving pumpkin\n(B) paragliding --> making jewelry --> shredding paper --> carving pumpkin\n(C) shredding paper --> carving pumpkin --> making jewelry --> paragliding\n(D) making jewelry --> paragliding --> carving pumpkin --> shredding paper\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2045, "doc": {"video_name": "order_130.mp4", "duration": 684.1, "question": "Arrange the following events from the video in the correct chronological order: (1)The blue and gray guy are on a seesaw and the blue guy jumps at the gray guy; (2)A man's image as he talks is imposed over trees and the man make gestures towards his mouth; (3)Both of the characters fall off the map; (4)We see a game of rock em sock em style robots on a computer screen cut with images of the man narrating in the lower left corner.\n(A) 4->2->1->3\n(B) 1->2->3->4\n(C) 2->4->1->3\n(D) 2->1->4->3\n", "candidates": ["4->2->1->3", "1->2->3->4", "2->4->1->3", "2->1->4->3"], "answer": "C", "task_type": "order", "question_id": "Q2045"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a55f14dfddbc565e5854639f892720b5c3a6dd01f44e16b59a4a0f908410fc7b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The blue and gray guy are on a seesaw and the blue guy jumps at the gray guy; (2)A man's image as he talks is imposed over trees and the man make gestures towards his mouth; (3)Both of the characters fall off the map; (4)We see a game of rock em sock em style robots on a computer screen cut with images of the man narrating in the lower left corner.\n(A) 4->2->1->3\n(B) 1->2->3->4\n(C) 2->4->1->3\n(D) 2->1->4->3\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The blue and gray guy are on a seesaw and the blue guy jumps at the gray guy; (2)A man's image as he talks is imposed over trees and the man make gestures towards his mouth; (3)Both of the characters fall off the map; (4)We see a game of rock em sock em style robots on a computer screen cut with images of the man narrating in the lower left corner.\n(A) 4->2->1->3\n(B) 1->2->3->4\n(C) 2->4->1->3\n(D) 2->1->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2049, "doc": {"video_name": "order_50.mp4", "duration": 707.3999999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)The woman starts working on her nails using bottles from a box next to her; (2)The words \"Love Food & Money with Angie Greenup\" appears on screen; (3)Her twitter handle and subscribe screen are shown while she holds her dogs; (4)The woman speaks to the camera from her living room while her dogs play fight behind her.\n(A) 2->4->1->3\n(B) 2->1->4->3\n(C) 4->2->1->3\n(D) 1->2->4->3\n", "candidates": ["2->4->1->3", "2->1->4->3", "4->2->1->3", "1->2->4->3"], "answer": "A", "task_type": "order", "question_id": "Q2049"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ab313748899f7df256f77e4de554c32a0183ba7cae4d5d789be97aa9dd6e532e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The woman starts working on her nails using bottles from a box next to her; (2)The words \"Love Food & Money with Angie Greenup\" appears on screen; (3)Her twitter handle and subscribe screen are shown while she holds her dogs; (4)The woman speaks to the camera from her living room while her dogs play fight behind her.\n(A) 2->4->1->3\n(B) 2->1->4->3\n(C) 4->2->1->3\n(D) 1->2->4->3\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The woman starts working on her nails using bottles from a box next to her; (2)The words \"Love Food & Money with Angie Greenup\" appears on screen; (3)Her twitter handle and subscribe screen are shown while she holds her dogs; (4)The woman speaks to the camera from her living room while her dogs play fight behind her.\n(A) 2->4->1->3\n(B) 2->1->4->3\n(C) 4->2->1->3\n(D) 1->2->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2053, "doc": {"video_name": "order_347.mp4", "duration": 490.02, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) making jewelry --> abseiling --> milking cow --> cooking sausages\n(B) cooking sausages --> abseiling --> milking cow --> making jewelry\n(C) milking cow --> abseiling --> making jewelry --> cooking sausages\n(D) abseiling --> milking cow --> cooking sausages --> making jewelry\n", "candidates": ["making jewelry --> abseiling --> milking cow --> cooking sausages", "cooking sausages --> abseiling --> milking cow --> making jewelry", "milking cow --> abseiling --> making jewelry --> cooking sausages", "abseiling --> milking cow --> cooking sausages --> making jewelry"], "answer": "A", "task_type": "order", "question_id": "Q2053"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a925115fb62ff64f245df1727ecebca7e2f922c07c7750600805c2d58758dd0a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) making jewelry --> abseiling --> milking cow --> cooking sausages\n(B) cooking sausages --> abseiling --> milking cow --> making jewelry\n(C) milking cow --> abseiling --> making jewelry --> cooking sausages\n(D) abseiling --> milking cow --> cooking sausages --> making jewelry\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) making jewelry --> abseiling --> milking cow --> cooking sausages\n(B) cooking sausages --> abseiling --> milking cow --> making jewelry\n(C) milking cow --> abseiling --> making jewelry --> cooking sausages\n(D) abseiling --> milking cow --> cooking sausages --> making jewelry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2057, "doc": {"video_name": "order_273.mp4", "duration": 490.02, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) baking cookies --> jetskiing --> water sliding --> paragliding\n(B) water sliding --> baking cookies --> jetskiing --> paragliding\n(C) baking cookies --> water sliding --> jetskiing --> paragliding\n(D) jetskiing --> water sliding --> paragliding --> baking cookies\n", "candidates": ["baking cookies --> jetskiing --> water sliding --> paragliding", "water sliding --> baking cookies --> jetskiing --> paragliding", "baking cookies --> water sliding --> jetskiing --> paragliding", "jetskiing --> water sliding --> paragliding --> baking cookies"], "answer": "B", "task_type": "order", "question_id": "Q2057"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "25f052130a7ea1c17919e8001e94bbe62e77897ba362c6f08acd8eb8262dc152", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) baking cookies --> jetskiing --> water sliding --> paragliding\n(B) water sliding --> baking cookies --> jetskiing --> paragliding\n(C) baking cookies --> water sliding --> jetskiing --> paragliding\n(D) jetskiing --> water sliding --> paragliding --> baking cookies\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) baking cookies --> jetskiing --> water sliding --> paragliding\n(B) water sliding --> baking cookies --> jetskiing --> paragliding\n(C) baking cookies --> water sliding --> jetskiing --> paragliding\n(D) jetskiing --> water sliding --> paragliding --> baking cookies\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2061, "doc": {"video_name": "order_87.mp4", "duration": 675.14, "question": "Arrange the following events from the video in the correct chronological order: (1) The slack line athletes finish competing and shake hands at the finish; (2) A bald headed man performs tricks on a yellow trick line; (3) Two men meet to compete in a slack line competition; (4) The camera pans back to the bald man performing trick on the slack line and back to the man wearing a baseball cap performing tricks on the traditional line.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->4->1\n(D) 2->3->1->4\n", "candidates": ["4->3->2->1", "1->2->3->4", "3->2->4->1", "2->3->1->4"], "answer": "C", "task_type": "order", "question_id": "Q2061"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1d582319d8f78a8a03340b372a22ada5704e695ecd00d6aba78754ff3117201c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) The slack line athletes finish competing and shake hands at the finish; (2) A bald headed man performs tricks on a yellow trick line; (3) Two men meet to compete in a slack line competition; (4) The camera pans back to the bald man performing trick on the slack line and back to the man wearing a baseball cap performing tricks on the traditional line.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->4->1\n(D) 2->3->1->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1) The slack line athletes finish competing and shake hands at the finish; (2) A bald headed man performs tricks on a yellow trick line; (3) Two men meet to compete in a slack line competition; (4) The camera pans back to the bald man performing trick on the slack line and back to the man wearing a baseball cap performing tricks on the traditional line.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->4->1\n(D) 2->3->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2065, "doc": {"video_name": "order_280.mp4", "duration": 490.16999999999996, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> cooking sausages --> baking cookies --> playing trombone\n(B) cooking sausages --> tossing coin --> baking cookies --> playing trombone\n(C) baking cookies --> tossing coin --> playing trombone --> cooking sausages\n(D) baking cookies --> tossing coin --> cooking sausages --> playing trombone\n", "candidates": ["tossing coin --> cooking sausages --> baking cookies --> playing trombone", "cooking sausages --> tossing coin --> baking cookies --> playing trombone", "baking cookies --> tossing coin --> playing trombone --> cooking sausages", "baking cookies --> tossing coin --> cooking sausages --> playing trombone"], "answer": "C", "task_type": "order", "question_id": "Q2065"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0900480b15769a7577ee8dc308a8e1e2339468982d00b87836d2c0849773d814", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> cooking sausages --> baking cookies --> playing trombone\n(B) cooking sausages --> tossing coin --> baking cookies --> playing trombone\n(C) baking cookies --> tossing coin --> playing trombone --> cooking sausages\n(D) baking cookies --> tossing coin --> cooking sausages --> playing trombone\n", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> cooking sausages --> baking cookies --> playing trombone\n(B) cooking sausages --> tossing coin --> baking cookies --> playing trombone\n(C) baking cookies --> tossing coin --> playing trombone --> cooking sausages\n(D) baking cookies --> tossing coin --> cooking sausages --> playing trombone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2069, "doc": {"video_name": "order_83.mp4", "duration": 673.5799999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)The instructor finishes the class; (2)A woman's indoor aerobic class is in process; (3)The logo 'Zumba Toning' appears on screen; (4)The camera briefly scans to the mirrored wall and then back to the class.\n(A) 2->1->3->4\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 3->2->4->1\n", "candidates": ["2->1->3->4", "4->3->2->1", "1->2->3->4", "3->2->4->1"], "answer": "D", "task_type": "order", "question_id": "Q2069"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9dd107e498229e1df0f1f5c56f76248d6286d5b37b4a873383ba66aa210391cd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The instructor finishes the class; (2)A woman's indoor aerobic class is in process; (3)The logo 'Zumba Toning' appears on screen; (4)The camera briefly scans to the mirrored wall and then back to the class.\n(A) 2->1->3->4\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 3->2->4->1\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The instructor finishes the class; (2)A woman's indoor aerobic class is in process; (3)The logo 'Zumba Toning' appears on screen; (4)The camera briefly scans to the mirrored wall and then back to the class.\n(A) 2->1->3->4\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 3->2->4->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2073, "doc": {"video_name": "order_272.mp4", "duration": 488.48, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) tossing coin --> jetskiing --> javelin throw --> carving pumpkin\n(B) jetskiing --> javelin throw --> tossing coin --> carving pumpkin\n(C) tossing coin --> jetskiing --> carving pumpkin --> javelin throw\n(D) carving pumpkin --> javelin throw --> tossing coin --> jetskiing\n", "candidates": ["tossing coin --> jetskiing --> javelin throw --> carving pumpkin", "jetskiing --> javelin throw --> tossing coin --> carving pumpkin", "tossing coin --> jetskiing --> carving pumpkin --> javelin throw", "carving pumpkin --> javelin throw --> tossing coin --> jetskiing"], "answer": "B", "task_type": "order", "question_id": "Q2073"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7299503d55489ee63a22e07187f4f8bc933d3e316fa953a66c8f9b27d8b23864", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) tossing coin --> jetskiing --> javelin throw --> carving pumpkin\n(B) jetskiing --> javelin throw --> tossing coin --> carving pumpkin\n(C) tossing coin --> jetskiing --> carving pumpkin --> javelin throw\n(D) carving pumpkin --> javelin throw --> tossing coin --> jetskiing\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) tossing coin --> jetskiing --> javelin throw --> carving pumpkin\n(B) jetskiing --> javelin throw --> tossing coin --> carving pumpkin\n(C) tossing coin --> jetskiing --> carving pumpkin --> javelin throw\n(D) carving pumpkin --> javelin throw --> tossing coin --> jetskiing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2077, "doc": {"video_name": "order_6.mp4", "duration": 711.8000000000001, "question": "Arrange the following events from the video in the correct chronological order: (1)A guy approaches a weight on a stage; (2)A man massages a guy's shoulders; (3)A guy lifts a weight on a stage and releases it; (4)A guy kisses the weight plates.\n(A) 1->3->2->4\n(B) 2->1->3->4\n(C) 2->3->1->4\n(D) 1->2->3->4\n", "candidates": ["1->3->2->4", "2->1->3->4", "2->3->1->4", "1->2->3->4"], "answer": "D", "task_type": "order", "question_id": "Q2077"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "13cc320df6807e0d66d0dc17105d92bf1bd653894de0e5ed39f4caf1d68170dd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A guy approaches a weight on a stage; (2)A man massages a guy's shoulders; (3)A guy lifts a weight on a stage and releases it; (4)A guy kisses the weight plates.\n(A) 1->3->2->4\n(B) 2->1->3->4\n(C) 2->3->1->4\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A guy approaches a weight on a stage; (2)A man massages a guy's shoulders; (3)A guy lifts a weight on a stage and releases it; (4)A guy kisses the weight plates.\n(A) 1->3->2->4\n(B) 2->1->3->4\n(C) 2->3->1->4\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2081, "doc": {"video_name": "order_261.mp4", "duration": 395.73999999999995, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) water sliding --> making jewelry --> paragliding --> playing trombone\n(B) making jewelry --> paragliding --> water sliding --> playing trombone\n(C) making jewelry --> paragliding --> playing trombone --> water sliding\n(D) paragliding --> water sliding --> making jewelry --> playing trombone\n", "candidates": ["water sliding --> making jewelry --> paragliding --> playing trombone", "making jewelry --> paragliding --> water sliding --> playing trombone", "making jewelry --> paragliding --> playing trombone --> water sliding", "paragliding --> water sliding --> making jewelry --> playing trombone"], "answer": "D", "task_type": "order", "question_id": "Q2081"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "77b69fb62175c7df79f79aee829d58b788407c6dc920099ea13722ead7879457", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) water sliding --> making jewelry --> paragliding --> playing trombone\n(B) making jewelry --> paragliding --> water sliding --> playing trombone\n(C) making jewelry --> paragliding --> playing trombone --> water sliding\n(D) paragliding --> water sliding --> making jewelry --> playing trombone\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) water sliding --> making jewelry --> paragliding --> playing trombone\n(B) making jewelry --> paragliding --> water sliding --> playing trombone\n(C) making jewelry --> paragliding --> playing trombone --> water sliding\n(D) paragliding --> water sliding --> making jewelry --> playing trombone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2085, "doc": {"video_name": "order_218.mp4", "duration": 245.11, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) javelin throw --> clean and jerk --> cooking sausages --> carving pumpkin\n(B) carving pumpkin --> clean and jerk --> javelin throw --> cooking sausages\n(C) cooking sausages --> javelin throw --> clean and jerk --> carving pumpkin\n(D) javelin throw --> carving pumpkin --> clean and jerk --> cooking sausages\n", "candidates": ["javelin throw --> clean and jerk --> cooking sausages --> carving pumpkin", "carving pumpkin --> clean and jerk --> javelin throw --> cooking sausages", "cooking sausages --> javelin throw --> clean and jerk --> carving pumpkin", "javelin throw --> carving pumpkin --> clean and jerk --> cooking sausages"], "answer": "B", "task_type": "order", "question_id": "Q2085"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1706f1a00b0ccfef82341667f33874a6f52e51b3c30ec267af3e11566132259e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) javelin throw --> clean and jerk --> cooking sausages --> carving pumpkin\n(B) carving pumpkin --> clean and jerk --> javelin throw --> cooking sausages\n(C) cooking sausages --> javelin throw --> clean and jerk --> carving pumpkin\n(D) javelin throw --> carving pumpkin --> clean and jerk --> cooking sausages\n", "task_type": "order", "pred_answer": "A", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) javelin throw --> clean and jerk --> cooking sausages --> carving pumpkin\n(B) carving pumpkin --> clean and jerk --> javelin throw --> cooking sausages\n(C) cooking sausages --> javelin throw --> clean and jerk --> carving pumpkin\n(D) javelin throw --> carving pumpkin --> clean and jerk --> cooking sausages\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2089, "doc": {"video_name": "order_42.mp4", "duration": 677.76, "question": "Arrange the following events from the video in the correct chronological order: (1)The bowler from the blue team hits an overhand ball to the batter; (2)The bowler raises his hand to claim that the batter has not made a run; (3)The bowler causes the batter to get out by hitting the stumps behind him, the entire team cheers; (4)The batter walks out and another batter from his team comes on the field.\n(A) 1->2->3->4\n(B) 3->2->1->4\n(C) 2->1->3->4\n(D) 4->3->2->1\n", "candidates": ["1->2->3->4", "3->2->1->4", "2->1->3->4", "4->3->2->1"], "answer": "A", "task_type": "order", "question_id": "Q2089"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4ea25a152e0efe0b88c30a8539456fe728910d03a8faad8c600afa8a3678c6ab", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The bowler from the blue team hits an overhand ball to the batter; (2)The bowler raises his hand to claim that the batter has not made a run; (3)The bowler causes the batter to get out by hitting the stumps behind him, the entire team cheers; (4)The batter walks out and another batter from his team comes on the field.\n(A) 1->2->3->4\n(B) 3->2->1->4\n(C) 2->1->3->4\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The bowler from the blue team hits an overhand ball to the batter; (2)The bowler raises his hand to claim that the batter has not made a run; (3)The bowler causes the batter to get out by hitting the stumps behind him, the entire team cheers; (4)The batter walks out and another batter from his team comes on the field.\n(A) 1->2->3->4\n(B) 3->2->1->4\n(C) 2->1->3->4\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2093, "doc": {"video_name": "order_225.mp4", "duration": 782.8, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) riding mule --> milking cow --> jetskiing --> playing harp\n(B) playing harp --> riding mule --> jetskiing --> milking cow\n(C) playing harp --> jetskiing --> riding mule --> milking cow\n(D) milking cow --> playing harp --> riding mule --> jetskiing\n", "candidates": ["riding mule --> milking cow --> jetskiing --> playing harp", "playing harp --> riding mule --> jetskiing --> milking cow", "playing harp --> jetskiing --> riding mule --> milking cow", "milking cow --> playing harp --> riding mule --> jetskiing"], "answer": "D", "task_type": "order", "question_id": "Q2093"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a781164b11d7334fc146adf380b05844531122f3adcec52b0ecc712fcce5a9ad", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) riding mule --> milking cow --> jetskiing --> playing harp\n(B) playing harp --> riding mule --> jetskiing --> milking cow\n(C) playing harp --> jetskiing --> riding mule --> milking cow\n(D) milking cow --> playing harp --> riding mule --> jetskiing\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) riding mule --> milking cow --> jetskiing --> playing harp\n(B) playing harp --> riding mule --> jetskiing --> milking cow\n(C) playing harp --> jetskiing --> riding mule --> milking cow\n(D) milking cow --> playing harp --> riding mule --> jetskiing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2097, "doc": {"video_name": "order_217.mp4", "duration": 485.42, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) jetskiing --> stomping grapes --> playing trombone --> water sliding\n(B) water sliding --> jetskiing --> playing trombone --> stomping grapes\n(C) stomping grapes --> playing trombone --> water sliding --> jetskiing\n(D) playing trombone --> water sliding --> jetskiing --> stomping grapes\n", "candidates": ["jetskiing --> stomping grapes --> playing trombone --> water sliding", "water sliding --> jetskiing --> playing trombone --> stomping grapes", "stomping grapes --> playing trombone --> water sliding --> jetskiing", "playing trombone --> water sliding --> jetskiing --> stomping grapes"], "answer": "C", "task_type": "order", "question_id": "Q2097"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "61f21e0e2e1ac9770cac646c029d0f6bb5ab59acf85d92fd26dc3a3f426a4a72", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) jetskiing --> stomping grapes --> playing trombone --> water sliding\n(B) water sliding --> jetskiing --> playing trombone --> stomping grapes\n(C) stomping grapes --> playing trombone --> water sliding --> jetskiing\n(D) playing trombone --> water sliding --> jetskiing --> stomping grapes\n", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) jetskiing --> stomping grapes --> playing trombone --> water sliding\n(B) water sliding --> jetskiing --> playing trombone --> stomping grapes\n(C) stomping grapes --> playing trombone --> water sliding --> jetskiing\n(D) playing trombone --> water sliding --> jetskiing --> stomping grapes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2101, "doc": {"video_name": "order_305.mp4", "duration": 575.0600000000001, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) stomping grapes --> paragliding --> milking cow --> shredding paper\n(B) paragliding --> shredding paper --> milking cow --> stomping grapes\n(C) stomping grapes --> shredding paper --> milking cow --> paragliding\n(D) milking cow --> stomping grapes --> paragliding --> shredding paper\n", "candidates": ["stomping grapes --> paragliding --> milking cow --> shredding paper", "paragliding --> shredding paper --> milking cow --> stomping grapes", "stomping grapes --> shredding paper --> milking cow --> paragliding", "milking cow --> stomping grapes --> paragliding --> shredding paper"], "answer": "A", "task_type": "order", "question_id": "Q2101"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9fb38b18c2cbb962d7dc9cce9770d3354f6c9a81020c641f66ae873f1b65b33b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) stomping grapes --> paragliding --> milking cow --> shredding paper\n(B) paragliding --> shredding paper --> milking cow --> stomping grapes\n(C) stomping grapes --> shredding paper --> milking cow --> paragliding\n(D) milking cow --> stomping grapes --> paragliding --> shredding paper\n", "task_type": "order", "pred_answer": "D", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) stomping grapes --> paragliding --> milking cow --> shredding paper\n(B) paragliding --> shredding paper --> milking cow --> stomping grapes\n(C) stomping grapes --> shredding paper --> milking cow --> paragliding\n(D) milking cow --> stomping grapes --> paragliding --> shredding paper\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2105, "doc": {"video_name": "order_118.mp4", "duration": 699.22, "question": "Arrange the following events from the video in the correct chronological order: (1)A white car drives by in the background; (2)A black car drives by in the background; (3)Two people walk by in the background; (4)The ball is kicked into the camera.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "3->2->1->4", "1->2->3->4", "2->1->3->4"], "answer": "C", "task_type": "order", "question_id": "Q2105"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d10e5fafeef3e922405c73b6237e791ed4c70250e7fbc3b9ad98daaca77cc92", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A white car drives by in the background; (2)A black car drives by in the background; (3)Two people walk by in the background; (4)The ball is kicked into the camera.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A white car drives by in the background; (2)A black car drives by in the background; (3)Two people walk by in the background; (4)The ball is kicked into the camera.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2109, "doc": {"video_name": "order_125.mp4", "duration": 825.7, "question": "Arrange the following events from the video in the correct chronological order: (1)The woman hangs the washed clothes on a line; (2)The woman fills a metal bucket with water; (3)The woman washes and scrubs clothes by hand; (4)The woman places a small wooden stool near a larger bucket.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 3->2->4->1\n(D) 2->4->3->1\n", "candidates": ["1->2->3->4", "4->3->2->1", "3->2->4->1", "2->4->3->1"], "answer": "D", "task_type": "order", "question_id": "Q2109"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cbf973d3521fafcdb59a1c7672f0e62bf947380c2098370ec8568b48eecd4e34", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The woman hangs the washed clothes on a line; (2)The woman fills a metal bucket with water; (3)The woman washes and scrubs clothes by hand; (4)The woman places a small wooden stool near a larger bucket.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 3->2->4->1\n(D) 2->4->3->1\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The woman hangs the washed clothes on a line; (2)The woman fills a metal bucket with water; (3)The woman washes and scrubs clothes by hand; (4)The woman places a small wooden stool near a larger bucket.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 3->2->4->1\n(D) 2->4->3->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2113, "doc": {"video_name": "order_279.mp4", "duration": 485.03, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) stomping grapes --> paragliding --> jetskiing --> playing trombone\n(B) paragliding --> jetskiing --> playing trombone --> stomping grapes\n(C) stomping grapes --> paragliding --> playing trombone --> jetskiing\n(D) playing trombone --> paragliding --> stomping grapes --> jetskiing\n", "candidates": ["stomping grapes --> paragliding --> jetskiing --> playing trombone", "paragliding --> jetskiing --> playing trombone --> stomping grapes", "stomping grapes --> paragliding --> playing trombone --> jetskiing", "playing trombone --> paragliding --> stomping grapes --> jetskiing"], "answer": "B", "task_type": "order", "question_id": "Q2113"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7b28537989a7f4d721ab6a3e535a1c00d1ef95a303a481b3ba4654fe9fd80669", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) stomping grapes --> paragliding --> jetskiing --> playing trombone\n(B) paragliding --> jetskiing --> playing trombone --> stomping grapes\n(C) stomping grapes --> paragliding --> playing trombone --> jetskiing\n(D) playing trombone --> paragliding --> stomping grapes --> jetskiing\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) stomping grapes --> paragliding --> jetskiing --> playing trombone\n(B) paragliding --> jetskiing --> playing trombone --> stomping grapes\n(C) stomping grapes --> paragliding --> playing trombone --> jetskiing\n(D) playing trombone --> paragliding --> stomping grapes --> jetskiing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2117, "doc": {"video_name": "order_106.mp4", "duration": 685.98, "question": "Arrange the following events from the video in the correct chronological order: (1)The camera focuses on an older man's face; (2)The two children dance together; (3)The camera focuses on a bug on the wall; (4)The two children interact with each other in a cluttered room.\n(A) 4->1->2->3\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 1->4->3->2\n", "candidates": ["4->1->2->3", "1->2->3->4", "4->3->2->1", "1->4->3->2"], "answer": "D", "task_type": "order", "question_id": "Q2117"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "22d957d24e53d30a9998f2eafc7745424355da7419cb606a8bd4345112fe841e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The camera focuses on an older man's face; (2)The two children dance together; (3)The camera focuses on a bug on the wall; (4)The two children interact with each other in a cluttered room.\n(A) 4->1->2->3\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 1->4->3->2\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The camera focuses on an older man's face; (2)The two children dance together; (3)The camera focuses on a bug on the wall; (4)The two children interact with each other in a cluttered room.\n(A) 4->1->2->3\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 1->4->3->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2121, "doc": {"video_name": "order_36.mp4", "duration": 703.61, "question": "Arrange the following events from the video in the correct chronological order: (1)The word BIKE is overlaid on a mountain scene; (2)Oregon daily emerald logo and title card pops up; (3)The instructions follow with a man in a white ensemble and purple hat; (4)REPAIR is then overlaid under BIKE, Becoming BIKE REPAIR.\n(A) 2->1->3->4\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 1->2->4->3\n", "candidates": ["2->1->3->4", "2->1->4->3", "1->2->3->4", "1->2->4->3"], "answer": "B", "task_type": "order", "question_id": "Q2121"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d75d732fab1d6222eb88c5153c61249fcb5b2b9a8202813d875c8e9fdf1a339", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The word BIKE is overlaid on a mountain scene; (2)Oregon daily emerald logo and title card pops up; (3)The instructions follow with a man in a white ensemble and purple hat; (4)REPAIR is then overlaid under BIKE, Becoming BIKE REPAIR.\n(A) 2->1->3->4\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 1->2->4->3\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The word BIKE is overlaid on a mountain scene; (2)Oregon daily emerald logo and title card pops up; (3)The instructions follow with a man in a white ensemble and purple hat; (4)REPAIR is then overlaid under BIKE, Becoming BIKE REPAIR.\n(A) 2->1->3->4\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 1->2->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2125, "doc": {"video_name": "order_33.mp4", "duration": 675.6500000000001, "question": "Arrange the following events from the video in the correct chronological order: (1)The man snowboards down a hill and turns around; (2)An old man holds a surfboard and puts on a helmet to snowboard; (3)A young person sits on the snow wearing a snowboard; (4)The man has a hot drink with other people.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 1->2->4->3\n(D) 2->1->4->3\n", "candidates": ["1->2->3->4", "2->1->3->4", "1->2->4->3", "2->1->4->3"], "answer": "D", "task_type": "order", "question_id": "Q2125"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d0df25da5034d8c7e180e709fe281763e668cad85e0c8386913ac7a456b1e362", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man snowboards down a hill and turns around; (2)An old man holds a surfboard and puts on a helmet to snowboard; (3)A young person sits on the snow wearing a snowboard; (4)The man has a hot drink with other people.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 1->2->4->3\n(D) 2->1->4->3\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man snowboards down a hill and turns around; (2)An old man holds a surfboard and puts on a helmet to snowboard; (3)A young person sits on the snow wearing a snowboard; (4)The man has a hot drink with other people.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 1->2->4->3\n(D) 2->1->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2129, "doc": {"video_name": "order_275.mp4", "duration": 483.82000000000005, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) abseiling --> baking cookies --> clean and jerk --> water sliding\n(B) water sliding --> clean and jerk --> baking cookies --> abseiling\n(C) clean and jerk --> abseiling --> baking cookies --> water sliding\n(D) abseiling --> water sliding --> clean and jerk --> baking cookies\n", "candidates": ["abseiling --> baking cookies --> clean and jerk --> water sliding", "water sliding --> clean and jerk --> baking cookies --> abseiling", "clean and jerk --> abseiling --> baking cookies --> water sliding", "abseiling --> water sliding --> clean and jerk --> baking cookies"], "answer": "A", "task_type": "order", "question_id": "Q2129"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1125bc4753ff7f4e7f09dfc8834255e4342fdba34aa79b3bb3d1fdbe0380bc15", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) abseiling --> baking cookies --> clean and jerk --> water sliding\n(B) water sliding --> clean and jerk --> baking cookies --> abseiling\n(C) clean and jerk --> abseiling --> baking cookies --> water sliding\n(D) abseiling --> water sliding --> clean and jerk --> baking cookies\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) abseiling --> baking cookies --> clean and jerk --> water sliding\n(B) water sliding --> clean and jerk --> baking cookies --> abseiling\n(C) clean and jerk --> abseiling --> baking cookies --> water sliding\n(D) abseiling --> water sliding --> clean and jerk --> baking cookies\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2133, "doc": {"video_name": "order_9.mp4", "duration": 677.82, "question": "Arrange the following events from the video in the correct chronological order: (1)The man attempts to walk across the rope but falls and holds onto the rope; (2)A seal sits on a rock near an ocean; (3)The man films from a beach cliff next to a tent; (4)The man walks across the rope all the way to the attached rock.\n(A) 3->1->2->4\n(B) 2->4->1->3\n(C) 1->2->3->4\n(D) 2->1->4->3\n", "candidates": ["3->1->2->4", "2->4->1->3", "1->2->3->4", "2->1->4->3"], "answer": "D", "task_type": "order", "question_id": "Q2133"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "633988d14c80e0d3c0b69da6b572119e38ea38d896595dfa62607c4947b43312", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man attempts to walk across the rope but falls and holds onto the rope; (2)A seal sits on a rock near an ocean; (3)The man films from a beach cliff next to a tent; (4)The man walks across the rope all the way to the attached rock.\n(A) 3->1->2->4\n(B) 2->4->1->3\n(C) 1->2->3->4\n(D) 2->1->4->3\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man attempts to walk across the rope but falls and holds onto the rope; (2)A seal sits on a rock near an ocean; (3)The man films from a beach cliff next to a tent; (4)The man walks across the rope all the way to the attached rock.\n(A) 3->1->2->4\n(B) 2->4->1->3\n(C) 1->2->3->4\n(D) 2->1->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2137, "doc": {"video_name": "order_112.mp4", "duration": 675.0699999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 3->2->1->4\n(B) 2->1->3->4\n(C) 1->2->3->4\n(D) 4->3->2->1\n", "candidates": ["3->2->1->4", "2->1->3->4", "1->2->3->4", "4->3->2->1"], "answer": "C", "task_type": "order", "question_id": "Q2137"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "19611615ccaff23f773b2839facf7648df1cd2724f59761775d4fda67daa3f01", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 3->2->1->4\n(B) 2->1->3->4\n(C) 1->2->3->4\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 3->2->1->4\n(B) 2->1->3->4\n(C) 1->2->3->4\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2141, "doc": {"video_name": "order_281.mp4", "duration": 605.5200000000001, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) javelin throw --> tossing coin --> stomping grapes --> milking cow\n(B) javelin throw --> milking cow --> tossing coin --> stomping grapes\n(C) milking cow --> tossing coin --> stomping grapes --> javelin throw\n(D) milking cow --> tossing coin --> javelin throw --> stomping grapes\n", "candidates": ["javelin throw --> tossing coin --> stomping grapes --> milking cow", "javelin throw --> milking cow --> tossing coin --> stomping grapes", "milking cow --> tossing coin --> stomping grapes --> javelin throw", "milking cow --> tossing coin --> javelin throw --> stomping grapes"], "answer": "C", "task_type": "order", "question_id": "Q2141"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e0d9420042c7f990e2240f9b03e29e479ba2ce4789c330ba08362b24c9e659b1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) javelin throw --> tossing coin --> stomping grapes --> milking cow\n(B) javelin throw --> milking cow --> tossing coin --> stomping grapes\n(C) milking cow --> tossing coin --> stomping grapes --> javelin throw\n(D) milking cow --> tossing coin --> javelin throw --> stomping grapes\n", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) javelin throw --> tossing coin --> stomping grapes --> milking cow\n(B) javelin throw --> milking cow --> tossing coin --> stomping grapes\n(C) milking cow --> tossing coin --> stomping grapes --> javelin throw\n(D) milking cow --> tossing coin --> javelin throw --> stomping grapes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2145, "doc": {"video_name": "order_263.mp4", "duration": 490.03000000000003, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) pole vault --> jetskiing --> zumba --> clean and jerk\n(B) pole vault --> clean and jerk --> zumba --> jetskiing\n(C) jetskiing --> pole vault --> zumba --> clean and jerk\n(D) clean and jerk --> jetskiing --> zumba --> pole vault\n", "candidates": ["pole vault --> jetskiing --> zumba --> clean and jerk", "pole vault --> clean and jerk --> zumba --> jetskiing", "jetskiing --> pole vault --> zumba --> clean and jerk", "clean and jerk --> jetskiing --> zumba --> pole vault"], "answer": "B", "task_type": "order", "question_id": "Q2145"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "53b0af30a2c064a9617310d5c33aa429f4adbebdcf98f673b1b0f18bbf594e3a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) pole vault --> jetskiing --> zumba --> clean and jerk\n(B) pole vault --> clean and jerk --> zumba --> jetskiing\n(C) jetskiing --> pole vault --> zumba --> clean and jerk\n(D) clean and jerk --> jetskiing --> zumba --> pole vault\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) pole vault --> jetskiing --> zumba --> clean and jerk\n(B) pole vault --> clean and jerk --> zumba --> jetskiing\n(C) jetskiing --> pole vault --> zumba --> clean and jerk\n(D) clean and jerk --> jetskiing --> zumba --> pole vault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2149, "doc": {"video_name": "order_270.mp4", "duration": 359.03999999999996, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) baking cookies --> playing trombone --> cleaning toilet --> playing harp\n(B) playing harp --> cleaning toilet --> playing trombone --> baking cookies\n(C) cleaning toilet --> baking cookies --> playing harp --> playing trombone\n(D) playing harp --> cleaning toilet --> baking cookies --> playing trombone\n", "candidates": ["baking cookies --> playing trombone --> cleaning toilet --> playing harp", "playing harp --> cleaning toilet --> playing trombone --> baking cookies", "cleaning toilet --> baking cookies --> playing harp --> playing trombone", "playing harp --> cleaning toilet --> baking cookies --> playing trombone"], "answer": "B", "task_type": "order", "question_id": "Q2149"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e9b2b9eba7a610776425f6dbc18651782ae6e64e4d2f4bbd8bba322cd9390cf8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) baking cookies --> playing trombone --> cleaning toilet --> playing harp\n(B) playing harp --> cleaning toilet --> playing trombone --> baking cookies\n(C) cleaning toilet --> baking cookies --> playing harp --> playing trombone\n(D) playing harp --> cleaning toilet --> baking cookies --> playing trombone\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) baking cookies --> playing trombone --> cleaning toilet --> playing harp\n(B) playing harp --> cleaning toilet --> playing trombone --> baking cookies\n(C) cleaning toilet --> baking cookies --> playing harp --> playing trombone\n(D) playing harp --> cleaning toilet --> baking cookies --> playing trombone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2153, "doc": {"video_name": "order_340.mp4", "duration": 368.05999999999995, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cleaning toilet --> paragliding --> abseiling --> jetskiing\n(B) jetskiing --> paragliding --> cleaning toilet --> abseiling\n(C) abseiling --> paragliding --> jetskiing --> cleaning toilet\n(D) abseiling --> paragliding --> cleaning toilet --> jetskiing\n", "candidates": ["cleaning toilet --> paragliding --> abseiling --> jetskiing", "jetskiing --> paragliding --> cleaning toilet --> abseiling", "abseiling --> paragliding --> jetskiing --> cleaning toilet", "abseiling --> paragliding --> cleaning toilet --> jetskiing"], "answer": "C", "task_type": "order", "question_id": "Q2153"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b0dc6417b3857acb57c3f3ecacaf12397abdda1ef813671839dd4583d62ea7a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cleaning toilet --> paragliding --> abseiling --> jetskiing\n(B) jetskiing --> paragliding --> cleaning toilet --> abseiling\n(C) abseiling --> paragliding --> jetskiing --> cleaning toilet\n(D) abseiling --> paragliding --> cleaning toilet --> jetskiing\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cleaning toilet --> paragliding --> abseiling --> jetskiing\n(B) jetskiing --> paragliding --> cleaning toilet --> abseiling\n(C) abseiling --> paragliding --> jetskiing --> cleaning toilet\n(D) abseiling --> paragliding --> cleaning toilet --> jetskiing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2157, "doc": {"video_name": "order_244.mp4", "duration": 324.64, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> zumba --> water sliding --> carving pumpkin\n(B) water sliding --> zumba --> baking cookies --> carving pumpkin\n(C) water sliding --> baking cookies --> zumba --> carving pumpkin\n(D) zumba --> water sliding --> carving pumpkin --> baking cookies\n", "candidates": ["baking cookies --> zumba --> water sliding --> carving pumpkin", "water sliding --> zumba --> baking cookies --> carving pumpkin", "water sliding --> baking cookies --> zumba --> carving pumpkin", "zumba --> water sliding --> carving pumpkin --> baking cookies"], "answer": "C", "task_type": "order", "question_id": "Q2157"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1790a71c8275d424713198747aed94de829672ed5465dcc7b865abc373dbf594", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> zumba --> water sliding --> carving pumpkin\n(B) water sliding --> zumba --> baking cookies --> carving pumpkin\n(C) water sliding --> baking cookies --> zumba --> carving pumpkin\n(D) zumba --> water sliding --> carving pumpkin --> baking cookies\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> zumba --> water sliding --> carving pumpkin\n(B) water sliding --> zumba --> baking cookies --> carving pumpkin\n(C) water sliding --> baking cookies --> zumba --> carving pumpkin\n(D) zumba --> water sliding --> carving pumpkin --> baking cookies\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2161, "doc": {"video_name": "order_24.mp4", "duration": 688.0699999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)Two people are paddling down rapids on a river in canoes; (2)One of them stops at a bank where there is a person in a blue canoe; (3)People are seen in a group large red tube rapids ride; (4)They pass by a building and then fall into the water.\n(A) 2->1->3->4\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 3->2->1->4\n", "candidates": ["2->1->3->4", "4->3->2->1", "1->2->3->4", "3->2->1->4"], "answer": "C", "task_type": "order", "question_id": "Q2161"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5e4d41a35c361eeee27a40e676c71007804b7005467ff03c35ff5d3f4b058d0f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Two people are paddling down rapids on a river in canoes; (2)One of them stops at a bank where there is a person in a blue canoe; (3)People are seen in a group large red tube rapids ride; (4)They pass by a building and then fall into the water.\n(A) 2->1->3->4\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 3->2->1->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Two people are paddling down rapids on a river in canoes; (2)One of them stops at a bank where there is a person in a blue canoe; (3)People are seen in a group large red tube rapids ride; (4)They pass by a building and then fall into the water.\n(A) 2->1->3->4\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 3->2->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2165, "doc": {"video_name": "order_20.mp4", "duration": 679.54, "question": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 2->1->4->3\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->4->1->2\n", "candidates": ["2->1->4->3", "1->2->3->4", "4->3->2->1", "3->4->1->2"], "answer": "A", "task_type": "order", "question_id": "Q2165"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "366a0daf04e238c789b332e40597f10aae0c936e28ac83cd1c110fe200048cd8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 2->1->4->3\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->4->1->2\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 2->1->4->3\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->4->1->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2169, "doc": {"video_name": "order_215.mp4", "duration": 246.05, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) paragliding --> pole vault --> playing harp --> riding mule\n(B) paragliding --> playing harp --> pole vault --> riding mule\n(C) playing harp --> pole vault --> riding mule --> paragliding\n(D) riding mule --> playing harp --> pole vault --> paragliding\n", "candidates": ["paragliding --> pole vault --> playing harp --> riding mule", "paragliding --> playing harp --> pole vault --> riding mule", "playing harp --> pole vault --> riding mule --> paragliding", "riding mule --> playing harp --> pole vault --> paragliding"], "answer": "C", "task_type": "order", "question_id": "Q2169"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "50fe3a7f595657adafd836e24d1ad7b551f06be18be3a1c38268ae755bc4f2ad", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) paragliding --> pole vault --> playing harp --> riding mule\n(B) paragliding --> playing harp --> pole vault --> riding mule\n(C) playing harp --> pole vault --> riding mule --> paragliding\n(D) riding mule --> playing harp --> pole vault --> paragliding\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) paragliding --> pole vault --> playing harp --> riding mule\n(B) paragliding --> playing harp --> pole vault --> riding mule\n(C) playing harp --> pole vault --> riding mule --> paragliding\n(D) riding mule --> playing harp --> pole vault --> paragliding\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2173, "doc": {"video_name": "order_310.mp4", "duration": 505.66, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> tossing coin --> abseiling --> clean and jerk\n(B) abseiling --> zumba --> tossing coin --> clean and jerk\n(C) clean and jerk --> tossing coin --> abseiling --> zumba\n(D) tossing coin --> zumba --> clean and jerk --> abseiling\n", "candidates": ["zumba --> tossing coin --> abseiling --> clean and jerk", "abseiling --> zumba --> tossing coin --> clean and jerk", "clean and jerk --> tossing coin --> abseiling --> zumba", "tossing coin --> zumba --> clean and jerk --> abseiling"], "answer": "C", "task_type": "order", "question_id": "Q2173"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7900fa6efe397da6e73180c78e0150c4a179409285205aa3207c9e54ad96249d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> tossing coin --> abseiling --> clean and jerk\n(B) abseiling --> zumba --> tossing coin --> clean and jerk\n(C) clean and jerk --> tossing coin --> abseiling --> zumba\n(D) tossing coin --> zumba --> clean and jerk --> abseiling\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> tossing coin --> abseiling --> clean and jerk\n(B) abseiling --> zumba --> tossing coin --> clean and jerk\n(C) clean and jerk --> tossing coin --> abseiling --> zumba\n(D) tossing coin --> zumba --> clean and jerk --> abseiling\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2, "doc": {"video_name": "needle_147.mp4", "duration": 622.0, "question": "Where is the senior businessman having a serious conversation on the cell phone?\n(A) By the sea shore\n(B) In a park\n(C) In his office\n(D) At a restaurant\n", "candidates": ["By the sea shore", "In a park", "In his office", "At a restaurant"], "answer": "A", "task_type": "needle", "question_id": "Q2"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b849a0f83bbc6a462dfeb11494d8b8be95c1cf0a13787796b03c68d5a137bec2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where is the senior businessman having a serious conversation on the cell phone?\n(A) By the sea shore\n(B) In a park\n(C) In his office\n(D) At a restaurant\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "Where is the senior businessman having a serious conversation on the cell phone?\n(A) By the sea shore\n(B) In a park\n(C) In his office\n(D) At a restaurant\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 6, "doc": {"video_name": "needle_142.mp4", "duration": 612.03, "question": "What is at the remembrance war memorial in Toronto, Canada?\n(A) A fountain\n(B) Thousands of Canadian flags\n(C) A large statue of a horse\n(D) A large statue of a soldier\n", "candidates": ["A fountain", "Thousands of Canadian flags", "A large statue of a horse", "A large statue of a soldier"], "answer": "B", "task_type": "needle", "question_id": "Q6"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85024cbb6f7d4a853dde2f4270a108e3dedd1a8b625350f4d0773586dd548dd8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is at the remembrance war memorial in Toronto, Canada?\n(A) A fountain\n(B) Thousands of Canadian flags\n(C) A large statue of a horse\n(D) A large statue of a soldier\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is at the remembrance war memorial in Toronto, Canada?\n(A) A fountain\n(B) Thousands of Canadian flags\n(C) A large statue of a horse\n(D) A large statue of a soldier\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 10, "doc": {"video_name": "needle_20.mp4", "duration": 458.58, "question": "What is the weather like as the car with a broken windshield rides through the mountain country village?\n(A) Autumn rainy day\n(B) Winter snowy day\n(C) Spring cloudy day\n(D) Summer sunny day\n", "candidates": ["Autumn rainy day", "Winter snowy day", "Spring cloudy day", "Summer sunny day"], "answer": "D", "task_type": "needle", "question_id": "Q10"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "80cb03611b3ef92f3a91d03ab137ba2915cb088f2bfc95af10d96085a954f6cc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the weather like as the car with a broken windshield rides through the mountain country village?\n(A) Autumn rainy day\n(B) Winter snowy day\n(C) Spring cloudy day\n(D) Summer sunny day\n", "task_type": "needle", "pred_answer": "A", "answer": "D"}, "input": "What is the weather like as the car with a broken windshield rides through the mountain country village?\n(A) Autumn rainy day\n(B) Winter snowy day\n(C) Spring cloudy day\n(D) Summer sunny day\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 14, "doc": {"video_name": "needle_20.mp4", "duration": 458.58, "question": "What is the obstacle in the middle of the road as the car with a broken windshield rides through the mountain country village?\n(A) A group of people\n(B) A fallen tree\n(C) A flock of sheep\n(D) A cow\n", "candidates": ["A group of people", "A fallen tree", "A flock of sheep", "A cow"], "answer": "D", "task_type": "needle", "question_id": "Q14"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "81f67184b8cee09a4ecd5d785fd236cc7a972148ca18923954e6b550e30f9841", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the obstacle in the middle of the road as the car with a broken windshield rides through the mountain country village?\n(A) A group of people\n(B) A fallen tree\n(C) A flock of sheep\n(D) A cow\n", "task_type": "needle", "pred_answer": "C", "answer": "D"}, "input": "What is the obstacle in the middle of the road as the car with a broken windshield rides through the mountain country village?\n(A) A group of people\n(B) A fallen tree\n(C) A flock of sheep\n(D) A cow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 18, "doc": {"video_name": "needle_19.mp4", "duration": 464.0, "question": "Where are the people tourists walking?\n(A) On a beach\n(B) In a forest\n(C) On the embankment in a fishing village\n(D) In a city\n", "candidates": ["On a beach", "In a forest", "On the embankment in a fishing village", "In a city"], "answer": "C", "task_type": "needle", "question_id": "Q18"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6efcc590aa674d5222bee51e1e30673ee6fee10f204f41ba561a292e8b783de9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where are the people tourists walking?\n(A) On a beach\n(B) In a forest\n(C) On the embankment in a fishing village\n(D) In a city\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "Where are the people tourists walking?\n(A) On a beach\n(B) In a forest\n(C) On the embankment in a fishing village\n(D) In a city\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 22, "doc": {"video_name": "needle_54.mp4", "duration": 469.72, "question": "What is the serious older mature businesswoman writing on the whiteboard?\n(A) Her personal diary\n(B) A poem\n(C) Notes\n(D) A novel\n", "candidates": ["Her personal diary", "A poem", "Notes", "A novel"], "answer": "C", "task_type": "needle", "question_id": "Q22"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0fe27e9f4061307e11e39cdc7d5b5bef8a329c5ad67dddf6d9af206bde213ab1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the serious older mature businesswoman writing on the whiteboard?\n(A) Her personal diary\n(B) A poem\n(C) Notes\n(D) A novel\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the serious older mature businesswoman writing on the whiteboard?\n(A) Her personal diary\n(B) A poem\n(C) Notes\n(D) A novel\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 26, "doc": {"video_name": "needle_69.mp4", "duration": 461.04, "question": "What season is it when the little Asian girl in a bikini is creating sand piles on the beach at sunset?\n(A) Autumn\n(B) Winter\n(C) Summer\n(D) Spring\n", "candidates": ["Autumn", "Winter", "Summer", "Spring"], "answer": "C", "task_type": "needle", "question_id": "Q26"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6178857e6b92b86851774ea8036a82a1faf6b08a6cfb6a29cf245c373eee38c5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What season is it when the little Asian girl in a bikini is creating sand piles on the beach at sunset?\n(A) Autumn\n(B) Winter\n(C) Summer\n(D) Spring\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What season is it when the little Asian girl in a bikini is creating sand piles on the beach at sunset?\n(A) Autumn\n(B) Winter\n(C) Summer\n(D) Spring\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 30, "doc": {"video_name": "needle_78.mp4", "duration": 588.31, "question": "What part of the doctor's face is shown in closeup in the video?\n(A) Eye\n(B) Ear\n(C) Mouth\n(D) Nose\n", "candidates": ["Eye", "Ear", "Mouth", "Nose"], "answer": "A", "task_type": "needle", "question_id": "Q30"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1286ef310906c1fa5fcff1f20492cbce8ec24d9e0db943e7e63552853c8987bc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What part of the doctor's face is shown in closeup in the video?\n(A) Eye\n(B) Ear\n(C) Mouth\n(D) Nose\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What part of the doctor's face is shown in closeup in the video?\n(A) Eye\n(B) Ear\n(C) Mouth\n(D) Nose\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 34, "doc": {"video_name": "needle_4.mp4", "duration": 510.0, "question": "What is the man in the video doing on the lake shore during the sunny summer?\n(A) Fishing\n(B) Catching the drone\n(C) Launching the drone\n(D) Sunbathing\n", "candidates": ["Fishing", "Catching the drone", "Launching the drone", "Sunbathing"], "answer": "B", "task_type": "needle", "question_id": "Q34"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "27c4439af06e8e6f46c21c74c65a21bdf41ddfa2284c5fd65b19818238da8ae5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the man in the video doing on the lake shore during the sunny summer?\n(A) Fishing\n(B) Catching the drone\n(C) Launching the drone\n(D) Sunbathing\n", "task_type": "needle", "pred_answer": "C.", "answer": "B"}, "input": "What is the man in the video doing on the lake shore during the sunny summer?\n(A) Fishing\n(B) Catching the drone\n(C) Launching the drone\n(D) Sunbathing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 38, "doc": {"video_name": "needle_52.mp4", "duration": 1507.07, "question": "Where is the remembrance war memorial with thousands of Canadian flags located?\n(A) New York, USA\n(B) London, UK\n(C) Toronto, Canada\n(D) Vancouver, Canada\n", "candidates": ["New York, USA", "London, UK", "Toronto, Canada", "Vancouver, Canada"], "answer": "C", "task_type": "needle", "question_id": "Q38"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "62954049f503347ac05880b3dd202a8063fe12e878add0198c5d49db084d0f80", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the remembrance war memorial with thousands of Canadian flags located?\n(A) New York, USA\n(B) London, UK\n(C) Toronto, Canada\n(D) Vancouver, Canada\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "Where is the remembrance war memorial with thousands of Canadian flags located?\n(A) New York, USA\n(B) London, UK\n(C) Toronto, Canada\n(D) Vancouver, Canada\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 42, "doc": {"video_name": "needle_3.mp4", "duration": 496.21999999999997, "question": "Who is the young attractive hispanic medical doctor discussing health issues with?\n(A) A colleague\n(B) A male patient\n(C) A senior patient\n(D) A child patient\n", "candidates": ["A colleague", "A male patient", "A senior patient", "A child patient"], "answer": "C", "task_type": "needle", "question_id": "Q42"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4c396227baf1812d4e1f2ce54a51cf771c2827fd45376cdae419d7e6e8b1cff2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who is the young attractive hispanic medical doctor discussing health issues with?\n(A) A colleague\n(B) A male patient\n(C) A senior patient\n(D) A child patient\n", "task_type": "needle", "pred_answer": "B", "answer": "C"}, "input": "Who is the young attractive hispanic medical doctor discussing health issues with?\n(A) A colleague\n(B) A male patient\n(C) A senior patient\n(D) A child patient\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 46, "doc": {"video_name": "needle_143.mp4", "duration": 624.8699999999999, "question": "What does the hand coming out of the computer do?\n(A) Delivers a product\n(B) Takes the woman's credit card\n(C) Points at something on the screen\n(D) Shakes the woman's hand\n", "candidates": ["Delivers a product", "Takes the woman's credit card", "Points at something on the screen", "Shakes the woman's hand"], "answer": "A", "task_type": "needle", "question_id": "Q46"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0560a947ebca58453ea95d3d9c68c5e2035a9785751a88a0b1b2ec60b4c930ad", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What does the hand coming out of the computer do?\n(A) Delivers a product\n(B) Takes the woman's credit card\n(C) Points at something on the screen\n(D) Shakes the woman's hand\n", "task_type": "needle", "pred_answer": "C", "answer": "A"}, "input": "What does the hand coming out of the computer do?\n(A) Delivers a product\n(B) Takes the woman's credit card\n(C) Points at something on the screen\n(D) Shakes the woman's hand\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 50, "doc": {"video_name": "needle_26.mp4", "duration": 469.08, "question": "What season is it when the black cows with yellow tags on their ears are eating hay at the farm outdoors?\n(A) Autumn\n(B) Spring\n(C) Winter\n(D) Summer\n", "candidates": ["Autumn", "Spring", "Winter", "Summer"], "answer": "D", "task_type": "needle", "question_id": "Q50"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "338c4d43d0f950d7646f1becc89537834f54aea75569269cce8abb424865837d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What season is it when the black cows with yellow tags on their ears are eating hay at the farm outdoors?\n(A) Autumn\n(B) Spring\n(C) Winter\n(D) Summer\n", "task_type": "needle", "pred_answer": "C", "answer": "D"}, "input": "What season is it when the black cows with yellow tags on their ears are eating hay at the farm outdoors?\n(A) Autumn\n(B) Spring\n(C) Winter\n(D) Summer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 54, "doc": {"video_name": "needle_96.mp4", "duration": 503.59, "question": "What is the ethnicity of the couple?\n(A) Multiethnic\n(B) Caucasian\n(C) African\n(D) Asian\n", "candidates": ["Multiethnic", "Caucasian", "African", "Asian"], "answer": "A", "task_type": "needle", "question_id": "Q54"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b9f38ec171e50452c8d1a4a4199cb36633ae56d5f241991b11d3edc3ce1b4b28", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the ethnicity of the couple?\n(A) Multiethnic\n(B) Caucasian\n(C) African\n(D) Asian\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the ethnicity of the couple?\n(A) Multiethnic\n(B) Caucasian\n(C) African\n(D) Asian\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 58, "doc": {"video_name": "needle_50.mp4", "duration": 458.64, "question": "What are the two kids doing in the paddy field?\n(A) Studying\n(B) Sleeping\n(C) Having fun\n(D) Playing football\n", "candidates": ["Studying", "Sleeping", "Having fun", "Playing football"], "answer": "C", "task_type": "needle", "question_id": "Q58"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "12be01af24f1872334b5af1f165f59699dbeeff4eba5a571f0aca4bcbf1ef1eb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the two kids doing in the paddy field?\n(A) Studying\n(B) Sleeping\n(C) Having fun\n(D) Playing football\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What are the two kids doing in the paddy field?\n(A) Studying\n(B) Sleeping\n(C) Having fun\n(D) Playing football\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 62, "doc": {"video_name": "needle_94.mp4", "duration": 857.22, "question": "Where is the cup that the female potter is stirring paint in?\n(A) On the floor\n(B) On the table\n(C) On the shelf\n(D) In her hand\n", "candidates": ["On the floor", "On the table", "On the shelf", "In her hand"], "answer": "B", "task_type": "needle", "question_id": "Q62"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "73c4edde34a3d2836b92032e0fba5149859f361d94a1116f2221ea68cb377c05", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where is the cup that the female potter is stirring paint in?\n(A) On the floor\n(B) On the table\n(C) On the shelf\n(D) In her hand\n", "task_type": "needle", "pred_answer": "D", "answer": "B"}, "input": "Where is the cup that the female potter is stirring paint in?\n(A) On the floor\n(B) On the table\n(C) On the shelf\n(D) In her hand\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 66, "doc": {"video_name": "needle_62.mp4", "duration": 497.05, "question": "What is the young beautiful woman looking at while preparing for the new year?\n(A) She is looking at the christmas lights\n(B) She is looking at the christmas stockings\n(C) She is looking at the presents\n(D) She is looking at the golden christmas toy\n", "candidates": ["She is looking at the christmas lights", "She is looking at the christmas stockings", "She is looking at the presents", "She is looking at the golden christmas toy"], "answer": "D", "task_type": "needle", "question_id": "Q66"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "15b46c76644df2a2b3c8d757527be88b941f951a95ae0ed6856237cebdd33759", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the young beautiful woman looking at while preparing for the new year?\n(A) She is looking at the christmas lights\n(B) She is looking at the christmas stockings\n(C) She is looking at the presents\n(D) She is looking at the golden christmas toy\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the young beautiful woman looking at while preparing for the new year?\n(A) She is looking at the christmas lights\n(B) She is looking at the christmas stockings\n(C) She is looking at the presents\n(D) She is looking at the golden christmas toy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 70, "doc": {"video_name": "needle_96.mp4", "duration": 503.59, "question": "What devices are the couple using while sitting on the couch?\n(A) Phone and TV\n(B) Tablet and phone\n(C) Laptop and phone\n(D) Laptop and TV\n", "candidates": ["Phone and TV", "Tablet and phone", "Laptop and phone", "Laptop and TV"], "answer": "C", "task_type": "needle", "question_id": "Q70"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cd9d4f2efe1d3a8bf087f2ff5f730d9f82714cdd3bb1da84c863523711053f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What devices are the couple using while sitting on the couch?\n(A) Phone and TV\n(B) Tablet and phone\n(C) Laptop and phone\n(D) Laptop and TV\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What devices are the couple using while sitting on the couch?\n(A) Phone and TV\n(B) Tablet and phone\n(C) Laptop and phone\n(D) Laptop and TV\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 74, "doc": {"video_name": "needle_64.mp4", "duration": 470.88, "question": "What season is depicted in the video segment where the forest fire is happening?\n(A) Spring season\n(B) Dry season\n(C) Winter season\n(D) Rainy season\n", "candidates": ["Spring season", "Dry season", "Winter season", "Rainy season"], "answer": "B", "task_type": "needle", "question_id": "Q74"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2851ed3db3959bcca2aacffe103ed470fce17a638ca7c92db82ff7f0d3a27005", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What season is depicted in the video segment where the forest fire is happening?\n(A) Spring season\n(B) Dry season\n(C) Winter season\n(D) Rainy season\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What season is depicted in the video segment where the forest fire is happening?\n(A) Spring season\n(B) Dry season\n(C) Winter season\n(D) Rainy season\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 78, "doc": {"video_name": "needle_66.mp4", "duration": 475.20000000000005, "question": "What is rising to the sky in the video segment of the power plant block?\n(A) Smoke\n(B) Balloons\n(C) Water vapor\n(D) Birds\n", "candidates": ["Smoke", "Balloons", "Water vapor", "Birds"], "answer": "C", "task_type": "needle", "question_id": "Q78"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f105b9dd501edcd0032c91bea960949fbe5dcee1a3215606eed4820f69eaada4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is rising to the sky in the video segment of the power plant block?\n(A) Smoke\n(B) Balloons\n(C) Water vapor\n(D) Birds\n", "task_type": "needle", "pred_answer": "A", "answer": "C"}, "input": "What is rising to the sky in the video segment of the power plant block?\n(A) Smoke\n(B) Balloons\n(C) Water vapor\n(D) Birds\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 82, "doc": {"video_name": "needle_17.mp4", "duration": 867.89, "question": "What type of dog is the owner in sports boots stroking?\n(A) Jack Russell\n(B) German Shepherd\n(C) Golden Retriever\n(D) Bulldog\n", "candidates": ["Jack Russell", "German Shepherd", "Golden Retriever", "Bulldog"], "answer": "A", "task_type": "needle", "question_id": "Q82"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc6cc0941245e9d1280713aad3cb7517ba82613f68960497b00643413070dd3c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of dog is the owner in sports boots stroking?\n(A) Jack Russell\n(B) German Shepherd\n(C) Golden Retriever\n(D) Bulldog\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What type of dog is the owner in sports boots stroking?\n(A) Jack Russell\n(B) German Shepherd\n(C) Golden Retriever\n(D) Bulldog\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 86, "doc": {"video_name": "needle_16.mp4", "duration": 467.08, "question": "What is the owner wearing while stroking his Jack Russell dog?\n(A) Sneakers\n(B) Barefoot\n(C) Flip flops\n(D) Sports boots\n", "candidates": ["Sneakers", "Barefoot", "Flip flops", "Sports boots"], "answer": "D", "task_type": "needle", "question_id": "Q86"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bcb34acc07d452a219d89d693c176138efcd263876a5307d1389d4a0ca955ae2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the owner wearing while stroking his Jack Russell dog?\n(A) Sneakers\n(B) Barefoot\n(C) Flip flops\n(D) Sports boots\n", "task_type": "needle", "pred_answer": "C", "answer": "D"}, "input": "What is the owner wearing while stroking his Jack Russell dog?\n(A) Sneakers\n(B) Barefoot\n(C) Flip flops\n(D) Sports boots\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 90, "doc": {"video_name": "needle_15.mp4", "duration": 498.41999999999996, "question": "Who are decorating the Christmas tree outdoors in the video?\n(A) A young mother and her daughter\n(B) An elderly couple\n(C) A young mother and her son\n(D) A young father and his son\n", "candidates": ["A young mother and her daughter", "An elderly couple", "A young mother and her son", "A young father and his son"], "answer": "C", "task_type": "needle", "question_id": "Q90"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bea47af69165292209837fc1b2277eca0ec1016a2d31dc2d38aab571ef12c325", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who are decorating the Christmas tree outdoors in the video?\n(A) A young mother and her daughter\n(B) An elderly couple\n(C) A young mother and her son\n(D) A young father and his son\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "Who are decorating the Christmas tree outdoors in the video?\n(A) A young mother and her daughter\n(B) An elderly couple\n(C) A young mother and her son\n(D) A young father and his son\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 94, "doc": {"video_name": "needle_89.mp4", "duration": 456.0, "question": "What logo is displayed on the screen in the meeting room?\n(A) Microsoft\n(B) Google\n(C) The Goldman Sachs Group\n(D) Apple\n", "candidates": ["Microsoft", "Google", "The Goldman Sachs Group", "Apple"], "answer": "C", "task_type": "needle", "question_id": "Q94"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c940e992d3bf35591c74a6f80abd3dbe4f883a86a0f810c10eae7a57e4e6dcb6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What logo is displayed on the screen in the meeting room?\n(A) Microsoft\n(B) Google\n(C) The Goldman Sachs Group\n(D) Apple\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What logo is displayed on the screen in the meeting room?\n(A) Microsoft\n(B) Google\n(C) The Goldman Sachs Group\n(D) Apple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 98, "doc": {"video_name": "needle_43.mp4", "duration": 1516.18, "question": "What is the condition of the highway where the SUV is parked for green screen or chroma key?\n(A) Under construction\n(B) Empty\n(C) Busy with traffic\n(D) Flooded\n", "candidates": ["Under construction", "Empty", "Busy with traffic", "Flooded"], "answer": "B", "task_type": "needle", "question_id": "Q98"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B) Empty"]], "filtered_resps": ["B) Empty"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a05274c3b695b26c784991c12f02d87b34caf35777693324ee2cffec80f61126", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the condition of the highway where the SUV is parked for green screen or chroma key?\n(A) Under construction\n(B) Empty\n(C) Busy with traffic\n(D) Flooded\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the condition of the highway where the SUV is parked for green screen or chroma key?\n(A) Under construction\n(B) Empty\n(C) Busy with traffic\n(D) Flooded\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 102, "doc": {"video_name": "needle_86.mp4", "duration": 470.40999999999997, "question": "What is the kid doing with the colorful party balloons in the video?\n(A) Sitting on the sand\n(B) Swimming in the pool\n(C) Running on the beach\n(D) Walking in the water\n", "candidates": ["Sitting on the sand", "Swimming in the pool", "Running on the beach", "Walking in the water"], "answer": "D", "task_type": "needle", "question_id": "Q102"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf9ecb997f3181bd2da9efba8d21a8496fb7ea4ce23e7933b45f87eb9cfb2bd6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the kid doing with the colorful party balloons in the video?\n(A) Sitting on the sand\n(B) Swimming in the pool\n(C) Running on the beach\n(D) Walking in the water\n", "task_type": "needle", "pred_answer": "A", "answer": "D"}, "input": "What is the kid doing with the colorful party balloons in the video?\n(A) Sitting on the sand\n(B) Swimming in the pool\n(C) Running on the beach\n(D) Walking in the water\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 106, "doc": {"video_name": "needle_81.mp4", "duration": 463.1, "question": "What happens to the egg when it falls on the glass floor in the video?\n(A) It breaks and makes a mess\n(B) Nothing happens to it\n(C) It cracks but doesn't break\n(D) It bounces back\n", "candidates": ["It breaks and makes a mess", "Nothing happens to it", "It cracks but doesn't break", "It bounces back"], "answer": "A", "task_type": "needle", "question_id": "Q106"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "007581d9bd0f18e4e2b9ac9cec4811e5ab38be6828a2b5f0376ea2ba0ddc9f66", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What happens to the egg when it falls on the glass floor in the video?\n(A) It breaks and makes a mess\n(B) Nothing happens to it\n(C) It cracks but doesn't break\n(D) It bounces back\n", "task_type": "needle", "pred_answer": "C", "answer": "A"}, "input": "What happens to the egg when it falls on the glass floor in the video?\n(A) It breaks and makes a mess\n(B) Nothing happens to it\n(C) It cracks but doesn't break\n(D) It bounces back\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 110, "doc": {"video_name": "needle_126.mp4", "duration": 6008.08, "question": "What type of beach is shown in the video?\n(A) Pebble beach\n(B) Man-made beach\n(C) Sandy beach\n(D) Wild rocky beach\n", "candidates": ["Pebble beach", "Man-made beach", "Sandy beach", "Wild rocky beach"], "answer": "D", "task_type": "needle", "question_id": "Q110"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b7eb590b0463a28640d0372cb5653ebd59b7da998f104cac97d1f9edf02bbbd7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of beach is shown in the video?\n(A) Pebble beach\n(B) Man-made beach\n(C) Sandy beach\n(D) Wild rocky beach\n", "task_type": "needle", "pred_answer": "C.", "answer": "D"}, "input": "What type of beach is shown in the video?\n(A) Pebble beach\n(B) Man-made beach\n(C) Sandy beach\n(D) Wild rocky beach\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 114, "doc": {"video_name": "needle_25.mp4", "duration": 866.49, "question": "Where is the group of blue fin trevally hunting in the video?\n(A) Great Barrier Reef, Australia\n(B) Caribbean Sea, Bahamas\n(C) Red Sea, Egypt\n(D) Yap, Micronesia\n", "candidates": ["Great Barrier Reef, Australia", "Caribbean Sea, Bahamas", "Red Sea, Egypt", "Yap, Micronesia"], "answer": "D", "task_type": "needle", "question_id": "Q114"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "36e9adc4e724a5668b44754159fbe2a06a450facdeee852197a6be9f4c585719", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where is the group of blue fin trevally hunting in the video?\n(A) Great Barrier Reef, Australia\n(B) Caribbean Sea, Bahamas\n(C) Red Sea, Egypt\n(D) Yap, Micronesia\n", "task_type": "needle", "pred_answer": "A", "answer": "D"}, "input": "Where is the group of blue fin trevally hunting in the video?\n(A) Great Barrier Reef, Australia\n(B) Caribbean Sea, Bahamas\n(C) Red Sea, Egypt\n(D) Yap, Micronesia\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 118, "doc": {"video_name": "needle_24.mp4", "duration": 462.03999999999996, "question": "What is the habitat of the blue fin trevally shown in the video?\n(A) Coral reef\n(B) Freshwater river\n(C) Open ocean\n(D) Mangrove forest\n", "candidates": ["Coral reef", "Freshwater river", "Open ocean", "Mangrove forest"], "answer": "A", "task_type": "needle", "question_id": "Q118"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a984ffc381f6eea20679f6724903673b6f2e2be47947d9e37b4acd98be5bc57d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the habitat of the blue fin trevally shown in the video?\n(A) Coral reef\n(B) Freshwater river\n(C) Open ocean\n(D) Mangrove forest\n", "task_type": "needle", "pred_answer": "C", "answer": "A"}, "input": "What is the habitat of the blue fin trevally shown in the video?\n(A) Coral reef\n(B) Freshwater river\n(C) Open ocean\n(D) Mangrove forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 122, "doc": {"video_name": "needle_24.mp4", "duration": 462.03999999999996, "question": "Where is the group of blue fin trevally hunting in the video?\n(A) Yap, Micronesia\n(B) Caribbean Sea, Bahamas\n(C) Red Sea, Egypt\n(D) Great Barrier Reef, Australia\n", "candidates": ["Yap, Micronesia", "Caribbean Sea, Bahamas", "Red Sea, Egypt", "Great Barrier Reef, Australia"], "answer": "A", "task_type": "needle", "question_id": "Q122"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "014092da2c173a1dd78fce26b54ef2868e17a63d0f4e1707390e20b47c3b0347", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where is the group of blue fin trevally hunting in the video?\n(A) Yap, Micronesia\n(B) Caribbean Sea, Bahamas\n(C) Red Sea, Egypt\n(D) Great Barrier Reef, Australia\n", "task_type": "needle", "pred_answer": "A.", "answer": "A"}, "input": "Where is the group of blue fin trevally hunting in the video?\n(A) Yap, Micronesia\n(B) Caribbean Sea, Bahamas\n(C) Red Sea, Egypt\n(D) Great Barrier Reef, Australia\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 126, "doc": {"video_name": "needle_27.mp4", "duration": 469.20000000000005, "question": "Where are the black cows with yellow tags on their ears eating hay on a sunny day?\n(A) At the farm\n(B) In the barn\n(C) In the forest\n(D) In the field\n", "candidates": ["At the farm", "In the barn", "In the forest", "In the field"], "answer": "A", "task_type": "needle", "question_id": "Q126"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a80413877722d923b41cf44030d6b1519ce9471896f3d2e08daae42b9bf81fe8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where are the black cows with yellow tags on their ears eating hay on a sunny day?\n(A) At the farm\n(B) In the barn\n(C) In the forest\n(D) In the field\n", "task_type": "needle", "pred_answer": "D", "answer": "A"}, "input": "Where are the black cows with yellow tags on their ears eating hay on a sunny day?\n(A) At the farm\n(B) In the barn\n(C) In the forest\n(D) In the field\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 130, "doc": {"video_name": "needle_130.mp4", "duration": 395.09999999999997, "question": "What is the owner wearing while stroking his Jack Russell dog?\n(A) Sneakers\n(B) Barefoot\n(C) Flip flops\n(D) Sports boots\n", "candidates": ["Sneakers", "Barefoot", "Flip flops", "Sports boots"], "answer": "D", "task_type": "needle", "question_id": "Q130"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bcb34acc07d452a219d89d693c176138efcd263876a5307d1389d4a0ca955ae2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the owner wearing while stroking his Jack Russell dog?\n(A) Sneakers\n(B) Barefoot\n(C) Flip flops\n(D) Sports boots\n", "task_type": "needle", "pred_answer": "A", "answer": "D"}, "input": "What is the owner wearing while stroking his Jack Russell dog?\n(A) Sneakers\n(B) Barefoot\n(C) Flip flops\n(D) Sports boots\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 134, "doc": {"video_name": "needle_143.mp4", "duration": 624.8699999999999, "question": "What is the woman doing on the computer?\n(A) Watching a movie\n(B) Playing a game\n(C) Shopping online\n(D) Writing an email\n", "candidates": ["Watching a movie", "Playing a game", "Shopping online", "Writing an email"], "answer": "C", "task_type": "needle", "question_id": "Q134"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "67f201510b54194355d395eef10cccc284f7f95a1ebe9865961a8a79efd4626d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the woman doing on the computer?\n(A) Watching a movie\n(B) Playing a game\n(C) Shopping online\n(D) Writing an email\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the woman doing on the computer?\n(A) Watching a movie\n(B) Playing a game\n(C) Shopping online\n(D) Writing an email\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 138, "doc": {"video_name": "needle_149.mp4", "duration": 324.02, "question": "What logo is displayed on the screen in the meeting room?\n(A) The Goldman Sachs Group\n(B) Google\n(C) Microsoft\n(D) Apple\n", "candidates": ["The Goldman Sachs Group", "Google", "Microsoft", "Apple"], "answer": "A", "task_type": "needle", "question_id": "Q138"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fe866e2aadcc78fa28e57c6c630d580cf43a9234b9898b353c5fbc8d934239b6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What logo is displayed on the screen in the meeting room?\n(A) The Goldman Sachs Group\n(B) Google\n(C) Microsoft\n(D) Apple\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What logo is displayed on the screen in the meeting room?\n(A) The Goldman Sachs Group\n(B) Google\n(C) Microsoft\n(D) Apple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 142, "doc": {"video_name": "needle_31.mp4", "duration": 1374.94, "question": "What is the setting of the highway road construction?\n(A) Underwater\n(B) In space\n(C) Indoor\n(D) Outdoor\n", "candidates": ["Underwater", "In space", "Indoor", "Outdoor"], "answer": "D", "task_type": "needle", "question_id": "Q142"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "482b439a70e793ee63a5c8d8cb7924423dead8c27366d66daa7caca89f338b64", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the setting of the highway road construction?\n(A) Underwater\n(B) In space\n(C) Indoor\n(D) Outdoor\n", "task_type": "needle", "pred_answer": "A", "answer": "D"}, "input": "What is the setting of the highway road construction?\n(A) Underwater\n(B) In space\n(C) Indoor\n(D) Outdoor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 146, "doc": {"video_name": "needle_13.mp4", "duration": 493.15, "question": "What are the volunteers doing to find the casualties of the rocket attack in the simulation drill?\n(A) Digging through the rubble\n(B) Using a metal detector\n(C) Using a search dog\n(D) Using a drone\n", "candidates": ["Digging through the rubble", "Using a metal detector", "Using a search dog", "Using a drone"], "answer": "A", "task_type": "needle", "question_id": "Q146"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "97db271751ca4b1172488194d61ee90ab0df33b511402fb99fe4b123b3b1ec4c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What are the volunteers doing to find the casualties of the rocket attack in the simulation drill?\n(A) Digging through the rubble\n(B) Using a metal detector\n(C) Using a search dog\n(D) Using a drone\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What are the volunteers doing to find the casualties of the rocket attack in the simulation drill?\n(A) Digging through the rubble\n(B) Using a metal detector\n(C) Using a search dog\n(D) Using a drone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 150, "doc": {"video_name": "needle_95.mp4", "duration": 464.54, "question": "Where is the cup that the female potter is stirring paint in?\n(A) In her hand\n(B) On the table\n(C) On the shelf\n(D) On the floor\n", "candidates": ["In her hand", "On the table", "On the shelf", "On the floor"], "answer": "B", "task_type": "needle", "question_id": "Q150"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7da824fd2febcad9298643dad3218d4f14288cfab05c43509e989a9e17dc9321", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where is the cup that the female potter is stirring paint in?\n(A) In her hand\n(B) On the table\n(C) On the shelf\n(D) On the floor\n", "task_type": "needle", "pred_answer": "A", "answer": "B"}, "input": "Where is the cup that the female potter is stirring paint in?\n(A) In her hand\n(B) On the table\n(C) On the shelf\n(D) On the floor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 154, "doc": {"video_name": "needle_33.mp4", "duration": 467.89000000000004, "question": "What unusual event happens while the woman is using the computer?\n(A) A hand comes out of the computer\n(B) The computer crashes\n(C) The screen goes blank\n(D) The computer starts talking\n", "candidates": ["A hand comes out of the computer", "The computer crashes", "The screen goes blank", "The computer starts talking"], "answer": "A", "task_type": "needle", "question_id": "Q154"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c2d8b0041ca9b6235f487dda9f832d2772bee24ecf0b743e772cef893ef85ff", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What unusual event happens while the woman is using the computer?\n(A) A hand comes out of the computer\n(B) The computer crashes\n(C) The screen goes blank\n(D) The computer starts talking\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What unusual event happens while the woman is using the computer?\n(A) A hand comes out of the computer\n(B) The computer crashes\n(C) The screen goes blank\n(D) The computer starts talking\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 158, "doc": {"video_name": "needle_12.mp4", "duration": 463.15, "question": "What are the volunteers searching for during the simulation drill?\n(A) Hidden treasure\n(B) Survivors of a natural disaster\n(C) Rocket attack casualties\n(D) Lost items\n", "candidates": ["Hidden treasure", "Survivors of a natural disaster", "Rocket attack casualties", "Lost items"], "answer": "C", "task_type": "needle", "question_id": "Q158"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e5da8258c57b930f5fc8b330354a1be1f35a69b8e548d15f399ba1e4c6373b42", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the volunteers searching for during the simulation drill?\n(A) Hidden treasure\n(B) Survivors of a natural disaster\n(C) Rocket attack casualties\n(D) Lost items\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What are the volunteers searching for during the simulation drill?\n(A) Hidden treasure\n(B) Survivors of a natural disaster\n(C) Rocket attack casualties\n(D) Lost items\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 162, "doc": {"video_name": "needle_67.mp4", "duration": 475.2, "question": "What time of the day is it in the video segment of the power plant block?\n(A) Midday\n(B) Dusk\n(C) Night\n(D) Dawn\n", "candidates": ["Midday", "Dusk", "Night", "Dawn"], "answer": "D", "task_type": "needle", "question_id": "Q162"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "92eded9cbdaeff52349f18dce4cdece50577f7671150bb63fa685f0df2576a74", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What time of the day is it in the video segment of the power plant block?\n(A) Midday\n(B) Dusk\n(C) Night\n(D) Dawn\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "What time of the day is it in the video segment of the power plant block?\n(A) Midday\n(B) Dusk\n(C) Night\n(D) Dawn\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 166, "doc": {"video_name": "needle_45.mp4", "duration": 500.52, "question": "What is the team of scientists doing in the laboratory room?\n(A) Conducting a physics experiment\n(B) Having a meeting\n(C) Teaching a class\n(D) Researching sample test with protection equipment and glasses\n", "candidates": ["Conducting a physics experiment", "Having a meeting", "Teaching a class", "Researching sample test with protection equipment and glasses"], "answer": "D", "task_type": "needle", "question_id": "Q166"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "01a07383dacb79e5ffcaa7221303b3ff97c952cf41133e4986c06596222daaf4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the team of scientists doing in the laboratory room?\n(A) Conducting a physics experiment\n(B) Having a meeting\n(C) Teaching a class\n(D) Researching sample test with protection equipment and glasses\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the team of scientists doing in the laboratory room?\n(A) Conducting a physics experiment\n(B) Having a meeting\n(C) Teaching a class\n(D) Researching sample test with protection equipment and glasses\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 170, "doc": {"video_name": "needle_96.mp4", "duration": 503.59, "question": "What are the couple planning and dreaming about?\n(A) New job\n(B) New home\n(C) Wedding\n(D) Vacation\n", "candidates": ["New job", "New home", "Wedding", "Vacation"], "answer": "B", "task_type": "needle", "question_id": "Q170"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f77f6c35da55ec52e4d4e21d253e546e63bc9ec8ed95bfcab601e1643fc2febb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What are the couple planning and dreaming about?\n(A) New job\n(B) New home\n(C) Wedding\n(D) Vacation\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What are the couple planning and dreaming about?\n(A) New job\n(B) New home\n(C) Wedding\n(D) Vacation\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 174, "doc": {"video_name": "needle_136.mp4", "duration": 7979.45, "question": "What is the serious older mature businesswoman writing on the whiteboard?\n(A) Notes\n(B) A novel\n(C) Her personal diary\n(D) A poem\n", "candidates": ["Notes", "A novel", "Her personal diary", "A poem"], "answer": "A", "task_type": "needle", "question_id": "Q174"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ee62427f66633ec80f53cb4fa16b41eb24a25b2989ee8e528d6076cbc05eef9c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the serious older mature businesswoman writing on the whiteboard?\n(A) Notes\n(B) A novel\n(C) Her personal diary\n(D) A poem\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the serious older mature businesswoman writing on the whiteboard?\n(A) Notes\n(B) A novel\n(C) Her personal diary\n(D) A poem\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 178, "doc": {"video_name": "needle_98.mp4", "duration": 461.7, "question": "What is the concept of the segment where the young girl in a tracksuit is doing yoga in the park?\n(A) Outdoor sports\n(B) Leisure and relaxation\n(C) Meditation in the fresh air\n(D) Physical fitness\n", "candidates": ["Outdoor sports", "Leisure and relaxation", "Meditation in the fresh air", "Physical fitness"], "answer": "C", "task_type": "needle", "question_id": "Q178"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c20543c05a32ce8ebae4b8de4bb830c8dcf3ad27371afe1cb58465d9d4caccc6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the concept of the segment where the young girl in a tracksuit is doing yoga in the park?\n(A) Outdoor sports\n(B) Leisure and relaxation\n(C) Meditation in the fresh air\n(D) Physical fitness\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the concept of the segment where the young girl in a tracksuit is doing yoga in the park?\n(A) Outdoor sports\n(B) Leisure and relaxation\n(C) Meditation in the fresh air\n(D) Physical fitness\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 182, "doc": {"video_name": "needle_44.mp4", "duration": 470.52, "question": "What type of protective gear are the scientists wearing in the laboratory?\n(A) Helmets\n(B) Gloves and glasses\n(C) Safety boots\n(D) None\n", "candidates": ["Helmets", "Gloves and glasses", "Safety boots", "None"], "answer": "B", "task_type": "needle", "question_id": "Q182"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ead18ee2ecb16f4ac07407b481d7540444e227a5ad77d22a87e694ece2fda5d1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of protective gear are the scientists wearing in the laboratory?\n(A) Helmets\n(B) Gloves and glasses\n(C) Safety boots\n(D) None\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What type of protective gear are the scientists wearing in the laboratory?\n(A) Helmets\n(B) Gloves and glasses\n(C) Safety boots\n(D) None\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 186, "doc": {"video_name": "needle_26.mp4", "duration": 469.08, "question": "What color are the cows eating hay from the stall at the farm outdoors on a summer sunny day?\n(A) Brown\n(B) Spotted\n(C) White\n(D) Black\n", "candidates": ["Brown", "Spotted", "White", "Black"], "answer": "D", "task_type": "needle", "question_id": "Q186"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c5465b117fb30413aaf564f403c3d3a7679de4ab25990e9ac2235216ad6ad24", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color are the cows eating hay from the stall at the farm outdoors on a summer sunny day?\n(A) Brown\n(B) Spotted\n(C) White\n(D) Black\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What color are the cows eating hay from the stall at the farm outdoors on a summer sunny day?\n(A) Brown\n(B) Spotted\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 190, "doc": {"video_name": "needle_40.mp4", "duration": 477.20000000000005, "question": "What is the state of movement of the American toad at the mouth of the den in the video?\n(A) Moderate movement\n(B) Very little movement\n(C) Constant movement\n(D) No movement at all\n", "candidates": ["Moderate movement", "Very little movement", "Constant movement", "No movement at all"], "answer": "B", "task_type": "needle", "question_id": "Q190"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B."]], "filtered_resps": ["B."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e44f4dfa1600e7437cf200b697161e899b4c03605035eb44575d4027ee08c97d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the state of movement of the American toad at the mouth of the den in the video?\n(A) Moderate movement\n(B) Very little movement\n(C) Constant movement\n(D) No movement at all\n", "task_type": "needle", "pred_answer": "B.", "answer": "B"}, "input": "What is the state of movement of the American toad at the mouth of the den in the video?\n(A) Moderate movement\n(B) Very little movement\n(C) Constant movement\n(D) No movement at all\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 194, "doc": {"video_name": "needle_96.mp4", "duration": 503.59, "question": "What are the couple surrounded by while sitting on the couch?\n(A) Furniture\n(B) Balloons\n(C) Pillows\n(D) Cardboard boxes\n", "candidates": ["Furniture", "Balloons", "Pillows", "Cardboard boxes"], "answer": "D", "task_type": "needle", "question_id": "Q194"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "839b684cfb86dfe172ee6476fe48ea5156c6da7e74a8bf8a6fe241de39c7b615", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What are the couple surrounded by while sitting on the couch?\n(A) Furniture\n(B) Balloons\n(C) Pillows\n(D) Cardboard boxes\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What are the couple surrounded by while sitting on the couch?\n(A) Furniture\n(B) Balloons\n(C) Pillows\n(D) Cardboard boxes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 198, "doc": {"video_name": "needle_59.mp4", "duration": 460.02000000000004, "question": "Where does the woman performing the microblasting routine work?\n(A) Spa\n(B) Beauty Salon\n(C) Hospital\n(D) Cosmetology Clinic\n", "candidates": ["Spa", "Beauty Salon", "Hospital", "Cosmetology Clinic"], "answer": "D", "task_type": "needle", "question_id": "Q198"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c03683c6663ebe5b67629fb85cf0356bd8b7a4db76f1de6df75a6b51c076ce75", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where does the woman performing the microblasting routine work?\n(A) Spa\n(B) Beauty Salon\n(C) Hospital\n(D) Cosmetology Clinic\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "Where does the woman performing the microblasting routine work?\n(A) Spa\n(B) Beauty Salon\n(C) Hospital\n(D) Cosmetology Clinic\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 202, "doc": {"video_name": "needle_93.mp4", "duration": 458.01, "question": "What is the woman wearing during the summer sunset?\n(A) A swimsuit\n(B) A winter coat\n(C) A dress and heels\n(D) A hat and sunglasses\n", "candidates": ["A swimsuit", "A winter coat", "A dress and heels", "A hat and sunglasses"], "answer": "D", "task_type": "needle", "question_id": "Q202"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "144920dd2ed4b40759de1bf36707599b6ca9bc4ce9b05d61079c23784bdc32a1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the woman wearing during the summer sunset?\n(A) A swimsuit\n(B) A winter coat\n(C) A dress and heels\n(D) A hat and sunglasses\n", "task_type": "needle", "pred_answer": "C.", "answer": "D"}, "input": "What is the woman wearing during the summer sunset?\n(A) A swimsuit\n(B) A winter coat\n(C) A dress and heels\n(D) A hat and sunglasses\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 206, "doc": {"video_name": "needle_11.mp4", "duration": 454.52, "question": "What is happening in the control room when the sound engineer is moving levers of a multitrack mixing console?\n(A) A recording session\n(B) A press conference\n(C) A live concert\n(D) A video shoot\n", "candidates": ["A recording session", "A press conference", "A live concert", "A video shoot"], "answer": "A", "task_type": "needle", "question_id": "Q206"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "61d095071a95b0eb3fd34e77e012d77a5fa6a8e91c8a762ac48a29105fbd5226", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is happening in the control room when the sound engineer is moving levers of a multitrack mixing console?\n(A) A recording session\n(B) A press conference\n(C) A live concert\n(D) A video shoot\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is happening in the control room when the sound engineer is moving levers of a multitrack mixing console?\n(A) A recording session\n(B) A press conference\n(C) A live concert\n(D) A video shoot\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 210, "doc": {"video_name": "needle_86.mp4", "duration": 470.40999999999997, "question": "What is the direction of the boy walking in the water with colorful party balloons towards?\n(A) Towards the snowy mountain\n(B) Towards the dark forest\n(C) Towards the sparkling sunset light\n(D) Towards the crowded city\n", "candidates": ["Towards the snowy mountain", "Towards the dark forest", "Towards the sparkling sunset light", "Towards the crowded city"], "answer": "C", "task_type": "needle", "question_id": "Q210"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3dc3f6c739066c26012ad8497af243cce6b0835fe11ae54187742d6c3a6f93e4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the direction of the boy walking in the water with colorful party balloons towards?\n(A) Towards the snowy mountain\n(B) Towards the dark forest\n(C) Towards the sparkling sunset light\n(D) Towards the crowded city\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the direction of the boy walking in the water with colorful party balloons towards?\n(A) Towards the snowy mountain\n(B) Towards the dark forest\n(C) Towards the sparkling sunset light\n(D) Towards the crowded city\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 214, "doc": {"video_name": "needle_10.mp4", "duration": 454.52, "question": "What is happening in the control room when the sound engineer is moving levers of a multitrack mixing console?\n(A) A live concert\n(B) A video shoot\n(C) A press conference\n(D) A recording session\n", "candidates": ["A live concert", "A video shoot", "A press conference", "A recording session"], "answer": "D", "task_type": "needle", "question_id": "Q214"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f0b064b55bfed014782389d8d023dce25e06e2d6c13764dd44e1ed2e118fb7f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is happening in the control room when the sound engineer is moving levers of a multitrack mixing console?\n(A) A live concert\n(B) A video shoot\n(C) A press conference\n(D) A recording session\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is happening in the control room when the sound engineer is moving levers of a multitrack mixing console?\n(A) A live concert\n(B) A video shoot\n(C) A press conference\n(D) A recording session\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 218, "doc": {"video_name": "needle_81.mp4", "duration": 463.1, "question": "What is the result of the egg falling on the glass floor?\n(A) The egg breaks and creates a mess\n(B) The egg disappears\n(C) The egg cracks slightly\n(D) The egg remains intact\n", "candidates": ["The egg breaks and creates a mess", "The egg disappears", "The egg cracks slightly", "The egg remains intact"], "answer": "A", "task_type": "needle", "question_id": "Q218"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "34efbc48a0456add92d2b4f0bbbaea3a0313b06b0848f149dfaebe5c5471701d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the result of the egg falling on the glass floor?\n(A) The egg breaks and creates a mess\n(B) The egg disappears\n(C) The egg cracks slightly\n(D) The egg remains intact\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the result of the egg falling on the glass floor?\n(A) The egg breaks and creates a mess\n(B) The egg disappears\n(C) The egg cracks slightly\n(D) The egg remains intact\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 222, "doc": {"video_name": "needle_41.mp4", "duration": 477.07, "question": "What is the American toad doing at the mouth of the den in the video?\n(A) Jumping\n(B) Breathing and waiting\n(C) Sleeping\n(D) Eating\n", "candidates": ["Jumping", "Breathing and waiting", "Sleeping", "Eating"], "answer": "B", "task_type": "needle", "question_id": "Q222"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "04c1101169fd2eebb5eee286b83e0057bae832531588b5fa8563f82462b29a59", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the American toad doing at the mouth of the den in the video?\n(A) Jumping\n(B) Breathing and waiting\n(C) Sleeping\n(D) Eating\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the American toad doing at the mouth of the den in the video?\n(A) Jumping\n(B) Breathing and waiting\n(C) Sleeping\n(D) Eating\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 226, "doc": {"video_name": "needle_65.mp4", "duration": 500.88, "question": "What season is depicted in the video segment where the forest fire is happening?\n(A) Dry season\n(B) Winter season\n(C) Rainy season\n(D) Spring season\n", "candidates": ["Dry season", "Winter season", "Rainy season", "Spring season"], "answer": "A", "task_type": "needle", "question_id": "Q226"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59dfab41ac50ebf9e0a8cd0180ec4b77ddb086bea5b08ece04b66f387a4ec6f5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What season is depicted in the video segment where the forest fire is happening?\n(A) Dry season\n(B) Winter season\n(C) Rainy season\n(D) Spring season\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What season is depicted in the video segment where the forest fire is happening?\n(A) Dry season\n(B) Winter season\n(C) Rainy season\n(D) Spring season\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 230, "doc": {"video_name": "needle_85.mp4", "duration": 475.83000000000004, "question": "What is the background when the engineer begins work with the drawings in the video?\n(A) Windmills\n(B) Mountains\n(C) Cityscape\n(D) Forest\n", "candidates": ["Windmills", "Mountains", "Cityscape", "Forest"], "answer": "A", "task_type": "needle", "question_id": "Q230"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ee6889e2609befeb5254a1bd1b36cefad3c4f071f5523f299f2c5f05e1021bf7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the background when the engineer begins work with the drawings in the video?\n(A) Windmills\n(B) Mountains\n(C) Cityscape\n(D) Forest\n", "task_type": "needle", "pred_answer": "A.", "answer": "A"}, "input": "What is the background when the engineer begins work with the drawings in the video?\n(A) Windmills\n(B) Mountains\n(C) Cityscape\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 234, "doc": {"video_name": "needle_34.mp4", "duration": 455.36, "question": "What are the dentist and his assistant doing to the patient's teeth in the video segment?\n(A) Whitening\n(B) Extracting\n(C) Cleaning\n(D) Treating\n", "candidates": ["Whitening", "Extracting", "Cleaning", "Treating"], "answer": "D", "task_type": "needle", "question_id": "Q234"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f991f1b1f6066ba0ddaacde7038eca43d76094175dbf945929088917d4fc4d6f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What are the dentist and his assistant doing to the patient's teeth in the video segment?\n(A) Whitening\n(B) Extracting\n(C) Cleaning\n(D) Treating\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What are the dentist and his assistant doing to the patient's teeth in the video segment?\n(A) Whitening\n(B) Extracting\n(C) Cleaning\n(D) Treating\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 238, "doc": {"video_name": "needle_36.mp4", "duration": 478.7, "question": "What type of beach is shown in the video?\n(A) Wild rocky beach\n(B) Pebble beach\n(C) Man-made beach\n(D) Sandy beach\n", "candidates": ["Wild rocky beach", "Pebble beach", "Man-made beach", "Sandy beach"], "answer": "A", "task_type": "needle", "question_id": "Q238"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c4b7d7750b9328f518fba2cbe1c382dbb553b309bb0b00a4450ff85b3105fa57", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of beach is shown in the video?\n(A) Wild rocky beach\n(B) Pebble beach\n(C) Man-made beach\n(D) Sandy beach\n", "task_type": "needle", "pred_answer": "D", "answer": "A"}, "input": "What type of beach is shown in the video?\n(A) Wild rocky beach\n(B) Pebble beach\n(C) Man-made beach\n(D) Sandy beach\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 242, "doc": {"video_name": "needle_124.mp4", "duration": 254.78, "question": "What is the weather like when the tourists are strolling in the Plaza de Espana in Seville?\n(A) Cloudy\n(B) Rainy\n(C) Snowy\n(D) Sunny\n", "candidates": ["Cloudy", "Rainy", "Snowy", "Sunny"], "answer": "D", "task_type": "needle", "question_id": "Q242"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c50e42ed0e35ecadef4bd6047a9ff061c8e42e9d264e209b551054ecaad48b49", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the weather like when the tourists are strolling in the Plaza de Espana in Seville?\n(A) Cloudy\n(B) Rainy\n(C) Snowy\n(D) Sunny\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the weather like when the tourists are strolling in the Plaza de Espana in Seville?\n(A) Cloudy\n(B) Rainy\n(C) Snowy\n(D) Sunny\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 246, "doc": {"video_name": "needle_118.mp4", "duration": 812.24, "question": "What is the gender of the patient the young attractive hispanic medical doctor is discussing health issues with?\n(A) Both male and female\n(B) Male\n(C) Female\n(D) Not specified\n", "candidates": ["Both male and female", "Male", "Female", "Not specified"], "answer": "C", "task_type": "needle", "question_id": "Q246"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "52ac2dd8fb93f9b4001804df7e13c0b6e0fb132ab79df18c32da87ad2ad3cd8a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the gender of the patient the young attractive hispanic medical doctor is discussing health issues with?\n(A) Both male and female\n(B) Male\n(C) Female\n(D) Not specified\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the gender of the patient the young attractive hispanic medical doctor is discussing health issues with?\n(A) Both male and female\n(B) Male\n(C) Female\n(D) Not specified\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 250, "doc": {"video_name": "needle_57.mp4", "duration": 448.06, "question": "Where is the woman working?\n(A) In a garment factory\n(B) In a school\n(C) In a hospital\n(D) In a restaurant\n", "candidates": ["In a garment factory", "In a school", "In a hospital", "In a restaurant"], "answer": "A", "task_type": "needle", "question_id": "Q250"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e5897ec672675626628f4ec84f970f558387d51c2ea3ba8e672e58cf9aef54a3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where is the woman working?\n(A) In a garment factory\n(B) In a school\n(C) In a hospital\n(D) In a restaurant\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "Where is the woman working?\n(A) In a garment factory\n(B) In a school\n(C) In a hospital\n(D) In a restaurant\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 254, "doc": {"video_name": "needle_73.mp4", "duration": 471.08, "question": "What is the woman doing in the flower shop?\n(A) Making a bouquet of fresh flowers\n(B) Arranging books\n(C) Watering the plants\n(D) Cleaning the shop\n", "candidates": ["Making a bouquet of fresh flowers", "Arranging books", "Watering the plants", "Cleaning the shop"], "answer": "A", "task_type": "needle", "question_id": "Q254"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a4fab52c5627a81df4049dee076bee8409b0f4ab969b1365b22fdb8c5f38ca7b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the woman doing in the flower shop?\n(A) Making a bouquet of fresh flowers\n(B) Arranging books\n(C) Watering the plants\n(D) Cleaning the shop\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the woman doing in the flower shop?\n(A) Making a bouquet of fresh flowers\n(B) Arranging books\n(C) Watering the plants\n(D) Cleaning the shop\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 258, "doc": {"video_name": "needle_51.mp4", "duration": 1504.7, "question": "What are the two kids doing in the paddy field?\n(A) Studying\n(B) Having fun\n(C) Playing football\n(D) Sleeping\n", "candidates": ["Studying", "Having fun", "Playing football", "Sleeping"], "answer": "B", "task_type": "needle", "question_id": "Q258"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9fca7a62da5c81cea50f7812abdda3e17a2fbeeb144cfbf979f40e1646711509", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What are the two kids doing in the paddy field?\n(A) Studying\n(B) Having fun\n(C) Playing football\n(D) Sleeping\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What are the two kids doing in the paddy field?\n(A) Studying\n(B) Having fun\n(C) Playing football\n(D) Sleeping\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 262, "doc": {"video_name": "needle_27.mp4", "duration": 469.20000000000005, "question": "What are the cows with yellow tags on their ears doing at the farm outdoors on a summer sunny day?\n(A) Sleeping\n(B) Running\n(C) Eating hay\n(D) Drinking water\n", "candidates": ["Sleeping", "Running", "Eating hay", "Drinking water"], "answer": "C", "task_type": "needle", "question_id": "Q262"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c64a3ab0df4dc831e92f8ac1750832bc9e506c2c5e2adb70009186fa26c49ca8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the cows with yellow tags on their ears doing at the farm outdoors on a summer sunny day?\n(A) Sleeping\n(B) Running\n(C) Eating hay\n(D) Drinking water\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What are the cows with yellow tags on their ears doing at the farm outdoors on a summer sunny day?\n(A) Sleeping\n(B) Running\n(C) Eating hay\n(D) Drinking water\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 266, "doc": {"video_name": "needle_118.mp4", "duration": 812.24, "question": "Where is the young attractive hispanic medical doctor discussing health issues with the senior patient?\n(A) Inside office\n(B) In a park\n(C) At the patient's home\n(D) In a hospital ward\n", "candidates": ["Inside office", "In a park", "At the patient's home", "In a hospital ward"], "answer": "A", "task_type": "needle", "question_id": "Q266"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cd00d5ec89c015554b461d48a153394e69ff1ad0df548aa9804cd5b9c0256173", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where is the young attractive hispanic medical doctor discussing health issues with the senior patient?\n(A) Inside office\n(B) In a park\n(C) At the patient's home\n(D) In a hospital ward\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "Where is the young attractive hispanic medical doctor discussing health issues with the senior patient?\n(A) Inside office\n(B) In a park\n(C) At the patient's home\n(D) In a hospital ward\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 270, "doc": {"video_name": "needle_8.mp4", "duration": 461.12, "question": "What is the friend doing in the video call?\n(A) Reading a book\n(B) Playing with a dog\n(C) Cooking dinner\n(D) Watching TV\n", "candidates": ["Reading a book", "Playing with a dog", "Cooking dinner", "Watching TV"], "answer": "B", "task_type": "needle", "question_id": "Q270"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e0cbf07076cea156ac92c2effdd30ee34ebbfa70f87ba32e71952738350682a1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the friend doing in the video call?\n(A) Reading a book\n(B) Playing with a dog\n(C) Cooking dinner\n(D) Watching TV\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the friend doing in the video call?\n(A) Reading a book\n(B) Playing with a dog\n(C) Cooking dinner\n(D) Watching TV\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 274, "doc": {"video_name": "needle_97.mp4", "duration": 473.59000000000003, "question": "What are the couple doing on the couch?\n(A) Choosing new home and ordering furniture online\n(B) Eating dinner\n(C) Watching TV\n(D) Reading a book\n", "candidates": ["Choosing new home and ordering furniture online", "Eating dinner", "Watching TV", "Reading a book"], "answer": "A", "task_type": "needle", "question_id": "Q274"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "94cecc9d777153c9da1659d1bed5f442fe36c91787699a23635fb72c60ef1c43", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What are the couple doing on the couch?\n(A) Choosing new home and ordering furniture online\n(B) Eating dinner\n(C) Watching TV\n(D) Reading a book\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What are the couple doing on the couch?\n(A) Choosing new home and ordering furniture online\n(B) Eating dinner\n(C) Watching TV\n(D) Reading a book\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 278, "doc": {"video_name": "needle_7.mp4", "duration": 471.23, "question": "What is the woman in the bikini holding while enjoying her summer holidays in the swimming pool?\n(A) A book\n(B) A cocktail\n(C) A towel\n(D) A beach ball\n", "candidates": ["A book", "A cocktail", "A towel", "A beach ball"], "answer": "B", "task_type": "needle", "question_id": "Q278"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d019ac99d8d00acdb781e249f28bd71d1132cf825652c929902d8c2d27997f4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the woman in the bikini holding while enjoying her summer holidays in the swimming pool?\n(A) A book\n(B) A cocktail\n(C) A towel\n(D) A beach ball\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the woman in the bikini holding while enjoying her summer holidays in the swimming pool?\n(A) A book\n(B) A cocktail\n(C) A towel\n(D) A beach ball\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 282, "doc": {"video_name": "needle_92.mp4", "duration": 458.13, "question": "What is the woman doing during the summer sunset?\n(A) Having a picnic\n(B) Swimming in the sea\n(C) Taking a moment to enjoy life\n(D) Reading a book\n", "candidates": ["Having a picnic", "Swimming in the sea", "Taking a moment to enjoy life", "Reading a book"], "answer": "C", "task_type": "needle", "question_id": "Q282"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f6218d135adcb3fb1e655704fa94245b1ce60a5786036f2499dd620e346e8ba6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the woman doing during the summer sunset?\n(A) Having a picnic\n(B) Swimming in the sea\n(C) Taking a moment to enjoy life\n(D) Reading a book\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the woman doing during the summer sunset?\n(A) Having a picnic\n(B) Swimming in the sea\n(C) Taking a moment to enjoy life\n(D) Reading a book\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 286, "doc": {"video_name": "needle_2.mp4", "duration": 1372.5, "question": "Who is the young attractive hispanic medical doctor discussing health issues with?\n(A) A senior patient\n(B) A male patient\n(C) A child patient\n(D) A colleague\n", "candidates": ["A senior patient", "A male patient", "A child patient", "A colleague"], "answer": "A", "task_type": "needle", "question_id": "Q286"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a3d6c1365403e7306e64b12c941cd68f97cb298922eac0b65504bb0133a2051c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Who is the young attractive hispanic medical doctor discussing health issues with?\n(A) A senior patient\n(B) A male patient\n(C) A child patient\n(D) A colleague\n", "task_type": "needle", "pred_answer": "B", "answer": "A"}, "input": "Who is the young attractive hispanic medical doctor discussing health issues with?\n(A) A senior patient\n(B) A male patient\n(C) A child patient\n(D) A colleague\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 290, "doc": {"video_name": "needle_111.mp4", "duration": 5487.650000000001, "question": "Where are the silhouette fishermen performing their popular activity during the monsoon season?\n(A) In a city\n(B) In a forest\n(C) On a boat\n(D) On the beach\n", "candidates": ["In a city", "In a forest", "On a boat", "On the beach"], "answer": "D", "task_type": "needle", "question_id": "Q290"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "212b4f44b3831b70f9b65b8d507fab5c9038ea16e6aa23624fd997c89f1f641f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where are the silhouette fishermen performing their popular activity during the monsoon season?\n(A) In a city\n(B) In a forest\n(C) On a boat\n(D) On the beach\n", "task_type": "needle", "pred_answer": "C", "answer": "D"}, "input": "Where are the silhouette fishermen performing their popular activity during the monsoon season?\n(A) In a city\n(B) In a forest\n(C) On a boat\n(D) On the beach\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 294, "doc": {"video_name": "needle_97.mp4", "duration": 473.59000000000003, "question": "What are the couple surrounded by while sitting on the couch?\n(A) Pillows\n(B) Balloons\n(C) Furniture\n(D) Cardboard boxes\n", "candidates": ["Pillows", "Balloons", "Furniture", "Cardboard boxes"], "answer": "D", "task_type": "needle", "question_id": "Q294"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ba5abdfd1bb9466675636ee47e327dbef873c3578edc117baa31bb16bf445cde", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What are the couple surrounded by while sitting on the couch?\n(A) Pillows\n(B) Balloons\n(C) Furniture\n(D) Cardboard boxes\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What are the couple surrounded by while sitting on the couch?\n(A) Pillows\n(B) Balloons\n(C) Furniture\n(D) Cardboard boxes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 298, "doc": {"video_name": "needle_119.mp4", "duration": 766.14, "question": "What is the weather condition on the basketball court in the video?\n(A) Sunny\n(B) Rainy\n(C) Beautiful sunset\n(D) Cloudy\n", "candidates": ["Sunny", "Rainy", "Beautiful sunset", "Cloudy"], "answer": "C", "task_type": "needle", "question_id": "Q298"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "188d56142e37f207aaa50f3dbd7f282422fb998aa93e3f4b25d93c01fd4faeb9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the weather condition on the basketball court in the video?\n(A) Sunny\n(B) Rainy\n(C) Beautiful sunset\n(D) Cloudy\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "What is the weather condition on the basketball court in the video?\n(A) Sunny\n(B) Rainy\n(C) Beautiful sunset\n(D) Cloudy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 302, "doc": {"video_name": "needle_135.mp4", "duration": 344.4, "question": "What is the man doing over the white background?\n(A) Singing\n(B) Crying\n(C) Dancing\n(D) Laughing and covering his mouth with hand\n", "candidates": ["Singing", "Crying", "Dancing", "Laughing and covering his mouth with hand"], "answer": "D", "task_type": "needle", "question_id": "Q302"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ccb318443e6c621c6554b7f5e4e71a8843504649af1cf54606d070469c4cff76", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the man doing over the white background?\n(A) Singing\n(B) Crying\n(C) Dancing\n(D) Laughing and covering his mouth with hand\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the man doing over the white background?\n(A) Singing\n(B) Crying\n(C) Dancing\n(D) Laughing and covering his mouth with hand\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 306, "doc": {"video_name": "needle_26.mp4", "duration": 469.08, "question": "What are the cows with yellow tags on their ears doing at the farm outdoors on a summer sunny day?\n(A) Running\n(B) Drinking water\n(C) Eating hay\n(D) Sleeping\n", "candidates": ["Running", "Drinking water", "Eating hay", "Sleeping"], "answer": "C", "task_type": "needle", "question_id": "Q306"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a0edd4c36e3240f63db1a3b1226c7aa31dcf3fbd9db4b1b32ad3b8dedec3f0de", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the cows with yellow tags on their ears doing at the farm outdoors on a summer sunny day?\n(A) Running\n(B) Drinking water\n(C) Eating hay\n(D) Sleeping\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What are the cows with yellow tags on their ears doing at the farm outdoors on a summer sunny day?\n(A) Running\n(B) Drinking water\n(C) Eating hay\n(D) Sleeping\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 310, "doc": {"video_name": "needle_96.mp4", "duration": 503.59, "question": "What are the couple doing on the couch?\n(A) Eating dinner\n(B) Choosing new home and ordering furniture online\n(C) Reading a book\n(D) Watching TV\n", "candidates": ["Eating dinner", "Choosing new home and ordering furniture online", "Reading a book", "Watching TV"], "answer": "B", "task_type": "needle", "question_id": "Q310"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6a9fd1d7abf7769d5821e7a5724a2b8baf5e74b8daa82c82b39c62ce7e07a705", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What are the couple doing on the couch?\n(A) Eating dinner\n(B) Choosing new home and ordering furniture online\n(C) Reading a book\n(D) Watching TV\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What are the couple doing on the couch?\n(A) Eating dinner\n(B) Choosing new home and ordering furniture online\n(C) Reading a book\n(D) Watching TV\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 314, "doc": {"video_name": "needle_82.mp4", "duration": 494.38, "question": "What is the man doing over the white background?\n(A) Singing\n(B) Laughing and covering his mouth with hand\n(C) Crying\n(D) Dancing\n", "candidates": ["Singing", "Laughing and covering his mouth with hand", "Crying", "Dancing"], "answer": "B", "task_type": "needle", "question_id": "Q314"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "67b597e61d5342de5ba214792a7c0fe6f86bc8ef423b0f0f3b07d8f6a86564bd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the man doing over the white background?\n(A) Singing\n(B) Laughing and covering his mouth with hand\n(C) Crying\n(D) Dancing\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the man doing over the white background?\n(A) Singing\n(B) Laughing and covering his mouth with hand\n(C) Crying\n(D) Dancing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 318, "doc": {"video_name": "needle_44.mp4", "duration": 470.52, "question": "What is the team of scientists doing in the laboratory room?\n(A) Conducting a physics experiment\n(B) Having a meeting\n(C) Researching sample test with protection equipment and glasses\n(D) Teaching a class\n", "candidates": ["Conducting a physics experiment", "Having a meeting", "Researching sample test with protection equipment and glasses", "Teaching a class"], "answer": "C", "task_type": "needle", "question_id": "Q318"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1f40479539043d715b67e2af20dc7f2783cda98269a453b6aef2c8db341e98c8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the team of scientists doing in the laboratory room?\n(A) Conducting a physics experiment\n(B) Having a meeting\n(C) Researching sample test with protection equipment and glasses\n(D) Teaching a class\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "What is the team of scientists doing in the laboratory room?\n(A) Conducting a physics experiment\n(B) Having a meeting\n(C) Researching sample test with protection equipment and glasses\n(D) Teaching a class\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 322, "doc": {"video_name": "needle_6.mp4", "duration": 471.23, "question": "Where is the woman in the bikini enjoying the summer sun and tanning during her holidays?\n(A) In a swimming pool\n(B) On a sun lounger\n(C) In a hot tub\n(D) At the beach\n", "candidates": ["In a swimming pool", "On a sun lounger", "In a hot tub", "At the beach"], "answer": "A", "task_type": "needle", "question_id": "Q322"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B."]], "filtered_resps": ["B."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b8afa8a04f357ab7212ead383266371956c0127d8023ec196d761bcbeac35acc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where is the woman in the bikini enjoying the summer sun and tanning during her holidays?\n(A) In a swimming pool\n(B) On a sun lounger\n(C) In a hot tub\n(D) At the beach\n", "task_type": "needle", "pred_answer": "B.", "answer": "A"}, "input": "Where is the woman in the bikini enjoying the summer sun and tanning during her holidays?\n(A) In a swimming pool\n(B) On a sun lounger\n(C) In a hot tub\n(D) At the beach\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 326, "doc": {"video_name": "needle_64.mp4", "duration": 470.88, "question": "What is happening to the old dry grass from last year in the video segment?\n(A) It is being watered\n(B) It is growing\n(C) It is being cut\n(D) It is burning\n", "candidates": ["It is being watered", "It is growing", "It is being cut", "It is burning"], "answer": "D", "task_type": "needle", "question_id": "Q326"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "646ec5e1be2fb4a5f387d19f488405958fdb0448d2b5b498983fb9bd65305942", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is happening to the old dry grass from last year in the video segment?\n(A) It is being watered\n(B) It is growing\n(C) It is being cut\n(D) It is burning\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is happening to the old dry grass from last year in the video segment?\n(A) It is being watered\n(B) It is growing\n(C) It is being cut\n(D) It is burning\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 330, "doc": {"video_name": "needle_5.mp4", "duration": 480.0, "question": "What is the man in the black silhouette catching on the lake shore?\n(A) A ball\n(B) A drone quadcopter\n(C) A bird\n(D) A frisbee\n", "candidates": ["A ball", "A drone quadcopter", "A bird", "A frisbee"], "answer": "B", "task_type": "needle", "question_id": "Q330"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d2855dd3a561183044ce8b504149aeaeb6230576d0783e55a43e446184374e09", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the man in the black silhouette catching on the lake shore?\n(A) A ball\n(B) A drone quadcopter\n(C) A bird\n(D) A frisbee\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the man in the black silhouette catching on the lake shore?\n(A) A ball\n(B) A drone quadcopter\n(C) A bird\n(D) A frisbee\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 334, "doc": {"video_name": "needle_59.mp4", "duration": 460.02000000000004, "question": "What is the profession of the woman performing the microblasting routine in the video?\n(A) Hair Stylist\n(B) Dentist\n(C) Cosmetologist\n(D) Nurse\n", "candidates": ["Hair Stylist", "Dentist", "Cosmetologist", "Nurse"], "answer": "C", "task_type": "needle", "question_id": "Q334"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b373e1c17b7f828ac5a21e79310a31db21160e070a02ba714c28715d607baee1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the profession of the woman performing the microblasting routine in the video?\n(A) Hair Stylist\n(B) Dentist\n(C) Cosmetologist\n(D) Nurse\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the profession of the woman performing the microblasting routine in the video?\n(A) Hair Stylist\n(B) Dentist\n(C) Cosmetologist\n(D) Nurse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 338, "doc": {"video_name": "needle_5.mp4", "duration": 480.0, "question": "What is the man in the video doing on the lake shore during the sunny summer?\n(A) Swimming\n(B) Sunbathing\n(C) Launching the drone\n(D) Catching the drone\n", "candidates": ["Swimming", "Sunbathing", "Launching the drone", "Catching the drone"], "answer": "D", "task_type": "needle", "question_id": "Q338"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "076d1694ec6612da5b374b7aa538d32014b75c30cf5e5ed068d7de2d464fce14", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the man in the video doing on the lake shore during the sunny summer?\n(A) Swimming\n(B) Sunbathing\n(C) Launching the drone\n(D) Catching the drone\n", "task_type": "needle", "pred_answer": "C.", "answer": "D"}, "input": "What is the man in the video doing on the lake shore during the sunny summer?\n(A) Swimming\n(B) Sunbathing\n(C) Launching the drone\n(D) Catching the drone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 342, "doc": {"video_name": "needle_39.mp4", "duration": 583.8199999999999, "question": "What activity are the silhouette fishermen doing on the beach during the monsoon season?\n(A) Fishing\n(B) Building sandcastles\n(C) Swimming\n(D) Sunbathing\n", "candidates": ["Fishing", "Building sandcastles", "Swimming", "Sunbathing"], "answer": "A", "task_type": "needle", "question_id": "Q342"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a9297154e21275b1eb9be174bc025c1aa3eab2384ab1266cd16b434b997e0526", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What activity are the silhouette fishermen doing on the beach during the monsoon season?\n(A) Fishing\n(B) Building sandcastles\n(C) Swimming\n(D) Sunbathing\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What activity are the silhouette fishermen doing on the beach during the monsoon season?\n(A) Fishing\n(B) Building sandcastles\n(C) Swimming\n(D) Sunbathing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 346, "doc": {"video_name": "needle_148.mp4", "duration": 6546.93, "question": "What is the child doing at home in the video?\n(A) Playing game on mobile phone\n(B) Sleeping\n(C) Playing with toys\n(D) Eating\n", "candidates": ["Playing game on mobile phone", "Sleeping", "Playing with toys", "Eating"], "answer": "A", "task_type": "needle", "question_id": "Q346"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "86c312d5f6ab0c784d2e45d7818645f1e28a26fcddeb44b9b3198bea9359903e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the child doing at home in the video?\n(A) Playing game on mobile phone\n(B) Sleeping\n(C) Playing with toys\n(D) Eating\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the child doing at home in the video?\n(A) Playing game on mobile phone\n(B) Sleeping\n(C) Playing with toys\n(D) Eating\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 350, "doc": {"video_name": "needle_133.mp4", "duration": 7421.88, "question": "What color are the cows eating hay from the stall at the farm outdoors on a summer sunny day?\n(A) Brown\n(B) Spotted\n(C) White\n(D) Black\n", "candidates": ["Brown", "Spotted", "White", "Black"], "answer": "D", "task_type": "needle", "question_id": "Q350"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c5465b117fb30413aaf564f403c3d3a7679de4ab25990e9ac2235216ad6ad24", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color are the cows eating hay from the stall at the farm outdoors on a summer sunny day?\n(A) Brown\n(B) Spotted\n(C) White\n(D) Black\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "What color are the cows eating hay from the stall at the farm outdoors on a summer sunny day?\n(A) Brown\n(B) Spotted\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 354, "doc": {"video_name": "needle_100.mp4", "duration": 751.67, "question": "Where is the chef preparing the dinner?\n(A) In a hotel\n(B) In a restaurant\n(C) In a kitchen\n(D) By the ocean shore on an island\n", "candidates": ["In a hotel", "In a restaurant", "In a kitchen", "By the ocean shore on an island"], "answer": "D", "task_type": "needle", "question_id": "Q354"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d5a3cbf2fc63eba217f92398a70b52b80a8c2fd94c2f8d74eb8e45a509713274", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where is the chef preparing the dinner?\n(A) In a hotel\n(B) In a restaurant\n(C) In a kitchen\n(D) By the ocean shore on an island\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "Where is the chef preparing the dinner?\n(A) In a hotel\n(B) In a restaurant\n(C) In a kitchen\n(D) By the ocean shore on an island\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 358, "doc": {"video_name": "surveil_11.mp4", "duration": 428.55, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Fighting\n(B) Abuse\n(C) Stealing\n(D) Shooting\n", "candidates": ["Fighting", "Abuse", "Stealing", "Shooting"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q358"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6cbcb9e8032a752a8fa7b50bf77a380e5005baee6f7aa0e8fb863327611b7163", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Fighting\n(B) Abuse\n(C) Stealing\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Fighting\n(B) Abuse\n(C) Stealing\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 362, "doc": {"video_name": "surveil_63.mp4", "duration": 607.54, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Normal\n(C) RoadAccidents\n(D) Shooting\n", "candidates": ["Assault", "Normal", "RoadAccidents", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q362"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6f68157d1974ec94d531bef189c1627fa8849a27c8fd2d52c2f6c7f193219ca6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Normal\n(C) RoadAccidents\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Assault\n(B) Normal\n(C) RoadAccidents\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 366, "doc": {"video_name": "surveil_192.mp4", "duration": 722.77, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Burglary\n(C) Vandalism\n(D) Shooting\n", "candidates": ["Shoplifting", "Burglary", "Vandalism", "Shooting"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q366"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "125cd4c1291004de7c190dc9b972591b3ea5cfd86f6d47dd7e535683dead3fa7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Burglary\n(C) Vandalism\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Burglary\n(C) Vandalism\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 370, "doc": {"video_name": "surveil_75.mp4", "duration": 381.04, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) Burglary\n(C) Explosion\n(D) Abuse\n", "candidates": ["Stealing", "Burglary", "Explosion", "Abuse"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q370"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e24c4665449b8a17dc794da906d271569c99756e0ede96dbfead1d9333e2992a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) Burglary\n(C) Explosion\n(D) Abuse\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) Burglary\n(C) Explosion\n(D) Abuse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 374, "doc": {"video_name": "surveil_149.mp4", "duration": 307.32, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Robbery\n(B) Burglary\n(C) Explosion\n(D) Shooting\n", "candidates": ["Robbery", "Burglary", "Explosion", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q374"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "52168e3dbca6737b3cacee98a495ed3ad950a0515ebd63ce1874d1b9364e9c7b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Robbery\n(B) Burglary\n(C) Explosion\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Robbery\n(B) Burglary\n(C) Explosion\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 378, "doc": {"video_name": "surveil_90.mp4", "duration": 198.2, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Shooting\n(C) Arson\n(D) Fighting\n", "candidates": ["RoadAccidents", "Shooting", "Arson", "Fighting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q378"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c842d446ba6f45e67925c528f7c6ba3a77e55b477ddd38d3a6642aaabb6c5b1c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Shooting\n(C) Arson\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Shooting\n(C) Arson\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 382, "doc": {"video_name": "surveil_5.mp4", "duration": 2016.3, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Stealing\n(C) Assault\n(D) Shooting\n", "candidates": ["Normal", "Stealing", "Assault", "Shooting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q382"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d0184d84d0212ca8f44b98284c61589aa0891736f6fc947d7349848fe9bc209b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Stealing\n(C) Assault\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Stealing\n(C) Assault\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 386, "doc": {"video_name": "surveil_138.mp4", "duration": 330.94, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shooting\n(B) Normal\n(C) Fighting\n(D) Arson\n", "candidates": ["Shooting", "Normal", "Fighting", "Arson"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q386"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5caa5b52f8f12057f93ac5b2a086f112449d8f3beefa3faac0616617f2863ff9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shooting\n(B) Normal\n(C) Fighting\n(D) Arson\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shooting\n(B) Normal\n(C) Fighting\n(D) Arson\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 390, "doc": {"video_name": "surveil_58.mp4", "duration": 184.66, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Stealing\n(C) RoadAccidents\n(D) Shooting\n", "candidates": ["Burglary", "Stealing", "RoadAccidents", "Shooting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q390"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0a4f5a0225844805173e963157a0465bd25cb8c4e6582966dc6dbcbb6a666419", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Stealing\n(C) RoadAccidents\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Stealing\n(C) RoadAccidents\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 394, "doc": {"video_name": "surveil_156.mp4", "duration": 258.17, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Stealing\n(C) Explosion\n(D) Arrest\n", "candidates": ["RoadAccidents", "Stealing", "Explosion", "Arrest"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q394"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e9d8598641f9ce26bebb62b5f4a0afa0c2fdf9cc28d2b7518b0fbd3c8ffa35e9", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Stealing\n(C) Explosion\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Stealing\n(C) Explosion\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 398, "doc": {"video_name": "surveil_178.mp4", "duration": 210.07, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) RoadAccidents\n(B) Stealing\n(C) Vandalism\n(D) Fighting\n", "candidates": ["RoadAccidents", "Stealing", "Vandalism", "Fighting"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q398"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5107919cc381f6fbdd98ece3b4f51869d39d3ff4afde43fb7db310daa8f44309", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) RoadAccidents\n(B) Stealing\n(C) Vandalism\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) RoadAccidents\n(B) Stealing\n(C) Vandalism\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 402, "doc": {"video_name": "surveil_187.mp4", "duration": 365.38, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Assault\n(C) Vandalism\n(D) Burglary\n", "candidates": ["Arson", "Assault", "Vandalism", "Burglary"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q402"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fdb74e8f957576f14297689b95a5f792c5565bf9d8633d3ba6ff122929595ba5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Assault\n(C) Vandalism\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Assault\n(C) Vandalism\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 406, "doc": {"video_name": "surveil_142.mp4", "duration": 273.74, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Explosion\n(C) Robbery\n(D) Normal\n", "candidates": ["Stealing", "Explosion", "Robbery", "Normal"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q406"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1fa8308b92f0983a4ef0c79ff3429b7d4a5ccd725a2fa74c5b6c65c8792f4a0e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Explosion\n(C) Robbery\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Explosion\n(C) Robbery\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 410, "doc": {"video_name": "surveil_115.mp4", "duration": 806.48, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Stealing\n(C) RoadAccidents\n(D) Shooting\n", "candidates": ["Shoplifting", "Stealing", "RoadAccidents", "Shooting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q410"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8a47e91ff1b00b992fe15b952ca6b844e9b6d31066ac9010684edab7a2f32192", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Stealing\n(C) RoadAccidents\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Stealing\n(C) RoadAccidents\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 414, "doc": {"video_name": "surveil_141.mp4", "duration": 471.52, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Explosion\n(C) Stealing\n(D) Abuse\n", "candidates": ["Arson", "Explosion", "Stealing", "Abuse"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q414"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c4213f4cbfce531700c18b39e5d5851bbbd24daa3a620bd5e20d4e7fa1b2609c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Explosion\n(C) Stealing\n(D) Abuse\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Explosion\n(C) Stealing\n(D) Abuse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 418, "doc": {"video_name": "surveil_164.mp4", "duration": 288.18, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Assault\n(C) Normal\n(D) Arrest\n", "candidates": ["Shooting", "Assault", "Normal", "Arrest"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q418"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e8e3549812d8e4841af9b5839619b19e4dd7a390eee83039bf4fda30c7a52b72", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Assault\n(C) Normal\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Assault\n(C) Normal\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 422, "doc": {"video_name": "surveil_96.mp4", "duration": 258.08, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Shooting\n(C) Burglary\n(D) Arrest\n", "candidates": ["Vandalism", "Shooting", "Burglary", "Arrest"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q422"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f397788ce03f5e1493b5238e049351506911d70e54b8b4eb662599a8eb8ad167", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Shooting\n(C) Burglary\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Shooting\n(C) Burglary\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 426, "doc": {"video_name": "surveil_26.mp4", "duration": 209.64, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Vandalism\n(C) Normal\n(D) RoadAccidents\n", "candidates": ["Fighting", "Vandalism", "Normal", "RoadAccidents"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q426"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "67941ecb40778fb4cb41b0ed84cc3b91da3f00be66557e1e5b9df2c2ad1abb26", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Vandalism\n(C) Normal\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Vandalism\n(C) Normal\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 430, "doc": {"video_name": "surveil_135.mp4", "duration": 180.14, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Assault\n(B) Shooting\n(C) Vandalism\n(D) Normal\n", "candidates": ["Assault", "Shooting", "Vandalism", "Normal"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q430"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b7af6bf2d8fa7849776ac9dc2b5c4f27e21724f4da6e40c5e7d15c51e09619d9", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Assault\n(B) Shooting\n(C) Vandalism\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Assault\n(B) Shooting\n(C) Vandalism\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 434, "doc": {"video_name": "surveil_161.mp4", "duration": 299.67, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Burglary\n(C) Shooting\n(D) Arrest\n", "candidates": ["Stealing", "Burglary", "Shooting", "Arrest"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q434"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e1d9767899e0d8033ce8af8360b06caedd79325b40ad77b3b5f6e779ea0256b9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Burglary\n(C) Shooting\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Burglary\n(C) Shooting\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 438, "doc": {"video_name": "surveil_43.mp4", "duration": 409.79, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Arrest\n(C) Shoplifting\n(D) Shooting\n", "candidates": ["Normal", "Arrest", "Shoplifting", "Shooting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q438"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1f9f816d746ff2d6860fa3111066d946b3cd8bd32188f642a1f59f75e64c04de", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Arrest\n(C) Shoplifting\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Arrest\n(C) Shoplifting\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 442, "doc": {"video_name": "surveil_68.mp4", "duration": 221.07999999999998, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Robbery\n(B) Burglary\n(C) Arrest\n(D) Arson\n", "candidates": ["Robbery", "Burglary", "Arrest", "Arson"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q442"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "06c8ed6e183cb2678b9131ac7bf478bdcbf6f38593c49afe3b36fe345cc321b2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Robbery\n(B) Burglary\n(C) Arrest\n(D) Arson\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Robbery\n(B) Burglary\n(C) Arrest\n(D) Arson\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 446, "doc": {"video_name": "surveil_118.mp4", "duration": 435.79, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Shooting\n(C) Burglary\n(D) Arson\n", "candidates": ["Robbery", "Shooting", "Burglary", "Arson"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q446"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b1cb3ed628b3582181930599779bc0649f0b0a1456e8aa2faef003b91081ad72", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Shooting\n(C) Burglary\n(D) Arson\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Shooting\n(C) Burglary\n(D) Arson\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 450, "doc": {"video_name": "surveil_84.mp4", "duration": 200.03, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Normal\n(C) Vandalism\n(D) Arson\n", "candidates": ["Fighting", "Normal", "Vandalism", "Arson"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q450"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5011f0227aec85c2cc335eb78f4be4ed6458b5efcb1d2f1268b6d348e31dd382", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Normal\n(C) Vandalism\n(D) Arson\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Normal\n(C) Vandalism\n(D) Arson\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 454, "doc": {"video_name": "surveil_29.mp4", "duration": 197.17000000000002, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arson\n(B) Fighting\n(C) Stealing\n(D) Arrest\n", "candidates": ["Arson", "Fighting", "Stealing", "Arrest"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q454"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "24ccae6474d4ccf6f44955dfcffa7e4e16386b0917d1eeacbfcb773a51bedb21", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arson\n(B) Fighting\n(C) Stealing\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arson\n(B) Fighting\n(C) Stealing\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 458, "doc": {"video_name": "surveil_76.mp4", "duration": 308.37, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) RoadAccidents\n(B) Explosion\n(C) Vandalism\n(D) Stealing\n", "candidates": ["RoadAccidents", "Explosion", "Vandalism", "Stealing"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q458"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9e5d151081aab28194f6f4d6bff8b6d334546a40bd91d4366f1bf366fd377b77", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) RoadAccidents\n(B) Explosion\n(C) Vandalism\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) RoadAccidents\n(B) Explosion\n(C) Vandalism\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 462, "doc": {"video_name": "surveil_100.mp4", "duration": 4730.04, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) Robbery\n(C) Arson\n(D) Shoplifting\n", "candidates": ["Explosion", "Robbery", "Arson", "Shoplifting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q462"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f30da2725392258f21644797d19b2ea8369291d864177879e599dd6b4d1e048f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) Robbery\n(C) Arson\n(D) Shoplifting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) Robbery\n(C) Arson\n(D) Shoplifting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 466, "doc": {"video_name": "surveil_191.mp4", "duration": 604.98, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) RoadAccidents\n(C) Shooting\n(D) Stealing\n", "candidates": ["Shoplifting", "RoadAccidents", "Shooting", "Stealing"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q466"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ee12935533ab02268036d109bec9c9b481fe18f35642cde83006480481015ec2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) RoadAccidents\n(C) Shooting\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) RoadAccidents\n(C) Shooting\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 470, "doc": {"video_name": "surveil_199.mp4", "duration": 2102.0, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arrest\n(B) Explosion\n(C) Assault\n(D) Shooting\n", "candidates": ["Arrest", "Explosion", "Assault", "Shooting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q470"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aae6574ab89848e07a82afec574b4e818fdd00f1d617a5ffaeef8ef63ca5c9a7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arrest\n(B) Explosion\n(C) Assault\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arrest\n(B) Explosion\n(C) Assault\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 474, "doc": {"video_name": "surveil_1.mp4", "duration": 182.88, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Stealing\n(C) Shoplifting\n(D) RoadAccidents\n", "candidates": ["Abuse", "Stealing", "Shoplifting", "RoadAccidents"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q474"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5e70b26a448138c178dff9d13946e02ea55c864311276a08d959a9bcf9805c36", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Stealing\n(C) Shoplifting\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Stealing\n(C) Shoplifting\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 478, "doc": {"video_name": "surveil_23.mp4", "duration": 949.28, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Abuse\n(B) Assault\n(C) Arson\n(D) Robbery\n", "candidates": ["Abuse", "Assault", "Arson", "Robbery"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q478"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "88e28e627a85ab7f9ee5d4d4a88b2f071d09de346e26c25d049745d8d694115a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Abuse\n(B) Assault\n(C) Arson\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Abuse\n(B) Assault\n(C) Arson\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 482, "doc": {"video_name": "surveil_121.mp4", "duration": 314.07, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Burglary\n(B) Shooting\n(C) Shooting\n(D) RoadAccidents\n", "candidates": ["Burglary", "Shooting", "Shooting", "RoadAccidents"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q482"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d27ffdb739d4736c40d13ccd2238811e443b0d786f738ad53e3394b07b40cfb", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Burglary\n(B) Shooting\n(C) Shooting\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Burglary\n(B) Shooting\n(C) Shooting\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 486, "doc": {"video_name": "surveil_93.mp4", "duration": 257.58, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arson\n(B) Normal\n(C) Shooting\n(D) RoadAccidents\n", "candidates": ["Arson", "Normal", "Shooting", "RoadAccidents"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q486"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d7f114f585da8856a4db6a402c4261c12cea64e2832d3d2454ab28068727946", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arson\n(B) Normal\n(C) Shooting\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arson\n(B) Normal\n(C) Shooting\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 490, "doc": {"video_name": "surveil_119.mp4", "duration": 519.99, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shooting\n(B) Vandalism\n(C) RoadAccidents\n(D) Burglary\n", "candidates": ["Shooting", "Vandalism", "RoadAccidents", "Burglary"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q490"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ab07c9b1fd37b2bc08dcc1e8edb99ba11a42db3e170d56d70fd17542a6ebdbd0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shooting\n(B) Vandalism\n(C) RoadAccidents\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "D"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Shooting\n(B) Vandalism\n(C) RoadAccidents\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 494, "doc": {"video_name": "surveil_120.mp4", "duration": 280.29, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Vandalism\n(B) Abuse\n(C) Assault\n(D) Burglary\n", "candidates": ["Vandalism", "Abuse", "Assault", "Burglary"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q494"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "46a579a453f769c025a10cadcdf99017a1d53cbbbdb7c306747389f0b20de2e4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Vandalism\n(B) Abuse\n(C) Assault\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Vandalism\n(B) Abuse\n(C) Assault\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 498, "doc": {"video_name": "surveil_13.mp4", "duration": 248.09, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) Robbery\n(C) Normal\n(D) Burglary\n", "candidates": ["Stealing", "Robbery", "Normal", "Burglary"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q498"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3b87806e2de750e7bbdb8850f91368e20ec8983c2b7a7db72ac5c0fae8d7d88e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) Robbery\n(C) Normal\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) Robbery\n(C) Normal\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 502, "doc": {"video_name": "surveil_65.mp4", "duration": 187.53, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Fighting\n(C) Stealing\n(D) Shooting\n", "candidates": ["Normal", "Fighting", "Stealing", "Shooting"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q502"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b99db835300b4ac84861b09795bd6df7a5f3201e4c5bb43f75a5cf1b49475603", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Fighting\n(C) Stealing\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Fighting\n(C) Stealing\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 506, "doc": {"video_name": "surveil_69.mp4", "duration": 229.1, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Vandalism\n(B) Shooting\n(C) Burglary\n(D) Shoplifting\n", "candidates": ["Vandalism", "Shooting", "Burglary", "Shoplifting"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q506"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0aa9a5f39be1c60aebc7658d529dc25fc3bf1bafc31b84a4871b01fdaf0502cd", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Vandalism\n(B) Shooting\n(C) Burglary\n(D) Shoplifting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Vandalism\n(B) Shooting\n(C) Burglary\n(D) Shoplifting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 510, "doc": {"video_name": "surveil_157.mp4", "duration": 255.0, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Explosion\n(C) Arson\n(D) Shooting\n", "candidates": ["Arrest", "Explosion", "Arson", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q510"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "11899216a474ae387b31c50b8162d3cab2257b92ebe8d85f58740b0377ae2a6e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Explosion\n(C) Arson\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "B"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Explosion\n(C) Arson\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 514, "doc": {"video_name": "surveil_131.mp4", "duration": 180.0, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Assault\n(B) Robbery\n(C) Fighting\n(D) Stealing\n", "candidates": ["Assault", "Robbery", "Fighting", "Stealing"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q514"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "74a4ed92c03c910ed7bec061201c848cf6bb664a3af53f60106bb51738c19a91", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Assault\n(B) Robbery\n(C) Fighting\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Assault\n(B) Robbery\n(C) Fighting\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 518, "doc": {"video_name": "surveil_126.mp4", "duration": 364.04, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Explosion\n(B) Arson\n(C) Shooting\n(D) Normal\n", "candidates": ["Explosion", "Arson", "Shooting", "Normal"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q518"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "965755a1709b2222d01628d0fc3b6fd90efa3be9d1e119b0c01e8b7c64ab4c0a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Explosion\n(B) Arson\n(C) Shooting\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Explosion\n(B) Arson\n(C) Shooting\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 522, "doc": {"video_name": "surveil_64.mp4", "duration": 360.0, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arrest\n(B) Robbery\n(C) Shooting\n(D) Explosion\n", "candidates": ["Arrest", "Robbery", "Shooting", "Explosion"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q522"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9c4f6f5a2ada39361aa6ad345c8dd960b2d01915f6e7d0025da7a542d85e895d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arrest\n(B) Robbery\n(C) Shooting\n(D) Explosion\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arrest\n(B) Robbery\n(C) Shooting\n(D) Explosion\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 526, "doc": {"video_name": "surveil_16.mp4", "duration": 190.11, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) Burglary\n(C) Vandalism\n(D) Fighting\n", "candidates": ["Explosion", "Burglary", "Vandalism", "Fighting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q526"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "117479e3bc6944bca1267970d076e68ab3fbff7f6f0ffbc61ad40b6fdbb2ad8e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) Burglary\n(C) Vandalism\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) Burglary\n(C) Vandalism\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 530, "doc": {"video_name": "surveil_36.mp4", "duration": 222.48, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) RoadAccidents\n(C) Shoplifting\n(D) Arrest\n", "candidates": ["Arson", "RoadAccidents", "Shoplifting", "Arrest"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q530"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fe7abe520592c22c243a4771d7a6456cc090f587e4cabc9038d31771845543a6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) RoadAccidents\n(C) Shoplifting\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) RoadAccidents\n(C) Shoplifting\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 534, "doc": {"video_name": "surveil_21.mp4", "duration": 180.0, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Shoplifting\n(C) Robbery\n(D) Shooting\n", "candidates": ["Shooting", "Shoplifting", "Robbery", "Shooting"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q534"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5a763627f235c18d1c0d88ed33f065ab04e1473659f8a421f4cc997c0b747438", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Shoplifting\n(C) Robbery\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Shoplifting\n(C) Robbery\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 538, "doc": {"video_name": "surveil_150.mp4", "duration": 223.52, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Fighting\n(B) Arson\n(C) RoadAccidents\n(D) Arrest\n", "candidates": ["Fighting", "Arson", "RoadAccidents", "Arrest"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q538"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "118f2cae2498793d2400bbc0ef60757291ebe1aa7a37381c3ca022cd85ccf20b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Fighting\n(B) Arson\n(C) RoadAccidents\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Fighting\n(B) Arson\n(C) RoadAccidents\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 542, "doc": {"video_name": "surveil_80.mp4", "duration": 411.3, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Assault\n(B) Shooting\n(C) Shoplifting\n(D) Vandalism\n", "candidates": ["Assault", "Shooting", "Shoplifting", "Vandalism"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q542"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "40153db9839c92a5b18d9d304b713b6f0cdb6421b20d56f23e3660083a408307", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Assault\n(B) Shooting\n(C) Shoplifting\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Assault\n(B) Shooting\n(C) Shoplifting\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 546, "doc": {"video_name": "surveil_122.mp4", "duration": 240.2, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Fighting\n(C) Normal\n(D) Shooting\n", "candidates": ["Robbery", "Fighting", "Normal", "Shooting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q546"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ab11e9053a0b61c684981ca2bc1cc3ac8c870c240e7f2fe16827c31caf8ea7cb", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Fighting\n(C) Normal\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Fighting\n(C) Normal\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 550, "doc": {"video_name": "surveil_108.mp4", "duration": 1620.85, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Explosion\n(C) Assault\n(D) Burglary\n", "candidates": ["Shooting", "Explosion", "Assault", "Burglary"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q550"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "318bd8d90259e26020f84b7f5fecb365c038bf18da4c9038c928063e384aee71", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Explosion\n(C) Assault\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Explosion\n(C) Assault\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 554, "doc": {"video_name": "surveil_92.mp4", "duration": 437.8, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Fighting\n(C) Burglary\n(D) Assault\n", "candidates": ["Arson", "Fighting", "Burglary", "Assault"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q554"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "74a86cb002933d8439f86aecba265b93bc50b6318cd8e02cf101cc2b93af67cd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Fighting\n(C) Burglary\n(D) Assault\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Fighting\n(C) Burglary\n(D) Assault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 558, "doc": {"video_name": "count_133.mp4", "duration": 520.05, "question": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 2\n(B) 1\n(C) 4\n(D) 6\n", "candidates": ["2", "1", "4", "6"], "answer": "C", "task_type": "count", "question_id": "Q558"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf1c824eb70cadbe7bb459816424a05ad0ef832ed2ddf0e588a95a874c80c0f4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 2\n(B) 1\n(C) 4\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 2\n(B) 1\n(C) 4\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 562, "doc": {"video_name": "count_78.mp4", "duration": 460.01, "question": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 4\n(B) 1\n(C) 3\n(D) 6\n", "candidates": ["4", "1", "3", "6"], "answer": "B", "task_type": "count", "question_id": "Q562"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7d91541f0ec04d93e217f32d41c09843137859ccb929f66a0f3ee5a7e94c3c3e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 4\n(B) 1\n(C) 3\n(D) 6\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 4\n(B) 1\n(C) 3\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 566, "doc": {"video_name": "count_74.mp4", "duration": 500.03999999999996, "question": "In this video, how many instances are there of the 'cleaning toilet' action scene in total?\n(A) 1\n(B) 5\n(C) 2\n(D) 4\n", "candidates": ["1", "5", "2", "4"], "answer": "B", "task_type": "count", "question_id": "Q566"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "83a2bb418954153940363a60b41966617b96eecb65c766a6cd0ad89a2a0f38fd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'cleaning toilet' action scene in total?\n(A) 1\n(B) 5\n(C) 2\n(D) 4\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many instances are there of the 'cleaning toilet' action scene in total?\n(A) 1\n(B) 5\n(C) 2\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 570, "doc": {"video_name": "count_71.mp4", "duration": 530.03, "question": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 5\n(B) 6\n(C) 3\n(D) 0\n", "candidates": ["5", "6", "3", "0"], "answer": "A", "task_type": "count", "question_id": "Q570"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "00e6a7139ddc71acea57894c52eec8f1593c0c2062722c57a5bc2ea8adff6545", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 5\n(B) 6\n(C) 3\n(D) 0\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 5\n(B) 6\n(C) 3\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 574, "doc": {"video_name": "count_17.mp4", "duration": 488.56, "question": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 0\n(B) 2\n(C) 4\n(D) 1\n", "candidates": ["0", "2", "4", "1"], "answer": "C", "task_type": "count", "question_id": "Q574"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cb912af3cd426cfc91a008f1d4a555a86578f83fd629757b40888247b50b2fe3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 0\n(B) 2\n(C) 4\n(D) 1\n", "task_type": "count", "pred_answer": "B", "answer": "C"}, "input": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 0\n(B) 2\n(C) 4\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 578, "doc": {"video_name": "count_14.mp4", "duration": 873.0, "question": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 0\n(B) 2\n(C) 3\n(D) 5\n", "candidates": ["0", "2", "3", "5"], "answer": "B", "task_type": "count", "question_id": "Q578"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5051a41b68bbb934a9532c3a06669bec592054da0df7e80e015755bf5ab98d68", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 0\n(B) 2\n(C) 3\n(D) 5\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 0\n(B) 2\n(C) 3\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 582, "doc": {"video_name": "count_11.mp4", "duration": 480.03, "question": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 3\n(B) 6\n(C) 5\n(D) 2\n", "candidates": ["3", "6", "5", "2"], "answer": "A", "task_type": "count", "question_id": "Q582"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a0c8fecfa32ddf1d643b2489101210c886f80bd7186e54304bc1cd93bdf1d75a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 3\n(B) 6\n(C) 5\n(D) 2\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 3\n(B) 6\n(C) 5\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 586, "doc": {"video_name": "count_187.mp4", "duration": 1372.54, "question": "In this video, how many instances are there of the 'paragliding' action scene in total?\n(A) 2\n(B) 6\n(C) 5\n(D) 1\n", "candidates": ["2", "6", "5", "1"], "answer": "A", "task_type": "count", "question_id": "Q586"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "49dbe586f5b95a416707ee63f7d76c9ff1cd61b35cc58e5c62015b27657bc793", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'paragliding' action scene in total?\n(A) 2\n(B) 6\n(C) 5\n(D) 1\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many instances are there of the 'paragliding' action scene in total?\n(A) 2\n(B) 6\n(C) 5\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 590, "doc": {"video_name": "count_239.mp4", "duration": 356.03, "question": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 3\n(B) 4\n(C) 0\n(D) 1\n", "candidates": ["3", "4", "0", "1"], "answer": "D", "task_type": "count", "question_id": "Q590"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "930b30ae742c15e7362ba176cae0021b87fdc9da1fcfcf31d12ad520aa0fdb58", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 3\n(B) 4\n(C) 0\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 3\n(B) 4\n(C) 0\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 594, "doc": {"video_name": "count_228.mp4", "duration": 467.63, "question": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 5\n(B) 3\n(C) 4\n(D) 2\n", "candidates": ["5", "3", "4", "2"], "answer": "D", "task_type": "count", "question_id": "Q594"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "95ed6c142db9e1d088493505bbbe006d73095fdb4ff5e6116a465b95ea1c4579", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 5\n(B) 3\n(C) 4\n(D) 2\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 5\n(B) 3\n(C) 4\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 598, "doc": {"video_name": "count_67.mp4", "duration": 484.05999999999995, "question": "In this video, how many times does the scene of the 'pole vault' action appear in total?\n(A) 5\n(B) 4\n(C) 6\n(D) 2\n", "candidates": ["5", "4", "6", "2"], "answer": "B", "task_type": "count", "question_id": "Q598"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a3fd7a184878567ff82390b043a3ccf026eeda02ee209b353e2caf3b01c75062", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'pole vault' action appear in total?\n(A) 5\n(B) 4\n(C) 6\n(D) 2\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many times does the scene of the 'pole vault' action appear in total?\n(A) 5\n(B) 4\n(C) 6\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 602, "doc": {"video_name": "count_65.mp4", "duration": 460.01, "question": "In this video, how many times does the scene of the 'pole vault' action appear in total?\n(A) 0\n(B) 4\n(C) 5\n(D) 1\n", "candidates": ["0", "4", "5", "1"], "answer": "D", "task_type": "count", "question_id": "Q602"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "94c56f45db164f13c39f1d3fa24513ffc0ee858a577266883befc0098f5bcea7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'pole vault' action appear in total?\n(A) 0\n(B) 4\n(C) 5\n(D) 1\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "In this video, how many times does the scene of the 'pole vault' action appear in total?\n(A) 0\n(B) 4\n(C) 5\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 606, "doc": {"video_name": "count_61.mp4", "duration": 479.74, "question": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 2\n(B) 3\n(C) 4\n(D) 1\n", "candidates": ["2", "3", "4", "1"], "answer": "C", "task_type": "count", "question_id": "Q606"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bf1386b3bee4e94eb1b67f454ce15ba92aa462f04bd7ed2e4843853f5ea698d4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 2\n(B) 3\n(C) 4\n(D) 1\n", "task_type": "count", "pred_answer": "B", "answer": "C"}, "input": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 2\n(B) 3\n(C) 4\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 610, "doc": {"video_name": "count_227.mp4", "duration": 352.64, "question": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 3\n(B) 4\n(C) 2\n(D) 1\n", "candidates": ["3", "4", "2", "1"], "answer": "B", "task_type": "count", "question_id": "Q610"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e9debb0cdef908a30558c0bf3868656c0db7c6565ae8b829fedac8a94dcae5b2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 3\n(B) 4\n(C) 2\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 3\n(B) 4\n(C) 2\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 614, "doc": {"video_name": "count_128.mp4", "duration": 466.94, "question": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 4\n(B) 1\n(C) 3\n(D) 2\n", "candidates": ["4", "1", "3", "2"], "answer": "D", "task_type": "count", "question_id": "Q614"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4df8632a5f647713b207d1768380de3929c331fbb56adfac820e91c80af99621", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 4\n(B) 1\n(C) 3\n(D) 2\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 4\n(B) 1\n(C) 3\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 618, "doc": {"video_name": "count_160.mp4", "duration": 500.10999999999996, "question": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 1\n(B) 2\n(C) 6\n(D) 5\n", "candidates": ["1", "2", "6", "5"], "answer": "D", "task_type": "count", "question_id": "Q618"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fbee423f48933d8e7b099cebc73cc5d29038a1b720bf5947c220a4f8086deb8d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 1\n(B) 2\n(C) 6\n(D) 5\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 1\n(B) 2\n(C) 6\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 622, "doc": {"video_name": "count_205.mp4", "duration": 7788.09, "question": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 6\n(B) 4\n(C) 3\n(D) 1\n", "candidates": ["6", "4", "3", "1"], "answer": "B", "task_type": "count", "question_id": "Q622"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e67aba53b072cd92bd3c553bba6d0a9a4bfb1af58d6404093a26e2a77e292111", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 6\n(B) 4\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 6\n(B) 4\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 626, "doc": {"video_name": "count_93.mp4", "duration": 460.05000000000007, "question": "In this video, how many times does the scene of the 'playing harp' action appear in total?\n(A) 0\n(B) 1\n(C) 3\n(D) 2\n", "candidates": ["0", "1", "3", "2"], "answer": "B", "task_type": "count", "question_id": "Q626"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0525ee6b76cffa5361a23cb368549fac28b321af6beb97f07b3a6101565b5545", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'playing harp' action appear in total?\n(A) 0\n(B) 1\n(C) 3\n(D) 2\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many times does the scene of the 'playing harp' action appear in total?\n(A) 0\n(B) 1\n(C) 3\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 630, "doc": {"video_name": "count_90.mp4", "duration": 530.02, "question": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 6\n(B) 5\n(C) 2\n(D) 1\n", "candidates": ["6", "5", "2", "1"], "answer": "B", "task_type": "count", "question_id": "Q630"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c52cf26c8a7df4c5705c15900b79a068acb63575a65db4444909d04eb714214a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 6\n(B) 5\n(C) 2\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 6\n(B) 5\n(C) 2\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 634, "doc": {"video_name": "count_59.mp4", "duration": 469.33000000000004, "question": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 2\n(B) 1\n(C) 3\n(D) 5\n", "candidates": ["2", "1", "3", "5"], "answer": "A", "task_type": "count", "question_id": "Q634"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6e1636392a615375a5b2d4975bac5223fdd0f6dd351a813a34381b114f6b89f3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 2\n(B) 1\n(C) 3\n(D) 5\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 2\n(B) 1\n(C) 3\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 638, "doc": {"video_name": "count_105.mp4", "duration": 460.01, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 5\n(B) 6\n(C) 3\n(D) 1\n", "candidates": ["5", "6", "3", "1"], "answer": "D", "task_type": "count", "question_id": "Q638"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6cc1623cd879c6410469247349e0256a1a23778077e74909f1026c2e6202aaab", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 5\n(B) 6\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 5\n(B) 6\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 642, "doc": {"video_name": "count_53.mp4", "duration": 490.02, "question": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 3\n(B) 0\n(C) 5\n(D) 4\n", "candidates": ["3", "0", "5", "4"], "answer": "D", "task_type": "count", "question_id": "Q642"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b876a59432d646491504c62aa293ec5d92737dee4cd63bf09ada835babebb1e6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 3\n(B) 0\n(C) 5\n(D) 4\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 3\n(B) 0\n(C) 5\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 646, "doc": {"video_name": "count_50.mp4", "duration": 490.02, "question": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 0\n(B) 6\n(C) 4\n(D) 1\n", "candidates": ["0", "6", "4", "1"], "answer": "C", "task_type": "count", "question_id": "Q646"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "855d4d775f1b50ee152cefe2f2e2ae3de045a521ca9fc01755abf03a40bf0080", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 0\n(B) 6\n(C) 4\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many times does the scene of the 'milking cow' action appear in total?\n(A) 0\n(B) 6\n(C) 4\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 650, "doc": {"video_name": "count_178.mp4", "duration": 490.10999999999996, "question": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 1\n(B) 6\n(C) 4\n(D) 0\n", "candidates": ["1", "6", "4", "0"], "answer": "C", "task_type": "count", "question_id": "Q650"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "673afa464e19feb63f90eb7ecbcb9f1da1e7c99903ce5560fdff306811306485", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 1\n(B) 6\n(C) 4\n(D) 0\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 1\n(B) 6\n(C) 4\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 654, "doc": {"video_name": "count_89.mp4", "duration": 470.02, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 6\n(B) 5\n(C) 0\n(D) 2\n", "candidates": ["6", "5", "0", "2"], "answer": "D", "task_type": "count", "question_id": "Q654"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4650968478ccd43d90c83851f0a2f2a21d4eb4c17e536fda9b72ffe19971ac41", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 6\n(B) 5\n(C) 0\n(D) 2\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 6\n(B) 5\n(C) 0\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 658, "doc": {"video_name": "count_86.mp4", "duration": 510.03, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 4\n(B) 3\n(C) 0\n(D) 6\n", "candidates": ["4", "3", "0", "6"], "answer": "B", "task_type": "count", "question_id": "Q658"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "53eb11378c71cfb0ab27d2dadf613780319f908c3911e612c0ff49e496edf86b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 4\n(B) 3\n(C) 0\n(D) 6\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 4\n(B) 3\n(C) 0\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 662, "doc": {"video_name": "count_84.mp4", "duration": 530.05, "question": "In this video, how many instances are there of the 'jetskiing' action scene in total?\n(A) 6\n(B) 1\n(C) 5\n(D) 3\n", "candidates": ["6", "1", "5", "3"], "answer": "C", "task_type": "count", "question_id": "Q662"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b26aa63c7e2b7057fe99cc07856a92148304dc934a333ebc3955194bb45451e8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'jetskiing' action scene in total?\n(A) 6\n(B) 1\n(C) 5\n(D) 3\n", "task_type": "count", "pred_answer": "D", "answer": "C"}, "input": "In this video, how many instances are there of the 'jetskiing' action scene in total?\n(A) 6\n(B) 1\n(C) 5\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 666, "doc": {"video_name": "count_81.mp4", "duration": 470.02, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 0\n(B) 2\n(C) 4\n(D) 3\n", "candidates": ["0", "2", "4", "3"], "answer": "B", "task_type": "count", "question_id": "Q666"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "61f2e528befabbe69fe175580925dc8a068b431f1d5cca8f66ef31884b70163b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 0\n(B) 2\n(C) 4\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 0\n(B) 2\n(C) 4\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 670, "doc": {"video_name": "count_229.mp4", "duration": 6980.260000000001, "question": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 4\n(B) 5\n(C) 6\n(D) 3\n", "candidates": ["4", "5", "6", "3"], "answer": "A", "task_type": "count", "question_id": "Q670"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7ad469d8e3cd52fde4316d3c30f353af6437beb70dc29c6956aaf045093df07f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 4\n(B) 5\n(C) 6\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 4\n(B) 5\n(C) 6\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 674, "doc": {"video_name": "count_173.mp4", "duration": 460.60999999999996, "question": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 4\n(B) 0\n(C) 2\n(D) 3\n", "candidates": ["4", "0", "2", "3"], "answer": "C", "task_type": "count", "question_id": "Q674"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3dae851a350631f493264e02cd9bcf79bb4c4c31100c54ee4879265027c6f4a4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 4\n(B) 0\n(C) 2\n(D) 3\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 4\n(B) 0\n(C) 2\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 678, "doc": {"video_name": "count_48.mp4", "duration": 470.09999999999997, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 2\n(B) 6\n(C) 3\n(D) 1\n", "candidates": ["2", "6", "3", "1"], "answer": "A", "task_type": "count", "question_id": "Q678"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "54bbef3436f6674685c8df8d642312178478f08429933c77006ac63db078b6d3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 2\n(B) 6\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 2\n(B) 6\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 682, "doc": {"video_name": "count_218.mp4", "duration": 504.05999999999995, "question": "In this video, how many instances are there of the 'shredding paper' action scene in total?\n(A) 1\n(B) 4\n(C) 6\n(D) 0\n", "candidates": ["1", "4", "6", "0"], "answer": "B", "task_type": "count", "question_id": "Q682"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f44d92463ab91263ea3798740216de0739eaff0ca68b53118f48622a960957ab", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'shredding paper' action scene in total?\n(A) 1\n(B) 4\n(C) 6\n(D) 0\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many instances are there of the 'shredding paper' action scene in total?\n(A) 1\n(B) 4\n(C) 6\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 686, "doc": {"video_name": "count_43.mp4", "duration": 496.82, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 4\n(B) 5\n(C) 1\n(D) 6\n", "candidates": ["4", "5", "1", "6"], "answer": "B", "task_type": "count", "question_id": "Q686"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5ac3b22f6992f023234f37696bd96a2f87db9953a8730683c8a1f10ff63a4eb6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 4\n(B) 5\n(C) 1\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 4\n(B) 5\n(C) 1\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 690, "doc": {"video_name": "count_225.mp4", "duration": 348.03999999999996, "question": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 3\n(B) 2\n(C) 0\n(D) 5\n", "candidates": ["3", "2", "0", "5"], "answer": "A", "task_type": "count", "question_id": "Q690"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dcc4a7983f1bf70cf40846ad70b8c08e717e00f7c478a3794b09ee91e8ccf012", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 3\n(B) 2\n(C) 0\n(D) 5\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 3\n(B) 2\n(C) 0\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 694, "doc": {"video_name": "count_136.mp4", "duration": 480.02000000000004, "question": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 5\n(B) 3\n(C) 2\n(D) 0\n", "candidates": ["5", "3", "2", "0"], "answer": "B", "task_type": "count", "question_id": "Q694"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c36d60135977d66e69efb7794c136c537253e26ccdf5654ab01c5848597973a1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 5\n(B) 3\n(C) 2\n(D) 0\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 5\n(B) 3\n(C) 2\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 698, "doc": {"video_name": "count_143.mp4", "duration": 469.07, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'javelin throw' action\n(A) 2\n(B) 6\n(C) 3\n(D) 4\n", "candidates": ["2", "6", "3", "4"], "answer": "A", "task_type": "count", "question_id": "Q698"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "385e6ca5f77e71ea2d460445661802eb1b59d33da04935462977646132d76d0c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'javelin throw' action\n(A) 2\n(B) 6\n(C) 3\n(D) 4\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'javelin throw' action\n(A) 2\n(B) 6\n(C) 3\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 702, "doc": {"video_name": "count_213.mp4", "duration": 377.11999999999995, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'javelin throw' action\n(A) 5\n(B) 1\n(C) 3\n(D) 6\n", "candidates": ["5", "1", "3", "6"], "answer": "A", "task_type": "count", "question_id": "Q702"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8b64c25d5b37b4ed82aab1e4cc9a8f18d401e964e0641bcaad8514cdd5d5bd96", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'javelin throw' action\n(A) 5\n(B) 1\n(C) 3\n(D) 6\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'javelin throw' action\n(A) 5\n(B) 1\n(C) 3\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 706, "doc": {"video_name": "count_124.mp4", "duration": 500.08, "question": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 0\n(B) 6\n(C) 5\n(D) 3\n", "candidates": ["0", "6", "5", "3"], "answer": "C", "task_type": "count", "question_id": "Q706"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1ef3f287830672ebabd5cca4f2991eb555f37647dfc1d92c377c743b6715b3eb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 0\n(B) 6\n(C) 5\n(D) 3\n", "task_type": "count", "pred_answer": "B", "answer": "C"}, "input": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 0\n(B) 6\n(C) 5\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 710, "doc": {"video_name": "count_37.mp4", "duration": 460.01, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 3\n(B) 1\n(C) 4\n(D) 6\n", "candidates": ["3", "1", "4", "6"], "answer": "B", "task_type": "count", "question_id": "Q710"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "febeeef86c76fc15572e19d831b3be57ae4d37c53001824149f27e6050a37dd4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 3\n(B) 1\n(C) 4\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 3\n(B) 1\n(C) 4\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 714, "doc": {"video_name": "count_113.mp4", "duration": 470.01, "question": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 2\n(B) 1\n(C) 3\n(D) 0\n", "candidates": ["2", "1", "3", "0"], "answer": "A", "task_type": "count", "question_id": "Q714"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a77ea03b3efb6cb431e6716a1c646080ddfaa1395ab8fe0fa8b027f6eb700744", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 2\n(B) 1\n(C) 3\n(D) 0\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 2\n(B) 1\n(C) 3\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 718, "doc": {"video_name": "count_201.mp4", "duration": 7125.52, "question": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 4\n(B) 2\n(C) 3\n(D) 5\n", "candidates": ["4", "2", "3", "5"], "answer": "B", "task_type": "count", "question_id": "Q718"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2744b1ff3e2732ca7b930c856176b549b76ef50040ca5c1a6ed59503f9994243", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 4\n(B) 2\n(C) 3\n(D) 5\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 4\n(B) 2\n(C) 3\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 722, "doc": {"video_name": "count_9.mp4", "duration": 470.03, "question": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 1\n(B) 4\n(C) 2\n(D) 6\n", "candidates": ["1", "4", "2", "6"], "answer": "C", "task_type": "count", "question_id": "Q722"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "718ebd9d1d338e904268a18ef908b50e207ccb0223e45e31b9dc312b3cf8ebc3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 1\n(B) 4\n(C) 2\n(D) 6\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 1\n(B) 4\n(C) 2\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 726, "doc": {"video_name": "count_186.mp4", "duration": 510.03, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 1\n(B) 0\n(C) 2\n(D) 3\n", "candidates": ["1", "0", "2", "3"], "answer": "D", "task_type": "count", "question_id": "Q726"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d95326cac2e2f196b906a30ea2ed125d8d81306c73f0d85251d7517d3b86828d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 1\n(B) 0\n(C) 2\n(D) 3\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 1\n(B) 0\n(C) 2\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 730, "doc": {"video_name": "count_175.mp4", "duration": 874.65, "question": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 4\n(B) 3\n(C) 0\n(D) 2\n", "candidates": ["4", "3", "0", "2"], "answer": "D", "task_type": "count", "question_id": "Q730"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7d5fd93cdee13793c21ec3373c372a15e165d3873711eee3bc351e2e12a5e79d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 4\n(B) 3\n(C) 0\n(D) 2\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "In this video, how many instances are there of the 'zumba' action scene in total?\n(A) 4\n(B) 3\n(C) 0\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 734, "doc": {"video_name": "count_167.mp4", "duration": 470.0999999999999, "question": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 3\n(B) 2\n(C) 1\n(D) 6\n", "candidates": ["3", "2", "1", "6"], "answer": "B", "task_type": "count", "question_id": "Q734"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b8eedb44efdd70f7bcdd116322ac1f00c3a517bc44f5236102993d94d38f3ff7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 3\n(B) 2\n(C) 1\n(D) 6\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 3\n(B) 2\n(C) 1\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 738, "doc": {"video_name": "count_219.mp4", "duration": 319.03999999999996, "question": "In this video, how many instances are there of the 'shredding paper' action scene in total?\n(A) 2\n(B) 6\n(C) 0\n(D) 3\n", "candidates": ["2", "6", "0", "3"], "answer": "D", "task_type": "count", "question_id": "Q738"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "072548a6922b8dc789232e6cb648044da2406baf0a2462043c3b8ec55b1ec64b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'shredding paper' action scene in total?\n(A) 2\n(B) 6\n(C) 0\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many instances are there of the 'shredding paper' action scene in total?\n(A) 2\n(B) 6\n(C) 0\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 742, "doc": {"video_name": "count_27.mp4", "duration": 460.01, "question": "In this video, how many instances are there of the 'clean and jerk' action scene in total?\n(A) 1\n(B) 4\n(C) 0\n(D) 6\n", "candidates": ["1", "4", "0", "6"], "answer": "A", "task_type": "count", "question_id": "Q742"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "08b88995581d4608a155a73c5a0ba55ba8048dcc8154857e654155047fecfcb9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'clean and jerk' action scene in total?\n(A) 1\n(B) 4\n(C) 0\n(D) 6\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "In this video, how many instances are there of the 'clean and jerk' action scene in total?\n(A) 1\n(B) 4\n(C) 0\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 746, "doc": {"video_name": "count_24.mp4", "duration": 495.59, "question": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 2\n(B) 1\n(C) 0\n(D) 5\n", "candidates": ["2", "1", "0", "5"], "answer": "D", "task_type": "count", "question_id": "Q746"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9d8fb9d91d953055b70331d049c7b3686d34182cb597f5eb03568256f8f94902", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 2\n(B) 1\n(C) 0\n(D) 5\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 2\n(B) 1\n(C) 0\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 750, "doc": {"video_name": "count_22.mp4", "duration": 470.01, "question": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 6\n(B) 1\n(C) 2\n(D) 0\n", "candidates": ["6", "1", "2", "0"], "answer": "C", "task_type": "count", "question_id": "Q750"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4bf38e199c3fb15a7a5046a7e2421a29b1fc5d913cf1a11254259b040b82968e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 6\n(B) 1\n(C) 2\n(D) 0\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 6\n(B) 1\n(C) 2\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 754, "doc": {"video_name": "count_170.mp4", "duration": 500.11999999999995, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 5\n(B) 4\n(C) 1\n(D) 2\n", "candidates": ["5", "4", "1", "2"], "answer": "A", "task_type": "count", "question_id": "Q754"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c5e4d5e9d452bd5112a51d955800e2368127b7d8ce16410b71a09745922901c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 5\n(B) 4\n(C) 1\n(D) 2\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 5\n(B) 4\n(C) 1\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 758, "doc": {"video_name": "count_119.mp4", "duration": 500.01, "question": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 2\n(B) 5\n(C) 4\n(D) 3\n", "candidates": ["2", "5", "4", "3"], "answer": "B", "task_type": "count", "question_id": "Q758"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "df799a2b56a1c9fba7eae7ce042b01845129bfff853643eff5fb59bb25315b48", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 2\n(B) 5\n(C) 4\n(D) 3\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 2\n(B) 5\n(C) 4\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 762, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Where was my card\n(A) under the bed\n(B) in my pocket\n(C) in my hand\n(D) on the table\n", "candidates": ["under the bed", "in my pocket", "in my hand", "on the table"], "answer": "C", "task_type": "ego", "question_id": "Q762"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "30be624884b370310a712f2c93c83468b29dfe314f573d6fe97b2956ab59e1c2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was my card\n(A) under the bed\n(B) in my pocket\n(C) in my hand\n(D) on the table\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was my card\n(A) under the bed\n(B) in my pocket\n(C) in my hand\n(D) on the table\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 766, "doc": {"video_name": "ego_73.mp4", "duration": 480.0, "question": "Where did I put the Silver container?\n(A) In the bathroom\n(B) In the kitchen cabinet\n(C) In the dustbin\n(D) On the bookshelf\n", "candidates": ["In the bathroom", "In the kitchen cabinet", "In the dustbin", "On the bookshelf"], "answer": "C", "task_type": "ego", "question_id": "Q766"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "09a8d078686350894805e5f8d53a09daeb34e6ad89d8b55b294a70121482638f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I put the Silver container?\n(A) In the bathroom\n(B) In the kitchen cabinet\n(C) In the dustbin\n(D) On the bookshelf\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "Where did I put the Silver container?\n(A) In the bathroom\n(B) In the kitchen cabinet\n(C) In the dustbin\n(D) On the bookshelf\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 770, "doc": {"video_name": "ego_30.mp4", "duration": 480.0, "question": "Where did I put the tools?\n(A) On the top shelf.\n(B) In the blue box.\n(C) In the red tray.\n(D) In the kitchen drawer.\n", "candidates": ["On the top shelf.", "In the blue box.", "In the red tray.", "In the kitchen drawer."], "answer": "C", "task_type": "ego", "question_id": "Q770"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f685e713a3fbbb44c208054b936a973e372077e03b5957b7cb2c0c372fbd3f83", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I put the tools?\n(A) On the top shelf.\n(B) In the blue box.\n(C) In the red tray.\n(D) In the kitchen drawer.\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "Where did I put the tools?\n(A) On the top shelf.\n(B) In the blue box.\n(C) In the red tray.\n(D) In the kitchen drawer.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 774, "doc": {"video_name": "ego_22.mp4", "duration": 480.0, "question": "Who did I interact with when I opened the door\n(A) a child\n(B) a woman\n(C) a dog\n(D) a man\n", "candidates": ["a child", "a woman", "a dog", "a man"], "answer": "B", "task_type": "ego", "question_id": "Q774"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5fb171b651dad8a959323b933af1e6cbd06f0b713529a9330260d4a9bbbb9c89", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who did I interact with when I opened the door\n(A) a child\n(B) a woman\n(C) a dog\n(D) a man\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Who did I interact with when I opened the door\n(A) a child\n(B) a woman\n(C) a dog\n(D) a man\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 778, "doc": {"video_name": "ego_21.mp4", "duration": 480.0, "question": "Where was the dust pan?\n(A) in the closet\n(B) near the window \n(C) under the bed\n(D) on the kitchen counter\n", "candidates": ["in the closet", "near the window ", "under the bed", "on the kitchen counter"], "answer": "B", "task_type": "ego", "question_id": "Q778"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1f60aff7b1335bb810a095b5e13ce1bfcabf451fbf92eac2e3c88e642b423993", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the dust pan?\n(A) in the closet\n(B) near the window \n(C) under the bed\n(D) on the kitchen counter\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the dust pan?\n(A) in the closet\n(B) near the window \n(C) under the bed\n(D) on the kitchen counter\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 782, "doc": {"video_name": "ego_20.mp4", "duration": 480.0, "question": "Where did I last put the electric screwdriver machine?\n(A) On the upper wooden block next to drill machine\n(B) In the toolbox\n(C) In the kitchen drawer\n(D) In the backyard shed\n", "candidates": ["On the upper wooden block next to drill machine", "In the toolbox", "In the kitchen drawer", "In the backyard shed"], "answer": "A", "task_type": "ego", "question_id": "Q782"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "466c58cafb3e7bd2bd209cd8967e20e45db47eea3df1277402e5ad32a7cb7011", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I last put the electric screwdriver machine?\n(A) On the upper wooden block next to drill machine\n(B) In the toolbox\n(C) In the kitchen drawer\n(D) In the backyard shed\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I last put the electric screwdriver machine?\n(A) On the upper wooden block next to drill machine\n(B) In the toolbox\n(C) In the kitchen drawer\n(D) In the backyard shed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 786, "doc": {"video_name": "ego_20.mp4", "duration": 480.0, "question": "How many nails did I attach?\n(A) Three\n(B) One\n(C) Five\n(D) Seven\n", "candidates": ["Three", "One", "Five", "Seven"], "answer": "A", "task_type": "ego", "question_id": "Q786"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fc7d7ed44830dc9a4280fad4028d0364ccb744981ecfab95f0f102afc97f2eb5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many nails did I attach?\n(A) Three\n(B) One\n(C) Five\n(D) Seven\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "How many nails did I attach?\n(A) Three\n(B) One\n(C) Five\n(D) Seven\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 790, "doc": {"video_name": "ego_5.mp4", "duration": 480.0, "question": "Did I pick up the yellow jar?\n(A) Yes.\n(B) Maybe.\n(C) No.\n(D) I don't know.\n", "candidates": ["Yes.", "Maybe.", "No.", "I don't know."], "answer": "A", "task_type": "ego", "question_id": "Q790"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dfd9a83aea0ccb7af27488e4f051f1577a7b03da84b0f710ea5ef7bf21a8d5c0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Did I pick up the yellow jar?\n(A) Yes.\n(B) Maybe.\n(C) No.\n(D) I don't know.\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "Did I pick up the yellow jar?\n(A) Yes.\n(B) Maybe.\n(C) No.\n(D) I don't know.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 794, "doc": {"video_name": "ego_47.mp4", "duration": 1200.0333333333333, "question": "How many holes did I first drill in the wood? \n(A) 4\n(B) 6\n(C) 2\n(D) 8\n", "candidates": ["4", "6", "2", "8"], "answer": "A", "task_type": "ego", "question_id": "Q794"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e2e95f7f9577c02c9b35617a344a774e819ad4b4a2b843a8b40298492efd4d59", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many holes did I first drill in the wood? \n(A) 4\n(B) 6\n(C) 2\n(D) 8\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "How many holes did I first drill in the wood? \n(A) 4\n(B) 6\n(C) 2\n(D) 8\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 798, "doc": {"video_name": "ego_3.mp4", "duration": 418.0333332874998, "question": "Where did I last put the white furry scarf?\n(A) In the car\n(B) In the backyard\n(C) On the marble counter\n(D) In the closet\n", "candidates": ["In the car", "In the backyard", "On the marble counter", "In the closet"], "answer": "C", "task_type": "ego", "question_id": "Q798"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4a257d54b26719deabf3b3ad13ce8e2507cba3e5a06cf9e3ba5a9c53dc98e0c3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I last put the white furry scarf?\n(A) In the car\n(B) In the backyard\n(C) On the marble counter\n(D) In the closet\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where did I last put the white furry scarf?\n(A) In the car\n(B) In the backyard\n(C) On the marble counter\n(D) In the closet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 802, "doc": {"video_name": "ego_3.mp4", "duration": 418.0333332874998, "question": "Where did I put the white color top hanger?\n(A) In the middle drawer\n(B) On the Lower right hanger\n(C) In the shoe rack\n(D) On the upper left hanger\n", "candidates": ["In the middle drawer", "On the Lower right hanger", "In the shoe rack", "On the upper left hanger"], "answer": "B", "task_type": "ego", "question_id": "Q802"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "273f62cdbd1a50c9102f9931dd092965fe905d3259100a27f78716d30eb11d73", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where did I put the white color top hanger?\n(A) In the middle drawer\n(B) On the Lower right hanger\n(C) In the shoe rack\n(D) On the upper left hanger\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where did I put the white color top hanger?\n(A) In the middle drawer\n(B) On the Lower right hanger\n(C) In the shoe rack\n(D) On the upper left hanger\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 806, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Where was the polishing machine?\n(A) in the box\n(B) outside the room\n(C) on the table\n(D) under the chair\n", "candidates": ["in the box", "outside the room", "on the table", "under the chair"], "answer": "A", "task_type": "ego", "question_id": "Q806"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cb93f22c8789e98f0657835bb265a693209e50c6b23bbb1c15156f0cb8be6d5a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the polishing machine?\n(A) in the box\n(B) outside the room\n(C) on the table\n(D) under the chair\n", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "Where was the polishing machine?\n(A) in the box\n(B) outside the room\n(C) on the table\n(D) under the chair\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 810, "doc": {"video_name": "ego_28.mp4", "duration": 480.0, "question": "How many boxes did I pick up? \n(A) four\n(B) one\n(C) two\n(D) three\n", "candidates": ["four", "one", "two", "three"], "answer": "C", "task_type": "ego", "question_id": "Q810"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b339aac4c92eba0435b1679e9463c71c949c506a216d9a9cb3479b13ce92e443", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many boxes did I pick up? \n(A) four\n(B) one\n(C) two\n(D) three\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "How many boxes did I pick up? \n(A) four\n(B) one\n(C) two\n(D) three\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 814, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "What jug did I take from the cupboard?\n(A) the jug with the red lid\n(B) the jug with the blue lid\n(C) the jug with the green lid\n(D) the jug with the yellow lid\n", "candidates": ["the jug with the red lid", "the jug with the blue lid", "the jug with the green lid", "the jug with the yellow lid"], "answer": "A", "task_type": "ego", "question_id": "Q814"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e4dcb48cbad018ed24916f6c541e62c8af7a4a525f484ae9b0f5ebb6dea02de4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What jug did I take from the cupboard?\n(A) the jug with the red lid\n(B) the jug with the blue lid\n(C) the jug with the green lid\n(D) the jug with the yellow lid\n", "task_type": "ego", "pred_answer": "D", "answer": "A"}, "input": "What jug did I take from the cupboard?\n(A) the jug with the red lid\n(B) the jug with the blue lid\n(C) the jug with the green lid\n(D) the jug with the yellow lid\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 818, "doc": {"video_name": "ego_48.mp4", "duration": 480.0, "question": "What did I put in the rack?\n(A) Spoon\n(B) Fork\n(C) Cup\n(D) Plate\n", "candidates": ["Spoon", "Fork", "Cup", "Plate"], "answer": "D", "task_type": "ego", "question_id": "Q818"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f3a180c51f0f0f985b9c40f20972a159ebb9db1b3e9694de676782f627cbe07", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I put in the rack?\n(A) Spoon\n(B) Fork\n(C) Cup\n(D) Plate\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "What did I put in the rack?\n(A) Spoon\n(B) Fork\n(C) Cup\n(D) Plate\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 822, "doc": {"video_name": "ego_28.mp4", "duration": 480.0, "question": "Where was the square bucket before I picked it up?\n(A) in the garden area\n(B) in the kitchen\n(C) on the roof\n(D) under the bed\n", "candidates": ["in the garden area", "in the kitchen", "on the roof", "under the bed"], "answer": "A", "task_type": "ego", "question_id": "Q822"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0a49acc7420474dd456dd2f3a389f6850ce89e613cb1ba65dc907dc67edd56af", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the square bucket before I picked it up?\n(A) in the garden area\n(B) in the kitchen\n(C) on the roof\n(D) under the bed\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where was the square bucket before I picked it up?\n(A) in the garden area\n(B) in the kitchen\n(C) on the roof\n(D) under the bed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 826, "doc": {"video_name": "ego_80.mp4", "duration": 480.03333333333336, "question": "where was the eyeglasses\n(A) on the table \n(B) in the drawer\n(C) under the chair\n(D) in the car\n", "candidates": ["on the table ", "in the drawer", "under the chair", "in the car"], "answer": "A", "task_type": "ego", "question_id": "Q826"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "829df96bd9519c22706bbf769513271dd33427576bde2161589bd3045ba87e1a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "where was the eyeglasses\n(A) on the table \n(B) in the drawer\n(C) under the chair\n(D) in the car\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "where was the eyeglasses\n(A) on the table \n(B) in the drawer\n(C) under the chair\n(D) in the car\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 830, "doc": {"video_name": "ego_62.mp4", "duration": 480.0, "question": "Where was the ATM machine?\n(A) At the top of the hill\n(B) Nearby window.\n(C) In the park\n(D) Inside the supermarket\n", "candidates": ["At the top of the hill", "Nearby window.", "In the park", "Inside the supermarket"], "answer": "B", "task_type": "ego", "question_id": "Q830"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "16be9d552ebc16aa6c63240fc27f6b6ea9e75b7497dd6457b37eb40c84910f64", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the ATM machine?\n(A) At the top of the hill\n(B) Nearby window.\n(C) In the park\n(D) Inside the supermarket\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the ATM machine?\n(A) At the top of the hill\n(B) Nearby window.\n(C) In the park\n(D) Inside the supermarket\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 834, "doc": {"video_name": "ego_41.mp4", "duration": 480.0333333333333, "question": "Where did I keep the cap of the tank?\n(A) on the car bonnet \n(B) under the bed\n(C) in the kitchen drawer\n(D) in the glove compartment\n", "candidates": ["on the car bonnet ", "under the bed", "in the kitchen drawer", "in the glove compartment"], "answer": "A", "task_type": "ego", "question_id": "Q834"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "233a5de01160824b4983f9f867de22b5581229a46e29d59805ed50ee3eb03675", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I keep the cap of the tank?\n(A) on the car bonnet \n(B) under the bed\n(C) in the kitchen drawer\n(D) in the glove compartment\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I keep the cap of the tank?\n(A) on the car bonnet \n(B) under the bed\n(C) in the kitchen drawer\n(D) in the glove compartment\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 838, "doc": {"video_name": "ego_19.mp4", "duration": 480.0, "question": "Where did I put the drillers?\n(A) in the shed\n(B) in the toolbox\n(C) in the garage\n(D) on the wooden scaffolding\n", "candidates": ["in the shed", "in the toolbox", "in the garage", "on the wooden scaffolding"], "answer": "D", "task_type": "ego", "question_id": "Q838"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eea0946f1d404c601fa2a431b2baaf9797fcec7e16355fab89d918815baa62a5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where did I put the drillers?\n(A) in the shed\n(B) in the toolbox\n(C) in the garage\n(D) on the wooden scaffolding\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where did I put the drillers?\n(A) in the shed\n(B) in the toolbox\n(C) in the garage\n(D) on the wooden scaffolding\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 842, "doc": {"video_name": "ego_81.mp4", "duration": 1200.0333333333333, "question": "Where did I put the butter?\n(A) on the counter top\n(B) in the refrigerator\n(C) in the pantry\n(D) in the microwave\n", "candidates": ["on the counter top", "in the refrigerator", "in the pantry", "in the microwave"], "answer": "A", "task_type": "ego", "question_id": "Q842"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4c16804600f537392acc9ba86745acabb75abef5c7b2aff3206d389b91fb9cd6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I put the butter?\n(A) on the counter top\n(B) in the refrigerator\n(C) in the pantry\n(D) in the microwave\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I put the butter?\n(A) on the counter top\n(B) in the refrigerator\n(C) in the pantry\n(D) in the microwave\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 846, "doc": {"video_name": "ego_19.mp4", "duration": 480.0, "question": "Where did I put the wood scaffold plank? \n(A) in the corner to save space\n(B) in the middle for balance\n(C) in the upward to attached the other wood\n(D) in the downward for stability\n", "candidates": ["in the corner to save space", "in the middle for balance", "in the upward to attached the other wood", "in the downward for stability"], "answer": "C", "task_type": "ego", "question_id": "Q846"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8ab4d7d59ff2faadab1d57a8f3ef59690be585a0907850a5e1d18ad9c9771d13", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I put the wood scaffold plank? \n(A) in the corner to save space\n(B) in the middle for balance\n(C) in the upward to attached the other wood\n(D) in the downward for stability\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where did I put the wood scaffold plank? \n(A) in the corner to save space\n(B) in the middle for balance\n(C) in the upward to attached the other wood\n(D) in the downward for stability\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 850, "doc": {"video_name": "ego_19.mp4", "duration": 480.0, "question": "What did I pick from the plastic box?\n(A) pen\n(B) needle\n(C) paper\n(D) scissors\n", "candidates": ["pen", "needle", "paper", "scissors"], "answer": "B", "task_type": "ego", "question_id": "Q850"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a703f8cce601ee1ae7ec7a260f6e6061cde058239000b94537cd922b8dc98230", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I pick from the plastic box?\n(A) pen\n(B) needle\n(C) paper\n(D) scissors\n", "task_type": "ego", "pred_answer": "A", "answer": "B"}, "input": "What did I pick from the plastic box?\n(A) pen\n(B) needle\n(C) paper\n(D) scissors\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 854, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "How many face masks were on the brown cabinet\n(A) 10\n(B) 3\n(C) 1\n(D) 5\n", "candidates": ["10", "3", "1", "5"], "answer": "B", "task_type": "ego", "question_id": "Q854"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1f93a210af97b10c896ab06568e31bfda805c5a8e01f7381f38a66c04d2025b1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many face masks were on the brown cabinet\n(A) 10\n(B) 3\n(C) 1\n(D) 5\n", "task_type": "ego", "pred_answer": "C", "answer": "B"}, "input": "How many face masks were on the brown cabinet\n(A) 10\n(B) 3\n(C) 1\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 858, "doc": {"video_name": "ego_80.mp4", "duration": 480.03333333333336, "question": "How many black bags are there?\n(A) 4 black bags\n(B) 1 black bag\n(C) 2 black bags\n(D) 3 black bags\n", "candidates": ["4 black bags", "1 black bag", "2 black bags", "3 black bags"], "answer": "C", "task_type": "ego", "question_id": "Q858"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2503c2d718d18a1a3f89fd796bf73b4e2dd7ab8b02299da3d4145d9305f95f72", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many black bags are there?\n(A) 4 black bags\n(B) 1 black bag\n(C) 2 black bags\n(D) 3 black bags\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "How many black bags are there?\n(A) 4 black bags\n(B) 1 black bag\n(C) 2 black bags\n(D) 3 black bags\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 862, "doc": {"video_name": "ego_51.mp4", "duration": 480.0333333333334, "question": "What did I fill in the pink bowl?\n(A) Soup\n(B) Milk\n(C) Orange juice\n(D) Water\n", "candidates": ["Soup", "Milk", "Orange juice", "Water"], "answer": "D", "task_type": "ego", "question_id": "Q862"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d7f057e26711453302c309a7460776847342144829d75d825fd90d270dcd94de", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I fill in the pink bowl?\n(A) Soup\n(B) Milk\n(C) Orange juice\n(D) Water\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What did I fill in the pink bowl?\n(A) Soup\n(B) Milk\n(C) Orange juice\n(D) Water\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 866, "doc": {"video_name": "ego_77.mp4", "duration": 468.23333328749993, "question": "What payment method did I use?\n(A) cash\n(B) credit card\n(C) Venmo\n(D) check\n", "candidates": ["cash", "credit card", "Venmo", "check"], "answer": "A", "task_type": "ego", "question_id": "Q866"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7ca219ce07017c99956e5fe815a7e73a7f19c348c069c65d25dd75e7de1346cb", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What payment method did I use?\n(A) cash\n(B) credit card\n(C) Venmo\n(D) check\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What payment method did I use?\n(A) cash\n(B) credit card\n(C) Venmo\n(D) check\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 870, "doc": {"video_name": "ego_73.mp4", "duration": 480.0, "question": "Where was the Brown bag?\n(A) On the red table in the kitchen\n(B) In the black drawer under the bed\n(C) On the blue chair beside the tv\n(D) On the white shelf in the bathroom\n", "candidates": ["On the red table in the kitchen", "In the black drawer under the bed", "On the blue chair beside the tv", "On the white shelf in the bathroom"], "answer": "C", "task_type": "ego", "question_id": "Q870"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d753b0fc3a93b537e0262bf785d8145ae9bea07e54ade8e446a9e76d1720859", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the Brown bag?\n(A) On the red table in the kitchen\n(B) In the black drawer under the bed\n(C) On the blue chair beside the tv\n(D) On the white shelf in the bathroom\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "Where was the Brown bag?\n(A) On the red table in the kitchen\n(B) In the black drawer under the bed\n(C) On the blue chair beside the tv\n(D) On the white shelf in the bathroom\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 874, "doc": {"video_name": "ego_27.mp4", "duration": 480.0, "question": "Where was the measurement tape?\n(A) hanging on the wall\n(B) in the pocket\n(C) in the toolbox\n(D) on the table\n", "candidates": ["hanging on the wall", "in the pocket", "in the toolbox", "on the table"], "answer": "B", "task_type": "ego", "question_id": "Q874"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "972b5b3374457c2aa7248f20341b732d93002a64c719d2b5aaf18d41a4ffa611", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the measurement tape?\n(A) hanging on the wall\n(B) in the pocket\n(C) in the toolbox\n(D) on the table\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the measurement tape?\n(A) hanging on the wall\n(B) in the pocket\n(C) in the toolbox\n(D) on the table\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 878, "doc": {"video_name": "ego_50.mp4", "duration": 480.0333333333333, "question": "Where did I put the white clothe?\n(A) In the kitchen table drawer\n(B) In the living room bookshelf\n(C) In the bathroom cabinet\n(D) In the bedroom closet\n", "candidates": ["In the kitchen table drawer", "In the living room bookshelf", "In the bathroom cabinet", "In the bedroom closet"], "answer": "A", "task_type": "ego", "question_id": "Q878"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b6b3e88e23ea488d1881d5aafcbaaff517dff7009a2513207c26f0826e39c0af", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I put the white clothe?\n(A) In the kitchen table drawer\n(B) In the living room bookshelf\n(C) In the bathroom cabinet\n(D) In the bedroom closet\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I put the white clothe?\n(A) In the kitchen table drawer\n(B) In the living room bookshelf\n(C) In the bathroom cabinet\n(D) In the bedroom closet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 882, "doc": {"video_name": "ego_56.mp4", "duration": 480.0000000000001, "question": "Where was the coffee container before pouring the coffee?\n(A) In the coffee maker\n(B) In the refrigerator\n(C) In the microwave\n(D) In the toaster\n", "candidates": ["In the coffee maker", "In the refrigerator", "In the microwave", "In the toaster"], "answer": "A", "task_type": "ego", "question_id": "Q882"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0541ed64b8d6cc612ee1276d058c8e807cd33ba29759499f98eb930d9eafec44", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the coffee container before pouring the coffee?\n(A) In the coffee maker\n(B) In the refrigerator\n(C) In the microwave\n(D) In the toaster\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where was the coffee container before pouring the coffee?\n(A) In the coffee maker\n(B) In the refrigerator\n(C) In the microwave\n(D) In the toaster\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 886, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "In what location did I last see the cat?\n(A) in the kitchen\n(B) on the dining chair\n(C) in the backyard\n(D) under the bed\n", "candidates": ["in the kitchen", "on the dining chair", "in the backyard", "under the bed"], "answer": "B", "task_type": "ego", "question_id": "Q886"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "378fca2fb7328f7efdadd5444d19e19cee3e5520eceaf566c3e87d3ddee451b2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In what location did I last see the cat?\n(A) in the kitchen\n(B) on the dining chair\n(C) in the backyard\n(D) under the bed\n", "task_type": "ego", "pred_answer": "A", "answer": "B"}, "input": "In what location did I last see the cat?\n(A) in the kitchen\n(B) on the dining chair\n(C) in the backyard\n(D) under the bed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 890, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Where was the black tape before I picked it up\n(A) on the kitchen counter\n(B) in the backyard\n(C) on the entrance stairs\n(D) under the couch\n", "candidates": ["on the kitchen counter", "in the backyard", "on the entrance stairs", "under the couch"], "answer": "C", "task_type": "ego", "question_id": "Q890"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59012d99a74b18951e246e513bbe7ed0f42c06b8b2d62489c02928fe979f7356", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the black tape before I picked it up\n(A) on the kitchen counter\n(B) in the backyard\n(C) on the entrance stairs\n(D) under the couch\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was the black tape before I picked it up\n(A) on the kitchen counter\n(B) in the backyard\n(C) on the entrance stairs\n(D) under the couch\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 894, "doc": {"video_name": "ego_30.mp4", "duration": 480.0, "question": "Where were the keys?\n(A) In the car.\n(B) In the fridge.\n(C) On the table.\n(D) In the mailbox.\n", "candidates": ["In the car.", "In the fridge.", "On the table.", "In the mailbox."], "answer": "C", "task_type": "ego", "question_id": "Q894"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d2c9bc53b2450a13b6031bcd99c4d3eb38805a3fbbf5a3f1202eca1e18c808ee", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where were the keys?\n(A) In the car.\n(B) In the fridge.\n(C) On the table.\n(D) In the mailbox.\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where were the keys?\n(A) In the car.\n(B) In the fridge.\n(C) On the table.\n(D) In the mailbox.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 898, "doc": {"video_name": "ego_34.mp4", "duration": 442.29999999999995, "question": "Did I drink water?\n(A) yes.\n(B) no\n(C) I don't know\n(D) maybe\n", "candidates": ["yes.", "no", "I don't know", "maybe"], "answer": "A", "task_type": "ego", "question_id": "Q898"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4df848fb8c00e20284ce7df0e5e6c6db1beff55487e3ff5c30a879ab243e645b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Did I drink water?\n(A) yes.\n(B) no\n(C) I don't know\n(D) maybe\n", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "Did I drink water?\n(A) yes.\n(B) no\n(C) I don't know\n(D) maybe\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 902, "doc": {"video_name": "ego_51.mp4", "duration": 480.0333333333334, "question": "Where was the plate after I putting a bread slice on it ?\n(A) the plate was in the cabinet before i put a bread slice on it.\n(B) the plate was in the refrigerator before i put a bread slice on it.\n(C) the plate was in the sink before i put a bread slice on it.\n(D) the plate was on the floor before i put a bread slice on it.\n", "candidates": ["the plate was in the cabinet before i put a bread slice on it.", "the plate was in the refrigerator before i put a bread slice on it.", "the plate was in the sink before i put a bread slice on it.", "the plate was on the floor before i put a bread slice on it."], "answer": "A", "task_type": "ego", "question_id": "Q902"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4010a258e3bfef2e8b5e3d56989291a3cb84d19129d6696176e283cd953173b8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the plate after I putting a bread slice on it ?\n(A) the plate was in the cabinet before i put a bread slice on it.\n(B) the plate was in the refrigerator before i put a bread slice on it.\n(C) the plate was in the sink before i put a bread slice on it.\n(D) the plate was on the floor before i put a bread slice on it.\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "Where was the plate after I putting a bread slice on it ?\n(A) the plate was in the cabinet before i put a bread slice on it.\n(B) the plate was in the refrigerator before i put a bread slice on it.\n(C) the plate was in the sink before i put a bread slice on it.\n(D) the plate was on the floor before i put a bread slice on it.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 906, "doc": {"video_name": "ego_28.mp4", "duration": 480.0, "question": "Did I throw the drill on the ground?\n(A) yes \n(B) I don't know\n(C) no\n(D) maybe\n", "candidates": ["yes ", "I don't know", "no", "maybe"], "answer": "A", "task_type": "ego", "question_id": "Q906"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7eec0693a1824ae5c267399dcedfde77245a649e364f51277f23437465341603", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Did I throw the drill on the ground?\n(A) yes \n(B) I don't know\n(C) no\n(D) maybe\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "Did I throw the drill on the ground?\n(A) yes \n(B) I don't know\n(C) no\n(D) maybe\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 910, "doc": {"video_name": "ego_16.mp4", "duration": 480.0, "question": "Where was the blue poly bag?\n(A) On the floor next to the small black sofa\n(B) Underneath the dining table\n(C) On the kitchen counter\n(D) In the bathroom cabinet\n", "candidates": ["On the floor next to the small black sofa", "Underneath the dining table", "On the kitchen counter", "In the bathroom cabinet"], "answer": "A", "task_type": "ego", "question_id": "Q910"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9d9350b701c80b2d16aa919734b1e980b7916928193f46d02a07912cc5c6f438", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the blue poly bag?\n(A) On the floor next to the small black sofa\n(B) Underneath the dining table\n(C) On the kitchen counter\n(D) In the bathroom cabinet\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where was the blue poly bag?\n(A) On the floor next to the small black sofa\n(B) Underneath the dining table\n(C) On the kitchen counter\n(D) In the bathroom cabinet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 914, "doc": {"video_name": "ego_15.mp4", "duration": 480.0, "question": "Did I leave the monitor screen on ?\n(A) Yes, but it was accidental.\n(B) Yes.\n(C) No, the monitor screen is off.\n(D) I'm not sure, I didn't see.\n", "candidates": ["Yes, but it was accidental.", "Yes.", "No, the monitor screen is off.", "I'm not sure, I didn't see."], "answer": "B", "task_type": "ego", "question_id": "Q914"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c290d1131e7932aff87962559709a17265cb001da199713990e0493536012f7d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did I leave the monitor screen on ?\n(A) Yes, but it was accidental.\n(B) Yes.\n(C) No, the monitor screen is off.\n(D) I'm not sure, I didn't see.\n", "task_type": "ego", "pred_answer": "C.", "answer": "B"}, "input": "Did I leave the monitor screen on ?\n(A) Yes, but it was accidental.\n(B) Yes.\n(C) No, the monitor screen is off.\n(D) I'm not sure, I didn't see.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 918, "doc": {"video_name": "ego_48.mp4", "duration": 480.0, "question": "Did I leave the door open?\n(A) maybe\n(B) yes\n(C) I don't know\n(D) no\n", "candidates": ["maybe", "yes", "I don't know", "no"], "answer": "B", "task_type": "ego", "question_id": "Q918"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b9a4671ed226ff11151c301001f8c8d0de99628f43dcc5dccda11ccf78258b0b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did I leave the door open?\n(A) maybe\n(B) yes\n(C) I don't know\n(D) no\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Did I leave the door open?\n(A) maybe\n(B) yes\n(C) I don't know\n(D) no\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 922, "doc": {"video_name": "ego_28.mp4", "duration": 480.0, "question": "Where did I keep the drill?\n(A) in the garage\n(B) near the stairs \n(C) under the bed\n(D) in the kitchen\n", "candidates": ["in the garage", "near the stairs ", "under the bed", "in the kitchen"], "answer": "B", "task_type": "ego", "question_id": "Q922"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d7604179eea5bedd4336a68233aa095e8aaa4d13f6d6eda10b8d8d8e85c9a24c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where did I keep the drill?\n(A) in the garage\n(B) near the stairs \n(C) under the bed\n(D) in the kitchen\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where did I keep the drill?\n(A) in the garage\n(B) near the stairs \n(C) under the bed\n(D) in the kitchen\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 926, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "What did I pour in the jug?\n(A) coffee\n(B) water\n(C) squeezed orange juice\n(D) milk\n", "candidates": ["coffee", "water", "squeezed orange juice", "milk"], "answer": "C", "task_type": "ego", "question_id": "Q926"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "34a00aaef007eddc13696c93d0797cfea708d2d5d91dcd09d08bea0b3e68a508", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did I pour in the jug?\n(A) coffee\n(B) water\n(C) squeezed orange juice\n(D) milk\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What did I pour in the jug?\n(A) coffee\n(B) water\n(C) squeezed orange juice\n(D) milk\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 930, "doc": {"video_name": "ego_68.mp4", "duration": 480.03333333333336, "question": "What word did I read on the paper stuck to the green glass ?\n(A) Golden Gate Bridge\n(B) Santana Row.\n(C) Alcatraz\n(D) San Francisco\n", "candidates": ["Golden Gate Bridge", "Santana Row.", "Alcatraz", "San Francisco"], "answer": "B", "task_type": "ego", "question_id": "Q930"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "acd433c98564cadc6b6c5ad955310ebcbdd92b9f33ec4d060e481eb38c8b90c8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What word did I read on the paper stuck to the green glass ?\n(A) Golden Gate Bridge\n(B) Santana Row.\n(C) Alcatraz\n(D) San Francisco\n", "task_type": "ego", "pred_answer": "D", "answer": "B"}, "input": "What word did I read on the paper stuck to the green glass ?\n(A) Golden Gate Bridge\n(B) Santana Row.\n(C) Alcatraz\n(D) San Francisco\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 934, "doc": {"video_name": "ego_24.mp4", "duration": 480.0, "question": "Where did I put three handful of nails?\n(A) In my backpack\n(B) In my left pocket.\n(C) On the table\n(D) In my right pocket\n", "candidates": ["In my backpack", "In my left pocket.", "On the table", "In my right pocket"], "answer": "B", "task_type": "ego", "question_id": "Q934"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6297ca992b0c6c8fe9de6928565a329a094afddd462ae1884b445792d0aa89cf", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where did I put three handful of nails?\n(A) In my backpack\n(B) In my left pocket.\n(C) On the table\n(D) In my right pocket\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where did I put three handful of nails?\n(A) In my backpack\n(B) In my left pocket.\n(C) On the table\n(D) In my right pocket\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 938, "doc": {"video_name": "ego_76.mp4", "duration": 480.03333333333336, "question": "Where was the wooden bamboo?\n(A) In the kitchen.\n(B) Nearby tractor.\n(C) Underneath the ocean.\n(D) On top of the mountain.\n", "candidates": ["In the kitchen.", "Nearby tractor.", "Underneath the ocean.", "On top of the mountain."], "answer": "B", "task_type": "ego", "question_id": "Q938"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0275a6c4b8e91514eb5dad73bcf2b882589d6c0e1ec33c39993905addce7c250", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the wooden bamboo?\n(A) In the kitchen.\n(B) Nearby tractor.\n(C) Underneath the ocean.\n(D) On top of the mountain.\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the wooden bamboo?\n(A) In the kitchen.\n(B) Nearby tractor.\n(C) Underneath the ocean.\n(D) On top of the mountain.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 942, "doc": {"video_name": "ego_81.mp4", "duration": 1200.0333333333333, "question": "What did I put in the plate\n(A) spaghetti\n(B) lettuce\n(C) yolk\n(D) bread\n", "candidates": ["spaghetti", "lettuce", "yolk", "bread"], "answer": "C", "task_type": "ego", "question_id": "Q942"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7697e72fbaa122a36ea3885009d494871cb7e7a7498ff2f74cd1f38158cd8269", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did I put in the plate\n(A) spaghetti\n(B) lettuce\n(C) yolk\n(D) bread\n", "task_type": "ego", "pred_answer": "D", "answer": "C"}, "input": "What did I put in the plate\n(A) spaghetti\n(B) lettuce\n(C) yolk\n(D) bread\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 946, "doc": {"video_name": "ego_22.mp4", "duration": 480.0, "question": "Where did I put the fur blanket \n(A) in the kitchen\n(B) on the bed\n(C) in the closet\n(D) on the couch\n", "candidates": ["in the kitchen", "on the bed", "in the closet", "on the couch"], "answer": "B", "task_type": "ego", "question_id": "Q946"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e2d8f3be7014a2ad9dc862001c4d83644ea1e3b76791475962543cd6808605c1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where did I put the fur blanket \n(A) in the kitchen\n(B) on the bed\n(C) in the closet\n(D) on the couch\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where did I put the fur blanket \n(A) in the kitchen\n(B) on the bed\n(C) in the closet\n(D) on the couch\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 950, "doc": {"video_name": "ego_45.mp4", "duration": 428.83333328749984, "question": "Where did I kept the brown packet?\n(A) On the floor\n(B) In the car\n(C) In the refrigerator\n(D) In the mailbox\n", "candidates": ["On the floor", "In the car", "In the refrigerator", "In the mailbox"], "answer": "A", "task_type": "ego", "question_id": "Q950"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "20706e5cd155b33981a19e39472b22d7974f470adb25d34ba9ca7d63c3f99957", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I kept the brown packet?\n(A) On the floor\n(B) In the car\n(C) In the refrigerator\n(D) In the mailbox\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "Where did I kept the brown packet?\n(A) On the floor\n(B) In the car\n(C) In the refrigerator\n(D) In the mailbox\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 954, "doc": {"video_name": "ego_80.mp4", "duration": 480.03333333333336, "question": "What word was written on the mirror door?\n(A) Skydive\n(B) Skylounge\n(C) Skyscraper\n(D) Skylight\n", "candidates": ["Skydive", "Skylounge", "Skyscraper", "Skylight"], "answer": "B", "task_type": "ego", "question_id": "Q954"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8f29940837377679961d12e2903050bf95124edc72f5a6e4f60e3c7bd334cde2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What word was written on the mirror door?\n(A) Skydive\n(B) Skylounge\n(C) Skyscraper\n(D) Skylight\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What word was written on the mirror door?\n(A) Skydive\n(B) Skylounge\n(C) Skyscraper\n(D) Skylight\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 958, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Where did I put the white bucket?\n(A) in the car\n(B) outside\n(C) in the room\n(D) in the kitchen\n", "candidates": ["in the car", "outside", "in the room", "in the kitchen"], "answer": "C", "task_type": "ego", "question_id": "Q958"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "11cd339761b3923b48934bf000370693e56e7363b2396b3f62d268377632ffdc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I put the white bucket?\n(A) in the car\n(B) outside\n(C) in the room\n(D) in the kitchen\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where did I put the white bucket?\n(A) in the car\n(B) outside\n(C) in the room\n(D) in the kitchen\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 962, "doc": {"video_name": "ego_81.mp4", "duration": 1200.0333333333333, "question": "What did I put in the plastic bag?\n(A) Apple core\n(B) Orange peel\n(C) Empty soda can\n(D) Banana peel\n", "candidates": ["Apple core", "Orange peel", "Empty soda can", "Banana peel"], "answer": "D", "task_type": "ego", "question_id": "Q962"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e69431b5d358a439a5d7d99e0586462f8ec089c86b97c36b8d8cd3b7de1875a6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I put in the plastic bag?\n(A) Apple core\n(B) Orange peel\n(C) Empty soda can\n(D) Banana peel\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "What did I put in the plastic bag?\n(A) Apple core\n(B) Orange peel\n(C) Empty soda can\n(D) Banana peel\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 966, "doc": {"video_name": "ego_1.mp4", "duration": 480.0, "question": "Did I leave the door open?\n(A) I don't know\n(B) no \n(C) maybe\n(D) yes\n", "candidates": ["I don't know", "no ", "maybe", "yes"], "answer": "B", "task_type": "ego", "question_id": "Q966"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b793f3dbced9163456b67052eeb48fe3a446e58f6e377e2cb059d70728e80b69", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did I leave the door open?\n(A) I don't know\n(B) no \n(C) maybe\n(D) yes\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Did I leave the door open?\n(A) I don't know\n(B) no \n(C) maybe\n(D) yes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 970, "doc": {"video_name": "ego_78.mp4", "duration": 480.0, "question": "Where did I put the drill machine?\n(A) Closet\n(B) Table\n(C) Shelf\n(D) Drawer\n", "candidates": ["Closet", "Table", "Shelf", "Drawer"], "answer": "B", "task_type": "ego", "question_id": "Q970"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "41f80bbf3bbafbb0dcb9fbb74ea8acffa681f29f73ae535bf4d56fd111f70125", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where did I put the drill machine?\n(A) Closet\n(B) Table\n(C) Shelf\n(D) Drawer\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where did I put the drill machine?\n(A) Closet\n(B) Table\n(C) Shelf\n(D) Drawer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 974, "doc": {"video_name": "ego_14.mp4", "duration": 480.0333333333334, "question": "Did I wash my hands ?\n(A) No\n(B) I don't know\n(C) Maybe\n(D) Yes.\n", "candidates": ["No", "I don't know", "Maybe", "Yes."], "answer": "D", "task_type": "ego", "question_id": "Q974"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43f66b59e3bfd73b843a5c83689a6f0b4c44e5ded38d6882a943c2930ea0e803", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Did I wash my hands ?\n(A) No\n(B) I don't know\n(C) Maybe\n(D) Yes.\n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "Did I wash my hands ?\n(A) No\n(B) I don't know\n(C) Maybe\n(D) Yes.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 978, "doc": {"video_name": "ego_13.mp4", "duration": 480.0, "question": "In what location did I see the second hint bottled water?\n(A) in the bedroom closet\n(B) in the kitchen pantry\n(C) in the bathroom cabinet\n(D) on the table by the couch\n", "candidates": ["in the bedroom closet", "in the kitchen pantry", "in the bathroom cabinet", "on the table by the couch"], "answer": "D", "task_type": "ego", "question_id": "Q978"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "716fd4186388876e63111ba3ff0ba0308544cb07dade9d26be215d306375bc49", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what location did I see the second hint bottled water?\n(A) in the bedroom closet\n(B) in the kitchen pantry\n(C) in the bathroom cabinet\n(D) on the table by the couch\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "In what location did I see the second hint bottled water?\n(A) in the bedroom closet\n(B) in the kitchen pantry\n(C) in the bathroom cabinet\n(D) on the table by the couch\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 982, "doc": {"video_name": "ego_80.mp4", "duration": 480.03333333333336, "question": "Where was the bar code\n(A) on the ceiling\n(B) on the floor\n(C) on the door \n(D) on the window\n", "candidates": ["on the ceiling", "on the floor", "on the door ", "on the window"], "answer": "C", "task_type": "ego", "question_id": "Q982"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "779ca04bdc807bc2a2b4fd2ae4ba1597b92453096af0465c558782d0e297f4f5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the bar code\n(A) on the ceiling\n(B) on the floor\n(C) on the door \n(D) on the window\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was the bar code\n(A) on the ceiling\n(B) on the floor\n(C) on the door \n(D) on the window\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 986, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Did I leave the front door open\n(A) No\n(B) I'm not sure\n(C) Yes\n(D) I don't remember\n", "candidates": ["No", "I'm not sure", "Yes", "I don't remember"], "answer": "C", "task_type": "ego", "question_id": "Q986"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8b12078778b8759e29b8a68385df45a59fd575ddc5237f9531af9ed3a0dfabe3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Did I leave the front door open\n(A) No\n(B) I'm not sure\n(C) Yes\n(D) I don't remember\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "Did I leave the front door open\n(A) No\n(B) I'm not sure\n(C) Yes\n(D) I don't remember\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 990, "doc": {"video_name": "ego_38.mp4", "duration": 480.0333333333333, "question": "What did I put in the black bin?\n(A) glass\n(B) paper\n(C) metal\n(D) plastic\n", "candidates": ["glass", "paper", "metal", "plastic"], "answer": "D", "task_type": "ego", "question_id": "Q990"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c7607bb4f48fde8a7c6ee47ddf7fd8e5ebbb328023a4af7facd5cda5dc4f5d77", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I put in the black bin?\n(A) glass\n(B) paper\n(C) metal\n(D) plastic\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "What did I put in the black bin?\n(A) glass\n(B) paper\n(C) metal\n(D) plastic\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 994, "doc": {"video_name": "ego_60.mp4", "duration": 480.0333333333334, "question": "What did I keep inside the refrigerator?\n(A) Juice packet.\n(B) Milk carton\n(C) Eggs carton\n(D) Bread loaf\n", "candidates": ["Juice packet.", "Milk carton", "Eggs carton", "Bread loaf"], "answer": "A", "task_type": "ego", "question_id": "Q994"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2da40b65f4e9024584f35bb5a84c2040cf6b60d4ab761366e10aa88f7cdcfb2e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I keep inside the refrigerator?\n(A) Juice packet.\n(B) Milk carton\n(C) Eggs carton\n(D) Bread loaf\n", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "What did I keep inside the refrigerator?\n(A) Juice packet.\n(B) Milk carton\n(C) Eggs carton\n(D) Bread loaf\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 998, "doc": {"video_name": "ego_41.mp4", "duration": 480.0333333333333, "question": "What did I put in the dustbin?\n\n(A) banana peel and coffee grounds\n(B) box and egg shell\n(C) paper and plastic\n(D) glass bottle and aluminum can\n", "candidates": ["banana peel and coffee grounds", "box and egg shell", "paper and plastic", "glass bottle and aluminum can"], "answer": "B", "task_type": "ego", "question_id": "Q998"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6da56b2ffa93e1e0326aed5b19343f45adf310a6ad9474787d60ea28691c257f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I put in the dustbin?\n\n(A) banana peel and coffee grounds\n(B) box and egg shell\n(C) paper and plastic\n(D) glass bottle and aluminum can\n", "task_type": "ego", "pred_answer": "C", "answer": "B"}, "input": "What did I put in the dustbin?\n\n(A) banana peel and coffee grounds\n(B) box and egg shell\n(C) paper and plastic\n(D) glass bottle and aluminum can\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1002, "doc": {"video_name": "ego_74.mp4", "duration": 480.0, "question": "Where was the scraper before I picked it up?\n(A) under the sink\n(B) in the drawer\n(C) on the cupboard\n(D) in the dishwasher\n", "candidates": ["under the sink", "in the drawer", "on the cupboard", "in the dishwasher"], "answer": "C", "task_type": "ego", "question_id": "Q1002"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1c57f56694b7749fef815e8f9affc23ad1a928593b8b5fcee6ca8a1650e6a6f1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the scraper before I picked it up?\n(A) under the sink\n(B) in the drawer\n(C) on the cupboard\n(D) in the dishwasher\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "Where was the scraper before I picked it up?\n(A) under the sink\n(B) in the drawer\n(C) on the cupboard\n(D) in the dishwasher\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1006, "doc": {"video_name": "ego_34.mp4", "duration": 442.29999999999995, "question": "Where was the mango after I smelled it?\n(A) in the refrigerator\n(B) on the kitchen counter\n(C) under the bed\n(D) in the person who wearing grey undershirt \n", "candidates": ["in the refrigerator", "on the kitchen counter", "under the bed", "in the person who wearing grey undershirt "], "answer": "D", "task_type": "ego", "question_id": "Q1006"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5152cff08cec9d1bd3b392a78ce28f4e34e3a8dfc405cebfa53d44b8d604ad7d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the mango after I smelled it?\n(A) in the refrigerator\n(B) on the kitchen counter\n(C) under the bed\n(D) in the person who wearing grey undershirt \n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the mango after I smelled it?\n(A) in the refrigerator\n(B) on the kitchen counter\n(C) under the bed\n(D) in the person who wearing grey undershirt \n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1010, "doc": {"video_name": "ego_72.mp4", "duration": 479.9999999999998, "question": "Where were green peas?\n(A) in a garden\n(B) in a refrigerator.\n(C) in a pantry\n(D) in a shoebox\n", "candidates": ["in a garden", "in a refrigerator.", "in a pantry", "in a shoebox"], "answer": "B", "task_type": "ego", "question_id": "Q1010"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c4aaa1dd05c7f5a4113c02905b5a7d8e04407b27cb517e18af2dc3c993133434", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where were green peas?\n(A) in a garden\n(B) in a refrigerator.\n(C) in a pantry\n(D) in a shoebox\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where were green peas?\n(A) in a garden\n(B) in a refrigerator.\n(C) in a pantry\n(D) in a shoebox\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1014, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Where did I put the white cup\n(A) in the sink\n(B) in the refrigerator\n(C) on the black table\n(D) on the white table\n", "candidates": ["in the sink", "in the refrigerator", "on the black table", "on the white table"], "answer": "D", "task_type": "ego", "question_id": "Q1014"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2dbaf64e81c181337d64ab9cd66ee2131e222f32429e4c53f62886c29d97ba1a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where did I put the white cup\n(A) in the sink\n(B) in the refrigerator\n(C) on the black table\n(D) on the white table\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "Where did I put the white cup\n(A) in the sink\n(B) in the refrigerator\n(C) on the black table\n(D) on the white table\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1018, "doc": {"video_name": "ego_48.mp4", "duration": 480.0, "question": "What did I put in the wardrobe?\n(A) Throw pillow\n(B) Shoes\n(C) Blanket\n(D) Coffee mug\n", "candidates": ["Throw pillow", "Shoes", "Blanket", "Coffee mug"], "answer": "A", "task_type": "ego", "question_id": "Q1018"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ad6d7c2d843a265832d50ca5065b010610af9207e66df37ef4e647f206a25928", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I put in the wardrobe?\n(A) Throw pillow\n(B) Shoes\n(C) Blanket\n(D) Coffee mug\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "What did I put in the wardrobe?\n(A) Throw pillow\n(B) Shoes\n(C) Blanket\n(D) Coffee mug\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1022, "doc": {"video_name": "ego_28.mp4", "duration": 480.0, "question": "Did I attached the drill into the drill machine?\n(A) yes\n(B) I don't know\n(C) maybe\n(D) no\n", "candidates": ["yes", "I don't know", "maybe", "no"], "answer": "A", "task_type": "ego", "question_id": "Q1022"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e8ebfe6a2d1049826ba91534879dee065a3c009f77cbcba84a7b30c19dc05287", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Did I attached the drill into the drill machine?\n(A) yes\n(B) I don't know\n(C) maybe\n(D) no\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Did I attached the drill into the drill machine?\n(A) yes\n(B) I don't know\n(C) maybe\n(D) no\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1026, "doc": {"video_name": "ego_12.mp4", "duration": 480.0, "question": "Where was the milk pack?\n(A) in the microwave\n(B) in the pantry\n(C) inside fridge\n(D) on the kitchen counter\n", "candidates": ["in the microwave", "in the pantry", "inside fridge", "on the kitchen counter"], "answer": "C", "task_type": "ego", "question_id": "Q1026"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "983ed4677abce3858efc7cc9e4a435d6b237b370bcbe322bb7e857a7385398b6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the milk pack?\n(A) in the microwave\n(B) in the pantry\n(C) inside fridge\n(D) on the kitchen counter\n", "task_type": "ego", "pred_answer": "D", "answer": "C"}, "input": "Where was the milk pack?\n(A) in the microwave\n(B) in the pantry\n(C) inside fridge\n(D) on the kitchen counter\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1030, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "Where did I put the knife?\n(A) on the living room table\n(B) outside in the garden\n(C) in the bedroom drawer\n(D) in the kitchen sink\n", "candidates": ["on the living room table", "outside in the garden", "in the bedroom drawer", "in the kitchen sink"], "answer": "D", "task_type": "ego", "question_id": "Q1030"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "660056f31b1c8005459e549a4ab134b3e8a661050890fb2ece9e9117bd02e07d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where did I put the knife?\n(A) on the living room table\n(B) outside in the garden\n(C) in the bedroom drawer\n(D) in the kitchen sink\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where did I put the knife?\n(A) on the living room table\n(B) outside in the garden\n(C) in the bedroom drawer\n(D) in the kitchen sink\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1034, "doc": {"video_name": "ego_9.mp4", "duration": 480.03333333333336, "question": "Did I throw away the black plastic?\n(A) I'm not sure\n(B) I don't know\n(C) no\n(D) yes\n", "candidates": ["I'm not sure", "I don't know", "no", "yes"], "answer": "D", "task_type": "ego", "question_id": "Q1034"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd97a31b4d16b074ceb77cd54fc0a0034afd154d7075ecfc1f740bcf8ffcdb85", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Did I throw away the black plastic?\n(A) I'm not sure\n(B) I don't know\n(C) no\n(D) yes\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "Did I throw away the black plastic?\n(A) I'm not sure\n(B) I don't know\n(C) no\n(D) yes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1038, "doc": {"video_name": "ego_9.mp4", "duration": 480.03333333333336, "question": "How many drilling machines did I keep on the stairs?\n(A) four\n(B) three\n(C) two\n(D) one\n", "candidates": ["four", "three", "two", "one"], "answer": "C", "task_type": "ego", "question_id": "Q1038"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "869e5f86cd8a7eeeb7ad8d95745d7c0b6ebffc3f48a9f71bba36abcebf79aff6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many drilling machines did I keep on the stairs?\n(A) four\n(B) three\n(C) two\n(D) one\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "How many drilling machines did I keep on the stairs?\n(A) four\n(B) three\n(C) two\n(D) one\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1042, "doc": {"video_name": "ego_3.mp4", "duration": 418.0333332874998, "question": "How many belts were kept on the display table?\n(A) Four.\n(B) Six\n(C) Eight\n(D) Two\n", "candidates": ["Four.", "Six", "Eight", "Two"], "answer": "A", "task_type": "ego", "question_id": "Q1042"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "13425eb0671a6a3a24fcf5c5a4d6a9c2ed72c57e4b6408cb1ef7b8f495d18903", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many belts were kept on the display table?\n(A) Four.\n(B) Six\n(C) Eight\n(D) Two\n", "task_type": "ego", "pred_answer": "D", "answer": "A"}, "input": "How many belts were kept on the display table?\n(A) Four.\n(B) Six\n(C) Eight\n(D) Two\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1046, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "How many oranges did I squeeze last?\n(A) 2 sliced oranges\n(B) 1 whole orange\n(C) half cut orange\n(D) 3 peeled oranges\n", "candidates": ["2 sliced oranges", "1 whole orange", "half cut orange", "3 peeled oranges"], "answer": "C", "task_type": "ego", "question_id": "Q1046"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "39e273e9ef70cfb91c0c6817ef91738300887605945ae7fe69ccfca26b2351e6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many oranges did I squeeze last?\n(A) 2 sliced oranges\n(B) 1 whole orange\n(C) half cut orange\n(D) 3 peeled oranges\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "How many oranges did I squeeze last?\n(A) 2 sliced oranges\n(B) 1 whole orange\n(C) half cut orange\n(D) 3 peeled oranges\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1050, "doc": {"video_name": "ego_69.mp4", "duration": 480.0, "question": "Where was the weighing scale?\n(A) On top of the refrigerator\n(B) Inside the pantry\n(C) In the bathroom\n(D) Next to the fruits\n", "candidates": ["On top of the refrigerator", "Inside the pantry", "In the bathroom", "Next to the fruits"], "answer": "D", "task_type": "ego", "question_id": "Q1050"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9bfaa0567a41dff01f6a4a4f1b40d75638657b8fd3e22f22b0b1096c02128180", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the weighing scale?\n(A) On top of the refrigerator\n(B) Inside the pantry\n(C) In the bathroom\n(D) Next to the fruits\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the weighing scale?\n(A) On top of the refrigerator\n(B) Inside the pantry\n(C) In the bathroom\n(D) Next to the fruits\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1054, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Where was the fevicol?\n(A) under the bed\n(B) on the shelf\n(C) outside the house\n(D) in the room\n", "candidates": ["under the bed", "on the shelf", "outside the house", "in the room"], "answer": "D", "task_type": "ego", "question_id": "Q1054"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e50b92ec577aed067bf4893bf88137980ecf56405ccb175b90c5218779cdac0f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the fevicol?\n(A) under the bed\n(B) on the shelf\n(C) outside the house\n(D) in the room\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the fevicol?\n(A) under the bed\n(B) on the shelf\n(C) outside the house\n(D) in the room\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1058, "doc": {"video_name": "ego_27.mp4", "duration": 480.0, "question": "Where was the pencil?\n(A) in the backpack\n(B) under the chair\n(C) on the table\n(D) in the pocket\n", "candidates": ["in the backpack", "under the chair", "on the table", "in the pocket"], "answer": "D", "task_type": "ego", "question_id": "Q1058"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "38c435057ec7b60a1948b37f65cd19577b161fed5f71db4b036288608b1a88d3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the pencil?\n(A) in the backpack\n(B) under the chair\n(C) on the table\n(D) in the pocket\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the pencil?\n(A) in the backpack\n(B) under the chair\n(C) on the table\n(D) in the pocket\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1062, "doc": {"video_name": "ego_79.mp4", "duration": 480.0, "question": "What game did I play after playing Connect 4?\n(A) sorry\n(B) Chess\n(C) Monopoly\n(D) Scrabble\n", "candidates": ["sorry", "Chess", "Monopoly", "Scrabble"], "answer": "A", "task_type": "ego", "question_id": "Q1062"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "34d9c6777f6e1188c959f1a9667037d5dc3bdce29004928e61a9541c726e63f4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What game did I play after playing Connect 4?\n(A) sorry\n(B) Chess\n(C) Monopoly\n(D) Scrabble\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What game did I play after playing Connect 4?\n(A) sorry\n(B) Chess\n(C) Monopoly\n(D) Scrabble\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1066, "doc": {"video_name": "ego_61.mp4", "duration": 480.0333333333333, "question": "What did I put under the cup?\n(A) tissue paper\n(B) pen\n(C) key\n(D) coin\n", "candidates": ["tissue paper", "pen", "key", "coin"], "answer": "A", "task_type": "ego", "question_id": "Q1066"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5f31b22f79e6084b54082a7443d607b292ffa4b08219174f85970f5260cfbbf7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I put under the cup?\n(A) tissue paper\n(B) pen\n(C) key\n(D) coin\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What did I put under the cup?\n(A) tissue paper\n(B) pen\n(C) key\n(D) coin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1070, "doc": {"video_name": "ego_80.mp4", "duration": 480.03333333333336, "question": "how  many cloth are hugging \n(A) 3 cloth are hugging\n(B) 12 cloth are hugging\n(C) 6 cloth are hugging\n(D) 9 cloth are hugging\n", "candidates": ["3 cloth are hugging", "12 cloth are hugging", "6 cloth are hugging", "9 cloth are hugging"], "answer": "C", "task_type": "ego", "question_id": "Q1070"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6dee56813f0e166d7e5ada32f84cdfa55f688fb507fff9eedd58eebd03f5c2be", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "how  many cloth are hugging \n(A) 3 cloth are hugging\n(B) 12 cloth are hugging\n(C) 6 cloth are hugging\n(D) 9 cloth are hugging\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "how  many cloth are hugging \n(A) 3 cloth are hugging\n(B) 12 cloth are hugging\n(C) 6 cloth are hugging\n(D) 9 cloth are hugging\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1074, "doc": {"video_name": "ego_81.mp4", "duration": 1200.0333333333333, "question": "How many jars did I see in the cabinet?\n(A) 2\n(B) 5\n(C) 10\n(D) \n", "candidates": ["2", "5", "10", ""], "answer": "D", "task_type": "ego", "question_id": "Q1074"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43274c3bb14ccedb70d5b3cec23f0cbfea8e5b9c6489fd164d3f1f0c7296f6e5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many jars did I see in the cabinet?\n(A) 2\n(B) 5\n(C) 10\n(D) \n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "How many jars did I see in the cabinet?\n(A) 2\n(B) 5\n(C) 10\n(D) \n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1078, "doc": {"video_name": "ego_52.mp4", "duration": 480.0, "question": "Where was the writing book ?\n(A) The writing book was in the drawer.\n(B) The writing book was on the table.\n(C) The writing book was on the shelf.\n(D) The writing book was in the backpack.\n", "candidates": ["The writing book was in the drawer.", "The writing book was on the table.", "The writing book was on the shelf.", "The writing book was in the backpack."], "answer": "A", "task_type": "ego", "question_id": "Q1078"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6a04d1aa6e6c1f560133adbcacdc173e01391681058ed11f119ab846d5511768", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the writing book ?\n(A) The writing book was in the drawer.\n(B) The writing book was on the table.\n(C) The writing book was on the shelf.\n(D) The writing book was in the backpack.\n", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "Where was the writing book ?\n(A) The writing book was in the drawer.\n(B) The writing book was on the table.\n(C) The writing book was on the shelf.\n(D) The writing book was in the backpack.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1082, "doc": {"video_name": "ego_61.mp4", "duration": 480.0333333333333, "question": "Did my friend give me coffee?\n(A) no, he did not\n(B) yes he did.\n(C) I don't like coffee\n(D) I don't have a friend\n", "candidates": ["no, he did not", "yes he did.", "I don't like coffee", "I don't have a friend"], "answer": "B", "task_type": "ego", "question_id": "Q1082"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d550039b944096ac398ca5620bab818fa7c5e9bbcd174979a6ea80856f863e83", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did my friend give me coffee?\n(A) no, he did not\n(B) yes he did.\n(C) I don't like coffee\n(D) I don't have a friend\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Did my friend give me coffee?\n(A) no, he did not\n(B) yes he did.\n(C) I don't like coffee\n(D) I don't have a friend\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1086, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Where was the brown bag\n(A) in the closet\n(B) on the floor\n(C) on the table\n(D) under the bed\n", "candidates": ["in the closet", "on the floor", "on the table", "under the bed"], "answer": "B", "task_type": "ego", "question_id": "Q1086"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b60829373056d25639b5fa493aa1b14f77342358056ee316541a29d7e99bc805", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the brown bag\n(A) in the closet\n(B) on the floor\n(C) on the table\n(D) under the bed\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the brown bag\n(A) in the closet\n(B) on the floor\n(C) on the table\n(D) under the bed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1090, "doc": {"video_name": "ego_41.mp4", "duration": 480.0333333333333, "question": "Did I open the car door to enter the car?\n(A) I can't remember\n(B) Yes \n(C) No\n(D) I don't know\n", "candidates": ["I can't remember", "Yes ", "No", "I don't know"], "answer": "B", "task_type": "ego", "question_id": "Q1090"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1f60999e8df86c2c56652191ddee29c4e7697c8c7d55f3b2f5e3708ba91969f7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did I open the car door to enter the car?\n(A) I can't remember\n(B) Yes \n(C) No\n(D) I don't know\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Did I open the car door to enter the car?\n(A) I can't remember\n(B) Yes \n(C) No\n(D) I don't know\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1094, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Did I cut down the wood strip?\n(A) no\n(B) I don't know\n(C) yes\n(D) maybe not\n", "candidates": ["no", "I don't know", "yes", "maybe not"], "answer": "C", "task_type": "ego", "question_id": "Q1094"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7fcb4fd916f250d0b3b67255ed74c5b5f5417e180020a5f529506e33b5e85d00", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Did I cut down the wood strip?\n(A) no\n(B) I don't know\n(C) yes\n(D) maybe not\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Did I cut down the wood strip?\n(A) no\n(B) I don't know\n(C) yes\n(D) maybe not\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1098, "doc": {"video_name": "ego_51.mp4", "duration": 480.0333333333334, "question": "Where was the Cat before I served the food?\n(A) in the room\n(B) outside the house\n(C) in the kitchen\n(D) at the vet\n", "candidates": ["in the room", "outside the house", "in the kitchen", "at the vet"], "answer": "A", "task_type": "ego", "question_id": "Q1098"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eddd63ea986feef3dedd3bf0e24b9f35e2a073b5030854ee29a8518257574883", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the Cat before I served the food?\n(A) in the room\n(B) outside the house\n(C) in the kitchen\n(D) at the vet\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "Where was the Cat before I served the food?\n(A) in the room\n(B) outside the house\n(C) in the kitchen\n(D) at the vet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1102, "doc": {"video_name": "ego_37.mp4", "duration": 480.0, "question": "Where did I put the cornflakes packet ?\n(A) in the pantry\n(B) in the trash can\n(C) in the refrigerator\n(D) on the store shelf.\n", "candidates": ["in the pantry", "in the trash can", "in the refrigerator", "on the store shelf."], "answer": "D", "task_type": "ego", "question_id": "Q1102"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cff2fb03c6078d91af2df38967272715b7cf3629155b173cf50131c3fba00d86", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where did I put the cornflakes packet ?\n(A) in the pantry\n(B) in the trash can\n(C) in the refrigerator\n(D) on the store shelf.\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where did I put the cornflakes packet ?\n(A) in the pantry\n(B) in the trash can\n(C) in the refrigerator\n(D) on the store shelf.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1106, "doc": {"video_name": "ego_73.mp4", "duration": 480.0, "question": "What did I put in the dustbin?\n(A) Glass waste\n(B) Food waste\n(C) Plastic waste\n(D) Paper waste\n", "candidates": ["Glass waste", "Food waste", "Plastic waste", "Paper waste"], "answer": "C", "task_type": "ego", "question_id": "Q1106"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8fd7363bd07a24b66c8d152688d728c2b8f39d423d9d8c8b472c8b71fa4a4fdb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did I put in the dustbin?\n(A) Glass waste\n(B) Food waste\n(C) Plastic waste\n(D) Paper waste\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "What did I put in the dustbin?\n(A) Glass waste\n(B) Food waste\n(C) Plastic waste\n(D) Paper waste\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1110, "doc": {"video_name": "ego_50.mp4", "duration": 480.0333333333333, "question": "What did I put in the plastic bag? \n(A) Apples\n(B) Bread\n(C) Toothpaste\n(D) Milk\n", "candidates": ["Apples", "Bread", "Toothpaste", "Milk"], "answer": "B", "task_type": "ego", "question_id": "Q1110"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bd81c52604256f43b1708a05ef9df424c7161adaa831954245eaccac2afa47d2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I put in the plastic bag? \n(A) Apples\n(B) Bread\n(C) Toothpaste\n(D) Milk\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What did I put in the plastic bag? \n(A) Apples\n(B) Bread\n(C) Toothpaste\n(D) Milk\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1114, "doc": {"video_name": "xiaoliyu_6.mp4", "duration": 400.0, "question": "How did the cartoon jellyfish leave the mire?\n(A) Carried by the cartoon shrimp\n(B) Carried by the cartoon turtle\n(C) Carried by the cartoon catfish\n(D) Carried by the cartoon seahorse\n", "candidates": ["Carried by the cartoon shrimp", "Carried by the cartoon turtle", "Carried by the cartoon catfish", "Carried by the cartoon seahorse"], "answer": "B", "task_type": "plotQA", "question_id": "Q1114"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c25a36f5cb5ee7071414217e13bafc6f20747878cab47082a4dff30e6f38859", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How did the cartoon jellyfish leave the mire?\n(A) Carried by the cartoon shrimp\n(B) Carried by the cartoon turtle\n(C) Carried by the cartoon catfish\n(D) Carried by the cartoon seahorse\n", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "How did the cartoon jellyfish leave the mire?\n(A) Carried by the cartoon shrimp\n(B) Carried by the cartoon turtle\n(C) Carried by the cartoon catfish\n(D) Carried by the cartoon seahorse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1118, "doc": {"video_name": "movie101_37.mp4", "duration": 457.0, "question": "How many people are at the staircase at the beginning of the video?\n(A) One\n(B) Two\n(C) Three\n(D) Four\n", "candidates": ["One", "Two", "Three", "Four"], "answer": "B", "task_type": "plotQA", "question_id": "Q1118"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "57cab1c089fa3afcc4ed6351dc321ef2e865fecb97bbe45df7bfa22357793130", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many people are at the staircase at the beginning of the video?\n(A) One\n(B) Two\n(C) Three\n(D) Four\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "How many people are at the staircase at the beginning of the video?\n(A) One\n(B) Two\n(C) Three\n(D) Four\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1122, "doc": {"video_name": "xiaoliyu_9.mp4", "duration": 302.0, "question": "What expression did the school of fish in the cage show when they saw the cartoon snake?\n(A) Surprised\n(B) Happy\n(C) Sad\n(D) Angry\n", "candidates": ["Surprised", "Happy", "Sad", "Angry"], "answer": "A", "task_type": "plotQA", "question_id": "Q1122"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ae525b80b8da86a64180f14ac7ed0a3279b876141f772798b9428b9404d24d76", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What expression did the school of fish in the cage show when they saw the cartoon snake?\n(A) Surprised\n(B) Happy\n(C) Sad\n(D) Angry\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What expression did the school of fish in the cage show when they saw the cartoon snake?\n(A) Surprised\n(B) Happy\n(C) Sad\n(D) Angry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1126, "doc": {"video_name": "en_tv_19.mp4", "duration": 583.0, "question": "In the opening scene of the video where three people are chatting, what color is the shirt of the man wearing glasses?\n(A) Blue\n(B) White\n(C) Green\n(D) Black\n", "candidates": ["Blue", "White", "Green", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1126"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5b4fc238502c0b993d8aaf8eeb1d76ff2401d5643e41fb0360933d0ba228f2b6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the opening scene of the video where three people are chatting, what color is the shirt of the man wearing glasses?\n(A) Blue\n(B) White\n(C) Green\n(D) Black\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "In the opening scene of the video where three people are chatting, what color is the shirt of the man wearing glasses?\n(A) Blue\n(B) White\n(C) Green\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1130, "doc": {"video_name": "movie101_87.mp4", "duration": 599.0, "question": "What color is the woman's clothing at the beginning of the video?\n(A) Green\n(B) White\n(C) Pink\n(D) Black\n", "candidates": ["Green", "White", "Pink", "Black"], "answer": "C", "task_type": "plotQA", "question_id": "Q1130"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "40d7b55fa4503cc768d70334cd4cbe6533645bbcad76c22e8b84f9db62c4b5ee", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the woman's clothing at the beginning of the video?\n(A) Green\n(B) White\n(C) Pink\n(D) Black\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the woman's clothing at the beginning of the video?\n(A) Green\n(B) White\n(C) Pink\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1134, "doc": {"video_name": "movie101_28.mp4", "duration": 234.0, "question": "What is the weather in the scene in the forest in the movie?\n(A) Snowy\n(B) Sunny\n(C) Rainy\n(D) Foggy\n", "candidates": ["Snowy", "Sunny", "Rainy", "Foggy"], "answer": "D", "task_type": "plotQA", "question_id": "Q1134"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9d7a6f82d57262d289d7d8dd9fe06145dbac3d27f17a8fd0a10d1463d9839a71", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the weather in the scene in the forest in the movie?\n(A) Snowy\n(B) Sunny\n(C) Rainy\n(D) Foggy\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What is the weather in the scene in the forest in the movie?\n(A) Snowy\n(B) Sunny\n(C) Rainy\n(D) Foggy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1138, "doc": {"video_name": "movie101_5.mp4", "duration": 795.0, "question": "What is the weather during the fight at the beginning of the video?\n(A) Snowstorm\n(B) Cloudy\n(C) Sunny\n(D) Rainy\n", "candidates": ["Snowstorm", "Cloudy", "Sunny", "Rainy"], "answer": "D", "task_type": "plotQA", "question_id": "Q1138"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f5c06b4449b30900e83b203916f5cdb227b8be0d96c5b232e89d958d15e9b84d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the weather during the fight at the beginning of the video?\n(A) Snowstorm\n(B) Cloudy\n(C) Sunny\n(D) Rainy\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What is the weather during the fight at the beginning of the video?\n(A) Snowstorm\n(B) Cloudy\n(C) Sunny\n(D) Rainy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1142, "doc": {"video_name": "movie101_34.mp4", "duration": 748.0, "question": "What color is the hair of the boy buying things in the convenience store in the video?\n(A) Black\n(B) Red\n(C) White\n(D) Yellow\n", "candidates": ["Black", "Red", "White", "Yellow"], "answer": "D", "task_type": "plotQA", "question_id": "Q1142"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7b7d7f436e6016a3ee5a387e8defbbdfc299da0a0d59886f3622d2bb4a2cc97d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the hair of the boy buying things in the convenience store in the video?\n(A) Black\n(B) Red\n(C) White\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the hair of the boy buying things in the convenience store in the video?\n(A) Black\n(B) Red\n(C) White\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1146, "doc": {"video_name": "movie101_27.mp4", "duration": 374.0, "question": "What color are the flowers on the table in the meeting scene in the video?\n(A) Black\n(B) White\n(C) Yellow\n(D) Blue\n", "candidates": ["Black", "White", "Yellow", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1146"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7119262cf5214044b0e32b5599429c2a0e5700ca48026aacb7a023af83cf900a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color are the flowers on the table in the meeting scene in the video?\n(A) Black\n(B) White\n(C) Yellow\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color are the flowers on the table in the meeting scene in the video?\n(A) Black\n(B) White\n(C) Yellow\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1150, "doc": {"video_name": "movie101_78.mp4", "duration": 607.0, "question": "What color is the woman's dress in the tavern?\n(A) Pink\n(B) Red\n(C) White\n(D) Green\n", "candidates": ["Pink", "Red", "White", "Green"], "answer": "D", "task_type": "plotQA", "question_id": "Q1150"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b465115eb4ca19ddf7c911f13079273632e9b3ce4383bf2b763107d54a902b4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the woman's dress in the tavern?\n(A) Pink\n(B) Red\n(C) White\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the woman's dress in the tavern?\n(A) Pink\n(B) Red\n(C) White\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1154, "doc": {"video_name": "movie101_51.mp4", "duration": 796.0, "question": "At the beginning of the video, why is the man in a white t-shirt panicking?\n(A) An earthquake occurred\n(B) Their house is being lifted into the air\n(C) Sudden change in weather\n(D) The house collapsed\n", "candidates": ["An earthquake occurred", "Their house is being lifted into the air", "Sudden change in weather", "The house collapsed"], "answer": "B", "task_type": "plotQA", "question_id": "Q1154"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "61d975a999bfb56794fd9b6059c86c612ad0ad67e5fb44d7208045d8b72c4a02", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "At the beginning of the video, why is the man in a white t-shirt panicking?\n(A) An earthquake occurred\n(B) Their house is being lifted into the air\n(C) Sudden change in weather\n(D) The house collapsed\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "At the beginning of the video, why is the man in a white t-shirt panicking?\n(A) An earthquake occurred\n(B) Their house is being lifted into the air\n(C) Sudden change in weather\n(D) The house collapsed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1158, "doc": {"video_name": "en_tv_22.mp4", "duration": 441.0, "question": "What color is the clothing of the man breaking the wall with a hammer?\n(A) Green\n(B) White\n(C) Purple\n(D) Black\n", "candidates": ["Green", "White", "Purple", "Black"], "answer": "D", "task_type": "plotQA", "question_id": "Q1158"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07aab3527bdfd78468b5513a987328655428844460d03089acc214a3fc3e1dc5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the clothing of the man breaking the wall with a hammer?\n(A) Green\n(B) White\n(C) Purple\n(D) Black\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What color is the clothing of the man breaking the wall with a hammer?\n(A) Green\n(B) White\n(C) Purple\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1162, "doc": {"video_name": "movie101_74.mp4", "duration": 323.0, "question": "What color is the police officer's uniform in the film?\n(A) White\n(B) Green\n(C) Black\n(D) Yellow\n", "candidates": ["White", "Green", "Black", "Yellow"], "answer": "B", "task_type": "plotQA", "question_id": "Q1162"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3b0f29545b50ea61e23d1a60d0fbb341db6c40383c22983c02fadb68a3bc1640", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the police officer's uniform in the film?\n(A) White\n(B) Green\n(C) Black\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the police officer's uniform in the film?\n(A) White\n(B) Green\n(C) Black\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1166, "doc": {"video_name": "tomjerry_9.mp4", "duration": 328.0, "question": "How does the cartoon dog get its teeth?\n(A) Installing false teeth\n(B) Glued on\n(C) Stuck on\n(D) Drawn on\n", "candidates": ["Installing false teeth", "Glued on", "Stuck on", "Drawn on"], "answer": "A", "task_type": "plotQA", "question_id": "Q1166"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "93ec303ebad408ab2ac31648c0a497305a907c7505ecb446edf476d82948816a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How does the cartoon dog get its teeth?\n(A) Installing false teeth\n(B) Glued on\n(C) Stuck on\n(D) Drawn on\n", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "How does the cartoon dog get its teeth?\n(A) Installing false teeth\n(B) Glued on\n(C) Stuck on\n(D) Drawn on\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1170, "doc": {"video_name": "en_tv_18.mp4", "duration": 568.0, "question": "What color is the shirt of the man who is talking to the man exercising?\n(A) Orange\n(B) Red\n(C) Green\n(D) Blue\n", "candidates": ["Orange", "Red", "Green", "Blue"], "answer": "D", "task_type": "plotQA", "question_id": "Q1170"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "09d7840c95885f939a3170dc2c7cc53f7aaff594648021f361352921948b0fcb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the shirt of the man who is talking to the man exercising?\n(A) Orange\n(B) Red\n(C) Green\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the shirt of the man who is talking to the man exercising?\n(A) Orange\n(B) Red\n(C) Green\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1174, "doc": {"video_name": "haimian_5.mp4", "duration": 335.0, "question": "What is the cartoon sponge preparing to do when it sees the cartoon shark?\n(A) Drinking milk\n(B) Drinking water\n(C) Drinking coffee\n(D) Drinking juice\n", "candidates": ["Drinking milk", "Drinking water", "Drinking coffee", "Drinking juice"], "answer": "C", "task_type": "plotQA", "question_id": "Q1174"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7dfa149bcd1908a5888ef2563ec091fe78b47bea2a08d08b968afc48e9b9a5f5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the cartoon sponge preparing to do when it sees the cartoon shark?\n(A) Drinking milk\n(B) Drinking water\n(C) Drinking coffee\n(D) Drinking juice\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What is the cartoon sponge preparing to do when it sees the cartoon shark?\n(A) Drinking milk\n(B) Drinking water\n(C) Drinking coffee\n(D) Drinking juice\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1178, "doc": {"video_name": "haimian_6.mp4", "duration": 324.0, "question": "What are the cartoon starfish and cartoon sponge doing in the room?\n(A) Sleeping\n(B) Watching TV\n(C) Playing games\n(D) Eating\n", "candidates": ["Sleeping", "Watching TV", "Playing games", "Eating"], "answer": "B", "task_type": "plotQA", "question_id": "Q1178"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b91646cd8226936a8bc2cdfca5e53a059959c955ba796fda292f6281aef6edd1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What are the cartoon starfish and cartoon sponge doing in the room?\n(A) Sleeping\n(B) Watching TV\n(C) Playing games\n(D) Eating\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What are the cartoon starfish and cartoon sponge doing in the room?\n(A) Sleeping\n(B) Watching TV\n(C) Playing games\n(D) Eating\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1182, "doc": {"video_name": "movie101_77.mp4", "duration": 206.0, "question": "What are the two people cutting with scissors?\n(A) Socks\n(B) Pants\n(C) The character for 'happiness'\n(D) Ribbon\n", "candidates": ["Socks", "Pants", "The character for 'happiness'", "Ribbon"], "answer": "C", "task_type": "plotQA", "question_id": "Q1182"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "27232b264c7fb6796ea2ed3f366834ba385ab2f0260e395ac8e2547a8b6a350d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the two people cutting with scissors?\n(A) Socks\n(B) Pants\n(C) The character for 'happiness'\n(D) Ribbon\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What are the two people cutting with scissors?\n(A) Socks\n(B) Pants\n(C) The character for 'happiness'\n(D) Ribbon\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1186, "doc": {"video_name": "haimian_6.mp4", "duration": 324.0, "question": "What color does the cartoon starfish turn the boat into?\n(A) Black\n(B) White\n(C) Blue\n(D) Green\n", "candidates": ["Black", "White", "Blue", "Green"], "answer": "A", "task_type": "plotQA", "question_id": "Q1186"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "baab19a9703a9d02a705a5f9651b589095480ca3ea772e0750fcc01137bc9771", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color does the cartoon starfish turn the boat into?\n(A) Black\n(B) White\n(C) Blue\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What color does the cartoon starfish turn the boat into?\n(A) Black\n(B) White\n(C) Blue\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1190, "doc": {"video_name": "en_tv_35.mp4", "duration": 505.0, "question": "What color is the woman's clothing at the beginning of the video?\n(A) Green\n(B) Black\n(C) White\n(D) Red\n", "candidates": ["Green", "Black", "White", "Red"], "answer": "C", "task_type": "plotQA", "question_id": "Q1190"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ad6c54a5407c0a49ceb53d94c415bec42552c18f5d15a249a442f9c99e27c68", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the woman's clothing at the beginning of the video?\n(A) Green\n(B) Black\n(C) White\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "What color is the woman's clothing at the beginning of the video?\n(A) Green\n(B) Black\n(C) White\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1194, "doc": {"video_name": "movie101_88.mp4", "duration": 348.0, "question": "What is the purpose of the two signing a contract?\n(A) Service provision\n(B) Copyright transfer\n(C) Blood transfusion\n(D) Product sales\n", "candidates": ["Service provision", "Copyright transfer", "Blood transfusion", "Product sales"], "answer": "C", "task_type": "plotQA", "question_id": "Q1194"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4c9b53f94190aaa618c39284efabaa8604a66c0e18bfd6dce57ac50511db725c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the purpose of the two signing a contract?\n(A) Service provision\n(B) Copyright transfer\n(C) Blood transfusion\n(D) Product sales\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "What is the purpose of the two signing a contract?\n(A) Service provision\n(B) Copyright transfer\n(C) Blood transfusion\n(D) Product sales\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1198, "doc": {"video_name": "movie101_37.mp4", "duration": 457.0, "question": "What color is the clothes of the person wearing a floral hat at the staircase in the video?\n(A) Purple\n(B) Blue\n(C) Red\n(D) Green\n", "candidates": ["Purple", "Blue", "Red", "Green"], "answer": "D", "task_type": "plotQA", "question_id": "Q1198"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "de3ba74ee4ae8ddf7edd6ca5fd5fa9d84e75c1c96cf658619763731cce7a3177", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the clothes of the person wearing a floral hat at the staircase in the video?\n(A) Purple\n(B) Blue\n(C) Red\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What color is the clothes of the person wearing a floral hat at the staircase in the video?\n(A) Purple\n(B) Blue\n(C) Red\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1202, "doc": {"video_name": "9.mp4", "duration": 471.67, "question": "How do the person in light-colored clothes and the person in dark-colored clothes climb onto the blue crane?\n(A) Through the crane's hook\n(B) By stepping on other people's shoulders\n(C) Jumping down from a helicopter\n(D) Jumping from the top of a truck\n", "candidates": ["Through the crane's hook", "By stepping on other people's shoulders", "Jumping down from a helicopter", "Jumping from the top of a truck"], "answer": "A", "task_type": "plotQA", "question_id": "Q1202"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fccc1d849eff32bcc5891bceb685131ee12c81aa7e0c0420d90160f05ae43765", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How do the person in light-colored clothes and the person in dark-colored clothes climb onto the blue crane?\n(A) Through the crane's hook\n(B) By stepping on other people's shoulders\n(C) Jumping down from a helicopter\n(D) Jumping from the top of a truck\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "How do the person in light-colored clothes and the person in dark-colored clothes climb onto the blue crane?\n(A) Through the crane's hook\n(B) By stepping on other people's shoulders\n(C) Jumping down from a helicopter\n(D) Jumping from the top of a truck\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1206, "doc": {"video_name": "movie101_77.mp4", "duration": 206.0, "question": "What color is the wedding dress the woman is wearing at the beginning of the video?\n(A) Black\n(B) Green\n(C) Pink\n(D) White\n", "candidates": ["Black", "Green", "Pink", "White"], "answer": "A", "task_type": "plotQA", "question_id": "Q1206"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "755e0e236fe6c362613b23383e9b98485a65d8f04f1b5929c5c33ca17e4cc5e0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the wedding dress the woman is wearing at the beginning of the video?\n(A) Black\n(B) Green\n(C) Pink\n(D) White\n", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "What color is the wedding dress the woman is wearing at the beginning of the video?\n(A) Black\n(B) Green\n(C) Pink\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1210, "doc": {"video_name": "movie101_104.mp4", "duration": 514.0, "question": "What is the girl's reaction after the phone call at the beginning of the video?\n(A) Angry\n(B) She walks on the road with a blank stare\n(C) Joyful\n(D) Happy\n", "candidates": ["Angry", "She walks on the road with a blank stare", "Joyful", "Happy"], "answer": "B", "task_type": "plotQA", "question_id": "Q1210"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d80865b7eef4a14c668255f8213fa1df6d5bbe98899e6358cddf35bda57c2cc", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the girl's reaction after the phone call at the beginning of the video?\n(A) Angry\n(B) She walks on the road with a blank stare\n(C) Joyful\n(D) Happy\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the girl's reaction after the phone call at the beginning of the video?\n(A) Angry\n(B) She walks on the road with a blank stare\n(C) Joyful\n(D) Happy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1214, "doc": {"video_name": "movie101_38.mp4", "duration": 539.0, "question": "What color is the hat that the policewoman wears at the beginning of the video?\n(A) Green\n(B) Blue\n(C) White\n(D) Black\n", "candidates": ["Green", "Blue", "White", "Black"], "answer": "C", "task_type": "plotQA", "question_id": "Q1214"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0818df741e6d367d0f64d819c6b5948c37d0d501d11c5df5f0b2238d91756aa0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the hat that the policewoman wears at the beginning of the video?\n(A) Green\n(B) Blue\n(C) White\n(D) Black\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "What color is the hat that the policewoman wears at the beginning of the video?\n(A) Green\n(B) Blue\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1218, "doc": {"video_name": "movie101_89.mp4", "duration": 677.0, "question": "What is the emotion of the old woman at the start of the video?\n(A) Neutral\n(B) Happy\n(C) Angry\n(D) Crying\n", "candidates": ["Neutral", "Happy", "Angry", "Crying"], "answer": "D", "task_type": "plotQA", "question_id": "Q1218"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e4c777c950ee43dafb0a9fe65d18bf066e4ca6212811787e0fb92c65d246ba9c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the emotion of the old woman at the start of the video?\n(A) Neutral\n(B) Happy\n(C) Angry\n(D) Crying\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What is the emotion of the old woman at the start of the video?\n(A) Neutral\n(B) Happy\n(C) Angry\n(D) Crying\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1222, "doc": {"video_name": "9.mp4", "duration": 471.67, "question": "At the beginning of the video, how do the person in light-colored clothes and the person in dark-colored clothes enter the scene?\n(A) Get off the car into the scene\n(B) They climb over an iron fence to enter the scene\n(C) Run from the field into the scene\n(D) Jump out of the water into the scene\n", "candidates": ["Get off the car into the scene", "They climb over an iron fence to enter the scene", "Run from the field into the scene", "Jump out of the water into the scene"], "answer": "B", "task_type": "plotQA", "question_id": "Q1222"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0549e3de698ab9fee7aae22a2102f7ee229c79186f5479164f583e50ed04caa5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "At the beginning of the video, how do the person in light-colored clothes and the person in dark-colored clothes enter the scene?\n(A) Get off the car into the scene\n(B) They climb over an iron fence to enter the scene\n(C) Run from the field into the scene\n(D) Jump out of the water into the scene\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "At the beginning of the video, how do the person in light-colored clothes and the person in dark-colored clothes enter the scene?\n(A) Get off the car into the scene\n(B) They climb over an iron fence to enter the scene\n(C) Run from the field into the scene\n(D) Jump out of the water into the scene\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1226, "doc": {"video_name": "1.mp4", "duration": 401.23, "question": "How does the man in the black sweater finally disappear from the screen?\n(A) He slowly moves to the edge of the screen, eventually disappearing from the screen.\n(B) He walks into the cabin, disappearing from the screen.\n(C) He walks into the door in the wall and leaves\n(D) He is blocked by other objects, gradually disappearing from the screen.\n", "candidates": ["He slowly moves to the edge of the screen, eventually disappearing from the screen.", "He walks into the cabin, disappearing from the screen.", "He walks into the door in the wall and leaves", "He is blocked by other objects, gradually disappearing from the screen."], "answer": "C", "task_type": "plotQA", "question_id": "Q1226"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aa7f42262ac8ed201063839a3956b6eac78cb35c8373ccc53ca91d10fd124d6e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How does the man in the black sweater finally disappear from the screen?\n(A) He slowly moves to the edge of the screen, eventually disappearing from the screen.\n(B) He walks into the cabin, disappearing from the screen.\n(C) He walks into the door in the wall and leaves\n(D) He is blocked by other objects, gradually disappearing from the screen.\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "How does the man in the black sweater finally disappear from the screen?\n(A) He slowly moves to the edge of the screen, eventually disappearing from the screen.\n(B) He walks into the cabin, disappearing from the screen.\n(C) He walks into the door in the wall and leaves\n(D) He is blocked by other objects, gradually disappearing from the screen.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1230, "doc": {"video_name": "tomjerry_9.mp4", "duration": 328.0, "question": "Why does the cartoon cat kiss the cartoon dog?\n(A) Because the cartoon cat likes the cartoon dog\n(B) Because the cartoon cat is happy\n(C) Because the cartoon cat is excited\n(D) Because the cartoon cat mistook the cartoon dog for the cartoon female cat\n", "candidates": ["Because the cartoon cat likes the cartoon dog", "Because the cartoon cat is happy", "Because the cartoon cat is excited", "Because the cartoon cat mistook the cartoon dog for the cartoon female cat"], "answer": "D", "task_type": "plotQA", "question_id": "Q1230"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c6117ae8dedba8d799648db913e0310c120fd3e04f8041f03a7f4a3b56e0929", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why does the cartoon cat kiss the cartoon dog?\n(A) Because the cartoon cat likes the cartoon dog\n(B) Because the cartoon cat is happy\n(C) Because the cartoon cat is excited\n(D) Because the cartoon cat mistook the cartoon dog for the cartoon female cat\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Why does the cartoon cat kiss the cartoon dog?\n(A) Because the cartoon cat likes the cartoon dog\n(B) Because the cartoon cat is happy\n(C) Because the cartoon cat is excited\n(D) Because the cartoon cat mistook the cartoon dog for the cartoon female cat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1234, "doc": {"video_name": "en_tv_22.mp4", "duration": 441.0, "question": "What color is the woman's hair in the room where the man is kidnapped?\n(A) Green\n(B) Black\n(C) Yellow\n(D) White\n", "candidates": ["Green", "Black", "Yellow", "White"], "answer": "C", "task_type": "plotQA", "question_id": "Q1234"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c16c03f045e322cef3e4550737891b626145eaeda4519e4e2c36e9fbd1e40778", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the woman's hair in the room where the man is kidnapped?\n(A) Green\n(B) Black\n(C) Yellow\n(D) White\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the woman's hair in the room where the man is kidnapped?\n(A) Green\n(B) Black\n(C) Yellow\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1238, "doc": {"video_name": "movie101_111.mp4", "duration": 720.0, "question": "Why did the man in the black coat enter the house of the woman in the white coat?\n(A) To commit a robbery\n(B) To investigate a suspect\n(C) To rescue the woman\n(D) To help the woman\n", "candidates": ["To commit a robbery", "To investigate a suspect", "To rescue the woman", "To help the woman"], "answer": "B", "task_type": "plotQA", "question_id": "Q1238"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59116c58122063c6708109e1424faa0c420dcb53b4dddb7bd5f487fd540ffa5f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why did the man in the black coat enter the house of the woman in the white coat?\n(A) To commit a robbery\n(B) To investigate a suspect\n(C) To rescue the woman\n(D) To help the woman\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Why did the man in the black coat enter the house of the woman in the white coat?\n(A) To commit a robbery\n(B) To investigate a suspect\n(C) To rescue the woman\n(D) To help the woman\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1242, "doc": {"video_name": "tomjerry_3.mp4", "duration": 303.0, "question": "How did the cartoon cat drink milk?\n(A) Drinking while standing\n(B) Drinking while sitting\n(C) Drinking from a cup\n(D) Drinking with a straw\n", "candidates": ["Drinking while standing", "Drinking while sitting", "Drinking from a cup", "Drinking with a straw"], "answer": "D", "task_type": "plotQA", "question_id": "Q1242"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a902f618306ea2f3993952a1988bad2c4dbd79f43d390d13949cf6a352dc124e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How did the cartoon cat drink milk?\n(A) Drinking while standing\n(B) Drinking while sitting\n(C) Drinking from a cup\n(D) Drinking with a straw\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "How did the cartoon cat drink milk?\n(A) Drinking while standing\n(B) Drinking while sitting\n(C) Drinking from a cup\n(D) Drinking with a straw\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1246, "doc": {"video_name": "movie101_64.mp4", "duration": 626.0, "question": "What mode of transportation do the boy and girl use to go to school in the movie?\n(A) Electric scooter\n(B) Bus\n(C) Subway\n(D) Bicycle\n", "candidates": ["Electric scooter", "Bus", "Subway", "Bicycle"], "answer": "D", "task_type": "plotQA", "question_id": "Q1246"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "960822bc56ea5a52ad0f657b2c66953f7f636de9d40d01439132aefdaf7f69bb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What mode of transportation do the boy and girl use to go to school in the movie?\n(A) Electric scooter\n(B) Bus\n(C) Subway\n(D) Bicycle\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "What mode of transportation do the boy and girl use to go to school in the movie?\n(A) Electric scooter\n(B) Bus\n(C) Subway\n(D) Bicycle\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1250, "doc": {"video_name": "movie101_9.mp4", "duration": 713.0, "question": "What color is the wild boar in the video?\n(A) White\n(B) Black\n(C) Yellow\n(D) Red\n", "candidates": ["White", "Black", "Yellow", "Red"], "answer": "B", "task_type": "plotQA", "question_id": "Q1250"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "096093cabed66b27ac1b2058b05d7afc024a61561bd87c28f18444eb5f6d815e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the wild boar in the video?\n(A) White\n(B) Black\n(C) Yellow\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the wild boar in the video?\n(A) White\n(B) Black\n(C) Yellow\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1254, "doc": {"video_name": "movie101_2.mp4", "duration": 259.0, "question": "Why does the man in black clothes stand on the edge of the cliff?\n(A) He is meditating or contemplating on the edge of the cliff\n(B) To avoid being hit by a truck\n(C) He is admiring the view from the cliff\n(D) He is taking photos on the edge of the cliff\n", "candidates": ["He is meditating or contemplating on the edge of the cliff", "To avoid being hit by a truck", "He is admiring the view from the cliff", "He is taking photos on the edge of the cliff"], "answer": "B", "task_type": "plotQA", "question_id": "Q1254"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "891863902ad4bbe72e4fab279f523491ee511bd0645c8c5565441911b9bbfa97", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why does the man in black clothes stand on the edge of the cliff?\n(A) He is meditating or contemplating on the edge of the cliff\n(B) To avoid being hit by a truck\n(C) He is admiring the view from the cliff\n(D) He is taking photos on the edge of the cliff\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Why does the man in black clothes stand on the edge of the cliff?\n(A) He is meditating or contemplating on the edge of the cliff\n(B) To avoid being hit by a truck\n(C) He is admiring the view from the cliff\n(D) He is taking photos on the edge of the cliff\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1258, "doc": {"video_name": "movie101_29.mp4", "duration": 482.0, "question": "What musical instrument appears in the movie?\n(A) Guqin\n(B) Harp\n(C) Pipa\n(D) Erhu\n", "candidates": ["Guqin", "Harp", "Pipa", "Erhu"], "answer": "A", "task_type": "plotQA", "question_id": "Q1258"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c7e5ab50625919e8fd730f24e5e78326145955f9c4631f22993ac649dab0434d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What musical instrument appears in the movie?\n(A) Guqin\n(B) Harp\n(C) Pipa\n(D) Erhu\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What musical instrument appears in the movie?\n(A) Guqin\n(B) Harp\n(C) Pipa\n(D) Erhu\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1262, "doc": {"video_name": "movie101_27.mp4", "duration": 374.0, "question": "What color is the jacket worn by the fencing coach in the video?\n(A) White\n(B) Green\n(C) Purple\n(D) Blue\n", "candidates": ["White", "Green", "Purple", "Blue"], "answer": "D", "task_type": "plotQA", "question_id": "Q1262"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "94825c6ab064232a3a71160294451729291600a504ad574127123c86ed180e21", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the jacket worn by the fencing coach in the video?\n(A) White\n(B) Green\n(C) Purple\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What color is the jacket worn by the fencing coach in the video?\n(A) White\n(B) Green\n(C) Purple\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1266, "doc": {"video_name": "movie101_19.mp4", "duration": 477.0, "question": "How many people are interacting at the staircase?\n(A) Four\n(B) Two\n(C) Five\n(D) Three\n", "candidates": ["Four", "Two", "Five", "Three"], "answer": "D", "task_type": "plotQA", "question_id": "Q1266"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b586cdfbf868818bea73749cf2902ee41666a38db948c7527eb17e62ba207716", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many people are interacting at the staircase?\n(A) Four\n(B) Two\n(C) Five\n(D) Three\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "How many people are interacting at the staircase?\n(A) Four\n(B) Two\n(C) Five\n(D) Three\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1270, "doc": {"video_name": "movie101_8.mp4", "duration": 750.0, "question": "When does the scene at the end of the video take place?\n(A) Morning\n(B) Dusk\n(C) Noon\n(D) Night\n", "candidates": ["Morning", "Dusk", "Noon", "Night"], "answer": "D", "task_type": "plotQA", "question_id": "Q1270"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6e956844b7e4d84da62dcf438db6bce6f883bd0555b881f47c7a6511b4002654", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "When does the scene at the end of the video take place?\n(A) Morning\n(B) Dusk\n(C) Noon\n(D) Night\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "When does the scene at the end of the video take place?\n(A) Morning\n(B) Dusk\n(C) Noon\n(D) Night\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1274, "doc": {"video_name": "movie101_53.mp4", "duration": 328.0, "question": "At the end of the video, what is the activity of the man in the blue jacket after entering the room?\n(A) Sitting alone at the table drinking\n(B) Sitting alone at the table eating\n(C) Sitting alone on the bed making a phone call\n(D) Sitting alone at the table playing on the computer\n", "candidates": ["Sitting alone at the table drinking", "Sitting alone at the table eating", "Sitting alone on the bed making a phone call", "Sitting alone at the table playing on the computer"], "answer": "C", "task_type": "plotQA", "question_id": "Q1274"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "be0b517f53b7e53a643cde61122d5aeb084f51775fae2544756178dc9bb8f958", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "At the end of the video, what is the activity of the man in the blue jacket after entering the room?\n(A) Sitting alone at the table drinking\n(B) Sitting alone at the table eating\n(C) Sitting alone on the bed making a phone call\n(D) Sitting alone at the table playing on the computer\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "At the end of the video, what is the activity of the man in the blue jacket after entering the room?\n(A) Sitting alone at the table drinking\n(B) Sitting alone at the table eating\n(C) Sitting alone on the bed making a phone call\n(D) Sitting alone at the table playing on the computer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1278, "doc": {"video_name": "movie101_13.mp4", "duration": 456.0, "question": "What color is the box of money at the beginning of the video?\n(A) Blue\n(B) White\n(C) Red\n(D) Purple\n", "candidates": ["Blue", "White", "Red", "Purple"], "answer": "C", "task_type": "plotQA", "question_id": "Q1278"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59f08f5b883b2b9d0981a9c51c215fdb93cc8995c12bb4fa8b8edcda412cdb64", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the box of money at the beginning of the video?\n(A) Blue\n(B) White\n(C) Red\n(D) Purple\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the box of money at the beginning of the video?\n(A) Blue\n(B) White\n(C) Red\n(D) Purple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1282, "doc": {"video_name": "en_tv_19.mp4", "duration": 583.0, "question": "In the scene where two people get out of the car, what color is the car?\n(A) Green\n(B) Black\n(C) Red\n(D) White\n", "candidates": ["Green", "Black", "Red", "White"], "answer": "B", "task_type": "plotQA", "question_id": "Q1282"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fca924d930164a7d313f10ff93eb31438a418636ca8bd3806efad3f78fdc176b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the scene where two people get out of the car, what color is the car?\n(A) Green\n(B) Black\n(C) Red\n(D) White\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "In the scene where two people get out of the car, what color is the car?\n(A) Green\n(B) Black\n(C) Red\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1286, "doc": {"video_name": "tomjerry_8.mp4", "duration": 308.0, "question": "Why did the cartoon mouse stop running?\n(A) A white cartoon female mouse\n(B) Cartoon dog\n(C) Cartoon cat\n(D) Cartoon mouse police\n", "candidates": ["A white cartoon female mouse", "Cartoon dog", "Cartoon cat", "Cartoon mouse police"], "answer": "A", "task_type": "plotQA", "question_id": "Q1286"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6c47a95e1c8fbcd37bc92e55f86f88413e45e8e6b5daf941b3d48b424a84b4f4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why did the cartoon mouse stop running?\n(A) A white cartoon female mouse\n(B) Cartoon dog\n(C) Cartoon cat\n(D) Cartoon mouse police\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "Why did the cartoon mouse stop running?\n(A) A white cartoon female mouse\n(B) Cartoon dog\n(C) Cartoon cat\n(D) Cartoon mouse police\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1290, "doc": {"video_name": "movie101_28.mp4", "duration": 234.0, "question": "What color is the man's clothes in the movie?\n(A) Blue\n(B) Brown\n(C) Teal\n(D) Purple\n", "candidates": ["Blue", "Brown", "Teal", "Purple"], "answer": "C", "task_type": "plotQA", "question_id": "Q1290"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3a9c81e94a6ed3650b410304f4c3f5f72944e534ef0df0b66974d3bcfadad336", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the man's clothes in the movie?\n(A) Blue\n(B) Brown\n(C) Teal\n(D) Purple\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the man's clothes in the movie?\n(A) Blue\n(B) Brown\n(C) Teal\n(D) Purple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1294, "doc": {"video_name": "movie101_85.mp4", "duration": 560.0, "question": "How many horses are there by the river at the beginning of the video?\n(A) 7\n(B) 6\n(C) 9\n(D) 8\n", "candidates": ["7", "6", "9", "8"], "answer": "D", "task_type": "plotQA", "question_id": "Q1294"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3479ec17a72e1986d6949ec9544d351faaab52ee0577841dc15d1975512dad79", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many horses are there by the river at the beginning of the video?\n(A) 7\n(B) 6\n(C) 9\n(D) 8\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "How many horses are there by the river at the beginning of the video?\n(A) 7\n(B) 6\n(C) 9\n(D) 8\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1298, "doc": {"video_name": "en_tv_20.mp4", "duration": 426.0, "question": "In the movie, two people have a conflict in a room, what color are the walls in the room?\n(A) Green\n(B) White\n(C) Red\n(D) Yellow\n", "candidates": ["Green", "White", "Red", "Yellow"], "answer": "C", "task_type": "plotQA", "question_id": "Q1298"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fe3e4117537ddb716d9b275f39d7993105d4134bab4494e6d645fd3047c1c7fd", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the movie, two people have a conflict in a room, what color are the walls in the room?\n(A) Green\n(B) White\n(C) Red\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "In the movie, two people have a conflict in a room, what color are the walls in the room?\n(A) Green\n(B) White\n(C) Red\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1302, "doc": {"video_name": "en_tv_4.mp4", "duration": 421.0, "question": "In the laboratory, what were the two people talking about and were attracted to at the same time?\n(A) Attracted by another person in the lab\n(B) Attracted by the preliminary results of a microbial experiment\n(C) Attracted by some kind of chemical in the lab\n(D) The information displayed on the screen\n", "candidates": ["Attracted by another person in the lab", "Attracted by the preliminary results of a microbial experiment", "Attracted by some kind of chemical in the lab", "The information displayed on the screen"], "answer": "D", "task_type": "plotQA", "question_id": "Q1302"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d6c9066c84abce1c426f25ab19829fca76c63e9f51c28bb4354c114dd32d4885", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the laboratory, what were the two people talking about and were attracted to at the same time?\n(A) Attracted by another person in the lab\n(B) Attracted by the preliminary results of a microbial experiment\n(C) Attracted by some kind of chemical in the lab\n(D) The information displayed on the screen\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "In the laboratory, what were the two people talking about and were attracted to at the same time?\n(A) Attracted by another person in the lab\n(B) Attracted by the preliminary results of a microbial experiment\n(C) Attracted by some kind of chemical in the lab\n(D) The information displayed on the screen\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1306, "doc": {"video_name": "movie101_51.mp4", "duration": 796.0, "question": "At the end of the video, why did everyone on the street run into a house?\n(A) They were being chased\n(B) To watch a person in white clothes cooking\n(C) They were hiding from the rain\n(D) They were looking for something\n", "candidates": ["They were being chased", "To watch a person in white clothes cooking", "They were hiding from the rain", "They were looking for something"], "answer": "B", "task_type": "plotQA", "question_id": "Q1306"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fc197d95567a24317d348250628016fa298977da31b7fe1d49ae0db89e1f49d6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "At the end of the video, why did everyone on the street run into a house?\n(A) They were being chased\n(B) To watch a person in white clothes cooking\n(C) They were hiding from the rain\n(D) They were looking for something\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "At the end of the video, why did everyone on the street run into a house?\n(A) They were being chased\n(B) To watch a person in white clothes cooking\n(C) They were hiding from the rain\n(D) They were looking for something\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1310, "doc": {"video_name": "en_tv_19.mp4", "duration": 583.0, "question": "In the scene where four people are dining and chatting, what color is the dress of the woman wearing a necklace?\n(A) Green\n(B) Purple\n(C) White\n(D) Black\n", "candidates": ["Green", "Purple", "White", "Black"], "answer": "D", "task_type": "plotQA", "question_id": "Q1310"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "785994bc4c02a94b31d65de505266f77447370ff7ff2a68c928bb46b0d585e07", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the scene where four people are dining and chatting, what color is the dress of the woman wearing a necklace?\n(A) Green\n(B) Purple\n(C) White\n(D) Black\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "In the scene where four people are dining and chatting, what color is the dress of the woman wearing a necklace?\n(A) Green\n(B) Purple\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1314, "doc": {"video_name": "movie101_87.mp4", "duration": 599.0, "question": "What mode of transportation does the man use to carry the woman?\n(A) Electric scooter\n(B) Rickshaw\n(C) Car\n(D) Bicycle\n", "candidates": ["Electric scooter", "Rickshaw", "Car", "Bicycle"], "answer": "D", "task_type": "plotQA", "question_id": "Q1314"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9c5f67821f780d71d77c7b9319726b572e7ba29a87f4b38f2d1802413b96bef2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What mode of transportation does the man use to carry the woman?\n(A) Electric scooter\n(B) Rickshaw\n(C) Car\n(D) Bicycle\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What mode of transportation does the man use to carry the woman?\n(A) Electric scooter\n(B) Rickshaw\n(C) Car\n(D) Bicycle\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1318, "doc": {"video_name": "haimian_10.mp4", "duration": 361.0, "question": "What did the cartoon starfish make for the cartoon sponge using wood?\n(A) A pair of pants\n(B) A toy\n(C) A pair of shoes\n(D) A piece of clothing\n", "candidates": ["A pair of pants", "A toy", "A pair of shoes", "A piece of clothing"], "answer": "C", "task_type": "plotQA", "question_id": "Q1318"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b0be2e86c20643247c5ea3af92c1a8459706b6bdc2b9556a957b8a94af89ecf5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did the cartoon starfish make for the cartoon sponge using wood?\n(A) A pair of pants\n(B) A toy\n(C) A pair of shoes\n(D) A piece of clothing\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What did the cartoon starfish make for the cartoon sponge using wood?\n(A) A pair of pants\n(B) A toy\n(C) A pair of shoes\n(D) A piece of clothing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1322, "doc": {"video_name": "haimian_1.mp4", "duration": 305.0, "question": "What did the sea animals see that shocked them?\n(A) A fight scene\n(B) Delicious food\n(C) A crowded crowd\n(D) Cartoon Sponge's muscular arms\n", "candidates": ["A fight scene", "Delicious food", "A crowded crowd", "Cartoon Sponge's muscular arms"], "answer": "D", "task_type": "plotQA", "question_id": "Q1322"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7680a6eb4e8c28b798a51811c8871da942182b9604ce12caf3b5ce0182730ade", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did the sea animals see that shocked them?\n(A) A fight scene\n(B) Delicious food\n(C) A crowded crowd\n(D) Cartoon Sponge's muscular arms\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What did the sea animals see that shocked them?\n(A) A fight scene\n(B) Delicious food\n(C) A crowded crowd\n(D) Cartoon Sponge's muscular arms\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1326, "doc": {"video_name": "movie101_27.mp4", "duration": 374.0, "question": "What color is the computer used by the man during the meeting?\n(A) Green\n(B) Blue\n(C) Silver\n(D) Purple\n", "candidates": ["Green", "Blue", "Silver", "Purple"], "answer": "C", "task_type": "plotQA", "question_id": "Q1326"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9cafd9fd8ef1b505df7ef9d4993146627e9f255400f811af3fba56f4cb48370e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the computer used by the man during the meeting?\n(A) Green\n(B) Blue\n(C) Silver\n(D) Purple\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the computer used by the man during the meeting?\n(A) Green\n(B) Blue\n(C) Silver\n(D) Purple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1330, "doc": {"video_name": "en_tv_31.mp4", "duration": 577.0, "question": "What color is the dress of the woman who enters while the two men are cooking at the beginning?\n(A) Blue\n(B) White\n(C) Green\n(D) Black\n", "candidates": ["Blue", "White", "Green", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1330"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f73441405845ac23972353272ecb2640da7128b439c880ac167fea0239be4e97", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the dress of the woman who enters while the two men are cooking at the beginning?\n(A) Blue\n(B) White\n(C) Green\n(D) Black\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the dress of the woman who enters while the two men are cooking at the beginning?\n(A) Blue\n(B) White\n(C) Green\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1334, "doc": {"video_name": "movie101_17.mp4", "duration": 613.0, "question": "What is the musical instrument that the man takes out from the secret room in the video?\n(A) Erhu\n(B) Flute\n(C) Pipa\n(D) Piano\n", "candidates": ["Erhu", "Flute", "Pipa", "Piano"], "answer": "C", "task_type": "plotQA", "question_id": "Q1334"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9afde275a5a86d1b43bc2cdcad273c59cc8d51ef1f8c96dd92718bb83e9ab978", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the musical instrument that the man takes out from the secret room in the video?\n(A) Erhu\n(B) Flute\n(C) Pipa\n(D) Piano\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What is the musical instrument that the man takes out from the secret room in the video?\n(A) Erhu\n(B) Flute\n(C) Pipa\n(D) Piano\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1338, "doc": {"video_name": "movie101_14.mp4", "duration": 465.0, "question": "What color are the pants worn by the woman standing at the door in the video?\n(A) Red\n(B) White\n(C) Blue\n(D) Green\n", "candidates": ["Red", "White", "Blue", "Green"], "answer": "B", "task_type": "plotQA", "question_id": "Q1338"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b9459372ce8f167c4977bd5bc953721d5b0a64c694e4fe5c8126ee0144624c4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color are the pants worn by the woman standing at the door in the video?\n(A) Red\n(B) White\n(C) Blue\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color are the pants worn by the woman standing at the door in the video?\n(A) Red\n(B) White\n(C) Blue\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1342, "doc": {"video_name": "en_tv_33.mp4", "duration": 532.0, "question": "What animal is the woman holding when she enters the room at the beginning of the video?\n(A) Rabbit\n(B) Dog\n(C) Pig\n(D) Cat\n", "candidates": ["Rabbit", "Dog", "Pig", "Cat"], "answer": "B", "task_type": "plotQA", "question_id": "Q1342"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5e6af8f01ebe592c05ec144fb94d399c836651dc9751775c361ee7cb8068c478", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What animal is the woman holding when she enters the room at the beginning of the video?\n(A) Rabbit\n(B) Dog\n(C) Pig\n(D) Cat\n", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "What animal is the woman holding when she enters the room at the beginning of the video?\n(A) Rabbit\n(B) Dog\n(C) Pig\n(D) Cat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1346, "doc": {"video_name": "en_tv_26.mp4", "duration": 522.0, "question": "What color is the woman's clothing in the scene where a man gives a woman a gift in a room?\n(A) Blue\n(B) White\n(C) Pink\n(D) Red\n", "candidates": ["Blue", "White", "Pink", "Red"], "answer": "A", "task_type": "plotQA", "question_id": "Q1346"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "009becf9c5fbbc3c48134d68f62fa9322faf31f0395b9649d92056de0cfd4cf5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the woman's clothing in the scene where a man gives a woman a gift in a room?\n(A) Blue\n(B) White\n(C) Pink\n(D) Red\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What color is the woman's clothing in the scene where a man gives a woman a gift in a room?\n(A) Blue\n(B) White\n(C) Pink\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1350, "doc": {"video_name": "movie101_13.mp4", "duration": 456.0, "question": "What color is the top that the little boy who picks up the box in the video wearing?\n(A) Green\n(B) Purple\n(C) Blue\n(D) White\n", "candidates": ["Green", "Purple", "Blue", "White"], "answer": "D", "task_type": "plotQA", "question_id": "Q1350"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6609c831af6d565865651dfb2db01aeb9c80654cf8510854c8d3acbac9a089eb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the top that the little boy who picks up the box in the video wearing?\n(A) Green\n(B) Purple\n(C) Blue\n(D) White\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the top that the little boy who picks up the box in the video wearing?\n(A) Green\n(B) Purple\n(C) Blue\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1354, "doc": {"video_name": "haimian_1.mp4", "duration": 305.0, "question": "What did the cartoon squirrel do after leaving the house?\n(A) Writing a letter\n(B) Driving\n(C) Leaving\n(D) Sleeping\n", "candidates": ["Writing a letter", "Driving", "Leaving", "Sleeping"], "answer": "A", "task_type": "plotQA", "question_id": "Q1354"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e5d3a99180fd790e4c5adbb16da1f04e0f993c3d2cb87031b8278728c80459e5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did the cartoon squirrel do after leaving the house?\n(A) Writing a letter\n(B) Driving\n(C) Leaving\n(D) Sleeping\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What did the cartoon squirrel do after leaving the house?\n(A) Writing a letter\n(B) Driving\n(C) Leaving\n(D) Sleeping\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1358, "doc": {"video_name": "tomjerry_5.mp4", "duration": 303.0, "question": "What did the cartoon mouse use to hit the cartoon cat's butt?\n(A) Hammer\n(B) Yellow board\n(C) Fly swatter\n(D) Wooden stick\n", "candidates": ["Hammer", "Yellow board", "Fly swatter", "Wooden stick"], "answer": "B", "task_type": "plotQA", "question_id": "Q1358"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1c35b9764f54f401a6d4f9e41a4c62827125f5a6a447226853241bd83badf68a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did the cartoon mouse use to hit the cartoon cat's butt?\n(A) Hammer\n(B) Yellow board\n(C) Fly swatter\n(D) Wooden stick\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What did the cartoon mouse use to hit the cartoon cat's butt?\n(A) Hammer\n(B) Yellow board\n(C) Fly swatter\n(D) Wooden stick\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1362, "doc": {"video_name": "movie101_24.mp4", "duration": 311.0, "question": "What color is the back of the mountain monster in the video?\n(A) Black\n(B) Green\n(C) White\n(D) Blue\n", "candidates": ["Black", "Green", "White", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1362"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bb9ddfab7b9e2bcd2c3a7556d4f4c7021f57165a223eec02d99912bab1afd0a9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the back of the mountain monster in the video?\n(A) Black\n(B) Green\n(C) White\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the back of the mountain monster in the video?\n(A) Black\n(B) Green\n(C) White\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1366, "doc": {"video_name": "movie101_88.mp4", "duration": 348.0, "question": "What transportation does the man take to arrive at the end of the video?\n(A) Helicopter\n(B) Bicycle\n(C) Car\n(D) Train\n", "candidates": ["Helicopter", "Bicycle", "Car", "Train"], "answer": "A", "task_type": "plotQA", "question_id": "Q1366"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7f4c2128022ff57495ede00d257b1a6ce80da363ba29a9349d05b4618932441f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What transportation does the man take to arrive at the end of the video?\n(A) Helicopter\n(B) Bicycle\n(C) Car\n(D) Train\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What transportation does the man take to arrive at the end of the video?\n(A) Helicopter\n(B) Bicycle\n(C) Car\n(D) Train\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1370, "doc": {"video_name": "movie101_18.mp4", "duration": 318.0, "question": "How does the man leave the restaurant in the video?\n(A) Rides a bicycle\n(B) Walks\n(C) Leaves by horse\n(D) Rides an electric scooter\n", "candidates": ["Rides a bicycle", "Walks", "Leaves by horse", "Rides an electric scooter"], "answer": "C", "task_type": "plotQA", "question_id": "Q1370"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2beb3162033e056f2e4b33efd3b3af147b410db4c2eb38de38022bb9203e94b4", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How does the man leave the restaurant in the video?\n(A) Rides a bicycle\n(B) Walks\n(C) Leaves by horse\n(D) Rides an electric scooter\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "How does the man leave the restaurant in the video?\n(A) Rides a bicycle\n(B) Walks\n(C) Leaves by horse\n(D) Rides an electric scooter\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1374, "doc": {"video_name": "tomjerry_11.mp4", "duration": 322.0, "question": "Why are all the cartoon cats lying outside?\n(A) They are resting\n(B) Because they were hit by the cartoon mouse\n(C) They were coerced by their owner\n(D) They are sick\n", "candidates": ["They are resting", "Because they were hit by the cartoon mouse", "They were coerced by their owner", "They are sick"], "answer": "B", "task_type": "plotQA", "question_id": "Q1374"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4f213aee21d78458639d43e4875a939175630d9d4ab81889b9bdd1c6407f3c71", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why are all the cartoon cats lying outside?\n(A) They are resting\n(B) Because they were hit by the cartoon mouse\n(C) They were coerced by their owner\n(D) They are sick\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Why are all the cartoon cats lying outside?\n(A) They are resting\n(B) Because they were hit by the cartoon mouse\n(C) They were coerced by their owner\n(D) They are sick\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1378, "doc": {"video_name": "en_tv_28.mp4", "duration": 575.0, "question": "What color is the man's clothes who was knocked down by the woman playing the cello in the video?\n(A) Black\n(B) Yellow\n(C) Blue\n(D) Red\n", "candidates": ["Black", "Yellow", "Blue", "Red"], "answer": "C", "task_type": "plotQA", "question_id": "Q1378"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "99297f89b913462661fc20dfba78d3296ceb9fb8f6ae1cdd7cb34d7d3f2fbf11", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the man's clothes who was knocked down by the woman playing the cello in the video?\n(A) Black\n(B) Yellow\n(C) Blue\n(D) Red\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What color is the man's clothes who was knocked down by the woman playing the cello in the video?\n(A) Black\n(B) Yellow\n(C) Blue\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1382, "doc": {"video_name": "8.mp4", "duration": 507.29, "question": "What is the weather when the main cartoon character meets his friends in the video?\n(A) Cloudy\n(B) Foggy\n(C) Sunny\n(D) Rainy\n", "candidates": ["Cloudy", "Foggy", "Sunny", "Rainy"], "answer": "C", "task_type": "plotQA", "question_id": "Q1382"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c2188d23af89e2f4042b7a11c767aebeb2df3f9fed2a068197c4582d5565c73d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the weather when the main cartoon character meets his friends in the video?\n(A) Cloudy\n(B) Foggy\n(C) Sunny\n(D) Rainy\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What is the weather when the main cartoon character meets his friends in the video?\n(A) Cloudy\n(B) Foggy\n(C) Sunny\n(D) Rainy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1386, "doc": {"video_name": "movie101_33.mp4", "duration": 240.0, "question": "How many of the five people sitting on the ground in the movie are wearing hats?\n(A) Three\n(B) Two\n(C) One\n(D) Four\n", "candidates": ["Three", "Two", "One", "Four"], "answer": "D", "task_type": "plotQA", "question_id": "Q1386"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "be3eb529af0ad2fccb45a3673e1bb71233f4d592d71e3f53c8b80cd93f329da3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many of the five people sitting on the ground in the movie are wearing hats?\n(A) Three\n(B) Two\n(C) One\n(D) Four\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "How many of the five people sitting on the ground in the movie are wearing hats?\n(A) Three\n(B) Two\n(C) One\n(D) Four\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1390, "doc": {"video_name": "movie101_31.mp4", "duration": 403.0, "question": "What color is the clothing of the injured officer in the police station?\n(A) Blue\n(B) White\n(C) Yellow\n(D) Green\n", "candidates": ["Blue", "White", "Yellow", "Green"], "answer": "D", "task_type": "plotQA", "question_id": "Q1390"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2e9c861bff64d8d6d185524249c8fff966dd5e2f876275a219ab77a49de93303", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the clothing of the injured officer in the police station?\n(A) Blue\n(B) White\n(C) Yellow\n(D) Green\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What color is the clothing of the injured officer in the police station?\n(A) Blue\n(B) White\n(C) Yellow\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1394, "doc": {"video_name": "movie101_36.mp4", "duration": 415.0, "question": "In the scene where two people are looking at each other in the woods, what color is the coat worn by the girl?\n(A) Yellow\n(B) Black\n(C) Green\n(D) White\n", "candidates": ["Yellow", "Black", "Green", "White"], "answer": "D", "task_type": "plotQA", "question_id": "Q1394"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3ef774316e7289e29c4caf2bfb73607ca53cf77d119a5eb6ecb4e7114ab11eb1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the scene where two people are looking at each other in the woods, what color is the coat worn by the girl?\n(A) Yellow\n(B) Black\n(C) Green\n(D) White\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "In the scene where two people are looking at each other in the woods, what color is the coat worn by the girl?\n(A) Yellow\n(B) Black\n(C) Green\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1398, "doc": {"video_name": "movie101_23.mp4", "duration": 473.0, "question": "What color is the top that the long-haired man is wearing in the scene where several people are communicating indoors in the video?\n(A) Red\n(B) Blue\n(C) Yellow\n(D) Green\n", "candidates": ["Red", "Blue", "Yellow", "Green"], "answer": "C", "task_type": "plotQA", "question_id": "Q1398"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "40b2c2a5f2ec6e90a87d6bf6f6ca1f7ef5f11788d330b72c866575e8ca3c66ec", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the top that the long-haired man is wearing in the scene where several people are communicating indoors in the video?\n(A) Red\n(B) Blue\n(C) Yellow\n(D) Green\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What color is the top that the long-haired man is wearing in the scene where several people are communicating indoors in the video?\n(A) Red\n(B) Blue\n(C) Yellow\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1402, "doc": {"video_name": "movie101_23.mp4", "duration": 473.0, "question": "What color is the backdrop of the stage where the elementary school student stands in the video?\n(A) White\n(B) Blue\n(C) Red\n(D) Green\n", "candidates": ["White", "Blue", "Red", "Green"], "answer": "C", "task_type": "plotQA", "question_id": "Q1402"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "95b99102750105878b47cb80dc3a3821d3281ec065032d924d1b6e42abc2f8cf", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the backdrop of the stage where the elementary school student stands in the video?\n(A) White\n(B) Blue\n(C) Red\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C.", "answer": "C"}, "input": "What color is the backdrop of the stage where the elementary school student stands in the video?\n(A) White\n(B) Blue\n(C) Red\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1406, "doc": {"video_name": "en_tv_37.mp4", "duration": 467.0, "question": "Where did the girl in black clothes get hurt?\n(A) Arm\n(B) Shoulder\n(C) Neck\n(D) Hand\n", "candidates": ["Arm", "Shoulder", "Neck", "Hand"], "answer": "C", "task_type": "plotQA", "question_id": "Q1406"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2be94b474390270c78b400011efa40ca1a1b59a8bd54bd206285365116a2a839", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did the girl in black clothes get hurt?\n(A) Arm\n(B) Shoulder\n(C) Neck\n(D) Hand\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "Where did the girl in black clothes get hurt?\n(A) Arm\n(B) Shoulder\n(C) Neck\n(D) Hand\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1410, "doc": {"video_name": "haimian_2.mp4", "duration": 356.0, "question": "What changes happened to the cartoon sponge after absorbing the liquid from the white stick?\n(A) Dried\n(B) Melted\n(C) Split\n(D) Its arms, legs, and head separated and enlarged\n", "candidates": ["Dried", "Melted", "Split", "Its arms, legs, and head separated and enlarged"], "answer": "D", "task_type": "plotQA", "question_id": "Q1410"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fa40a76a302fbb8eb12159f91d3ab4423ee7f55ea962704ec45b5157ef4947c4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What changes happened to the cartoon sponge after absorbing the liquid from the white stick?\n(A) Dried\n(B) Melted\n(C) Split\n(D) Its arms, legs, and head separated and enlarged\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What changes happened to the cartoon sponge after absorbing the liquid from the white stick?\n(A) Dried\n(B) Melted\n(C) Split\n(D) Its arms, legs, and head separated and enlarged\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1414, "doc": {"video_name": "en_tv_8.mp4", "duration": 460.0, "question": "Why did the man and woman stop kissing in the pool?\n(A) Shot by a gun\n(B) Bitten by a dog\n(C) Warned\n(D) Interrupted by a man in a black T-shirt\n", "candidates": ["Shot by a gun", "Bitten by a dog", "Warned", "Interrupted by a man in a black T-shirt"], "answer": "D", "task_type": "plotQA", "question_id": "Q1414"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fb3d250a5adea173070faa612f869fa0e6d40e8eaeb511bb417bd45247f71896", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why did the man and woman stop kissing in the pool?\n(A) Shot by a gun\n(B) Bitten by a dog\n(C) Warned\n(D) Interrupted by a man in a black T-shirt\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Why did the man and woman stop kissing in the pool?\n(A) Shot by a gun\n(B) Bitten by a dog\n(C) Warned\n(D) Interrupted by a man in a black T-shirt\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1418, "doc": {"video_name": "movie101_36.mp4", "duration": 415.0, "question": "What color is the hat worn by the girl being rescued in the hospital in the video?\n(A) Green\n(B) Black\n(C) Blue\n(D) Yellow\n", "candidates": ["Green", "Black", "Blue", "Yellow"], "answer": "C", "task_type": "plotQA", "question_id": "Q1418"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a5894b1c1c7e5174434cac47df49a6960057129fa2a6ecccf9e6c3322d384f83", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the hat worn by the girl being rescued in the hospital in the video?\n(A) Green\n(B) Black\n(C) Blue\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the hat worn by the girl being rescued in the hospital in the video?\n(A) Green\n(B) Black\n(C) Blue\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1422, "doc": {"video_name": "movie101_26.mp4", "duration": 763.0, "question": "What shape of musical instrument is the building in the video?\n(A) Flute\n(B) Guzheng\n(C) Erhu\n(D) Guitar\n", "candidates": ["Flute", "Guzheng", "Erhu", "Guitar"], "answer": "D", "task_type": "plotQA", "question_id": "Q1422"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b323328b34ce04376838aaa87a180569f5af59eb0c7b98109c611b85c27c803c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What shape of musical instrument is the building in the video?\n(A) Flute\n(B) Guzheng\n(C) Erhu\n(D) Guitar\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What shape of musical instrument is the building in the video?\n(A) Flute\n(B) Guzheng\n(C) Erhu\n(D) Guitar\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1426, "doc": {"video_name": "tomjerry_5.mp4", "duration": 303.0, "question": "Where did the cartoon cat hide in the end?\n(A) Under a large cartoon dog\n(B) Under the piano\n(C) In the car\n(D) On the tree\n", "candidates": ["Under a large cartoon dog", "Under the piano", "In the car", "On the tree"], "answer": "A", "task_type": "plotQA", "question_id": "Q1426"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7b9f35351c2b94d5734e1b4e87fa0680473cb8d1857af5fb1a632256125e3061", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did the cartoon cat hide in the end?\n(A) Under a large cartoon dog\n(B) Under the piano\n(C) In the car\n(D) On the tree\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "Where did the cartoon cat hide in the end?\n(A) Under a large cartoon dog\n(B) Under the piano\n(C) In the car\n(D) On the tree\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1430, "doc": {"video_name": "xiaoliyu_4.mp4", "duration": 399.0, "question": "What color is the blanket that flew away with the cartoon animals?\n(A) Golden\n(B) White\n(C) Black\n(D) Green\n", "candidates": ["Golden", "White", "Black", "Green"], "answer": "A", "task_type": "plotQA", "question_id": "Q1430"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6c92466426fefe3bfb2b3f0f3f7b7b52d1afa478c3df10b4ec6e9413d28df765", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the blanket that flew away with the cartoon animals?\n(A) Golden\n(B) White\n(C) Black\n(D) Green\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the blanket that flew away with the cartoon animals?\n(A) Golden\n(B) White\n(C) Black\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1434, "doc": {"video_name": "movie101_31.mp4", "duration": 403.0, "question": "What color is the bag placed on the table at the end of the video?\n(A) Blue\n(B) White\n(C) Green\n(D) Yellow\n", "candidates": ["Blue", "White", "Green", "Yellow"], "answer": "C", "task_type": "plotQA", "question_id": "Q1434"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9634fdf3ce93294c5c94d0dab7bd3600a4f09544beb641fa593d283e604bd3ca", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the bag placed on the table at the end of the video?\n(A) Blue\n(B) White\n(C) Green\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the bag placed on the table at the end of the video?\n(A) Blue\n(B) White\n(C) Green\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1438, "doc": {"video_name": "movie101_106.mp4", "duration": 746.0, "question": "Why does the man in the bedroom run away in a hurry in the video?\n(A) Because a woman enters the bedroom\n(B) Flood\n(C) Earthquake\n(D) Fire\n", "candidates": ["Because a woman enters the bedroom", "Flood", "Earthquake", "Fire"], "answer": "A", "task_type": "plotQA", "question_id": "Q1438"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8c2da4d680f2c2d99b6b98caafc4213ec2285395213500b28387b2a34890d68a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why does the man in the bedroom run away in a hurry in the video?\n(A) Because a woman enters the bedroom\n(B) Flood\n(C) Earthquake\n(D) Fire\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "Why does the man in the bedroom run away in a hurry in the video?\n(A) Because a woman enters the bedroom\n(B) Flood\n(C) Earthquake\n(D) Fire\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1442, "doc": {"video_name": "movie101_41.mp4", "duration": 319.0, "question": "What is the mask worn by the diving wild man made of?\n(A) Plastic\n(B) Bone\n(C) Wood\n(D) Stone\n", "candidates": ["Plastic", "Bone", "Wood", "Stone"], "answer": "B", "task_type": "plotQA", "question_id": "Q1442"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d501be7f5ed12b428522c81f71d767ab10714f701fbcc24095cacfe89ddbf602", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the mask worn by the diving wild man made of?\n(A) Plastic\n(B) Bone\n(C) Wood\n(D) Stone\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What is the mask worn by the diving wild man made of?\n(A) Plastic\n(B) Bone\n(C) Wood\n(D) Stone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1446, "doc": {"video_name": "xiaoliyu_5.mp4", "duration": 415.0, "question": "Who captured the cartoon turtle?\n(A) Cartoon mouse\n(B) Cartoon fish\n(C) Cartoon cat\n(D) Cartoon snake\n", "candidates": ["Cartoon mouse", "Cartoon fish", "Cartoon cat", "Cartoon snake"], "answer": "D", "task_type": "plotQA", "question_id": "Q1446"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f244dd83f8483a4a4f67d8cb77c1236023b352b556cdba97e02fa3ab08b6f206", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Who captured the cartoon turtle?\n(A) Cartoon mouse\n(B) Cartoon fish\n(C) Cartoon cat\n(D) Cartoon snake\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Who captured the cartoon turtle?\n(A) Cartoon mouse\n(B) Cartoon fish\n(C) Cartoon cat\n(D) Cartoon snake\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1450, "doc": {"video_name": "en_tv_39.mp4", "duration": 476.0, "question": "What color is the clothing of the elderly person on the hospital bed in the video?\n(A) Blue\n(B) Red\n(C) Black\n(D) Yellow\n", "candidates": ["Blue", "Red", "Black", "Yellow"], "answer": "A", "task_type": "plotQA", "question_id": "Q1450"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a7bf229cbafd3fec6f443c05ff22c31450ce59200fb1ce9e085b5054cd3be149", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the clothing of the elderly person on the hospital bed in the video?\n(A) Blue\n(B) Red\n(C) Black\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the clothing of the elderly person on the hospital bed in the video?\n(A) Blue\n(B) Red\n(C) Black\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1454, "doc": {"video_name": "movie101_85.mp4", "duration": 560.0, "question": "What does the little girl in red turn into?\n(A) Cat\n(B) Dolphin\n(C) Dog\n(D) Shark\n", "candidates": ["Cat", "Dolphin", "Dog", "Shark"], "answer": "B", "task_type": "plotQA", "question_id": "Q1454"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b9f9cc019db20a93934f12d9f92bff63679f8bc778bf5fff3222827ecc210de6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What does the little girl in red turn into?\n(A) Cat\n(B) Dolphin\n(C) Dog\n(D) Shark\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What does the little girl in red turn into?\n(A) Cat\n(B) Dolphin\n(C) Dog\n(D) Shark\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1458, "doc": {"video_name": "movie101_8.mp4", "duration": 750.0, "question": "What is the color of the child's hair in the video?\n(A) Black\n(B) Green\n(C) Red\n(D) Blue\n", "candidates": ["Black", "Green", "Red", "Blue"], "answer": "A", "task_type": "plotQA", "question_id": "Q1458"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6ea095a8feda2588c99d62cf71cbafa1abd5403826afddcd5ec350a53aa504f1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the color of the child's hair in the video?\n(A) Black\n(B) Green\n(C) Red\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What is the color of the child's hair in the video?\n(A) Black\n(B) Green\n(C) Red\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1462, "doc": {"video_name": "tomjerry_1.mp4", "duration": 311.0, "question": "What does the cartoon cat use to drag the cartoon mouse?\n(A) Pushcart\n(B) Fishing rod\n(C) Rope\n(D) Tow truck\n", "candidates": ["Pushcart", "Fishing rod", "Rope", "Tow truck"], "answer": "B", "task_type": "plotQA", "question_id": "Q1462"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5fec7cf5e582b947b169ec7eee0fb41b1e1f0169fc221f2988553cfca65f59d2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What does the cartoon cat use to drag the cartoon mouse?\n(A) Pushcart\n(B) Fishing rod\n(C) Rope\n(D) Tow truck\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What does the cartoon cat use to drag the cartoon mouse?\n(A) Pushcart\n(B) Fishing rod\n(C) Rope\n(D) Tow truck\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1466, "doc": {"video_name": "movie101_62.mp4", "duration": 601.0, "question": "What color is the drink the bald man is having at the bar?\n(A) Blue\n(B) Green\n(C) Red\n(D) Yellow\n", "candidates": ["Blue", "Green", "Red", "Yellow"], "answer": "A", "task_type": "plotQA", "question_id": "Q1466"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1bf248477017cfe8bd2b01629d92044551a54e7214ae6fd1855bc92d369055f9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the drink the bald man is having at the bar?\n(A) Blue\n(B) Green\n(C) Red\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the drink the bald man is having at the bar?\n(A) Blue\n(B) Green\n(C) Red\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1470, "doc": {"video_name": "movie101_25.mp4", "duration": 737.0, "question": "What color is the hat worn by the man smoking on the bridge?\n(A) Blue\n(B) Yellow\n(C) White\n(D) Black\n", "candidates": ["Blue", "Yellow", "White", "Black"], "answer": "D", "task_type": "plotQA", "question_id": "Q1470"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "27c46a0cef065f935f40bb6015897d543c42bccfb4dfad09c16d16994cad320e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the hat worn by the man smoking on the bridge?\n(A) Blue\n(B) Yellow\n(C) White\n(D) Black\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the hat worn by the man smoking on the bridge?\n(A) Blue\n(B) Yellow\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1474, "doc": {"video_name": "en_tv_34.mp4", "duration": 492.0, "question": "What is the emotion on the face of the allergic woman when the three people return?\n(A) Happy\n(B) Neutral\n(C) Crying\n(D) Surprised\n", "candidates": ["Happy", "Neutral", "Crying", "Surprised"], "answer": "D", "task_type": "plotQA", "question_id": "Q1474"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "054d901b8cee0c30cb809e779eeffacf61ea5bbc4fb88750125d1492dba8f6dd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the emotion on the face of the allergic woman when the three people return?\n(A) Happy\n(B) Neutral\n(C) Crying\n(D) Surprised\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What is the emotion on the face of the allergic woman when the three people return?\n(A) Happy\n(B) Neutral\n(C) Crying\n(D) Surprised\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1478, "doc": {"video_name": "haimian_9.mp4", "duration": 444.0, "question": "What did the cartoon sponge squeeze onto the bread?\n(A) Juice from the cartoon jellyfish\n(B) Salad Dressing\n(C) Meat Sauce\n(D) Jam\n", "candidates": ["Juice from the cartoon jellyfish", "Salad Dressing", "Meat Sauce", "Jam"], "answer": "A", "task_type": "plotQA", "question_id": "Q1478"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "52f9529f201a7ade657fec8bfcd45075098a367daaec76a95bf740c9964097d6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did the cartoon sponge squeeze onto the bread?\n(A) Juice from the cartoon jellyfish\n(B) Salad Dressing\n(C) Meat Sauce\n(D) Jam\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What did the cartoon sponge squeeze onto the bread?\n(A) Juice from the cartoon jellyfish\n(B) Salad Dressing\n(C) Meat Sauce\n(D) Jam\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1482, "doc": {"video_name": "xiaoliyu_6.mp4", "duration": 400.0, "question": "What caught the cartoon turtle?\n(A) Cartoon shrimp\n(B) Cartoon catfish\n(C) Cartoon snake\n(D) Giant octopus\n", "candidates": ["Cartoon shrimp", "Cartoon catfish", "Cartoon snake", "Giant octopus"], "answer": "D", "task_type": "plotQA", "question_id": "Q1482"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d5b5ff505bb23f2e2c65a4bb2627816e63001c8c2a248faf2c558eac6c70f8c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What caught the cartoon turtle?\n(A) Cartoon shrimp\n(B) Cartoon catfish\n(C) Cartoon snake\n(D) Giant octopus\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "What caught the cartoon turtle?\n(A) Cartoon shrimp\n(B) Cartoon catfish\n(C) Cartoon snake\n(D) Giant octopus\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1486, "doc": {"video_name": "movie101_15.mp4", "duration": 441.0, "question": "What color is the animal that appears in the video?\n(A) Brown\n(B) Black\n(C) White\n(D) Green\n", "candidates": ["Brown", "Black", "White", "Green"], "answer": "A", "task_type": "plotQA", "question_id": "Q1486"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f1e7e678f6ee8b7f4c3542cd248aa34b314c80449efcb94552023e5bffcfec4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the animal that appears in the video?\n(A) Brown\n(B) Black\n(C) White\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What color is the animal that appears in the video?\n(A) Brown\n(B) Black\n(C) White\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1490, "doc": {"video_name": "movie101_79.mp4", "duration": 590.0, "question": "What color is the swimsuit worn by the woman diving in the sea?\n(A) Black\n(B) Red\n(C) White\n(D) Yellow\n", "candidates": ["Black", "Red", "White", "Yellow"], "answer": "C", "task_type": "plotQA", "question_id": "Q1490"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "16d75accb51a293d37b74c70db7ffcb12eafa20d568c7337c6618ca049d0b6d3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the swimsuit worn by the woman diving in the sea?\n(A) Black\n(B) Red\n(C) White\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "C.", "answer": "C"}, "input": "What color is the swimsuit worn by the woman diving in the sea?\n(A) Black\n(B) Red\n(C) White\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1494, "doc": {"video_name": "en_tv_26.mp4", "duration": 522.0, "question": "What color is the woman's hair in the scene where two people are talking?\n(A) White\n(B) Red\n(C) Blonde\n(D) Pink\n", "candidates": ["White", "Red", "Blonde", "Pink"], "answer": "C", "task_type": "plotQA", "question_id": "Q1494"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f27a6a596363cf6d14259be1f6900127770070667fc09456a814a616320ca6d3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the woman's hair in the scene where two people are talking?\n(A) White\n(B) Red\n(C) Blonde\n(D) Pink\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the woman's hair in the scene where two people are talking?\n(A) White\n(B) Red\n(C) Blonde\n(D) Pink\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1498, "doc": {"video_name": "movie101_16.mp4", "duration": 740.0, "question": "What color is the table in the scene where one person gives money to another at the end of the video?\n(A) White\n(B) Blue\n(C) Green\n(D) Yellow\n", "candidates": ["White", "Blue", "Green", "Yellow"], "answer": "B", "task_type": "plotQA", "question_id": "Q1498"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d75b2a9db80c3de038b9732e872d40d2f1306b3e63a890b316933ec2a789e5df", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the table in the scene where one person gives money to another at the end of the video?\n(A) White\n(B) Blue\n(C) Green\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the table in the scene where one person gives money to another at the end of the video?\n(A) White\n(B) Blue\n(C) Green\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1502, "doc": {"video_name": "movie101_32.mp4", "duration": 247.0, "question": "At what time does the car repair scene in the movie take place?\n(A) Noon\n(B) Evening\n(C) Night\n(D) Morning\n", "candidates": ["Noon", "Evening", "Night", "Morning"], "answer": "C", "task_type": "plotQA", "question_id": "Q1502"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "46e2e113452376730ee4fac1562176cf67f4b8530366e76da2e100b990c34dc7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "At what time does the car repair scene in the movie take place?\n(A) Noon\n(B) Evening\n(C) Night\n(D) Morning\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "At what time does the car repair scene in the movie take place?\n(A) Noon\n(B) Evening\n(C) Night\n(D) Morning\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1506, "doc": {"video_name": "movie101_102.mp4", "duration": 607.0, "question": "Why does the horse-riding soldier enter the temple?\n(A) To eat\n(B) To rest\n(C) To pursue the wounded\n(D) To steal\n", "candidates": ["To eat", "To rest", "To pursue the wounded", "To steal"], "answer": "C", "task_type": "plotQA", "question_id": "Q1506"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d08ba2422e04db0e1fb161e328eef65149f911bc7a3432cb1377e84933705675", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why does the horse-riding soldier enter the temple?\n(A) To eat\n(B) To rest\n(C) To pursue the wounded\n(D) To steal\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Why does the horse-riding soldier enter the temple?\n(A) To eat\n(B) To rest\n(C) To pursue the wounded\n(D) To steal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1510, "doc": {"video_name": "en_tv_11.mp4", "duration": 465.0, "question": "Why does the woman in leather clothing want to attract attention?\n(A) To complete a mission\n(B) For fun\n(C) To help another woman sneak into the house\n(D) Unintentionally\n", "candidates": ["To complete a mission", "For fun", "To help another woman sneak into the house", "Unintentionally"], "answer": "C", "task_type": "plotQA", "question_id": "Q1510"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "504cd9d4f8a4e47cba858e1938dd1efe2e0778a19e520ee07963d4e467ccfa44", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why does the woman in leather clothing want to attract attention?\n(A) To complete a mission\n(B) For fun\n(C) To help another woman sneak into the house\n(D) Unintentionally\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Why does the woman in leather clothing want to attract attention?\n(A) To complete a mission\n(B) For fun\n(C) To help another woman sneak into the house\n(D) Unintentionally\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1514, "doc": {"video_name": "movie101_55.mp4", "duration": 315.0, "question": "What is the first expression of the person in the gray suit jacket after receiving the item from the little girl in front of the ruins?\n(A) Smile\n(B) Neutral\n(C) Excited\n(D) Sad\n", "candidates": ["Smile", "Neutral", "Excited", "Sad"], "answer": "A", "task_type": "plotQA", "question_id": "Q1514"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b6392b9f44d97aae345db97d58d0b9fb4581f5dcbf2272ccdf75fa27e7a2c820", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the first expression of the person in the gray suit jacket after receiving the item from the little girl in front of the ruins?\n(A) Smile\n(B) Neutral\n(C) Excited\n(D) Sad\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What is the first expression of the person in the gray suit jacket after receiving the item from the little girl in front of the ruins?\n(A) Smile\n(B) Neutral\n(C) Excited\n(D) Sad\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1518, "doc": {"video_name": "xiaoliyu_1.mp4", "duration": 381.0, "question": "What happened to the cartoon turtle after it ate the seaweed?\n(A) Swimming\n(B) Playing\n(C) Vomiting\n(D) Sleeping\n", "candidates": ["Swimming", "Playing", "Vomiting", "Sleeping"], "answer": "C", "task_type": "plotQA", "question_id": "Q1518"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c85b0bd1e9dde60e56f5a81d4f778a32c57fd185794643138a32b061dff17307", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What happened to the cartoon turtle after it ate the seaweed?\n(A) Swimming\n(B) Playing\n(C) Vomiting\n(D) Sleeping\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What happened to the cartoon turtle after it ate the seaweed?\n(A) Swimming\n(B) Playing\n(C) Vomiting\n(D) Sleeping\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1522, "doc": {"video_name": "tomjerry_2.mp4", "duration": 303.0, "question": "Where does the cartoon cat re-enter the house from?\n(A) Tunnel\n(B) Chimney\n(C) Window\n(D) Door\n", "candidates": ["Tunnel", "Chimney", "Window", "Door"], "answer": "D", "task_type": "plotQA", "question_id": "Q1522"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5974da8dad792bb3ae8e3b401feb10e35ac98da500c02ff6fcd762905dbf7251", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where does the cartoon cat re-enter the house from?\n(A) Tunnel\n(B) Chimney\n(C) Window\n(D) Door\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "Where does the cartoon cat re-enter the house from?\n(A) Tunnel\n(B) Chimney\n(C) Window\n(D) Door\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1526, "doc": {"video_name": "haimian_8.mp4", "duration": 445.0, "question": "What was the reaction of the cartoon sponge after eating grass?\n(A) Laughed\n(B) Cried\n(C) Vomited\n(D) Changed color\n", "candidates": ["Laughed", "Cried", "Vomited", "Changed color"], "answer": "C", "task_type": "plotQA", "question_id": "Q1526"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "33477c1bf59e0b635a7940f6f5f4c0f689b04161eb13d5501eb9541b726b6ae3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What was the reaction of the cartoon sponge after eating grass?\n(A) Laughed\n(B) Cried\n(C) Vomited\n(D) Changed color\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What was the reaction of the cartoon sponge after eating grass?\n(A) Laughed\n(B) Cried\n(C) Vomited\n(D) Changed color\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1530, "doc": {"video_name": "movie101_13.mp4", "duration": 456.0, "question": "What color is the hat that the man in the video wears when conducting business at the bank?\n(A) Green\n(B) Purple\n(C) Blue\n(D) Black\n", "candidates": ["Green", "Purple", "Blue", "Black"], "answer": "D", "task_type": "plotQA", "question_id": "Q1530"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "78c20fde91612b3076e2f85ae4d2fdef0cb5d9041e57ba7dbe28bdc74a423ffc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the hat that the man in the video wears when conducting business at the bank?\n(A) Green\n(B) Purple\n(C) Blue\n(D) Black\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the hat that the man in the video wears when conducting business at the bank?\n(A) Green\n(B) Purple\n(C) Blue\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1534, "doc": {"video_name": "haimian_5.mp4", "duration": 335.0, "question": "What does the cartoon starfish do before sitting on the cartoon rocking chair?\n(A) Playing with toys\n(B) Eating snacks\n(C) Drinking water\n(D) Inserting a coin\n", "candidates": ["Playing with toys", "Eating snacks", "Drinking water", "Inserting a coin"], "answer": "D", "task_type": "plotQA", "question_id": "Q1534"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eef976366531524dcac7790467f46b76b685e145ad027ee9f79ec281f96ee5b3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What does the cartoon starfish do before sitting on the cartoon rocking chair?\n(A) Playing with toys\n(B) Eating snacks\n(C) Drinking water\n(D) Inserting a coin\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What does the cartoon starfish do before sitting on the cartoon rocking chair?\n(A) Playing with toys\n(B) Eating snacks\n(C) Drinking water\n(D) Inserting a coin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1538, "doc": {"video_name": "movie101_32.mp4", "duration": 247.0, "question": "What color are the pants of the man in the blue shirt in the horse riding scene in the movie?\n(A) Yellow\n(B) Blue\n(C) White\n(D) Red\n", "candidates": ["Yellow", "Blue", "White", "Red"], "answer": "D", "task_type": "plotQA", "question_id": "Q1538"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c6c814e9bb75b8a04a3f45e4c662db54d988b20b5964b5593000639e8a4cd9dc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color are the pants of the man in the blue shirt in the horse riding scene in the movie?\n(A) Yellow\n(B) Blue\n(C) White\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color are the pants of the man in the blue shirt in the horse riding scene in the movie?\n(A) Yellow\n(B) Blue\n(C) White\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1542, "doc": {"video_name": "movie101_61.mp4", "duration": 627.0, "question": "What is the man's reaction upon seeing the woman in the video?\n(A) Indifferent\n(B) Sad\n(C) Joyful\n(D) Neutral\n", "candidates": ["Indifferent", "Sad", "Joyful", "Neutral"], "answer": "A", "task_type": "plotQA", "question_id": "Q1542"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "06bc1090abfb86a13483113b5df6b02f116db7b2142ac03a23faf80b0c9cc413", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the man's reaction upon seeing the woman in the video?\n(A) Indifferent\n(B) Sad\n(C) Joyful\n(D) Neutral\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What is the man's reaction upon seeing the woman in the video?\n(A) Indifferent\n(B) Sad\n(C) Joyful\n(D) Neutral\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1546, "doc": {"video_name": "movie101_38.mp4", "duration": 539.0, "question": "What color is the liquid bomb in the video?\n(A) White\n(B) Green\n(C) Black\n(D) Blue\n", "candidates": ["White", "Green", "Black", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1546"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2533ea981bf27f28b16d8646538efb799098c1e0dd27c8c798aabd86969aedf8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the liquid bomb in the video?\n(A) White\n(B) Green\n(C) Black\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the liquid bomb in the video?\n(A) White\n(B) Green\n(C) Black\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1550, "doc": {"video_name": "movie101_39.mp4", "duration": 473.0, "question": "What color is the top of the person doing the woman's makeup?\n(A) Green\n(B) Black\n(C) Yellow\n(D) Red\n", "candidates": ["Green", "Black", "Yellow", "Red"], "answer": "D", "task_type": "plotQA", "question_id": "Q1550"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "52fb86123c3f859029e193cd26a4fa0eba1a6dde345b5fb2b23b7b1f500af529", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the top of the person doing the woman's makeup?\n(A) Green\n(B) Black\n(C) Yellow\n(D) Red\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What color is the top of the person doing the woman's makeup?\n(A) Green\n(B) Black\n(C) Yellow\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1554, "doc": {"video_name": "haimian_8.mp4", "duration": 445.0, "question": "What did the cartoon sponge give to the cartoon starfish?\n(A) An orange\n(B) A ball\n(C) A small fishing net\n(D) An apple\n", "candidates": ["An orange", "A ball", "A small fishing net", "An apple"], "answer": "C", "task_type": "plotQA", "question_id": "Q1554"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ee8f5ac6705e4ae4c0ab4cd9150047fa54aeb02ea3ba90f43c018e3a1b2e61e7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did the cartoon sponge give to the cartoon starfish?\n(A) An orange\n(B) A ball\n(C) A small fishing net\n(D) An apple\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What did the cartoon sponge give to the cartoon starfish?\n(A) An orange\n(B) A ball\n(C) A small fishing net\n(D) An apple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1558, "doc": {"video_name": "haimian_5.mp4", "duration": 335.0, "question": "What are the cartoon starfish and cartoon sponge doing while they are talking?\n(A) Playing with toys\n(B) Drinking water\n(C) Eating snacks\n(D) Riding a cartoon seahorse seat\n", "candidates": ["Playing with toys", "Drinking water", "Eating snacks", "Riding a cartoon seahorse seat"], "answer": "D", "task_type": "plotQA", "question_id": "Q1558"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d0cfee6252f9acc9ed1171774071ba7cae44591a4395b5a0b7d4f4dd06722bf", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What are the cartoon starfish and cartoon sponge doing while they are talking?\n(A) Playing with toys\n(B) Drinking water\n(C) Eating snacks\n(D) Riding a cartoon seahorse seat\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What are the cartoon starfish and cartoon sponge doing while they are talking?\n(A) Playing with toys\n(B) Drinking water\n(C) Eating snacks\n(D) Riding a cartoon seahorse seat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1562, "doc": {"video_name": "haimian_2.mp4", "duration": 356.0, "question": "What action did the cartoon octopus do before giving the speech?\n(A) Squatted down\n(B) Took a deep breath\n(C) Went for a walk\n(D) Cleared its throat\n", "candidates": ["Squatted down", "Took a deep breath", "Went for a walk", "Cleared its throat"], "answer": "D", "task_type": "plotQA", "question_id": "Q1562"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fa66a2457b7983cafea325544bfe4b59c15a91e309bf434c74dd0326187afa6d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What action did the cartoon octopus do before giving the speech?\n(A) Squatted down\n(B) Took a deep breath\n(C) Went for a walk\n(D) Cleared its throat\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What action did the cartoon octopus do before giving the speech?\n(A) Squatted down\n(B) Took a deep breath\n(C) Went for a walk\n(D) Cleared its throat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1566, "doc": {"video_name": "movie101_41.mp4", "duration": 319.0, "question": "What are the hunting tools used by the wild man made of?\n(A) Knife\n(B) Sword\n(C) Gun\n(D) Wooden Stick, Stone and Hemp Rope\n", "candidates": ["Knife", "Sword", "Gun", "Wooden Stick, Stone and Hemp Rope"], "answer": "D", "task_type": "plotQA", "question_id": "Q1566"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ea06ef2ec6256ffc9125119958fac7dbe7a789057fa5ff677f5d07efac012992", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What are the hunting tools used by the wild man made of?\n(A) Knife\n(B) Sword\n(C) Gun\n(D) Wooden Stick, Stone and Hemp Rope\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What are the hunting tools used by the wild man made of?\n(A) Knife\n(B) Sword\n(C) Gun\n(D) Wooden Stick, Stone and Hemp Rope\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1570, "doc": {"video_name": "en_tv_30.mp4", "duration": 584.0, "question": "What are the many people in the video hitting the man with?\n(A) Eggs\n(B) Leather ball\n(C) Vegetables\n(D) Paper\n", "candidates": ["Eggs", "Leather ball", "Vegetables", "Paper"], "answer": "B", "task_type": "plotQA", "question_id": "Q1570"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e184099e24fbbf7d805f46767cb1d2afc877989b390c0fa45b9213a3700d18df", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What are the many people in the video hitting the man with?\n(A) Eggs\n(B) Leather ball\n(C) Vegetables\n(D) Paper\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What are the many people in the video hitting the man with?\n(A) Eggs\n(B) Leather ball\n(C) Vegetables\n(D) Paper\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1574, "doc": {"video_name": "movie101_11.mp4", "duration": 206.0, "question": "What color is the child's clothes in the video?\n(A) Blue\n(B) Red\n(C) Black\n(D) White\n", "candidates": ["Blue", "Red", "Black", "White"], "answer": "A", "task_type": "plotQA", "question_id": "Q1574"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f43f2cdb792d4c4f02f1bd4a700a85d8420e602b54a007830ba686f29607f634", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the child's clothes in the video?\n(A) Blue\n(B) Red\n(C) Black\n(D) White\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the child's clothes in the video?\n(A) Blue\n(B) Red\n(C) Black\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1578, "doc": {"video_name": "movie101_15.mp4", "duration": 441.0, "question": "What color is the hair of the person who gets spit on by the llama in the video?\n(A) Blue\n(B) Green\n(C) White\n(D) Red\n", "candidates": ["Blue", "Green", "White", "Red"], "answer": "C", "task_type": "plotQA", "question_id": "Q1578"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d366ddcb697c868a5af8c2abba066d94b3602cc062746976b917835388736952", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the hair of the person who gets spit on by the llama in the video?\n(A) Blue\n(B) Green\n(C) White\n(D) Red\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the hair of the person who gets spit on by the llama in the video?\n(A) Blue\n(B) Green\n(C) White\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1582, "doc": {"video_name": "en_tv_37.mp4", "duration": 467.0, "question": "What color of clothes does the woman who appears at the end wear?\n(A) Red\n(B) Black\n(C) White\n(D) Green\n", "candidates": ["Red", "Black", "White", "Green"], "answer": "A", "task_type": "plotQA", "question_id": "Q1582"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4edbd363479d8edc3e9a0ece43cd93d81ec09813ad27b035dc4d31685364ebaf", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color of clothes does the woman who appears at the end wear?\n(A) Red\n(B) Black\n(C) White\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What color of clothes does the woman who appears at the end wear?\n(A) Red\n(B) Black\n(C) White\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1586, "doc": {"video_name": "en_tv_21.mp4", "duration": 538.0, "question": "In the scene where two people are conversing, what color is the dress of the woman wearing the necklace?\n(A) Green\n(B) Orange\n(C) Rose Red\n(D) Blue\n", "candidates": ["Green", "Orange", "Rose Red", "Blue"], "answer": "C", "task_type": "plotQA", "question_id": "Q1586"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0c7dcbd94a2f375cb7ca3bd4d5a63e80c2557c7a713a3f5027ff6a067ac018e0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the scene where two people are conversing, what color is the dress of the woman wearing the necklace?\n(A) Green\n(B) Orange\n(C) Rose Red\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "In the scene where two people are conversing, what color is the dress of the woman wearing the necklace?\n(A) Green\n(B) Orange\n(C) Rose Red\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1590, "doc": {"video_name": "movie101_108.mp4", "duration": 330.0, "question": "At the beginning of the video, what do the two little girls sneak off to do?\n(A) Play\n(B) Drink\n(C) Eat\n(D) They vandalize the fire alarm\n", "candidates": ["Play", "Drink", "Eat", "They vandalize the fire alarm"], "answer": "D", "task_type": "plotQA", "question_id": "Q1590"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8270b1134c86baa85a9df6ea08845f6d61f1d60c169fff52a2e27c0aa7867038", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "At the beginning of the video, what do the two little girls sneak off to do?\n(A) Play\n(B) Drink\n(C) Eat\n(D) They vandalize the fire alarm\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "At the beginning of the video, what do the two little girls sneak off to do?\n(A) Play\n(B) Drink\n(C) Eat\n(D) They vandalize the fire alarm\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1594, "doc": {"video_name": "movie101_36.mp4", "duration": 415.0, "question": "What color is the top worn by the woman holding a white cloth in her hand?\n(A) Yellow\n(B) Red\n(C) Green\n(D) Black\n", "candidates": ["Yellow", "Red", "Green", "Black"], "answer": "C", "task_type": "plotQA", "question_id": "Q1594"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f680ccc165c230d5d586ebae10a330b3b0ebc5e5d3793da011d9c6238025574b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the top worn by the woman holding a white cloth in her hand?\n(A) Yellow\n(B) Red\n(C) Green\n(D) Black\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What color is the top worn by the woman holding a white cloth in her hand?\n(A) Yellow\n(B) Red\n(C) Green\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1598, "doc": {"video_name": "movie101_110.mp4", "duration": 423.0, "question": "At the beginning of the video, why is water sprayed on the face of a woman in pain?\n(A) Washing face\n(B) Rinsing a wound\n(C) Execution\n(D) Because they are acting\n", "candidates": ["Washing face", "Rinsing a wound", "Execution", "Because they are acting"], "answer": "D", "task_type": "plotQA", "question_id": "Q1598"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1186a549e390956dc9a2a630aaecfdfdeb8e2ffd5ca9eae5c82e4704c6d91c29", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "At the beginning of the video, why is water sprayed on the face of a woman in pain?\n(A) Washing face\n(B) Rinsing a wound\n(C) Execution\n(D) Because they are acting\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "At the beginning of the video, why is water sprayed on the face of a woman in pain?\n(A) Washing face\n(B) Rinsing a wound\n(C) Execution\n(D) Because they are acting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1602, "doc": {"video_name": "movie101_10.mp4", "duration": 716.0, "question": "What is the expression of the person being kidnapped in the video?\n(A) Happiness\n(B) Joy\n(C) Fear\n(D) Anger\n", "candidates": ["Happiness", "Joy", "Fear", "Anger"], "answer": "C", "task_type": "plotQA", "question_id": "Q1602"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "64f92c2b4d76f621cfa10b5f6ebf542037b34809441f5822e1451e74c8392537", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the expression of the person being kidnapped in the video?\n(A) Happiness\n(B) Joy\n(C) Fear\n(D) Anger\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What is the expression of the person being kidnapped in the video?\n(A) Happiness\n(B) Joy\n(C) Fear\n(D) Anger\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1606, "doc": {"video_name": "movie101_60.mp4", "duration": 402.0, "question": "What kind of shop does the man run?\n(A) Pharmacy\n(B) Restaurant\n(C) Coffee shop\n(D) Pharmacy\n", "candidates": ["Pharmacy", "Restaurant", "Coffee shop", "Pharmacy"], "answer": "A", "task_type": "plotQA", "question_id": "Q1606"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d97e12e52d32f90c99284a7069ed96a16d4c8b267f25361964b8f99869b2fa4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What kind of shop does the man run?\n(A) Pharmacy\n(B) Restaurant\n(C) Coffee shop\n(D) Pharmacy\n", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "What kind of shop does the man run?\n(A) Pharmacy\n(B) Restaurant\n(C) Coffee shop\n(D) Pharmacy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1610, "doc": {"video_name": "movie101_2.mp4", "duration": 259.0, "question": "How does the man in black clothes escape from indoors to outdoors?\n(A) He escapes to the outdoors by rushing out the door\n(B) He escapes to the outdoors through an underground passage\n(C) He escapes to the outdoors by breaking through the window\n(D) He escapes to the outdoors through an emergency exit\n", "candidates": ["He escapes to the outdoors by rushing out the door", "He escapes to the outdoors through an underground passage", "He escapes to the outdoors by breaking through the window", "He escapes to the outdoors through an emergency exit"], "answer": "C", "task_type": "plotQA", "question_id": "Q1610"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b228d5662e32e864bc14c0b1edd0e45ca62d7fc40c5ff82e5264fb4bd742000a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How does the man in black clothes escape from indoors to outdoors?\n(A) He escapes to the outdoors by rushing out the door\n(B) He escapes to the outdoors through an underground passage\n(C) He escapes to the outdoors by breaking through the window\n(D) He escapes to the outdoors through an emergency exit\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "How does the man in black clothes escape from indoors to outdoors?\n(A) He escapes to the outdoors by rushing out the door\n(B) He escapes to the outdoors through an underground passage\n(C) He escapes to the outdoors by breaking through the window\n(D) He escapes to the outdoors through an emergency exit\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1614, "doc": {"video_name": "movie101_62.mp4", "duration": 601.0, "question": "What does the blonde man do after chatting with the woman?\n(A) Surfs\n(B) Eats\n(C) Drinks\n(D) Plays games\n", "candidates": ["Surfs", "Eats", "Drinks", "Plays games"], "answer": "C", "task_type": "plotQA", "question_id": "Q1614"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ef53bb4c8f1201a243398a27e76bde66e5d53be4b2f96465ece759c7003b81c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What does the blonde man do after chatting with the woman?\n(A) Surfs\n(B) Eats\n(C) Drinks\n(D) Plays games\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What does the blonde man do after chatting with the woman?\n(A) Surfs\n(B) Eats\n(C) Drinks\n(D) Plays games\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1618, "doc": {"video_name": "en_tv_15.mp4", "duration": 541.0, "question": "What is the weather during the scene where two people are chatting at the beginning of the video?\n(A) Sunny\n(B) Snowy\n(C) Stormy\n(D) Rainy\n", "candidates": ["Sunny", "Snowy", "Stormy", "Rainy"], "answer": "B", "task_type": "plotQA", "question_id": "Q1618"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "48ed9c6da4e8007bb6e677ac2446e610597b0018f4b5cb59fb130bafa7954ed8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the weather during the scene where two people are chatting at the beginning of the video?\n(A) Sunny\n(B) Snowy\n(C) Stormy\n(D) Rainy\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the weather during the scene where two people are chatting at the beginning of the video?\n(A) Sunny\n(B) Snowy\n(C) Stormy\n(D) Rainy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1622, "doc": {"video_name": "movie101_52.mp4", "duration": 338.0, "question": "What is the fate of the person who found the watch?\n(A) He hides in a trench\n(B) He retreats safely\n(C) He is knocked down by a bomb\n(D) He is shot down by an enemy hiding in the dark\n", "candidates": ["He hides in a trench", "He retreats safely", "He is knocked down by a bomb", "He is shot down by an enemy hiding in the dark"], "answer": "D", "task_type": "plotQA", "question_id": "Q1622"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "27fb2f5152c42e342db288ffd28d291410a23cb539f8d5c337713b7b3b8254db", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the fate of the person who found the watch?\n(A) He hides in a trench\n(B) He retreats safely\n(C) He is knocked down by a bomb\n(D) He is shot down by an enemy hiding in the dark\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What is the fate of the person who found the watch?\n(A) He hides in a trench\n(B) He retreats safely\n(C) He is knocked down by a bomb\n(D) He is shot down by an enemy hiding in the dark\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1626, "doc": {"video_name": "movie101_78.mp4", "duration": 607.0, "question": "What did the woman split open with a knife?\n(A) Mat\n(B) Cloth\n(C) Wood\n(D) Paper\n", "candidates": ["Mat", "Cloth", "Wood", "Paper"], "answer": "A", "task_type": "plotQA", "question_id": "Q1626"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aabcbe958dbe2eac4b2fe84966e9cbc569f9a80c9a3dc84f6f1cf515c1771659", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did the woman split open with a knife?\n(A) Mat\n(B) Cloth\n(C) Wood\n(D) Paper\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What did the woman split open with a knife?\n(A) Mat\n(B) Cloth\n(C) Wood\n(D) Paper\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1630, "doc": {"video_name": "movie101_26.mp4", "duration": 763.0, "question": "What color is the pajamas the little girl in the movie is wearing?\n(A) Green\n(B) Blue\n(C) Red\n(D) Pink\n", "candidates": ["Green", "Blue", "Red", "Pink"], "answer": "D", "task_type": "plotQA", "question_id": "Q1630"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05d568b7c369ca1534867f4e180b793488b3e88824199bf2e721d28d9cb35dd6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the pajamas the little girl in the movie is wearing?\n(A) Green\n(B) Blue\n(C) Red\n(D) Pink\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the pajamas the little girl in the movie is wearing?\n(A) Green\n(B) Blue\n(C) Red\n(D) Pink\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1634, "doc": {"video_name": "movie101_63.mp4", "duration": 748.0, "question": "What are the group of people in white blowing?\n(A) Suona\n(B) Trumpet\n(C) Bull horn\n(D) Flute\n", "candidates": ["Suona", "Trumpet", "Bull horn", "Flute"], "answer": "C", "task_type": "plotQA", "question_id": "Q1634"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aa204ca7c3dbe972b7a5cddaf7af1d02c08678959faadf9be019c0ee5833e77f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the group of people in white blowing?\n(A) Suona\n(B) Trumpet\n(C) Bull horn\n(D) Flute\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What are the group of people in white blowing?\n(A) Suona\n(B) Trumpet\n(C) Bull horn\n(D) Flute\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1638, "doc": {"video_name": "movie101_112.mp4", "duration": 322.0, "question": "What is the final fate of the person holding the pipe?\n(A) He was shot dead by an arrow\n(B) He escaped\n(C) He was captured\n(D) He was killed by a sword\n", "candidates": ["He was shot dead by an arrow", "He escaped", "He was captured", "He was killed by a sword"], "answer": "A", "task_type": "plotQA", "question_id": "Q1638"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e342ada980ed12fa5eb3876658123c726c0e662cb0842850c5b2d95bda5f6195", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the final fate of the person holding the pipe?\n(A) He was shot dead by an arrow\n(B) He escaped\n(C) He was captured\n(D) He was killed by a sword\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What is the final fate of the person holding the pipe?\n(A) He was shot dead by an arrow\n(B) He escaped\n(C) He was captured\n(D) He was killed by a sword\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1642, "doc": {"video_name": "movie101_25.mp4", "duration": 737.0, "question": "What color is the vehicle driving on the road in the movie?\n(A) Red\n(B) White\n(C) Yellow\n(D) Blue\n", "candidates": ["Red", "White", "Yellow", "Blue"], "answer": "A", "task_type": "plotQA", "question_id": "Q1642"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "efa4a752a432e34701e5b5908f4613015df126d37157439473a29a30081ce8e9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the vehicle driving on the road in the movie?\n(A) Red\n(B) White\n(C) Yellow\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the vehicle driving on the road in the movie?\n(A) Red\n(B) White\n(C) Yellow\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1646, "doc": {"video_name": "haimian_10.mp4", "duration": 361.0, "question": "What is the reaction of the cartoon animals to the cartoon sponge hanging in the air?\n(A) Laughing\n(B) Panicking\n(C) Expressionless\n(D) Crying\n", "candidates": ["Laughing", "Panicking", "Expressionless", "Crying"], "answer": "A", "task_type": "plotQA", "question_id": "Q1646"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b4e345d4a2fbcdf5aa67a56d56e6631a844d667b09b6928890ab7fc6b1a06c8a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the reaction of the cartoon animals to the cartoon sponge hanging in the air?\n(A) Laughing\n(B) Panicking\n(C) Expressionless\n(D) Crying\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What is the reaction of the cartoon animals to the cartoon sponge hanging in the air?\n(A) Laughing\n(B) Panicking\n(C) Expressionless\n(D) Crying\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1650, "doc": {"video_name": "en_tv_20.mp4", "duration": 426.0, "question": "What color is the hair of the woman who walks into the room in the movie?\n(A) Yellow\n(B) Green\n(C) White\n(D) Black\n", "candidates": ["Yellow", "Green", "White", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1650"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "098dae2b2853bc02838f954a13475f22a1008224f867bd01fbfc7a8fcfae9b84", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the hair of the woman who walks into the room in the movie?\n(A) Yellow\n(B) Green\n(C) White\n(D) Black\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the hair of the woman who walks into the room in the movie?\n(A) Yellow\n(B) Green\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1654, "doc": {"video_name": "movie101_83.mp4", "duration": 640.0, "question": "What type of film is this?\n(A) Mystery\n(B) Comedy\n(C) Romance\n(D) Action\n", "candidates": ["Mystery", "Comedy", "Romance", "Action"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1654"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e4fe16c96d544aa68b238ce9c95bfa53ae2df5dfbc914874e381ea3af73057de", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Mystery\n(B) Comedy\n(C) Romance\n(D) Action\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of film is this?\n(A) Mystery\n(B) Comedy\n(C) Romance\n(D) Action\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1658, "doc": {"video_name": "movie101_86.mp4", "duration": 611.0, "question": "What story does the whole video tell?\n(A) Criminal Investigation\n(B) Wedding Scene\n(C) Drama Performance\n(D) Chase Incident\n", "candidates": ["Criminal Investigation", "Wedding Scene", "Drama Performance", "Chase Incident"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1658"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "71eb365070237a2148ad54d5b9e4417d619e3becaa264518cc83ef834eb24126", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What story does the whole video tell?\n(A) Criminal Investigation\n(B) Wedding Scene\n(C) Drama Performance\n(D) Chase Incident\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "A"}, "input": "What story does the whole video tell?\n(A) Criminal Investigation\n(B) Wedding Scene\n(C) Drama Performance\n(D) Chase Incident\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1662, "doc": {"video_name": "movie101_36.mp4", "duration": 415.0, "question": "What is the setting of the scene in the video?\n(A) City\n(B) Island\n(C) Snowy Mountain\n(D) Forest\n", "candidates": ["City", "Island", "Snowy Mountain", "Forest"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1662"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fbb7a8224ddb299ad818023b5927689303d7d925a0841fd20deca5409aba91af", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the setting of the scene in the video?\n(A) City\n(B) Island\n(C) Snowy Mountain\n(D) Forest\n", "task_type": "topic_reasoning", "pred_answer": "C.", "answer": "A"}, "input": "What is the setting of the scene in the video?\n(A) City\n(B) Island\n(C) Snowy Mountain\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1666, "doc": {"video_name": "232.mp4", "duration": 480.0, "question": "What is the video related to?\n(A) The video is related to traditional culture\n(B) The video is related to holidays\n(C) The video is related to nature\n(D) The video is related to food\n", "candidates": ["The video is related to traditional culture", "The video is related to holidays", "The video is related to nature", "The video is related to food"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1666"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "070cab17b66aad24f25f26b1e170f4d60545af4b29e5cd1c575988251dcb7636", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the video related to?\n(A) The video is related to traditional culture\n(B) The video is related to holidays\n(C) The video is related to nature\n(D) The video is related to food\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the video related to?\n(A) The video is related to traditional culture\n(B) The video is related to holidays\n(C) The video is related to nature\n(D) The video is related to food\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1670, "doc": {"video_name": "movie101_72.mp4", "duration": 289.0, "question": "In what setting does the majority of the video take place?\n(A) Ancient Folk\n(B) Modern City\n(C) Modern Rural\n(D) Ancient Palace\n", "candidates": ["Ancient Folk", "Modern City", "Modern Rural", "Ancient Palace"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1670"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f1353b529050ebb90ab9794cf381ccc2548b079b5f808c0676da85f7e1afa4f6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what setting does the majority of the video take place?\n(A) Ancient Folk\n(B) Modern City\n(C) Modern Rural\n(D) Ancient Palace\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "In what setting does the majority of the video take place?\n(A) Ancient Folk\n(B) Modern City\n(C) Modern Rural\n(D) Ancient Palace\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1674, "doc": {"video_name": "xiaoliyu_1.mp4", "duration": 381.0, "question": "What is the background of the video?\n(A) Forest\n(B) Desert\n(C) Underwater\n(D) Beach\n", "candidates": ["Forest", "Desert", "Underwater", "Beach"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1674"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e25cc3d7cd5c58482d1c39038fb1e00f1dbdde3dba9b89b526ea3228e4e773e8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the background of the video?\n(A) Forest\n(B) Desert\n(C) Underwater\n(D) Beach\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the background of the video?\n(A) Forest\n(B) Desert\n(C) Underwater\n(D) Beach\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1678, "doc": {"video_name": "movie101_69.mp4", "duration": 385.0, "question": "What event is depicted in the entire video?\n(A) Police drug bust\n(B) Technology research\n(C) Love story\n(D) Action fight\n", "candidates": ["Police drug bust", "Technology research", "Love story", "Action fight"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1678"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0cd39588f8089ea850ac252563711d9270c30838bd5b482750c1e72e27224f1f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What event is depicted in the entire video?\n(A) Police drug bust\n(B) Technology research\n(C) Love story\n(D) Action fight\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What event is depicted in the entire video?\n(A) Police drug bust\n(B) Technology research\n(C) Love story\n(D) Action fight\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1682, "doc": {"video_name": "AWA-12.mp4", "duration": 450.0, "question": "What is the main environment in the video?\n(A) Grassland\n(B) Gobi\n(C) Forest\n(D) Desert\n", "candidates": ["Grassland", "Gobi", "Forest", "Desert"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1682"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c3be15984fcdee6eda1a2adc12b5a74d38d125f516a9fc1edadeb91c967c3370", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main environment in the video?\n(A) Grassland\n(B) Gobi\n(C) Forest\n(D) Desert\n", "task_type": "topic_reasoning", "pred_answer": "A.", "answer": "A"}, "input": "What is the main environment in the video?\n(A) Grassland\n(B) Gobi\n(C) Forest\n(D) Desert\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1686, "doc": {"video_name": "tomjerry_11.mp4", "duration": 322.0, "question": "What character appears the most in the video?\n(A) Cartoon fish\n(B) Cartoon dog\n(C) Cartoon bear\n(D) Cartoon mouse\n", "candidates": ["Cartoon fish", "Cartoon dog", "Cartoon bear", "Cartoon mouse"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1686"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ade7f68cdc115c68eadf84f42c7b17bbd0065664050d5a655b53d2269537e120", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What character appears the most in the video?\n(A) Cartoon fish\n(B) Cartoon dog\n(C) Cartoon bear\n(D) Cartoon mouse\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What character appears the most in the video?\n(A) Cartoon fish\n(B) Cartoon dog\n(C) Cartoon bear\n(D) Cartoon mouse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1690, "doc": {"video_name": "movie101_16.mp4", "duration": 740.0, "question": "What type of movie is the scene in the video?\n(A) Science Fiction\n(B) Comedy\n(C) Horror\n(D) Documentary\n", "candidates": ["Science Fiction", "Comedy", "Horror", "Documentary"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1690"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05102071922fc3c7bc164edd1bc08e65ab201d1ac3a736b1da3a74ba5b0fc3d4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of movie is the scene in the video?\n(A) Science Fiction\n(B) Comedy\n(C) Horror\n(D) Documentary\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "D"}, "input": "What type of movie is the scene in the video?\n(A) Science Fiction\n(B) Comedy\n(C) Horror\n(D) Documentary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1694, "doc": {"video_name": "AWD-7.mp4", "duration": 450.13, "question": "What type of video is this?\n(A) Traditional Festivals\n(B) Food Flavor\n(C) History and Culture\n(D) Natural Science\n", "candidates": ["Traditional Festivals", "Food Flavor", "History and Culture", "Natural Science"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1694"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6f522caebaed3208c2dfbd3994ca56d0bdc91f3c0c195ae85ed43ee95794a823", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Traditional Festivals\n(B) Food Flavor\n(C) History and Culture\n(D) Natural Science\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of video is this?\n(A) Traditional Festivals\n(B) Food Flavor\n(C) History and Culture\n(D) Natural Science\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1698, "doc": {"video_name": "ego_12.mp4", "duration": 480.02, "question": "What is the weather like in the video?\n(A) Snowy\n(B) Windy\n(C) Sunny\n(D) Rainy\n", "candidates": ["Snowy", "Windy", "Sunny", "Rainy"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1698"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2367a0e4803d95add2a4c5b60fedeca6438f1a84d184d00a6fe92760a99c8a31", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the weather like in the video?\n(A) Snowy\n(B) Windy\n(C) Sunny\n(D) Rainy\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the weather like in the video?\n(A) Snowy\n(B) Windy\n(C) Sunny\n(D) Rainy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1702, "doc": {"video_name": "movie101_22.mp4", "duration": 335.0, "question": "What type of movie is the scene in the video from?\n(A) Horror\n(B) Comedy\n(C) Historical drama\n(D) Science fiction\n", "candidates": ["Horror", "Comedy", "Historical drama", "Science fiction"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1702"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8bd0b90bce57093eb0be4895c6839b79aab87f29e759d406fa7dfc9add002e08", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of movie is the scene in the video from?\n(A) Horror\n(B) Comedy\n(C) Historical drama\n(D) Science fiction\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of movie is the scene in the video from?\n(A) Horror\n(B) Comedy\n(C) Historical drama\n(D) Science fiction\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1706, "doc": {"video_name": "AWE-4.mp4", "duration": 450.0, "question": "What is the main content of the video related to?\n(A) Weather\n(B) Food\n(C) Animals\n(D) Plants\n", "candidates": ["Weather", "Food", "Animals", "Plants"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1706"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "21127855bc87e25825e315bb01c50ab5093120f317ee72c439555da6cff49d8c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the main content of the video related to?\n(A) Weather\n(B) Food\n(C) Animals\n(D) Plants\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the main content of the video related to?\n(A) Weather\n(B) Food\n(C) Animals\n(D) Plants\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1710, "doc": {"video_name": "223.mp4", "duration": 480.0, "question": "What type of video is this?\n(A) Natural Science\n(B) Food Culture\n(C) Natural Animals\n(D) Traditional Customs\n", "candidates": ["Natural Science", "Food Culture", "Natural Animals", "Traditional Customs"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1710"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e7492865c0b2ee8450476496a912faf314e24fd3a9b81884b20a84b1bb3e77c8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Natural Science\n(B) Food Culture\n(C) Natural Animals\n(D) Traditional Customs\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What type of video is this?\n(A) Natural Science\n(B) Food Culture\n(C) Natural Animals\n(D) Traditional Customs\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1714, "doc": {"video_name": "movie101_111.mp4", "duration": 720.0, "question": "What is the main plot shown in the video?\n(A) Police solving a case\n(B) Basketball match\n(C) Friends gathering\n(D) Traveling and sightseeing\n", "candidates": ["Police solving a case", "Basketball match", "Friends gathering", "Traveling and sightseeing"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1714"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "39b362fdaaade5253702820a55f29772a03e39a40e71125a47dde184a5147adc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main plot shown in the video?\n(A) Police solving a case\n(B) Basketball match\n(C) Friends gathering\n(D) Traveling and sightseeing\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the main plot shown in the video?\n(A) Police solving a case\n(B) Basketball match\n(C) Friends gathering\n(D) Traveling and sightseeing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1718, "doc": {"video_name": "en_tv_2.mp4", "duration": 509.0, "question": "What type of video is this?\n(A) Movie clip\n(B) Music video\n(C) Cartoon\n(D) Documentary\n", "candidates": ["Movie clip", "Music video", "Cartoon", "Documentary"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1718"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "33716c4325e845150aefcb1c0d86f8d67fe5f3bffcc4631de88eef724860bdf5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Movie clip\n(B) Music video\n(C) Cartoon\n(D) Documentary\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What type of video is this?\n(A) Movie clip\n(B) Music video\n(C) Cartoon\n(D) Documentary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1722, "doc": {"video_name": "movie101_68.mp4", "duration": 445.0, "question": "What is the genre of this movie?\n(A) Romance\n(B) Comedy\n(C) Science Fiction\n(D) Mystery\n", "candidates": ["Romance", "Comedy", "Science Fiction", "Mystery"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1722"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c2a0ccbcc886b88cff2b1a92c5c3eb0a3847de8e49201b2051c2f49f2a900b99", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the genre of this movie?\n(A) Romance\n(B) Comedy\n(C) Science Fiction\n(D) Mystery\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "B"}, "input": "What is the genre of this movie?\n(A) Romance\n(B) Comedy\n(C) Science Fiction\n(D) Mystery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1726, "doc": {"video_name": "xiaoliyu_6.mp4", "duration": 400.0, "question": "What is the relationship between the cartoon characters of carp, jellyfish, seahorse, and turtle?\n(A) Friends\n(B) Lovers\n(C) Teacher-student\n(D) Enemies\n", "candidates": ["Friends", "Lovers", "Teacher-student", "Enemies"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1726"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7076bf94c8ccc77161085ecb0048b0959e9285bac37b0e9c976633c9ecf57322", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the relationship between the cartoon characters of carp, jellyfish, seahorse, and turtle?\n(A) Friends\n(B) Lovers\n(C) Teacher-student\n(D) Enemies\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the relationship between the cartoon characters of carp, jellyfish, seahorse, and turtle?\n(A) Friends\n(B) Lovers\n(C) Teacher-student\n(D) Enemies\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1730, "doc": {"video_name": "206.mp4", "duration": 480.0, "question": "What type of video is this?\n(A) Science Fiction\n(B) Comedy\n(C) Animal\n(D) Action\n", "candidates": ["Science Fiction", "Comedy", "Animal", "Action"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1730"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b4538d87aa874cd09d642f2f50af71a1cc6ad84a624e6bf098c94afbfe4ce032", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Science Fiction\n(B) Comedy\n(C) Animal\n(D) Action\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of video is this?\n(A) Science Fiction\n(B) Comedy\n(C) Animal\n(D) Action\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1734, "doc": {"video_name": "210.mp4", "duration": 480.0, "question": "In what environment does the main event in the video occur?\n(A) Sky\n(B) Water area\n(C) Forest\n(D) Desert\n", "candidates": ["Sky", "Water area", "Forest", "Desert"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1734"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5060bfd8b413d902295762ac9d18c3ec3b7843c74da48030e9a495ef20faa119", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In what environment does the main event in the video occur?\n(A) Sky\n(B) Water area\n(C) Forest\n(D) Desert\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "In what environment does the main event in the video occur?\n(A) Sky\n(B) Water area\n(C) Forest\n(D) Desert\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1738, "doc": {"video_name": "movie101_54.mp4", "duration": 580.0, "question": "Which activity is not included in the police's actions in the video?\n(A) Rescuing the injured\n(B) Gunfight\n(C) Arresting\n(D) Escorting prisoners\n", "candidates": ["Rescuing the injured", "Gunfight", "Arresting", "Escorting prisoners"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1738"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8dc9a53fe9b508d2cc778edffed4f676cfc92ca028a9c1c82e4cf4708632f1e1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which activity is not included in the police's actions in the video?\n(A) Rescuing the injured\n(B) Gunfight\n(C) Arresting\n(D) Escorting prisoners\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "D"}, "input": "Which activity is not included in the police's actions in the video?\n(A) Rescuing the injured\n(B) Gunfight\n(C) Arresting\n(D) Escorting prisoners\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1742, "doc": {"video_name": "en_tv_30.mp4", "duration": 584.0, "question": "Where does the story of the video take place?\n(A) Countryside\n(B) Seaside\n(C) City\n(D) Desert\n", "candidates": ["Countryside", "Seaside", "City", "Desert"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1742"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "48a8e84f9a5de05cbda39e1036b8a47483822fd7ef3c65c772da183fe8959378", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where does the story of the video take place?\n(A) Countryside\n(B) Seaside\n(C) City\n(D) Desert\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "Where does the story of the video take place?\n(A) Countryside\n(B) Seaside\n(C) City\n(D) Desert\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1746, "doc": {"video_name": "movie101_67.mp4", "duration": 530.0, "question": "What genre is this movie?\n(A) Sci-fi\n(B) Comedy\n(C) Thriller\n(D) Romance\n", "candidates": ["Sci-fi", "Comedy", "Thriller", "Romance"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1746"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e66b609e706c932ca93b02bb51e9fec27b039628beaa1a558e5d04be9e2272b2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What genre is this movie?\n(A) Sci-fi\n(B) Comedy\n(C) Thriller\n(D) Romance\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "B"}, "input": "What genre is this movie?\n(A) Sci-fi\n(B) Comedy\n(C) Thriller\n(D) Romance\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1750, "doc": {"video_name": "ego_15.mp4", "duration": 480.02, "question": "In this first-person video, what is the first-person character doing?\n(A) Making the bed\n(B) Organizing kitchen utensils\n(C) Organizing the wardrobe\n(D) Organizing the bookshelf\n", "candidates": ["Making the bed", "Organizing kitchen utensils", "Organizing the wardrobe", "Organizing the bookshelf"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1750"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6db586e3fd71789e0829b4570c7cb6e4f36f5508daf272e6b74461eb0c1e0932", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this first-person video, what is the first-person character doing?\n(A) Making the bed\n(B) Organizing kitchen utensils\n(C) Organizing the wardrobe\n(D) Organizing the bookshelf\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "In this first-person video, what is the first-person character doing?\n(A) Making the bed\n(B) Organizing kitchen utensils\n(C) Organizing the wardrobe\n(D) Organizing the bookshelf\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1754, "doc": {"video_name": "movie101_81.mp4", "duration": 252.0, "question": "In what setting does the entire video take place?\n(A) Amusement park\n(B) Library\n(C) Wedding\n(D) School\n", "candidates": ["Amusement park", "Library", "Wedding", "School"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1754"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1e6354c214f5249af755017ed2f34341a4890afd49802d8b8a3f7537dc7c293b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what setting does the entire video take place?\n(A) Amusement park\n(B) Library\n(C) Wedding\n(D) School\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "In what setting does the entire video take place?\n(A) Amusement park\n(B) Library\n(C) Wedding\n(D) School\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1758, "doc": {"video_name": "xiaoliyu_5.mp4", "duration": 415.0, "question": "What is the main task of the three cartoon animals in the entire clip?\n(A) Playing\n(B) Going home\n(C) Arguing\n(D) Looking for the cartoon turtle\n", "candidates": ["Playing", "Going home", "Arguing", "Looking for the cartoon turtle"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1758"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "15fb1cc3db876d6dd7a3f77c45ad4778408d04e8c70384eb22bb245ab466af5d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the main task of the three cartoon animals in the entire clip?\n(A) Playing\n(B) Going home\n(C) Arguing\n(D) Looking for the cartoon turtle\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the main task of the three cartoon animals in the entire clip?\n(A) Playing\n(B) Going home\n(C) Arguing\n(D) Looking for the cartoon turtle\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1762, "doc": {"video_name": "movie101_0.mp4", "duration": 789.0, "question": "What is the weather in the video?\n(A) Sunny\n(B) Blizzard\n(C) Heavy Rain\n(D) Overcast\n", "candidates": ["Sunny", "Blizzard", "Heavy Rain", "Overcast"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1762"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A) Sunny"]], "filtered_resps": ["A) Sunny"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "829e746f4d4c361ce93a32f641fb93571720793eb10c2a796262517b60b650f6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the weather in the video?\n(A) Sunny\n(B) Blizzard\n(C) Heavy Rain\n(D) Overcast\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the weather in the video?\n(A) Sunny\n(B) Blizzard\n(C) Heavy Rain\n(D) Overcast\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1766, "doc": {"video_name": "movie101_68.mp4", "duration": 445.0, "question": "In what environment does the story take place?\n(A) Snowy mountains\n(B) Ocean\n(C) River\n(D) Forest\n", "candidates": ["Snowy mountains", "Ocean", "River", "Forest"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1766"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8e4d0c7c3f275a5b3d8e6d0b002fa77e300b0e1e04a6cca0c610ad56ca238a3b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what environment does the story take place?\n(A) Snowy mountains\n(B) Ocean\n(C) River\n(D) Forest\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "In what environment does the story take place?\n(A) Snowy mountains\n(B) Ocean\n(C) River\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1770, "doc": {"video_name": "movie101_84.mp4", "duration": 387.0, "question": "What is the main setting shown in the video?\n(A) Desert\n(B) City\n(C) Ocean\n(D) Forest\n", "candidates": ["Desert", "City", "Ocean", "Forest"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1770"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d60a6f22693e35047c0cb0cb90c73d2308d1c2d0110668d644271748a0d8d014", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main setting shown in the video?\n(A) Desert\n(B) City\n(C) Ocean\n(D) Forest\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the main setting shown in the video?\n(A) Desert\n(B) City\n(C) Ocean\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1774, "doc": {"video_name": "movie101_25.mp4", "duration": 737.0, "question": "What is the genre of the movie clip?\n(A) Horror\n(B) Modern\n(C) War\n(D) Comedy\n", "candidates": ["Horror", "Modern", "War", "Comedy"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1774"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c81712d0d7aa8fda4f0be47abbfc7da4cde1d3e1a175f3b0837a7026427f4bc5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the genre of the movie clip?\n(A) Horror\n(B) Modern\n(C) War\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the genre of the movie clip?\n(A) Horror\n(B) Modern\n(C) War\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1778, "doc": {"video_name": "haimian_8.mp4", "duration": 445.0, "question": "What type of video is this?\n(A) Mystery\n(B) Cartoon\n(C) Science Fiction\n(D) Thriller\n", "candidates": ["Mystery", "Cartoon", "Science Fiction", "Thriller"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1778"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8976cbe38d0f5d1bc97aca5687fc5d4155d47e90a5042ae8ce03575749d06415", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Mystery\n(B) Cartoon\n(C) Science Fiction\n(D) Thriller\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Mystery\n(B) Cartoon\n(C) Science Fiction\n(D) Thriller\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1782, "doc": {"video_name": "AWC-10.mp4", "duration": 450.0, "question": "What type of video is this?\n(A) Traditional Festivals\n(B) Natural Science Popularization\n(C) Food Flavor\n(D) Historical Culture\n", "candidates": ["Traditional Festivals", "Natural Science Popularization", "Food Flavor", "Historical Culture"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1782"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3eadf4cfd573f1417085ac73db44f83be0e7e5d35fce77b2b7943ef178d0e52c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Traditional Festivals\n(B) Natural Science Popularization\n(C) Food Flavor\n(D) Historical Culture\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Traditional Festivals\n(B) Natural Science Popularization\n(C) Food Flavor\n(D) Historical Culture\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1786, "doc": {"video_name": "movie101_32.mp4", "duration": 247.0, "question": "What is the genre of this film clip?\n(A) Comedy\n(B) War\n(C) Horror\n(D) Modern\n", "candidates": ["Comedy", "War", "Horror", "Modern"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1786"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3a95acd99e2a25fc90886b8b40c634f689e576aa5cf06358fc62b99bfe93cd12", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the genre of this film clip?\n(A) Comedy\n(B) War\n(C) Horror\n(D) Modern\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the genre of this film clip?\n(A) Comedy\n(B) War\n(C) Horror\n(D) Modern\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1790, "doc": {"video_name": "AWG-6.mp4", "duration": 450.0, "question": "What is the main content related to in the video?\n(A) Animals\n(B) Food\n(C) Weather\n(D) Plants\n", "candidates": ["Animals", "Food", "Weather", "Plants"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1790"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f40fb8d9b75d1b050a876feebe1909fbc38025cf62a57e6e29c254952d99e8a7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main content related to in the video?\n(A) Animals\n(B) Food\n(C) Weather\n(D) Plants\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the main content related to in the video?\n(A) Animals\n(B) Food\n(C) Weather\n(D) Plants\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1794, "doc": {"video_name": "movie101_87.mp4", "duration": 599.0, "question": "What is the main setting shown in the video?\n(A) Hospital\n(B) Campus\n(C) Countryside\n(D) Temple\n", "candidates": ["Hospital", "Campus", "Countryside", "Temple"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1794"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2bf0d560a38ef7e6c3ba9beff43b43c1ea9a8879163bad64b33cda5493fe416e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main setting shown in the video?\n(A) Hospital\n(B) Campus\n(C) Countryside\n(D) Temple\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the main setting shown in the video?\n(A) Hospital\n(B) Campus\n(C) Countryside\n(D) Temple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1798, "doc": {"video_name": "1.mp4", "duration": 401.23, "question": "In what environment is the man in the black sweater?\n(A) Desert\n(B) Grassland\n(C) Forest\n(D) Ocean\n", "candidates": ["Desert", "Grassland", "Forest", "Ocean"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1798"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "134db031ddabc5fe5e755be56824e67e302412a97c2d1f087993ad2edb991397", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what environment is the man in the black sweater?\n(A) Desert\n(B) Grassland\n(C) Forest\n(D) Ocean\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "In what environment is the man in the black sweater?\n(A) Desert\n(B) Grassland\n(C) Forest\n(D) Ocean\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1802, "doc": {"video_name": "en_tv_25.mp4", "duration": 478.0, "question": "What genre of film is the clip in the video from?\n(A) War film\n(B) Horror film\n(C) Documentary\n(D) Action film\n", "candidates": ["War film", "Horror film", "Documentary", "Action film"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1802"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e6122766ff82f5ea36dfc889dc8a87b9847ecadf858ef3aa126b370e9945ffc2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What genre of film is the clip in the video from?\n(A) War film\n(B) Horror film\n(C) Documentary\n(D) Action film\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What genre of film is the clip in the video from?\n(A) War film\n(B) Horror film\n(C) Documentary\n(D) Action film\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1806, "doc": {"video_name": "AWB-11.mp4", "duration": 450.0, "question": "What is the main environment in the video?\n(A) Desert\n(B) Gobi\n(C) Forest\n(D) Grassland\n", "candidates": ["Desert", "Gobi", "Forest", "Grassland"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1806"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D."]], "filtered_resps": ["D."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6e20bb8c6e407dc9e19d12a849a1bd58532eabd90dbebe768431762f3e4e0aad", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the main environment in the video?\n(A) Desert\n(B) Gobi\n(C) Forest\n(D) Grassland\n", "task_type": "topic_reasoning", "pred_answer": "D.", "answer": "D"}, "input": "What is the main environment in the video?\n(A) Desert\n(B) Gobi\n(C) Forest\n(D) Grassland\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1810, "doc": {"video_name": "AWA-4.mp4", "duration": 450.0, "question": "What type of video is this?\n(A) Food and Flavors\n(B) Lifestyle\n(C) Traditional Festivals\n(D) Nature and Animals\n", "candidates": ["Food and Flavors", "Lifestyle", "Traditional Festivals", "Nature and Animals"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1810"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "037ea38b5db3d84c9aa2aad8733b12913f6c04a7067529a20661fc7d5b7be574", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Food and Flavors\n(B) Lifestyle\n(C) Traditional Festivals\n(D) Nature and Animals\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of video is this?\n(A) Food and Flavors\n(B) Lifestyle\n(C) Traditional Festivals\n(D) Nature and Animals\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1814, "doc": {"video_name": "ego_11.mp4", "duration": 480.02, "question": "Where is the person in the video?\n(A) Living room\n(B) Lounge\n(C) Hall\n(D) Living room\n", "candidates": ["Living room", "Lounge", "Hall", "Living room"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1814"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07e3e999520ee5d113e8b427ae0882121ac076fe180760b93f01b6d7533ba74f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where is the person in the video?\n(A) Living room\n(B) Lounge\n(C) Hall\n(D) Living room\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "A"}, "input": "Where is the person in the video?\n(A) Living room\n(B) Lounge\n(C) Hall\n(D) Living room\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1818, "doc": {"video_name": "movie101_39.mp4", "duration": 473.0, "question": "What is the genre of the film clip?\n(A) War\n(B) Horror\n(C) Action\n(D) Comedy\n", "candidates": ["War", "Horror", "Action", "Comedy"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1818"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9abdd3dbe8b435264d56379c3b0c2d6042b05acd67ad04e081f1bc5d91d70966", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the genre of the film clip?\n(A) War\n(B) Horror\n(C) Action\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the genre of the film clip?\n(A) War\n(B) Horror\n(C) Action\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1822, "doc": {"video_name": "movie101_23.mp4", "duration": 473.0, "question": "What type of movie is the scene in the video?\n(A) Action\n(B) Horror\n(C) Science Fiction\n(D) Documentary\n", "candidates": ["Action", "Horror", "Science Fiction", "Documentary"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1822"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85f8cd160c8576a28c8c5327cbecbe878992bf16c2a4511492d6a56a5aa3394d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of movie is the scene in the video?\n(A) Action\n(B) Horror\n(C) Science Fiction\n(D) Documentary\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "D"}, "input": "What type of movie is the scene in the video?\n(A) Action\n(B) Horror\n(C) Science Fiction\n(D) Documentary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1826, "doc": {"video_name": "movie101_109.mp4", "duration": 670.0, "question": "What is the setting of the video?\n(A) Ocean\n(B) Forest\n(C) Prairie\n(D) City\n", "candidates": ["Ocean", "Forest", "Prairie", "City"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1826"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "99862cd8fc1101b4be6e8a540458c1102c1ec02cb6be38101a62a5c31eb55a5b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the setting of the video?\n(A) Ocean\n(B) Forest\n(C) Prairie\n(D) City\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the setting of the video?\n(A) Ocean\n(B) Forest\n(C) Prairie\n(D) City\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1830, "doc": {"video_name": "movie101_105.mp4", "duration": 744.0, "question": "What event is primarily narrated in the video?\n(A) Tsunami\n(B) People's beautiful life is disrupted by an earthquake\n(C) Flood\n(D) Sandstorm\n", "candidates": ["Tsunami", "People's beautiful life is disrupted by an earthquake", "Flood", "Sandstorm"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1830"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3dfc79cb3cb899c92f99dfa71237ff61c6c45be6539fcff8a245184cba217c1d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What event is primarily narrated in the video?\n(A) Tsunami\n(B) People's beautiful life is disrupted by an earthquake\n(C) Flood\n(D) Sandstorm\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What event is primarily narrated in the video?\n(A) Tsunami\n(B) People's beautiful life is disrupted by an earthquake\n(C) Flood\n(D) Sandstorm\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1834, "doc": {"video_name": "en_tv_15.mp4", "duration": 541.0, "question": "In what kind of setting does the scene in the video take place?\n(A) Snowy mountain\n(B) Island\n(C) City\n(D) Forest\n", "candidates": ["Snowy mountain", "Island", "City", "Forest"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1834"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ee58d2943c07bb87086f83189a4bf179f5eb59605438252c24e7db19cfcc2c36", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what kind of setting does the scene in the video take place?\n(A) Snowy mountain\n(B) Island\n(C) City\n(D) Forest\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "In what kind of setting does the scene in the video take place?\n(A) Snowy mountain\n(B) Island\n(C) City\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1838, "doc": {"video_name": "movie101_108.mp4", "duration": 330.0, "question": "What is the main subject shown in the video?\n(A) Two little boys\n(B) Three little girls\n(C) Two little girls\n(D) A boy and a girl\n", "candidates": ["Two little boys", "Three little girls", "Two little girls", "A boy and a girl"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1838"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a0c83b738ff3361f71b8a9895ce0c89e3f27778f3322b1591de7c84669f7b8a1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the main subject shown in the video?\n(A) Two little boys\n(B) Three little girls\n(C) Two little girls\n(D) A boy and a girl\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the main subject shown in the video?\n(A) Two little boys\n(B) Three little girls\n(C) Two little girls\n(D) A boy and a girl\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1842, "doc": {"video_name": "movie101_40.mp4", "duration": 450.0, "question": "What genre of movie clip is this?\n(A) Horror\n(B) War\n(C) Action\n(D) Comedy\n", "candidates": ["Horror", "War", "Action", "Comedy"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1842"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "84338476e7cc913fa32b23182ee49540433564de0606f48ae1f983d6b132e63c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What genre of movie clip is this?\n(A) Horror\n(B) War\n(C) Action\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What genre of movie clip is this?\n(A) Horror\n(B) War\n(C) Action\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1846, "doc": {"video_name": "AWB-10.mp4", "duration": 450.0, "question": "What is the main environment in the video?\n(A) Desert\n(B) Grassland\n(C) Gobi\n(D) Forest\n", "candidates": ["Desert", "Grassland", "Gobi", "Forest"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1846"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B."]], "filtered_resps": ["B."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "36ec218ad86265f98574a905340e6d0db57074d5c5cc186b2b19c851bbb25458", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main environment in the video?\n(A) Desert\n(B) Grassland\n(C) Gobi\n(D) Forest\n", "task_type": "topic_reasoning", "pred_answer": "B.", "answer": "B"}, "input": "What is the main environment in the video?\n(A) Desert\n(B) Grassland\n(C) Gobi\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1850, "doc": {"video_name": "haimian_2.mp4", "duration": 356.0, "question": "Who is the main character of the video?\n(A) Cartoon Whale\n(B) Cartoon Starfish\n(C) Cartoon Sponge and Cartoon Octopus\n(D) Cartoon Shark\n", "candidates": ["Cartoon Whale", "Cartoon Starfish", "Cartoon Sponge and Cartoon Octopus", "Cartoon Shark"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1850"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "86292b5a92715662653f2798fa2dc97544732cc8be63b0f594ab23ab74427a49", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who is the main character of the video?\n(A) Cartoon Whale\n(B) Cartoon Starfish\n(C) Cartoon Sponge and Cartoon Octopus\n(D) Cartoon Shark\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "Who is the main character of the video?\n(A) Cartoon Whale\n(B) Cartoon Starfish\n(C) Cartoon Sponge and Cartoon Octopus\n(D) Cartoon Shark\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1854, "doc": {"video_name": "movie101_35.mp4", "duration": 481.0, "question": "What is the weather in the video scene?\n(A) Snowy\n(B) Foggy\n(C) Rainy\n(D) Sunny\n", "candidates": ["Snowy", "Foggy", "Rainy", "Sunny"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1854"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e3ce934fd5a010a1f39607d1d53cb8de603a049a28904a63ce0719690116a444", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the weather in the video scene?\n(A) Snowy\n(B) Foggy\n(C) Rainy\n(D) Sunny\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "D"}, "input": "What is the weather in the video scene?\n(A) Snowy\n(B) Foggy\n(C) Rainy\n(D) Sunny\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1858, "doc": {"video_name": "ego_21.mp4", "duration": 480.02, "question": "Where is the person in the video?\n(A) Bedroom\n(B) Living room\n(C) Kitchen\n(D) Bathroom\n", "candidates": ["Bedroom", "Living room", "Kitchen", "Bathroom"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1858"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "636e2e97d35a140bd2dcdcc678ca906adf5c904c920e5d12a5d8830e5a5eeca7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where is the person in the video?\n(A) Bedroom\n(B) Living room\n(C) Kitchen\n(D) Bathroom\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "Where is the person in the video?\n(A) Bedroom\n(B) Living room\n(C) Kitchen\n(D) Bathroom\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1862, "doc": {"video_name": "AWA-5.mp4", "duration": 450.0, "question": "What is the main setting of the video?\n(A) City\n(B) Grassland\n(C) Desert\n(D) Ocean\n", "candidates": ["City", "Grassland", "Desert", "Ocean"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1862"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c7bb478e491341ce2cc844c35ff904b8d493584ef68afa6e3d0dd0788535c63d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main setting of the video?\n(A) City\n(B) Grassland\n(C) Desert\n(D) Ocean\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the main setting of the video?\n(A) City\n(B) Grassland\n(C) Desert\n(D) Ocean\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1866, "doc": {"video_name": "231.mp4", "duration": 480.0, "question": "What scenery is mainly shown in the video?\n(A) Space scenery\n(B) Grassland scenery\n(C) Ocean scenery\n(D) Desert scenery\n", "candidates": ["Space scenery", "Grassland scenery", "Ocean scenery", "Desert scenery"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1866"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5051cca3e4ddf250ca23700f6854a1ce0877e7750d4a9d5704c03e90df138c62", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What scenery is mainly shown in the video?\n(A) Space scenery\n(B) Grassland scenery\n(C) Ocean scenery\n(D) Desert scenery\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What scenery is mainly shown in the video?\n(A) Space scenery\n(B) Grassland scenery\n(C) Ocean scenery\n(D) Desert scenery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1870, "doc": {"video_name": "movie101_79.mp4", "duration": 590.0, "question": "What type of film is this?\n(A) Thriller\n(B) Action\n(C) Mystery\n(D) Comedy\n", "candidates": ["Thriller", "Action", "Mystery", "Comedy"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1870"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ca6441dfa2905e5f470eaebb50916fac05f2c6c212f7d7b58e49f1a8acff4e89", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Thriller\n(B) Action\n(C) Mystery\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of film is this?\n(A) Thriller\n(B) Action\n(C) Mystery\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1874, "doc": {"video_name": "game_5.mp4", "duration": 208.05, "question": "What are the characteristics of the object being built in the video?\n(A) green\n(B) high\n(C) solid\n(D) cylindroid\n", "candidates": ["green", "high", "solid", "cylindroid"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1874"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8141cf670ac7f388434cca96cdd64814303ccc1d0bc53cf812e7ddcb830b1bdd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What are the characteristics of the object being built in the video?\n(A) green\n(B) high\n(C) solid\n(D) cylindroid\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "B"}, "input": "What are the characteristics of the object being built in the video?\n(A) green\n(B) high\n(C) solid\n(D) cylindroid\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1878, "doc": {"video_name": "game_43.mp4", "duration": 229.04, "question": "What is the person in the game doing?\n(A) Building an automatic farm\n(B) Fighting with a game boss\n(C) Exploring a haunted house\n(D) Designing a character's outfit\n", "candidates": ["Building an automatic farm", "Fighting with a game boss", "Exploring a haunted house", "Designing a character's outfit"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1878"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2a30e18e2f802342e1600573e9230d2beff2e99fe0237851279acfd10328a121", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the person in the game doing?\n(A) Building an automatic farm\n(B) Fighting with a game boss\n(C) Exploring a haunted house\n(D) Designing a character's outfit\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the person in the game doing?\n(A) Building an automatic farm\n(B) Fighting with a game boss\n(C) Exploring a haunted house\n(D) Designing a character's outfit\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1882, "doc": {"video_name": "game_22.mp4", "duration": 253.68, "question": "What is being built in this video?\n(A) A floating platform\n(B) An underwater base\n(C) A treehouse\n(D) A fortress\n", "candidates": ["A floating platform", "An underwater base", "A treehouse", "A fortress"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1882"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "12912e1d43f9a870128d2de1346c7d96d5bb267a1cacdcfc3d4d94fc1a904675", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is being built in this video?\n(A) A floating platform\n(B) An underwater base\n(C) A treehouse\n(D) A fortress\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is being built in this video?\n(A) A floating platform\n(B) An underwater base\n(C) A treehouse\n(D) A fortress\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1886, "doc": {"video_name": "game_11.mp4", "duration": 264.89, "question": "What is the type of this video?\n(A) Minecraft gameplay video\n(B) Minecraft strategy guide\n(C) Minecraft mod review\n(D) Minecraft developer diary\n", "candidates": ["Minecraft gameplay video", "Minecraft strategy guide", "Minecraft mod review", "Minecraft developer diary"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1886"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5f398a0820dc550ae0ec47a9776a35f676f59e7c8cb8e68b96435a80792c965d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the type of this video?\n(A) Minecraft gameplay video\n(B) Minecraft strategy guide\n(C) Minecraft mod review\n(D) Minecraft developer diary\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the type of this video?\n(A) Minecraft gameplay video\n(B) Minecraft strategy guide\n(C) Minecraft mod review\n(D) Minecraft developer diary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1890, "doc": {"video_name": "game_38.mp4", "duration": 331.3, "question": "What is the video mainly about?\n(A) A person livestreaming product promotion\n(B) A person demonstrating how to build a castle in the game.\n(C) A person demonstrating a jungle crossing in the game\n(D) A person demonstrating a castle adventure in the game\n", "candidates": ["A person livestreaming product promotion", "A person demonstrating how to build a castle in the game.", "A person demonstrating a jungle crossing in the game", "A person demonstrating a castle adventure in the game"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1890"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "155dee1aa37dbe7b752034ad8cd411378c776b5792677879763f6a5138b650d3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the video mainly about?\n(A) A person livestreaming product promotion\n(B) A person demonstrating how to build a castle in the game.\n(C) A person demonstrating a jungle crossing in the game\n(D) A person demonstrating a castle adventure in the game\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the video mainly about?\n(A) A person livestreaming product promotion\n(B) A person demonstrating how to build a castle in the game.\n(C) A person demonstrating a jungle crossing in the game\n(D) A person demonstrating a castle adventure in the game\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1894, "doc": {"video_name": "game_7.mp4", "duration": 258.07, "question": "What is the object built by the mission in the video?\n(A) Chair\n(B) Gun\n(C) Table\n(D) flying device\n", "candidates": ["Chair", "Gun", "Table", "flying device"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1894"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2551272a71d0186666b253e7e7a9a5236b80cb3fce342abc5c347e31f0a3fb12", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the object built by the mission in the video?\n(A) Chair\n(B) Gun\n(C) Table\n(D) flying device\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "C"}, "input": "What is the object built by the mission in the video?\n(A) Chair\n(B) Gun\n(C) Table\n(D) flying device\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1898, "doc": {"video_name": "game_44.mp4", "duration": 340.61, "question": "What is this video about?\n(A) A game player digging underground tunnels in the game\n(B) A game player fighting monsters in the game\n(C) A game player building a castle in the game\n(D) A game player exploring a forest in the game\n", "candidates": ["A game player digging underground tunnels in the game", "A game player fighting monsters in the game", "A game player building a castle in the game", "A game player exploring a forest in the game"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1898"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85abf220a95aca3c3e0ff0cb08caecfa736e02dc868545ef70917a348712c4fc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is this video about?\n(A) A game player digging underground tunnels in the game\n(B) A game player fighting monsters in the game\n(C) A game player building a castle in the game\n(D) A game player exploring a forest in the game\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is this video about?\n(A) A game player digging underground tunnels in the game\n(B) A game player fighting monsters in the game\n(C) A game player building a castle in the game\n(D) A game player exploring a forest in the game\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1902, "doc": {"video_name": "game_24.mp4", "duration": 350.23, "question": "What is the protagonist of the game doing?\n(A) Building a bridge and setting up a trap\n(B) Planting trees and setting a fence\n(C) Constructing a house and installing windows\n(D) Digging a well and placing a ladder\n", "candidates": ["Building a bridge and setting up a trap", "Planting trees and setting a fence", "Constructing a house and installing windows", "Digging a well and placing a ladder"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1902"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a9c56914ee468422500c3f9a72af8c1f8913269d1533f843492baa686c79b4bc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the protagonist of the game doing?\n(A) Building a bridge and setting up a trap\n(B) Planting trees and setting a fence\n(C) Constructing a house and installing windows\n(D) Digging a well and placing a ladder\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "D"}, "input": "What is the protagonist of the game doing?\n(A) Building a bridge and setting up a trap\n(B) Planting trees and setting a fence\n(C) Constructing a house and installing windows\n(D) Digging a well and placing a ladder\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1906, "doc": {"video_name": "game_16.mp4", "duration": 294.99, "question": "Question: What is the object being built in this video?\n(A) An underground bunker\n(B) A floating platform\n(C) A rooftop garden\n(D) A water tower\n", "candidates": ["An underground bunker", "A floating platform", "A rooftop garden", "A water tower"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1906"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "683fab3c89197e253833d746559c15e48c34d34bef286b8f5f6a2c25d86c8b76", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Question: What is the object being built in this video?\n(A) An underground bunker\n(B) A floating platform\n(C) A rooftop garden\n(D) A water tower\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "Question: What is the object being built in this video?\n(A) An underground bunker\n(B) A floating platform\n(C) A rooftop garden\n(D) A water tower\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1910, "doc": {"video_name": "game_34.mp4", "duration": 232.22, "question": "What is the protagonist mainly doing in the game?\n(A) Raising pets\n(B) Planting trees\n(C) Constructing buildings\n(D) Digging holes\n", "candidates": ["Raising pets", "Planting trees", "Constructing buildings", "Digging holes"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1910"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8eea88d72697e83a8aad32147934b652b9d3fa20ee5b94ae1f2fc91795d0889a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the protagonist mainly doing in the game?\n(A) Raising pets\n(B) Planting trees\n(C) Constructing buildings\n(D) Digging holes\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the protagonist mainly doing in the game?\n(A) Raising pets\n(B) Planting trees\n(C) Constructing buildings\n(D) Digging holes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1914, "doc": {"video_name": "game_23.mp4", "duration": 247.36, "question": "What is the type of this video?\n(A) Movie trailer\n(B) Video game\n(C) Documentary\n(D) Tutorial\n", "candidates": ["Movie trailer", "Video game", "Documentary", "Tutorial"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1914"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b55087c075243e45263f4c11c276fbc5046454bf9c75ad6200076274ade1f33e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the type of this video?\n(A) Movie trailer\n(B) Video game\n(C) Documentary\n(D) Tutorial\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the type of this video?\n(A) Movie trailer\n(B) Video game\n(C) Documentary\n(D) Tutorial\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1918, "doc": {"video_name": "order_288.mp4", "duration": 511.25, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) water sliding --> making jewelry --> abseiling --> javelin throw\n(B) abseiling --> water sliding --> javelin throw --> making jewelry\n(C) javelin throw --> water sliding --> abseiling --> making jewelry\n(D) water sliding --> javelin throw --> abseiling --> making jewelry\n", "candidates": ["water sliding --> making jewelry --> abseiling --> javelin throw", "abseiling --> water sliding --> javelin throw --> making jewelry", "javelin throw --> water sliding --> abseiling --> making jewelry", "water sliding --> javelin throw --> abseiling --> making jewelry"], "answer": "C", "task_type": "order", "question_id": "Q1918"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf3c31405f64d6e064e90efde329f47635dd56152e34c728751a8884fa738933", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) water sliding --> making jewelry --> abseiling --> javelin throw\n(B) abseiling --> water sliding --> javelin throw --> making jewelry\n(C) javelin throw --> water sliding --> abseiling --> making jewelry\n(D) water sliding --> javelin throw --> abseiling --> making jewelry\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) water sliding --> making jewelry --> abseiling --> javelin throw\n(B) abseiling --> water sliding --> javelin throw --> making jewelry\n(C) javelin throw --> water sliding --> abseiling --> making jewelry\n(D) water sliding --> javelin throw --> abseiling --> making jewelry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1922, "doc": {"video_name": "order_221.mp4", "duration": 637.64, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) jetskiing --> tossing coin --> shredding paper --> zumba\n(B) zumba --> tossing coin --> jetskiing --> shredding paper\n(C) jetskiing --> shredding paper --> zumba --> tossing coin\n(D) zumba --> tossing coin --> shredding paper --> jetskiing\n", "candidates": ["jetskiing --> tossing coin --> shredding paper --> zumba", "zumba --> tossing coin --> jetskiing --> shredding paper", "jetskiing --> shredding paper --> zumba --> tossing coin", "zumba --> tossing coin --> shredding paper --> jetskiing"], "answer": "C", "task_type": "order", "question_id": "Q1922"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8ae5ed2669f6a247ca56dc9e6fb99f11b4bbf65049ee650a226efdc34b7db4ca", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) jetskiing --> tossing coin --> shredding paper --> zumba\n(B) zumba --> tossing coin --> jetskiing --> shredding paper\n(C) jetskiing --> shredding paper --> zumba --> tossing coin\n(D) zumba --> tossing coin --> shredding paper --> jetskiing\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) jetskiing --> tossing coin --> shredding paper --> zumba\n(B) zumba --> tossing coin --> jetskiing --> shredding paper\n(C) jetskiing --> shredding paper --> zumba --> tossing coin\n(D) zumba --> tossing coin --> shredding paper --> jetskiing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1926, "doc": {"video_name": "order_79.mp4", "duration": 1225.45, "question": "Arrange the following events from the video in the correct chronological order: (1)The man melts the wax with the tool and wipes the ski; (2)The man adds a substance from a jug to the ski and wipes it with a paper towel; (3)The man exchanges skis and waxes the second one with the tool; (4)The man then scrapes the wax off the ski and uses a different tools and paper towel on the ski.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "3->2->1->4", "1->2->3->4", "2->1->3->4"], "answer": "D", "task_type": "order", "question_id": "Q1926"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5cc1d94f98995c1535ff34f441e42edff77c3320717847134c857b26e287b5e4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man melts the wax with the tool and wipes the ski; (2)The man adds a substance from a jug to the ski and wipes it with a paper towel; (3)The man exchanges skis and waxes the second one with the tool; (4)The man then scrapes the wax off the ski and uses a different tools and paper towel on the ski.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man melts the wax with the tool and wipes the ski; (2)The man adds a substance from a jug to the ski and wipes it with a paper towel; (3)The man exchanges skis and waxes the second one with the tool; (4)The man then scrapes the wax off the ski and uses a different tools and paper towel on the ski.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1930, "doc": {"video_name": "order_76.mp4", "duration": 1093.34, "question": "Arrange the following events from the video in the correct chronological order: (1)Woman begins ripping the wrapping paper with her hands; (2)Woman sets the box on top of the wrapping paper and begins wrapping the box; (3)Woman lifts up a box and sets it on the table; (4)Woman grabs a pair of scissors and tape.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 3->4->1->2\n(D) 4->3->2->1\n", "candidates": ["2->1->3->4", "1->2->3->4", "3->4->1->2", "4->3->2->1"], "answer": "C", "task_type": "order", "question_id": "Q1930"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43dfc63fa7db224d3c6c1788e6abc3bcc94c349cebd84ee9b2e49be45d692267", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Woman begins ripping the wrapping paper with her hands; (2)Woman sets the box on top of the wrapping paper and begins wrapping the box; (3)Woman lifts up a box and sets it on the table; (4)Woman grabs a pair of scissors and tape.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 3->4->1->2\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Woman begins ripping the wrapping paper with her hands; (2)Woman sets the box on top of the wrapping paper and begins wrapping the box; (3)Woman lifts up a box and sets it on the table; (4)Woman grabs a pair of scissors and tape.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 3->4->1->2\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1934, "doc": {"video_name": "order_107.mp4", "duration": 685.98, "question": "Arrange the following events from the video in the correct chronological order: (1)The camera focuses on an older man's face; (2)The two children dance together; (3)The camera focuses on a bug on the wall; (4)The two children interact with each other in a cluttered room.\n(A) 4->3->2->1\n(B) 1->4->3->2\n(C) 4->1->2->3\n(D) 1->2->3->4\n", "candidates": ["4->3->2->1", "1->4->3->2", "4->1->2->3", "1->2->3->4"], "answer": "B", "task_type": "order", "question_id": "Q1934"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a03c0b12a46d74e7120f20b2089d8650c6c9184c1d95fe8fda83dc9f57442624", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The camera focuses on an older man's face; (2)The two children dance together; (3)The camera focuses on a bug on the wall; (4)The two children interact with each other in a cluttered room.\n(A) 4->3->2->1\n(B) 1->4->3->2\n(C) 4->1->2->3\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The camera focuses on an older man's face; (2)The two children dance together; (3)The camera focuses on a bug on the wall; (4)The two children interact with each other in a cluttered room.\n(A) 4->3->2->1\n(B) 1->4->3->2\n(C) 4->1->2->3\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1938, "doc": {"video_name": "order_132.mp4", "duration": 688.49, "question": "Arrange the following events from the video in the correct chronological order: (1)The man starts to demonstrate playing the bongos in a lesson; (2)LP and Giovanni Logo appear on the black screen opening; (3)The lesson continues, alternating between color and black and white footage; (4)A man sits behind a set of bongo drums.\n(A) 2->4->1->3\n(B) 2->1->4->3\n(C) 4->2->1->3\n(D) 1->2->3->4\n", "candidates": ["2->4->1->3", "2->1->4->3", "4->2->1->3", "1->2->3->4"], "answer": "A", "task_type": "order", "question_id": "Q1938"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9597fa579a6e20799f811d7cdc4a96970444264fa6695807ef091e7cac26ad66", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man starts to demonstrate playing the bongos in a lesson; (2)LP and Giovanni Logo appear on the black screen opening; (3)The lesson continues, alternating between color and black and white footage; (4)A man sits behind a set of bongo drums.\n(A) 2->4->1->3\n(B) 2->1->4->3\n(C) 4->2->1->3\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man starts to demonstrate playing the bongos in a lesson; (2)LP and Giovanni Logo appear on the black screen opening; (3)The lesson continues, alternating between color and black and white footage; (4)A man sits behind a set of bongo drums.\n(A) 2->4->1->3\n(B) 2->1->4->3\n(C) 4->2->1->3\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1942, "doc": {"video_name": "order_19.mp4", "duration": 679.45, "question": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 1->2->3->4\n(B) 3->4->1->2\n(C) 2->1->4->3\n(D) 4->3->2->1\n", "candidates": ["1->2->3->4", "3->4->1->2", "2->1->4->3", "4->3->2->1"], "answer": "C", "task_type": "order", "question_id": "Q1942"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a1527bb2d989a7002620058e3826f9f54e8bb838b75e7adfd0afe0d1557c6c66", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 1->2->3->4\n(B) 3->4->1->2\n(C) 2->1->4->3\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 1->2->3->4\n(B) 3->4->1->2\n(C) 2->1->4->3\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1946, "doc": {"video_name": "order_114.mp4", "duration": 676.67, "question": "Arrange the following events from the video in the correct chronological order: (1)A lady in black reads names a man hands her and passes out prize buckets to the kids; (2)A group of kids is building a moat filled with water around a sand castle; (3)We see an opening title screen; (4)We see kids across the beach working on their castles in the wet sand.\n(A) 1->4->2->3\n(B) 4->1->2->3\n(C) 3->2->4->1\n(D) 2->3->1->4\n", "candidates": ["1->4->2->3", "4->1->2->3", "3->2->4->1", "2->3->1->4"], "answer": "C", "task_type": "order", "question_id": "Q1946"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ccc63205a17dd6dfb52d43bda157b1b65835701b1d5994d8bb14e6659829ac52", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A lady in black reads names a man hands her and passes out prize buckets to the kids; (2)A group of kids is building a moat filled with water around a sand castle; (3)We see an opening title screen; (4)We see kids across the beach working on their castles in the wet sand.\n(A) 1->4->2->3\n(B) 4->1->2->3\n(C) 3->2->4->1\n(D) 2->3->1->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A lady in black reads names a man hands her and passes out prize buckets to the kids; (2)A group of kids is building a moat filled with water around a sand castle; (3)We see an opening title screen; (4)We see kids across the beach working on their castles in the wet sand.\n(A) 1->4->2->3\n(B) 4->1->2->3\n(C) 3->2->4->1\n(D) 2->3->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1950, "doc": {"video_name": "order_14.mp4", "duration": 687.24, "question": "Arrange the following events from the video in the correct chronological order: (1)The batter is poured into bowls and dye is added; (2)Ingredients are shown on a counter; (3)The cake is frosted with blue frosting and sprinkles are added; (4)The pans are greased and the different colored batter is poured into them.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 2->1->4->3\n(D) 3->4->1->2\n", "candidates": ["4->3->2->1", "1->2->3->4", "2->1->4->3", "3->4->1->2"], "answer": "C", "task_type": "order", "question_id": "Q1950"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "33b7157b02b25fac86b7143e96f07a9a7cfc929a0cf7e005f2d6d7e1151a7ca1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The batter is poured into bowls and dye is added; (2)Ingredients are shown on a counter; (3)The cake is frosted with blue frosting and sprinkles are added; (4)The pans are greased and the different colored batter is poured into them.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 2->1->4->3\n(D) 3->4->1->2\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The batter is poured into bowls and dye is added; (2)Ingredients are shown on a counter; (3)The cake is frosted with blue frosting and sprinkles are added; (4)The pans are greased and the different colored batter is poured into them.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 2->1->4->3\n(D) 3->4->1->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1954, "doc": {"video_name": "order_12.mp4", "duration": 687.24, "question": "Arrange the following events from the video in the correct chronological order: (1)The batter is poured into bowls and dye is added; (2)Ingredients are shown on a counter; (3)The cake is frosted with blue frosting and sprinkles are added; (4)The pans are greased and the different colored batter is poured into them.\n(A) 4->3->2->1\n(B) 3->4->1->2\n(C) 2->1->4->3\n(D) 1->2->3->4\n", "candidates": ["4->3->2->1", "3->4->1->2", "2->1->4->3", "1->2->3->4"], "answer": "C", "task_type": "order", "question_id": "Q1954"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1e555128d6fd062ff28b34d6f814c8f04d82be5b5ebaa0d74507a1b15594cce6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The batter is poured into bowls and dye is added; (2)Ingredients are shown on a counter; (3)The cake is frosted with blue frosting and sprinkles are added; (4)The pans are greased and the different colored batter is poured into them.\n(A) 4->3->2->1\n(B) 3->4->1->2\n(C) 2->1->4->3\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The batter is poured into bowls and dye is added; (2)Ingredients are shown on a counter; (3)The cake is frosted with blue frosting and sprinkles are added; (4)The pans are greased and the different colored batter is poured into them.\n(A) 4->3->2->1\n(B) 3->4->1->2\n(C) 2->1->4->3\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1958, "doc": {"video_name": "order_10.mp4", "duration": 677.73, "question": "Arrange the following events from the video in the correct chronological order: (1)The man attempts to walk across the rope but falls and holds onto the rope; (2)A seal sits on a rock near an ocean; (3)The man films from a beach cliff next to a tent; (4)The man walks across the rope all the way to the attached rock.\n(A) 3->1->2->4\n(B) 1->2->3->4\n(C) 2->4->1->3\n(D) 2->1->4->3\n", "candidates": ["3->1->2->4", "1->2->3->4", "2->4->1->3", "2->1->4->3"], "answer": "D", "task_type": "order", "question_id": "Q1958"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a312a0e621cf9cdfce5285f59d9e1107df2ef7d45b427ce31f0a0946a0c77723", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man attempts to walk across the rope but falls and holds onto the rope; (2)A seal sits on a rock near an ocean; (3)The man films from a beach cliff next to a tent; (4)The man walks across the rope all the way to the attached rock.\n(A) 3->1->2->4\n(B) 1->2->3->4\n(C) 2->4->1->3\n(D) 2->1->4->3\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man attempts to walk across the rope but falls and holds onto the rope; (2)A seal sits on a rock near an ocean; (3)The man films from a beach cliff next to a tent; (4)The man walks across the rope all the way to the attached rock.\n(A) 3->1->2->4\n(B) 1->2->3->4\n(C) 2->4->1->3\n(D) 2->1->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1962, "doc": {"video_name": "order_265.mp4", "duration": 641.0899999999999, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) pole vault --> playing harp --> paragliding --> cooking sausages\n(B) cooking sausages --> paragliding --> pole vault --> playing harp\n(C) cooking sausages --> pole vault --> paragliding --> playing harp\n(D) playing harp --> pole vault --> cooking sausages --> paragliding\n", "candidates": ["pole vault --> playing harp --> paragliding --> cooking sausages", "cooking sausages --> paragliding --> pole vault --> playing harp", "cooking sausages --> pole vault --> paragliding --> playing harp", "playing harp --> pole vault --> cooking sausages --> paragliding"], "answer": "A", "task_type": "order", "question_id": "Q1962"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1b3727d59cdb2f8ff6e656a78cecdc6c73756db441e662a4adb121e64cc09b9d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) pole vault --> playing harp --> paragliding --> cooking sausages\n(B) cooking sausages --> paragliding --> pole vault --> playing harp\n(C) cooking sausages --> pole vault --> paragliding --> playing harp\n(D) playing harp --> pole vault --> cooking sausages --> paragliding\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) pole vault --> playing harp --> paragliding --> cooking sausages\n(B) cooking sausages --> paragliding --> pole vault --> playing harp\n(C) cooking sausages --> pole vault --> paragliding --> playing harp\n(D) playing harp --> pole vault --> cooking sausages --> paragliding\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1966, "doc": {"video_name": "order_169.mp4", "duration": 484.91, "question": "Arrange the following events from the video in the correct chronological order: (1)A lady in blue talks about the Extreme Dog Grooming company; (2)A poodle is groomed and dyed with different colors; (3)A dog painted to resemble a zebra is shown; (4)A dog with spots resembling a giraffe is brought to an elementary school for kids to play with.\n(A) 4->3->2->1\n(B) 3->4->1->2\n(C) 2->1->3->4\n(D) 1->2->4->3\n", "candidates": ["4->3->2->1", "3->4->1->2", "2->1->3->4", "1->2->4->3"], "answer": "D", "task_type": "order", "question_id": "Q1966"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eb795553edf061188f91b39186ee5ff788c3a4c1a89e6b8cbec2b41f024a20f5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A lady in blue talks about the Extreme Dog Grooming company; (2)A poodle is groomed and dyed with different colors; (3)A dog painted to resemble a zebra is shown; (4)A dog with spots resembling a giraffe is brought to an elementary school for kids to play with.\n(A) 4->3->2->1\n(B) 3->4->1->2\n(C) 2->1->3->4\n(D) 1->2->4->3\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A lady in blue talks about the Extreme Dog Grooming company; (2)A poodle is groomed and dyed with different colors; (3)A dog painted to resemble a zebra is shown; (4)A dog with spots resembling a giraffe is brought to an elementary school for kids to play with.\n(A) 4->3->2->1\n(B) 3->4->1->2\n(C) 2->1->3->4\n(D) 1->2->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1970, "doc": {"video_name": "order_102.mp4", "duration": 1728.94, "question": "Arrange the following events from the video in the correct chronological order: (1)A lady in blue talks about the Extreme Dog Grooming company; (2)A poodle is groomed and dyed with different colors; (3)A dog painted to resemble a zebra is shown; (4)A dog with spots resembling a giraffe is brought to an elementary school for kids to play with.\n(A) 4->3->2->1\n(B) 2->1->3->4\n(C) 1->2->4->3\n(D) 3->4->1->2\n", "candidates": ["4->3->2->1", "2->1->3->4", "1->2->4->3", "3->4->1->2"], "answer": "C", "task_type": "order", "question_id": "Q1970"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f22ebd4dd81a1717a7686313326ad0f9b8c0ae0873b93040970eb06848feda10", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A lady in blue talks about the Extreme Dog Grooming company; (2)A poodle is groomed and dyed with different colors; (3)A dog painted to resemble a zebra is shown; (4)A dog with spots resembling a giraffe is brought to an elementary school for kids to play with.\n(A) 4->3->2->1\n(B) 2->1->3->4\n(C) 1->2->4->3\n(D) 3->4->1->2\n", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A lady in blue talks about the Extreme Dog Grooming company; (2)A poodle is groomed and dyed with different colors; (3)A dog painted to resemble a zebra is shown; (4)A dog with spots resembling a giraffe is brought to an elementary school for kids to play with.\n(A) 4->3->2->1\n(B) 2->1->3->4\n(C) 1->2->4->3\n(D) 3->4->1->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1974, "doc": {"video_name": "order_327.mp4", "duration": 513.06, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing harp --> paragliding --> shredding paper --> jetskiing\n(B) jetskiing --> shredding paper --> playing harp --> paragliding\n(C) jetskiing --> paragliding --> playing harp --> shredding paper\n(D) jetskiing --> shredding paper --> paragliding --> playing harp\n", "candidates": ["playing harp --> paragliding --> shredding paper --> jetskiing", "jetskiing --> shredding paper --> playing harp --> paragliding", "jetskiing --> paragliding --> playing harp --> shredding paper", "jetskiing --> shredding paper --> paragliding --> playing harp"], "answer": "C", "task_type": "order", "question_id": "Q1974"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cd0e772a945a2edbb4b055c12f0b54c9a780d3c5fa77dab58a43f0585cfa6b57", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing harp --> paragliding --> shredding paper --> jetskiing\n(B) jetskiing --> shredding paper --> playing harp --> paragliding\n(C) jetskiing --> paragliding --> playing harp --> shredding paper\n(D) jetskiing --> shredding paper --> paragliding --> playing harp\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing harp --> paragliding --> shredding paper --> jetskiing\n(B) jetskiing --> shredding paper --> playing harp --> paragliding\n(C) jetskiing --> paragliding --> playing harp --> shredding paper\n(D) jetskiing --> shredding paper --> paragliding --> playing harp\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1978, "doc": {"video_name": "order_271.mp4", "duration": 520.02, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cleaning toilet --> making jewelry --> abseiling --> javelin throw\n(B) javelin throw --> making jewelry --> abseiling --> cleaning toilet\n(C) cleaning toilet --> javelin throw --> making jewelry --> abseiling\n(D) abseiling --> javelin throw --> making jewelry --> cleaning toilet\n", "candidates": ["cleaning toilet --> making jewelry --> abseiling --> javelin throw", "javelin throw --> making jewelry --> abseiling --> cleaning toilet", "cleaning toilet --> javelin throw --> making jewelry --> abseiling", "abseiling --> javelin throw --> making jewelry --> cleaning toilet"], "answer": "A", "task_type": "order", "question_id": "Q1978"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a486a19377bcc96f2e0a860b59abf84367bd9a84aea3ca91b37c4e00218523ea", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cleaning toilet --> making jewelry --> abseiling --> javelin throw\n(B) javelin throw --> making jewelry --> abseiling --> cleaning toilet\n(C) cleaning toilet --> javelin throw --> making jewelry --> abseiling\n(D) abseiling --> javelin throw --> making jewelry --> cleaning toilet\n", "task_type": "order", "pred_answer": "D", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cleaning toilet --> making jewelry --> abseiling --> javelin throw\n(B) javelin throw --> making jewelry --> abseiling --> cleaning toilet\n(C) cleaning toilet --> javelin throw --> making jewelry --> abseiling\n(D) abseiling --> javelin throw --> making jewelry --> cleaning toilet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1982, "doc": {"video_name": "order_253.mp4", "duration": 1365.11, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) shredding paper --> making jewelry --> playing harp --> cooking sausages\n(B) shredding paper --> cooking sausages --> making jewelry --> playing harp\n(C) making jewelry --> playing harp --> cooking sausages --> shredding paper\n(D) playing harp --> making jewelry --> cooking sausages --> shredding paper\n", "candidates": ["shredding paper --> making jewelry --> playing harp --> cooking sausages", "shredding paper --> cooking sausages --> making jewelry --> playing harp", "making jewelry --> playing harp --> cooking sausages --> shredding paper", "playing harp --> making jewelry --> cooking sausages --> shredding paper"], "answer": "B", "task_type": "order", "question_id": "Q1982"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dac377ce6f7666af700ff51fc2305153df16bf25ce53505d20edc50a09ce37fa", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) shredding paper --> making jewelry --> playing harp --> cooking sausages\n(B) shredding paper --> cooking sausages --> making jewelry --> playing harp\n(C) making jewelry --> playing harp --> cooking sausages --> shredding paper\n(D) playing harp --> making jewelry --> cooking sausages --> shredding paper\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) shredding paper --> making jewelry --> playing harp --> cooking sausages\n(B) shredding paper --> cooking sausages --> making jewelry --> playing harp\n(C) making jewelry --> playing harp --> cooking sausages --> shredding paper\n(D) playing harp --> making jewelry --> cooking sausages --> shredding paper\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1986, "doc": {"video_name": "order_67.mp4", "duration": 686.23, "question": "Arrange the following events from the video in the correct chronological order: (1)The mix is poured into cupcake liners; (2)A cake with a Hershey shape is placed on a white plate; (3)Eggs, flour, and other ingredients are mixed in a bowl; (4)The cake is cut into a piece and served on a white plate.\n(A) 1->2->3->4\n(B) 3->1->4->2\n(C) 2->3->1->4\n(D) 3->1->2->4\n", "candidates": ["1->2->3->4", "3->1->4->2", "2->3->1->4", "3->1->2->4"], "answer": "D", "task_type": "order", "question_id": "Q1986"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "492e875e982158fffe5aa8e52df8a8e0430133ec365d34de75086446334fdeb1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The mix is poured into cupcake liners; (2)A cake with a Hershey shape is placed on a white plate; (3)Eggs, flour, and other ingredients are mixed in a bowl; (4)The cake is cut into a piece and served on a white plate.\n(A) 1->2->3->4\n(B) 3->1->4->2\n(C) 2->3->1->4\n(D) 3->1->2->4\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The mix is poured into cupcake liners; (2)A cake with a Hershey shape is placed on a white plate; (3)Eggs, flour, and other ingredients are mixed in a bowl; (4)The cake is cut into a piece and served on a white plate.\n(A) 1->2->3->4\n(B) 3->1->4->2\n(C) 2->3->1->4\n(D) 3->1->2->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1990, "doc": {"video_name": "order_235.mp4", "duration": 646.57, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) water sliding --> stomping grapes --> abseiling --> cleaning toilet\n(B) stomping grapes --> cleaning toilet --> water sliding --> abseiling\n(C) abseiling --> water sliding --> stomping grapes --> cleaning toilet\n(D) cleaning toilet --> abseiling --> stomping grapes --> water sliding\n", "candidates": ["water sliding --> stomping grapes --> abseiling --> cleaning toilet", "stomping grapes --> cleaning toilet --> water sliding --> abseiling", "abseiling --> water sliding --> stomping grapes --> cleaning toilet", "cleaning toilet --> abseiling --> stomping grapes --> water sliding"], "answer": "A", "task_type": "order", "question_id": "Q1990"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f0a965ee1759b8fa16ccff9a407c4dd0493d5945df55dcf3a2038a3240c671e7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) water sliding --> stomping grapes --> abseiling --> cleaning toilet\n(B) stomping grapes --> cleaning toilet --> water sliding --> abseiling\n(C) abseiling --> water sliding --> stomping grapes --> cleaning toilet\n(D) cleaning toilet --> abseiling --> stomping grapes --> water sliding\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) water sliding --> stomping grapes --> abseiling --> cleaning toilet\n(B) stomping grapes --> cleaning toilet --> water sliding --> abseiling\n(C) abseiling --> water sliding --> stomping grapes --> cleaning toilet\n(D) cleaning toilet --> abseiling --> stomping grapes --> water sliding\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1994, "doc": {"video_name": "order_260.mp4", "duration": 6236.110000000001, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) jetskiing --> paragliding --> abseiling --> zumba\n(B) abseiling --> zumba --> paragliding --> jetskiing\n(C) zumba --> jetskiing --> abseiling --> paragliding\n(D) zumba --> jetskiing --> paragliding --> abseiling\n", "candidates": ["jetskiing --> paragliding --> abseiling --> zumba", "abseiling --> zumba --> paragliding --> jetskiing", "zumba --> jetskiing --> abseiling --> paragliding", "zumba --> jetskiing --> paragliding --> abseiling"], "answer": "A", "task_type": "order", "question_id": "Q1994"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bd7b777261e82c9c9d4803beae390c6527c36858b959b725fbcfa92a79dbcb66", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) jetskiing --> paragliding --> abseiling --> zumba\n(B) abseiling --> zumba --> paragliding --> jetskiing\n(C) zumba --> jetskiing --> abseiling --> paragliding\n(D) zumba --> jetskiing --> paragliding --> abseiling\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) jetskiing --> paragliding --> abseiling --> zumba\n(B) abseiling --> zumba --> paragliding --> jetskiing\n(C) zumba --> jetskiing --> abseiling --> paragliding\n(D) zumba --> jetskiing --> paragliding --> abseiling\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1998, "doc": {"video_name": "order_61.mp4", "duration": 688.21, "question": "Arrange the following events from the video in the correct chronological order: (1)The chef pours soy sauce into the cup; (2)The chef shows off shredded garlic before throwing it into the cup; (3)The chef grabs a bowl of salad and shows it off; (4)The chef grabs a cup of nuts and throws it on top of a salad.\n(A) 1->2->3->4\n(B) 3->4->1->2\n(C) 2->1->3->4\n(D) 4->3->2->1\n", "candidates": ["1->2->3->4", "3->4->1->2", "2->1->3->4", "4->3->2->1"], "answer": "B", "task_type": "order", "question_id": "Q1998"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a0cbd9ab3f6e0b7b95e902831fefdbf2a816c12ed8f715637f83c11f4a9b5dad", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The chef pours soy sauce into the cup; (2)The chef shows off shredded garlic before throwing it into the cup; (3)The chef grabs a bowl of salad and shows it off; (4)The chef grabs a cup of nuts and throws it on top of a salad.\n(A) 1->2->3->4\n(B) 3->4->1->2\n(C) 2->1->3->4\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The chef pours soy sauce into the cup; (2)The chef shows off shredded garlic before throwing it into the cup; (3)The chef grabs a bowl of salad and shows it off; (4)The chef grabs a cup of nuts and throws it on top of a salad.\n(A) 1->2->3->4\n(B) 3->4->1->2\n(C) 2->1->3->4\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2002, "doc": {"video_name": "order_227.mp4", "duration": 520.05, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) making jewelry --> abseiling --> milking cow --> jetskiing\n(B) jetskiing --> abseiling --> making jewelry --> milking cow\n(C) jetskiing --> making jewelry --> abseiling --> milking cow\n(D) jetskiing --> making jewelry --> milking cow --> abseiling\n", "candidates": ["making jewelry --> abseiling --> milking cow --> jetskiing", "jetskiing --> abseiling --> making jewelry --> milking cow", "jetskiing --> making jewelry --> abseiling --> milking cow", "jetskiing --> making jewelry --> milking cow --> abseiling"], "answer": "B", "task_type": "order", "question_id": "Q2002"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5cabf19ecb9df0145e18ee9768ff327e9c64730771141265418088328122269b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) making jewelry --> abseiling --> milking cow --> jetskiing\n(B) jetskiing --> abseiling --> making jewelry --> milking cow\n(C) jetskiing --> making jewelry --> abseiling --> milking cow\n(D) jetskiing --> making jewelry --> milking cow --> abseiling\n", "task_type": "order", "pred_answer": "D", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) making jewelry --> abseiling --> milking cow --> jetskiing\n(B) jetskiing --> abseiling --> making jewelry --> milking cow\n(C) jetskiing --> making jewelry --> abseiling --> milking cow\n(D) jetskiing --> making jewelry --> milking cow --> abseiling\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2006, "doc": {"video_name": "order_146.mp4", "duration": 851.67, "question": "Arrange the following events from the video in the correct chronological order: (1)The man snowboards down a hill and turns around; (2)An old man holds a surfboard and puts on a helmet to snowboard; (3)A young person sits on the snow wearing a snowboard; (4)The man has a hot drink with other people.\n(A) 1->2->4->3\n(B) 2->1->4->3\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "candidates": ["1->2->4->3", "2->1->4->3", "2->1->3->4", "1->2->3->4"], "answer": "B", "task_type": "order", "question_id": "Q2006"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1de2cac183f6851eab7066fb9f657f01bd573c4293ad99f2ecaf770b0c52a785", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man snowboards down a hill and turns around; (2)An old man holds a surfboard and puts on a helmet to snowboard; (3)A young person sits on the snow wearing a snowboard; (4)The man has a hot drink with other people.\n(A) 1->2->4->3\n(B) 2->1->4->3\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man snowboards down a hill and turns around; (2)An old man holds a surfboard and puts on a helmet to snowboard; (3)A young person sits on the snow wearing a snowboard; (4)The man has a hot drink with other people.\n(A) 1->2->4->3\n(B) 2->1->4->3\n(C) 2->1->3->4\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2010, "doc": {"video_name": "order_128.mp4", "duration": 665.3399999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)Woman tapes her hands with white tape; (2)Woman starts boxing in the ring with a guy; (3)Woman does sit ups on a towel on the beach; (4)Pictures of woman in her bikini are shown.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "candidates": ["4->3->2->1", "3->2->1->4", "2->1->3->4", "1->2->3->4"], "answer": "D", "task_type": "order", "question_id": "Q2010"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6a6a4feedc95ab465e00f94eec2226f98e12d738fc09cf3e1881e1cfca565ae8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Woman tapes her hands with white tape; (2)Woman starts boxing in the ring with a guy; (3)Woman does sit ups on a towel on the beach; (4)Pictures of woman in her bikini are shown.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Woman tapes her hands with white tape; (2)Woman starts boxing in the ring with a guy; (3)Woman does sit ups on a towel on the beach; (4)Pictures of woman in her bikini are shown.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 2->1->3->4\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2014, "doc": {"video_name": "order_241.mp4", "duration": 490.15, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> baking cookies --> playing trombone --> water sliding\n(B) riding mule --> water sliding --> playing trombone --> baking cookies\n(C) water sliding --> baking cookies --> riding mule --> playing trombone\n(D) playing trombone --> riding mule --> water sliding --> baking cookies\n", "candidates": ["riding mule --> baking cookies --> playing trombone --> water sliding", "riding mule --> water sliding --> playing trombone --> baking cookies", "water sliding --> baking cookies --> riding mule --> playing trombone", "playing trombone --> riding mule --> water sliding --> baking cookies"], "answer": "D", "task_type": "order", "question_id": "Q2014"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2bdafd76cc269dc5d81dbf613ef9f73e67fe0117d8a17d195a15fe5dc699d08e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> baking cookies --> playing trombone --> water sliding\n(B) riding mule --> water sliding --> playing trombone --> baking cookies\n(C) water sliding --> baking cookies --> riding mule --> playing trombone\n(D) playing trombone --> riding mule --> water sliding --> baking cookies\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> baking cookies --> playing trombone --> water sliding\n(B) riding mule --> water sliding --> playing trombone --> baking cookies\n(C) water sliding --> baking cookies --> riding mule --> playing trombone\n(D) playing trombone --> riding mule --> water sliding --> baking cookies\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2018, "doc": {"video_name": "order_223.mp4", "duration": 506.7, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> jetskiing --> baking cookies --> playing harp\n(B) baking cookies --> cooking sausages --> jetskiing --> playing harp\n(C) playing harp --> jetskiing --> cooking sausages --> baking cookies\n(D) cooking sausages --> baking cookies --> playing harp --> jetskiing\n", "candidates": ["cooking sausages --> jetskiing --> baking cookies --> playing harp", "baking cookies --> cooking sausages --> jetskiing --> playing harp", "playing harp --> jetskiing --> cooking sausages --> baking cookies", "cooking sausages --> baking cookies --> playing harp --> jetskiing"], "answer": "A", "task_type": "order", "question_id": "Q2018"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7647aaf2459babdaddb33275e07787a551ef0943e3a18a2bad47fc3d9bf683fa", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> jetskiing --> baking cookies --> playing harp\n(B) baking cookies --> cooking sausages --> jetskiing --> playing harp\n(C) playing harp --> jetskiing --> cooking sausages --> baking cookies\n(D) cooking sausages --> baking cookies --> playing harp --> jetskiing\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> jetskiing --> baking cookies --> playing harp\n(B) baking cookies --> cooking sausages --> jetskiing --> playing harp\n(C) playing harp --> jetskiing --> cooking sausages --> baking cookies\n(D) cooking sausages --> baking cookies --> playing harp --> jetskiing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2022, "doc": {"video_name": "order_97.mp4", "duration": 1220.92, "question": "Arrange the following events from the video in the correct chronological order: (1)A person in a red coat cleans the snow off their car; (2)The trunk of the car is lifted open; (3)A person in a tan coat cleans off the front of the car; (4)A man in a white jacket starts to clear the snow off of another car.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->2->1->4\n", "candidates": ["2->1->3->4", "1->2->3->4", "4->3->2->1", "3->2->1->4"], "answer": "B", "task_type": "order", "question_id": "Q2022"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c9593a190277dba4921d68bcb9d93f0f251ca3a7a69f5628a71a37e6f7d76dc8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A person in a red coat cleans the snow off their car; (2)The trunk of the car is lifted open; (3)A person in a tan coat cleans off the front of the car; (4)A man in a white jacket starts to clear the snow off of another car.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->2->1->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A person in a red coat cleans the snow off their car; (2)The trunk of the car is lifted open; (3)A person in a tan coat cleans off the front of the car; (4)A man in a white jacket starts to clear the snow off of another car.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->2->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2026, "doc": {"video_name": "order_127.mp4", "duration": 665.34, "question": "Arrange the following events from the video in the correct chronological order: (1)Woman tapes her hands with white tape; (2)Woman starts boxing in the ring with a guy; (3)Woman does sit ups on a towel on the beach; (4)Pictures of woman in her bikini are shown.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 3->2->1->4\n", "candidates": ["1->2->3->4", "4->3->2->1", "2->1->3->4", "3->2->1->4"], "answer": "A", "task_type": "order", "question_id": "Q2026"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "856f3532b435dc1d18f86d1d790c37d86984773de0626bb4d1128f99eaeaa256", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Woman tapes her hands with white tape; (2)Woman starts boxing in the ring with a guy; (3)Woman does sit ups on a towel on the beach; (4)Pictures of woman in her bikini are shown.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 3->2->1->4\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Woman tapes her hands with white tape; (2)Woman starts boxing in the ring with a guy; (3)Woman does sit ups on a towel on the beach; (4)Pictures of woman in her bikini are shown.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 3->2->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2030, "doc": {"video_name": "order_92.mp4", "duration": 1578.99, "question": "Arrange the following events from the video in the correct chronological order: (1)Man is on lake side talking to the camera like other couples as well; (2)Man is talking to the camera; (3)People are kayaking on calm river and have a good picnic day; (4)People are standing on a side of a rock wall.\n(A) 2->1->4->3\n(B) 2->4->1->3\n(C) 1->2->3->4\n(D) 4->2->1->3\n", "candidates": ["2->1->4->3", "2->4->1->3", "1->2->3->4", "4->2->1->3"], "answer": "B", "task_type": "order", "question_id": "Q2030"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d61cbe6dbbae9a05806624d8dfdc971aef8ac17c5269ad0ddeb3b2b709ec581", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Man is on lake side talking to the camera like other couples as well; (2)Man is talking to the camera; (3)People are kayaking on calm river and have a good picnic day; (4)People are standing on a side of a rock wall.\n(A) 2->1->4->3\n(B) 2->4->1->3\n(C) 1->2->3->4\n(D) 4->2->1->3\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Man is on lake side talking to the camera like other couples as well; (2)Man is talking to the camera; (3)People are kayaking on calm river and have a good picnic day; (4)People are standing on a side of a rock wall.\n(A) 2->1->4->3\n(B) 2->4->1->3\n(C) 1->2->3->4\n(D) 4->2->1->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2034, "doc": {"video_name": "order_222.mp4", "duration": 520.03, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) cleaning toilet --> milking cow --> abseiling --> playing trombone\n(B) milking cow --> cleaning toilet --> playing trombone --> abseiling\n(C) milking cow --> abseiling --> cleaning toilet --> playing trombone\n(D) playing trombone --> milking cow --> cleaning toilet --> abseiling\n", "candidates": ["cleaning toilet --> milking cow --> abseiling --> playing trombone", "milking cow --> cleaning toilet --> playing trombone --> abseiling", "milking cow --> abseiling --> cleaning toilet --> playing trombone", "playing trombone --> milking cow --> cleaning toilet --> abseiling"], "answer": "C", "task_type": "order", "question_id": "Q2034"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "711626135967b97ea09e93c101a69f71ea3edf3482c4618443cc8fd881c6821e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) cleaning toilet --> milking cow --> abseiling --> playing trombone\n(B) milking cow --> cleaning toilet --> playing trombone --> abseiling\n(C) milking cow --> abseiling --> cleaning toilet --> playing trombone\n(D) playing trombone --> milking cow --> cleaning toilet --> abseiling\n", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) cleaning toilet --> milking cow --> abseiling --> playing trombone\n(B) milking cow --> cleaning toilet --> playing trombone --> abseiling\n(C) milking cow --> abseiling --> cleaning toilet --> playing trombone\n(D) playing trombone --> milking cow --> cleaning toilet --> abseiling\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2038, "doc": {"video_name": "order_278.mp4", "duration": 497.05999999999995, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) shredding paper --> jetskiing --> riding mule --> playing trombone\n(B) playing trombone --> shredding paper --> jetskiing --> riding mule\n(C) jetskiing --> playing trombone --> riding mule --> shredding paper\n(D) shredding paper --> riding mule --> jetskiing --> playing trombone\n", "candidates": ["shredding paper --> jetskiing --> riding mule --> playing trombone", "playing trombone --> shredding paper --> jetskiing --> riding mule", "jetskiing --> playing trombone --> riding mule --> shredding paper", "shredding paper --> riding mule --> jetskiing --> playing trombone"], "answer": "B", "task_type": "order", "question_id": "Q2038"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "79b14debb9b94f22f754555b86c4d09314a4431c892b2ba274e1c390cce97db8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) shredding paper --> jetskiing --> riding mule --> playing trombone\n(B) playing trombone --> shredding paper --> jetskiing --> riding mule\n(C) jetskiing --> playing trombone --> riding mule --> shredding paper\n(D) shredding paper --> riding mule --> jetskiing --> playing trombone\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) shredding paper --> jetskiing --> riding mule --> playing trombone\n(B) playing trombone --> shredding paper --> jetskiing --> riding mule\n(C) jetskiing --> playing trombone --> riding mule --> shredding paper\n(D) shredding paper --> riding mule --> jetskiing --> playing trombone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2042, "doc": {"video_name": "order_105.mp4", "duration": 715.98, "question": "Arrange the following events from the video in the correct chronological order: (1)The camera focuses on an older man's face; (2)The two children dance together; (3)The camera focuses on a bug on the wall; (4)The two children interact with each other in a cluttered room.\n(A) 1->4->3->2\n(B) 1->2->3->4\n(C) 4->1->2->3\n(D) 4->3->2->1\n", "candidates": ["1->4->3->2", "1->2->3->4", "4->1->2->3", "4->3->2->1"], "answer": "A", "task_type": "order", "question_id": "Q2042"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "70385c880afb6f2066608eb9903014e1f48e49b2baa9d4310fc4d492fa3dcd9d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The camera focuses on an older man's face; (2)The two children dance together; (3)The camera focuses on a bug on the wall; (4)The two children interact with each other in a cluttered room.\n(A) 1->4->3->2\n(B) 1->2->3->4\n(C) 4->1->2->3\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "D", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The camera focuses on an older man's face; (2)The two children dance together; (3)The camera focuses on a bug on the wall; (4)The two children interact with each other in a cluttered room.\n(A) 1->4->3->2\n(B) 1->2->3->4\n(C) 4->1->2->3\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2046, "doc": {"video_name": "order_53.mp4", "duration": 665.78, "question": "Arrange the following events from the video in the correct chronological order: (1)A guy sits and talks inside; (2)A man surfs on a body of water; (3)A lady spins on skates; (4)The credits of the video are shown.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 4->3->2->1\n(D) 3->2->1->4\n", "candidates": ["1->2->3->4", "2->1->3->4", "4->3->2->1", "3->2->1->4"], "answer": "A", "task_type": "order", "question_id": "Q2046"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ec86ccbd489ebb996a7bb057de59124ec46e599aeedd5f96057821a7e34c1772", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A guy sits and talks inside; (2)A man surfs on a body of water; (3)A lady spins on skates; (4)The credits of the video are shown.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 4->3->2->1\n(D) 3->2->1->4\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A guy sits and talks inside; (2)A man surfs on a body of water; (3)A lady spins on skates; (4)The credits of the video are shown.\n(A) 1->2->3->4\n(B) 2->1->3->4\n(C) 4->3->2->1\n(D) 3->2->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2050, "doc": {"video_name": "order_122.mp4", "duration": 673.46, "question": "Arrange the following events from the video in the correct chronological order: (1)The interviewer plays with the dogs; (2)A man has dogs on a city street near a car; (3)We see a title screen over the UK flag; (4)We see a banner across the bottom of the screen and the man kneeling playing with his dogs.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "1->2->3->4", "3->2->1->4", "2->1->3->4"], "answer": "C", "task_type": "order", "question_id": "Q2050"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ffc213787f2fe5f6d185b8331f36742a12ed0519942260fdb24eb8f4298dd6c8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The interviewer plays with the dogs; (2)A man has dogs on a city street near a car; (3)We see a title screen over the UK flag; (4)We see a banner across the bottom of the screen and the man kneeling playing with his dogs.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The interviewer plays with the dogs; (2)A man has dogs on a city street near a car; (3)We see a title screen over the UK flag; (4)We see a banner across the bottom of the screen and the man kneeling playing with his dogs.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2054, "doc": {"video_name": "order_266.mp4", "duration": 358.03999999999996, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> javelin throw --> jetskiing --> making jewelry\n(B) baking cookies --> javelin throw --> making jewelry --> jetskiing\n(C) baking cookies --> making jewelry --> javelin throw --> jetskiing\n(D) jetskiing --> making jewelry --> baking cookies --> javelin throw\n", "candidates": ["baking cookies --> javelin throw --> jetskiing --> making jewelry", "baking cookies --> javelin throw --> making jewelry --> jetskiing", "baking cookies --> making jewelry --> javelin throw --> jetskiing", "jetskiing --> making jewelry --> baking cookies --> javelin throw"], "answer": "C", "task_type": "order", "question_id": "Q2054"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0c8b29043fa8dd0a6b3c5f4dfdc5eaf8f2cc937746791ddec078407d8187895c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> javelin throw --> jetskiing --> making jewelry\n(B) baking cookies --> javelin throw --> making jewelry --> jetskiing\n(C) baking cookies --> making jewelry --> javelin throw --> jetskiing\n(D) jetskiing --> making jewelry --> baking cookies --> javelin throw\n", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) baking cookies --> javelin throw --> jetskiing --> making jewelry\n(B) baking cookies --> javelin throw --> making jewelry --> jetskiing\n(C) baking cookies --> making jewelry --> javelin throw --> jetskiing\n(D) jetskiing --> making jewelry --> baking cookies --> javelin throw\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2058, "doc": {"video_name": "order_336.mp4", "duration": 388.05, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) zumba --> paragliding --> stomping grapes --> baking cookies\n(B) baking cookies --> stomping grapes --> zumba --> paragliding\n(C) stomping grapes --> zumba --> baking cookies --> paragliding\n(D) zumba --> stomping grapes --> baking cookies --> paragliding\n", "candidates": ["zumba --> paragliding --> stomping grapes --> baking cookies", "baking cookies --> stomping grapes --> zumba --> paragliding", "stomping grapes --> zumba --> baking cookies --> paragliding", "zumba --> stomping grapes --> baking cookies --> paragliding"], "answer": "D", "task_type": "order", "question_id": "Q2058"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d5df419f951528331cc9b3272efb977e22c823bc719daca689f25e4c90fe0346", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) zumba --> paragliding --> stomping grapes --> baking cookies\n(B) baking cookies --> stomping grapes --> zumba --> paragliding\n(C) stomping grapes --> zumba --> baking cookies --> paragliding\n(D) zumba --> stomping grapes --> baking cookies --> paragliding\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) zumba --> paragliding --> stomping grapes --> baking cookies\n(B) baking cookies --> stomping grapes --> zumba --> paragliding\n(C) stomping grapes --> zumba --> baking cookies --> paragliding\n(D) zumba --> stomping grapes --> baking cookies --> paragliding\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2062, "doc": {"video_name": "order_255.mp4", "duration": 784.1899999999999, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> clean and jerk --> carving pumpkin --> milking cow\n(B) tossing coin --> milking cow --> clean and jerk --> carving pumpkin\n(C) clean and jerk --> tossing coin --> carving pumpkin --> milking cow\n(D) milking cow --> clean and jerk --> tossing coin --> carving pumpkin\n", "candidates": ["tossing coin --> clean and jerk --> carving pumpkin --> milking cow", "tossing coin --> milking cow --> clean and jerk --> carving pumpkin", "clean and jerk --> tossing coin --> carving pumpkin --> milking cow", "milking cow --> clean and jerk --> tossing coin --> carving pumpkin"], "answer": "D", "task_type": "order", "question_id": "Q2062"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a628152726357bee41adc1506d301f503f49ddf94085b0fe37d86aa6de82a713", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> clean and jerk --> carving pumpkin --> milking cow\n(B) tossing coin --> milking cow --> clean and jerk --> carving pumpkin\n(C) clean and jerk --> tossing coin --> carving pumpkin --> milking cow\n(D) milking cow --> clean and jerk --> tossing coin --> carving pumpkin\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> clean and jerk --> carving pumpkin --> milking cow\n(B) tossing coin --> milking cow --> clean and jerk --> carving pumpkin\n(C) clean and jerk --> tossing coin --> carving pumpkin --> milking cow\n(D) milking cow --> clean and jerk --> tossing coin --> carving pumpkin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2066, "doc": {"video_name": "order_343.mp4", "duration": 520.02, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> riding mule --> pole vault --> javelin throw\n(B) riding mule --> pole vault --> javelin throw --> tossing coin\n(C) tossing coin --> riding mule --> javelin throw --> pole vault\n(D) pole vault --> riding mule --> javelin throw --> tossing coin\n", "candidates": ["tossing coin --> riding mule --> pole vault --> javelin throw", "riding mule --> pole vault --> javelin throw --> tossing coin", "tossing coin --> riding mule --> javelin throw --> pole vault", "pole vault --> riding mule --> javelin throw --> tossing coin"], "answer": "B", "task_type": "order", "question_id": "Q2066"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "57bb9de690b06b60bcb46bf1268787bfbb7bf5571b8d5ba41309eeece0bd2af1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> riding mule --> pole vault --> javelin throw\n(B) riding mule --> pole vault --> javelin throw --> tossing coin\n(C) tossing coin --> riding mule --> javelin throw --> pole vault\n(D) pole vault --> riding mule --> javelin throw --> tossing coin\n", "task_type": "order", "pred_answer": "A", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> riding mule --> pole vault --> javelin throw\n(B) riding mule --> pole vault --> javelin throw --> tossing coin\n(C) tossing coin --> riding mule --> javelin throw --> pole vault\n(D) pole vault --> riding mule --> javelin throw --> tossing coin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2070, "doc": {"video_name": "order_82.mp4", "duration": 703.5799999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)The instructor finishes the class; (2)A woman's indoor aerobic class is in process; (3)The logo 'Zumba Toning' appears on screen; (4)The camera briefly scans to the mirrored wall and then back to the class.\n(A) 4->3->2->1\n(B) 3->2->4->1\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "candidates": ["4->3->2->1", "3->2->4->1", "2->1->3->4", "1->2->3->4"], "answer": "B", "task_type": "order", "question_id": "Q2070"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3bc94f1e50252cb717b94df4d9644422b2e1c5aebec0db4b6f7acd04d85ae722", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The instructor finishes the class; (2)A woman's indoor aerobic class is in process; (3)The logo 'Zumba Toning' appears on screen; (4)The camera briefly scans to the mirrored wall and then back to the class.\n(A) 4->3->2->1\n(B) 3->2->4->1\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The instructor finishes the class; (2)A woman's indoor aerobic class is in process; (3)The logo 'Zumba Toning' appears on screen; (4)The camera briefly scans to the mirrored wall and then back to the class.\n(A) 4->3->2->1\n(B) 3->2->4->1\n(C) 2->1->3->4\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2074, "doc": {"video_name": "order_335.mp4", "duration": 647.04, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> milking cow --> clean and jerk --> baking cookies\n(B) milking cow --> baking cookies --> jetskiing --> clean and jerk\n(C) clean and jerk --> jetskiing --> milking cow --> baking cookies\n(D) milking cow --> clean and jerk --> baking cookies --> jetskiing\n", "candidates": ["jetskiing --> milking cow --> clean and jerk --> baking cookies", "milking cow --> baking cookies --> jetskiing --> clean and jerk", "clean and jerk --> jetskiing --> milking cow --> baking cookies", "milking cow --> clean and jerk --> baking cookies --> jetskiing"], "answer": "D", "task_type": "order", "question_id": "Q2074"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5210cd3782b8c99fc1e951855bf90fae29993498982fd700c9613ba97b84d44d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> milking cow --> clean and jerk --> baking cookies\n(B) milking cow --> baking cookies --> jetskiing --> clean and jerk\n(C) clean and jerk --> jetskiing --> milking cow --> baking cookies\n(D) milking cow --> clean and jerk --> baking cookies --> jetskiing\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> milking cow --> clean and jerk --> baking cookies\n(B) milking cow --> baking cookies --> jetskiing --> clean and jerk\n(C) clean and jerk --> jetskiing --> milking cow --> baking cookies\n(D) milking cow --> clean and jerk --> baking cookies --> jetskiing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2078, "doc": {"video_name": "order_342.mp4", "duration": 515.1899999999999, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) making jewelry --> cooking sausages --> water sliding --> cleaning toilet\n(B) cooking sausages --> water sliding --> making jewelry --> cleaning toilet\n(C) making jewelry --> cleaning toilet --> water sliding --> cooking sausages\n(D) making jewelry --> cleaning toilet --> cooking sausages --> water sliding\n", "candidates": ["making jewelry --> cooking sausages --> water sliding --> cleaning toilet", "cooking sausages --> water sliding --> making jewelry --> cleaning toilet", "making jewelry --> cleaning toilet --> water sliding --> cooking sausages", "making jewelry --> cleaning toilet --> cooking sausages --> water sliding"], "answer": "B", "task_type": "order", "question_id": "Q2078"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8633cd0477d721d14a747a8e67e39af3a2b1b622fac5276f589b796632a22fcb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) making jewelry --> cooking sausages --> water sliding --> cleaning toilet\n(B) cooking sausages --> water sliding --> making jewelry --> cleaning toilet\n(C) making jewelry --> cleaning toilet --> water sliding --> cooking sausages\n(D) making jewelry --> cleaning toilet --> cooking sausages --> water sliding\n", "task_type": "order", "pred_answer": "D", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) making jewelry --> cooking sausages --> water sliding --> cleaning toilet\n(B) cooking sausages --> water sliding --> making jewelry --> cleaning toilet\n(C) making jewelry --> cleaning toilet --> water sliding --> cooking sausages\n(D) making jewelry --> cleaning toilet --> cooking sausages --> water sliding\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2082, "doc": {"video_name": "order_47.mp4", "duration": 673.61, "question": "Arrange the following events from the video in the correct chronological order: (1)The man in red cap stands outside a barbershop talking; (2)The man pretends to be asleep during his haircut; (3)The man points out the cameras and explains it to the barber; (4)The man appears to fall out of the chair.\n(A) 3->2->1->4\n(B) 1->3->2->4\n(C) 2->1->3->4\n(D) 1->2->4->3\n", "candidates": ["3->2->1->4", "1->3->2->4", "2->1->3->4", "1->2->4->3"], "answer": "D", "task_type": "order", "question_id": "Q2082"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "47c8cb9cbb582f9696dc6f52c43c5888774ab1f2b6a81318b223b66c73a19de3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man in red cap stands outside a barbershop talking; (2)The man pretends to be asleep during his haircut; (3)The man points out the cameras and explains it to the barber; (4)The man appears to fall out of the chair.\n(A) 3->2->1->4\n(B) 1->3->2->4\n(C) 2->1->3->4\n(D) 1->2->4->3\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man in red cap stands outside a barbershop talking; (2)The man pretends to be asleep during his haircut; (3)The man points out the cameras and explains it to the barber; (4)The man appears to fall out of the chair.\n(A) 3->2->1->4\n(B) 1->3->2->4\n(C) 2->1->3->4\n(D) 1->2->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2086, "doc": {"video_name": "order_44.mp4", "duration": 677.8399999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)The bowler from the blue team hits an overhand ball to the batter; (2)The bowler raises his hand to claim that the batter has not made a run; (3)The bowler causes the batter to get out by hitting the stumps behind him, the entire team cheers; (4)The batter walks out and another batter from his team comes on the field.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->2->1->4\n", "candidates": ["2->1->3->4", "1->2->3->4", "4->3->2->1", "3->2->1->4"], "answer": "B", "task_type": "order", "question_id": "Q2086"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b3d13296f07e17692d1b42b9c35fe197d754235c510a516e4648a63dac81d47b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The bowler from the blue team hits an overhand ball to the batter; (2)The bowler raises his hand to claim that the batter has not made a run; (3)The bowler causes the batter to get out by hitting the stumps behind him, the entire team cheers; (4)The batter walks out and another batter from his team comes on the field.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->2->1->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The bowler from the blue team hits an overhand ball to the batter; (2)The bowler raises his hand to claim that the batter has not made a run; (3)The bowler causes the batter to get out by hitting the stumps behind him, the entire team cheers; (4)The batter walks out and another batter from his team comes on the field.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->2->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2090, "doc": {"video_name": "order_41.mp4", "duration": 678.6899999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)The man replaces the tire on the front rim and pumps it up; (2)The man removes the front tire of the bike from the frame; (3)The man installs a headlamp to the bike; (4)The man reinstalls the front tire onto the bike frame.\n(A) 4->3->2->1\n(B) 2->1->4->3\n(C) 3->4->1->2\n(D) 1->2->3->4\n", "candidates": ["4->3->2->1", "2->1->4->3", "3->4->1->2", "1->2->3->4"], "answer": "B", "task_type": "order", "question_id": "Q2090"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "83329f840c4038aa7274eef9dbec2f4ab74094d3655d1fdc5364386a2ee22096", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man replaces the tire on the front rim and pumps it up; (2)The man removes the front tire of the bike from the frame; (3)The man installs a headlamp to the bike; (4)The man reinstalls the front tire onto the bike frame.\n(A) 4->3->2->1\n(B) 2->1->4->3\n(C) 3->4->1->2\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man replaces the tire on the front rim and pumps it up; (2)The man removes the front tire of the bike from the frame; (3)The man installs a headlamp to the bike; (4)The man reinstalls the front tire onto the bike frame.\n(A) 4->3->2->1\n(B) 2->1->4->3\n(C) 3->4->1->2\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2094, "doc": {"video_name": "order_250.mp4", "duration": 490.03, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) pole vault --> baking cookies --> tossing coin --> abseiling\n(B) tossing coin --> pole vault --> baking cookies --> abseiling\n(C) baking cookies --> tossing coin --> pole vault --> abseiling\n(D) baking cookies --> abseiling --> tossing coin --> pole vault\n", "candidates": ["pole vault --> baking cookies --> tossing coin --> abseiling", "tossing coin --> pole vault --> baking cookies --> abseiling", "baking cookies --> tossing coin --> pole vault --> abseiling", "baking cookies --> abseiling --> tossing coin --> pole vault"], "answer": "C", "task_type": "order", "question_id": "Q2094"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c175631e3376bb6a8207c385b7b9c74330f770b558b28d157e32dbdbb9ff084a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) pole vault --> baking cookies --> tossing coin --> abseiling\n(B) tossing coin --> pole vault --> baking cookies --> abseiling\n(C) baking cookies --> tossing coin --> pole vault --> abseiling\n(D) baking cookies --> abseiling --> tossing coin --> pole vault\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) pole vault --> baking cookies --> tossing coin --> abseiling\n(B) tossing coin --> pole vault --> baking cookies --> abseiling\n(C) baking cookies --> tossing coin --> pole vault --> abseiling\n(D) baking cookies --> abseiling --> tossing coin --> pole vault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2098, "doc": {"video_name": "order_320.mp4", "duration": 490.03999999999996, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> shredding paper --> milking cow --> baking cookies\n(B) milking cow --> jetskiing --> shredding paper --> baking cookies\n(C) milking cow --> jetskiing --> baking cookies --> shredding paper\n(D) baking cookies --> jetskiing --> milking cow --> shredding paper\n", "candidates": ["jetskiing --> shredding paper --> milking cow --> baking cookies", "milking cow --> jetskiing --> shredding paper --> baking cookies", "milking cow --> jetskiing --> baking cookies --> shredding paper", "baking cookies --> jetskiing --> milking cow --> shredding paper"], "answer": "B", "task_type": "order", "question_id": "Q2098"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "037d0521aca44c8a1e6210d8d342a19ca7a96fee70633d6c1ac9ddf669b80c87", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> shredding paper --> milking cow --> baking cookies\n(B) milking cow --> jetskiing --> shredding paper --> baking cookies\n(C) milking cow --> jetskiing --> baking cookies --> shredding paper\n(D) baking cookies --> jetskiing --> milking cow --> shredding paper\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> shredding paper --> milking cow --> baking cookies\n(B) milking cow --> jetskiing --> shredding paper --> baking cookies\n(C) milking cow --> jetskiing --> baking cookies --> shredding paper\n(D) baking cookies --> jetskiing --> milking cow --> shredding paper\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2102, "doc": {"video_name": "order_2.mp4", "duration": 695.74, "question": "Arrange the following events from the video in the correct chronological order: (1)A young and a kid are doing balance in a balance rope; (2)People are walking in a bridge to see a competition of men doing tricks on top of a balance rope; (3)A man is jumping and doing tricks in a balance rope above a cold river; (4)The boy is in a competition in snowy path doing tricks on a balance rope with people behind a fence watching him.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->2->1->4\n", "candidates": ["2->1->3->4", "1->2->3->4", "4->3->2->1", "3->2->1->4"], "answer": "B", "task_type": "order", "question_id": "Q2102"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf348153ca8b9274d6a9b4d69c3dcf40e3bb1f011008a93380c87535edd31b4e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A young and a kid are doing balance in a balance rope; (2)People are walking in a bridge to see a competition of men doing tricks on top of a balance rope; (3)A man is jumping and doing tricks in a balance rope above a cold river; (4)The boy is in a competition in snowy path doing tricks on a balance rope with people behind a fence watching him.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->2->1->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A young and a kid are doing balance in a balance rope; (2)People are walking in a bridge to see a competition of men doing tricks on top of a balance rope; (3)A man is jumping and doing tricks in a balance rope above a cold river; (4)The boy is in a competition in snowy path doing tricks on a balance rope with people behind a fence watching him.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->2->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2106, "doc": {"video_name": "order_312.mp4", "duration": 514.23, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) javelin throw --> cooking sausages --> riding mule --> pole vault\n(B) javelin throw --> riding mule --> cooking sausages --> pole vault\n(C) riding mule --> pole vault --> javelin throw --> cooking sausages\n(D) cooking sausages --> riding mule --> javelin throw --> pole vault\n", "candidates": ["javelin throw --> cooking sausages --> riding mule --> pole vault", "javelin throw --> riding mule --> cooking sausages --> pole vault", "riding mule --> pole vault --> javelin throw --> cooking sausages", "cooking sausages --> riding mule --> javelin throw --> pole vault"], "answer": "B", "task_type": "order", "question_id": "Q2106"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "92ac7c0bca84c805f7c1ba2772d894d0c60417fcd0aa13b8c81dfb9ff84034da", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) javelin throw --> cooking sausages --> riding mule --> pole vault\n(B) javelin throw --> riding mule --> cooking sausages --> pole vault\n(C) riding mule --> pole vault --> javelin throw --> cooking sausages\n(D) cooking sausages --> riding mule --> javelin throw --> pole vault\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) javelin throw --> cooking sausages --> riding mule --> pole vault\n(B) javelin throw --> riding mule --> cooking sausages --> pole vault\n(C) riding mule --> pole vault --> javelin throw --> cooking sausages\n(D) cooking sausages --> riding mule --> javelin throw --> pole vault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2110, "doc": {"video_name": "order_301.mp4", "duration": 781.49, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) milking cow --> tossing coin --> making jewelry --> jetskiing\n(B) jetskiing --> making jewelry --> milking cow --> tossing coin\n(C) milking cow --> jetskiing --> tossing coin --> making jewelry\n(D) making jewelry --> jetskiing --> milking cow --> tossing coin\n", "candidates": ["milking cow --> tossing coin --> making jewelry --> jetskiing", "jetskiing --> making jewelry --> milking cow --> tossing coin", "milking cow --> jetskiing --> tossing coin --> making jewelry", "making jewelry --> jetskiing --> milking cow --> tossing coin"], "answer": "B", "task_type": "order", "question_id": "Q2110"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e2ee511a55c38c81a6be9da70948b062455e6aad9c14b0a6b312d57b9ab7ac5b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) milking cow --> tossing coin --> making jewelry --> jetskiing\n(B) jetskiing --> making jewelry --> milking cow --> tossing coin\n(C) milking cow --> jetskiing --> tossing coin --> making jewelry\n(D) making jewelry --> jetskiing --> milking cow --> tossing coin\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) milking cow --> tossing coin --> making jewelry --> jetskiing\n(B) jetskiing --> making jewelry --> milking cow --> tossing coin\n(C) milking cow --> jetskiing --> tossing coin --> making jewelry\n(D) making jewelry --> jetskiing --> milking cow --> tossing coin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2114, "doc": {"video_name": "order_3.mp4", "duration": 708.41, "question": "Arrange the following events from the video in the correct chronological order: (1)The group begins to dance in unison; (2)Some of the group are on their feet; (3)A group gathers to the center of a gym floor; (4)Some are in wheel chairs.\n(A) 3->1->2->4\n(B) 2->3->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "candidates": ["3->1->2->4", "2->3->1->4", "4->3->2->1", "1->2->3->4"], "answer": "A", "task_type": "order", "question_id": "Q2114"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "735cfe27ce7d0ca445314eec7b1f7f3c53af71c2816fc2f1c8ade918260ba8ab", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The group begins to dance in unison; (2)Some of the group are on their feet; (3)A group gathers to the center of a gym floor; (4)Some are in wheel chairs.\n(A) 3->1->2->4\n(B) 2->3->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The group begins to dance in unison; (2)Some of the group are on their feet; (3)A group gathers to the center of a gym floor; (4)Some are in wheel chairs.\n(A) 3->1->2->4\n(B) 2->3->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2118, "doc": {"video_name": "order_39.mp4", "duration": 678.69, "question": "Arrange the following events from the video in the correct chronological order: (1)The man replaces the tire on the front rim and pumps it up; (2)The man removes the front tire of the bike from the frame; (3)The man installs a headlamp to the bike; (4)The man reinstalls the front tire onto the bike frame.\n(A) 2->1->4->3\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 3->4->1->2\n", "candidates": ["2->1->4->3", "4->3->2->1", "1->2->3->4", "3->4->1->2"], "answer": "A", "task_type": "order", "question_id": "Q2118"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f7b5837c07270e2fcde851bc96bc3a549881e007a8a77b62b7d16d4ca054d7b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man replaces the tire on the front rim and pumps it up; (2)The man removes the front tire of the bike from the frame; (3)The man installs a headlamp to the bike; (4)The man reinstalls the front tire onto the bike frame.\n(A) 2->1->4->3\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 3->4->1->2\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man replaces the tire on the front rim and pumps it up; (2)The man removes the front tire of the bike from the frame; (3)The man installs a headlamp to the bike; (4)The man reinstalls the front tire onto the bike frame.\n(A) 2->1->4->3\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 3->4->1->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2122, "doc": {"video_name": "order_35.mp4", "duration": 675.78, "question": "Arrange the following events from the video in the correct chronological order: (1)The man snowboards down a hill and turns around; (2)An old man holds a surfboard and puts on a helmet to snowboard; (3)A young person sits on the snow wearing a snowboard; (4)The man has a hot drink with other people.\n(A) 2->1->3->4\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 1->2->4->3\n", "candidates": ["2->1->3->4", "2->1->4->3", "1->2->3->4", "1->2->4->3"], "answer": "B", "task_type": "order", "question_id": "Q2122"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ae3db9af93db20c123a85ef34a8a38a6fb40e557857bdcd52783b5f59f192825", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man snowboards down a hill and turns around; (2)An old man holds a surfboard and puts on a helmet to snowboard; (3)A young person sits on the snow wearing a snowboard; (4)The man has a hot drink with other people.\n(A) 2->1->3->4\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 1->2->4->3\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man snowboards down a hill and turns around; (2)An old man holds a surfboard and puts on a helmet to snowboard; (3)A young person sits on the snow wearing a snowboard; (4)The man has a hot drink with other people.\n(A) 2->1->3->4\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 1->2->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2126, "doc": {"video_name": "order_32.mp4", "duration": 665.06, "question": "Arrange the following events from the video in the correct chronological order: (1)A man on the street with a poster sign tries to get customers; (2)A university swim team is doing a fund raiser washing cars; (3)The students thank people in the video and to come support them; (4)A black screen appears with a website address.\n(A) 1->2->3->4\n(B) 2->3->1->4\n(C) 2->1->3->4\n(D) 1->3->2->4\n", "candidates": ["1->2->3->4", "2->3->1->4", "2->1->3->4", "1->3->2->4"], "answer": "C", "task_type": "order", "question_id": "Q2126"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ee6343e8786a9b81f48eb387990239e962b53dfa2a8b26b2f6d385625105a611", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A man on the street with a poster sign tries to get customers; (2)A university swim team is doing a fund raiser washing cars; (3)The students thank people in the video and to come support them; (4)A black screen appears with a website address.\n(A) 1->2->3->4\n(B) 2->3->1->4\n(C) 2->1->3->4\n(D) 1->3->2->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A man on the street with a poster sign tries to get customers; (2)A university swim team is doing a fund raiser washing cars; (3)The students thank people in the video and to come support them; (4)A black screen appears with a website address.\n(A) 1->2->3->4\n(B) 2->3->1->4\n(C) 2->1->3->4\n(D) 1->3->2->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2130, "doc": {"video_name": "order_30.mp4", "duration": 665.06, "question": "Arrange the following events from the video in the correct chronological order: (1)A man on the street with a poster sign tries to get customers; (2)A university swim team is doing a fund raiser washing cars; (3)The students thank people in the video and to come support them; (4)A black screen appears with a website address.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 2->3->1->4\n(D) 1->3->2->4\n", "candidates": ["2->1->3->4", "1->2->3->4", "2->3->1->4", "1->3->2->4"], "answer": "A", "task_type": "order", "question_id": "Q2130"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d52355ec97b2edeb0f49e2d3726b040d8b9a4ea1404251c6bb2accace75f81a0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A man on the street with a poster sign tries to get customers; (2)A university swim team is doing a fund raiser washing cars; (3)The students thank people in the video and to come support them; (4)A black screen appears with a website address.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 2->3->1->4\n(D) 1->3->2->4\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A man on the street with a poster sign tries to get customers; (2)A university swim team is doing a fund raiser washing cars; (3)The students thank people in the video and to come support them; (4)A black screen appears with a website address.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 2->3->1->4\n(D) 1->3->2->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2134, "doc": {"video_name": "order_179.mp4", "duration": 485.51000000000005, "question": "Arrange the following events from the video in the correct chronological order: (1)The man starts to demonstrate playing the bongos in a lesson; (2)LP and Giovanni Logo appear on the black screen opening; (3)The lesson continues, alternating between color and black and white footage; (4)A man sits behind a set of bongo drums.\n(A) 2->1->4->3\n(B) 4->2->1->3\n(C) 1->2->3->4\n(D) 2->4->1->3\n", "candidates": ["2->1->4->3", "4->2->1->3", "1->2->3->4", "2->4->1->3"], "answer": "D", "task_type": "order", "question_id": "Q2134"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "67d8caea6c85c017ce404aac8eb00cd1f300afd1105885c46cee6523109e1c1c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man starts to demonstrate playing the bongos in a lesson; (2)LP and Giovanni Logo appear on the black screen opening; (3)The lesson continues, alternating between color and black and white footage; (4)A man sits behind a set of bongo drums.\n(A) 2->1->4->3\n(B) 4->2->1->3\n(C) 1->2->3->4\n(D) 2->4->1->3\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man starts to demonstrate playing the bongos in a lesson; (2)LP and Giovanni Logo appear on the black screen opening; (3)The lesson continues, alternating between color and black and white footage; (4)A man sits behind a set of bongo drums.\n(A) 2->1->4->3\n(B) 4->2->1->3\n(C) 1->2->3->4\n(D) 2->4->1->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2138, "doc": {"video_name": "order_249.mp4", "duration": 777.05, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) paragliding --> stomping grapes --> carving pumpkin --> shredding paper\n(B) paragliding --> shredding paper --> carving pumpkin --> stomping grapes\n(C) shredding paper --> carving pumpkin --> stomping grapes --> paragliding\n(D) stomping grapes --> shredding paper --> carving pumpkin --> paragliding\n", "candidates": ["paragliding --> stomping grapes --> carving pumpkin --> shredding paper", "paragliding --> shredding paper --> carving pumpkin --> stomping grapes", "shredding paper --> carving pumpkin --> stomping grapes --> paragliding", "stomping grapes --> shredding paper --> carving pumpkin --> paragliding"], "answer": "D", "task_type": "order", "question_id": "Q2138"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d538f77a40194d5243106275c59393efb08ce112277e2a5221ea6fff86c2bdb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) paragliding --> stomping grapes --> carving pumpkin --> shredding paper\n(B) paragliding --> shredding paper --> carving pumpkin --> stomping grapes\n(C) shredding paper --> carving pumpkin --> stomping grapes --> paragliding\n(D) stomping grapes --> shredding paper --> carving pumpkin --> paragliding\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) paragliding --> stomping grapes --> carving pumpkin --> shredding paper\n(B) paragliding --> shredding paper --> carving pumpkin --> stomping grapes\n(C) shredding paper --> carving pumpkin --> stomping grapes --> paragliding\n(D) stomping grapes --> shredding paper --> carving pumpkin --> paragliding\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2142, "doc": {"video_name": "order_101.mp4", "duration": 702.54, "question": "Arrange the following events from the video in the correct chronological order: (1)The trainer and class step in a circle and up on the platform; (2)The trainer leads an aerobic class with people in a gym; (3)The trainer and class walk over then in reverse over the platform; (4)The trainer and class step up sideways on the platform.\n(A) 2->1->4->3\n(B) 4->3->2->1\n(C) 3->4->1->2\n(D) 1->2->3->4\n", "candidates": ["2->1->4->3", "4->3->2->1", "3->4->1->2", "1->2->3->4"], "answer": "A", "task_type": "order", "question_id": "Q2142"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "65fdcc90d39ce510f5802a0dd1ac53d4b20485f831d193ff0622e6818b414ab0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The trainer and class step in a circle and up on the platform; (2)The trainer leads an aerobic class with people in a gym; (3)The trainer and class walk over then in reverse over the platform; (4)The trainer and class step up sideways on the platform.\n(A) 2->1->4->3\n(B) 4->3->2->1\n(C) 3->4->1->2\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The trainer and class step in a circle and up on the platform; (2)The trainer leads an aerobic class with people in a gym; (3)The trainer and class walk over then in reverse over the platform; (4)The trainer and class step up sideways on the platform.\n(A) 2->1->4->3\n(B) 4->3->2->1\n(C) 3->4->1->2\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2146, "doc": {"video_name": "order_326.mp4", "duration": 490.03999999999996, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) tossing coin --> clean and jerk --> zumba --> stomping grapes\n(B) zumba --> tossing coin --> clean and jerk --> stomping grapes\n(C) zumba --> stomping grapes --> clean and jerk --> tossing coin\n(D) clean and jerk --> tossing coin --> zumba --> stomping grapes\n", "candidates": ["tossing coin --> clean and jerk --> zumba --> stomping grapes", "zumba --> tossing coin --> clean and jerk --> stomping grapes", "zumba --> stomping grapes --> clean and jerk --> tossing coin", "clean and jerk --> tossing coin --> zumba --> stomping grapes"], "answer": "B", "task_type": "order", "question_id": "Q2146"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a49c4034a6641455af162200fcd0dadb42a9d27fea65b94a39a2c8c85e4d726f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) tossing coin --> clean and jerk --> zumba --> stomping grapes\n(B) zumba --> tossing coin --> clean and jerk --> stomping grapes\n(C) zumba --> stomping grapes --> clean and jerk --> tossing coin\n(D) clean and jerk --> tossing coin --> zumba --> stomping grapes\n", "task_type": "order", "pred_answer": "D", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) tossing coin --> clean and jerk --> zumba --> stomping grapes\n(B) zumba --> tossing coin --> clean and jerk --> stomping grapes\n(C) zumba --> stomping grapes --> clean and jerk --> tossing coin\n(D) clean and jerk --> tossing coin --> zumba --> stomping grapes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2150, "doc": {"video_name": "order_333.mp4", "duration": 486.18, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) clean and jerk --> paragliding --> making jewelry --> carving pumpkin\n(B) making jewelry --> paragliding --> carving pumpkin --> clean and jerk\n(C) making jewelry --> carving pumpkin --> clean and jerk --> paragliding\n(D) making jewelry --> paragliding --> clean and jerk --> carving pumpkin\n", "candidates": ["clean and jerk --> paragliding --> making jewelry --> carving pumpkin", "making jewelry --> paragliding --> carving pumpkin --> clean and jerk", "making jewelry --> carving pumpkin --> clean and jerk --> paragliding", "making jewelry --> paragliding --> clean and jerk --> carving pumpkin"], "answer": "A", "task_type": "order", "question_id": "Q2150"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3731ba4c8da1596ed0c82bb00296605f72b333bbb526b87c07a09344f9e91ba1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) clean and jerk --> paragliding --> making jewelry --> carving pumpkin\n(B) making jewelry --> paragliding --> carving pumpkin --> clean and jerk\n(C) making jewelry --> carving pumpkin --> clean and jerk --> paragliding\n(D) making jewelry --> paragliding --> clean and jerk --> carving pumpkin\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) clean and jerk --> paragliding --> making jewelry --> carving pumpkin\n(B) making jewelry --> paragliding --> carving pumpkin --> clean and jerk\n(C) making jewelry --> carving pumpkin --> clean and jerk --> paragliding\n(D) making jewelry --> paragliding --> clean and jerk --> carving pumpkin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2154, "doc": {"video_name": "order_325.mp4", "duration": 489.15, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) clean and jerk --> shredding paper --> cooking sausages --> zumba\n(B) zumba --> cooking sausages --> clean and jerk --> shredding paper\n(C) shredding paper --> clean and jerk --> cooking sausages --> zumba\n(D) clean and jerk --> zumba --> cooking sausages --> shredding paper\n", "candidates": ["clean and jerk --> shredding paper --> cooking sausages --> zumba", "zumba --> cooking sausages --> clean and jerk --> shredding paper", "shredding paper --> clean and jerk --> cooking sausages --> zumba", "clean and jerk --> zumba --> cooking sausages --> shredding paper"], "answer": "D", "task_type": "order", "question_id": "Q2154"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f853add6a083ea14acac9ef0bb7ec912c623213186ff88e192f0253708a672e7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) clean and jerk --> shredding paper --> cooking sausages --> zumba\n(B) zumba --> cooking sausages --> clean and jerk --> shredding paper\n(C) shredding paper --> clean and jerk --> cooking sausages --> zumba\n(D) clean and jerk --> zumba --> cooking sausages --> shredding paper\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) clean and jerk --> shredding paper --> cooking sausages --> zumba\n(B) zumba --> cooking sausages --> clean and jerk --> shredding paper\n(C) shredding paper --> clean and jerk --> cooking sausages --> zumba\n(D) clean and jerk --> zumba --> cooking sausages --> shredding paper\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2158, "doc": {"video_name": "order_27.mp4", "duration": 679.6700000000001, "question": "Arrange the following events from the video in the correct chronological order: (1) Cheese is sprinkled on the spaghetti; (2) A plate of spaghetti is shown; (3) Vegetables are added to the pot; (4) All of the contents get mixed and cooked.\n(A) 2->1->3->4\n(B) 2->3->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "candidates": ["2->1->3->4", "2->3->1->4", "4->3->2->1", "1->2->3->4"], "answer": "A", "task_type": "order", "question_id": "Q2158"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b4c4cfd53fb1efd7fc2ec6471c30d2c7abcc6dd4407049364385a80b741cc35", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) Cheese is sprinkled on the spaghetti; (2) A plate of spaghetti is shown; (3) Vegetables are added to the pot; (4) All of the contents get mixed and cooked.\n(A) 2->1->3->4\n(B) 2->3->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1) Cheese is sprinkled on the spaghetti; (2) A plate of spaghetti is shown; (3) Vegetables are added to the pot; (4) All of the contents get mixed and cooked.\n(A) 2->1->3->4\n(B) 2->3->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2162, "doc": {"video_name": "order_23.mp4", "duration": 667.42, "question": "Arrange the following events from the video in the correct chronological order: (1)Mestre Calango performs by the water on the pier; (2)View of a large body of water with a city around it; (3)Mestre Calango takes his shirt and shoes off and performs on the beach; (4)Credits overlay a black screen.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 2->3->1->4\n", "candidates": ["4->3->2->1", "1->2->3->4", "2->1->3->4", "2->3->1->4"], "answer": "C", "task_type": "order", "question_id": "Q2162"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf556e467e67142aa106e0ea13d27e9ec514a27cb5c71166baa2d318fe0d7df5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Mestre Calango performs by the water on the pier; (2)View of a large body of water with a city around it; (3)Mestre Calango takes his shirt and shoes off and performs on the beach; (4)Credits overlay a black screen.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 2->3->1->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Mestre Calango performs by the water on the pier; (2)View of a large body of water with a city around it; (3)Mestre Calango takes his shirt and shoes off and performs on the beach; (4)Credits overlay a black screen.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 2->3->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2166, "doc": {"video_name": "order_208.mp4", "duration": 465.28999999999996, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) paragliding --> playing harp --> riding mule --> making jewelry\n(B) making jewelry --> paragliding --> playing harp --> riding mule\n(C) paragliding --> riding mule --> playing harp --> making jewelry\n(D) riding mule --> making jewelry --> paragliding --> playing harp\n", "candidates": ["paragliding --> playing harp --> riding mule --> making jewelry", "making jewelry --> paragliding --> playing harp --> riding mule", "paragliding --> riding mule --> playing harp --> making jewelry", "riding mule --> making jewelry --> paragliding --> playing harp"], "answer": "B", "task_type": "order", "question_id": "Q2166"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bad65abcb0b0075d501bf569d5d19c75b1ed18d47bc0b852122c4619826e9c02", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) paragliding --> playing harp --> riding mule --> making jewelry\n(B) making jewelry --> paragliding --> playing harp --> riding mule\n(C) paragliding --> riding mule --> playing harp --> making jewelry\n(D) riding mule --> making jewelry --> paragliding --> playing harp\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) paragliding --> playing harp --> riding mule --> making jewelry\n(B) making jewelry --> paragliding --> playing harp --> riding mule\n(C) paragliding --> riding mule --> playing harp --> making jewelry\n(D) riding mule --> making jewelry --> paragliding --> playing harp\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2170, "doc": {"video_name": "order_240.mp4", "duration": 505.03999999999996, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> playing harp --> paragliding --> stomping grapes\n(B) playing harp --> playing trombone --> paragliding --> stomping grapes\n(C) paragliding --> playing harp --> playing trombone --> stomping grapes\n(D) paragliding --> stomping grapes --> playing trombone --> playing harp\n", "candidates": ["playing trombone --> playing harp --> paragliding --> stomping grapes", "playing harp --> playing trombone --> paragliding --> stomping grapes", "paragliding --> playing harp --> playing trombone --> stomping grapes", "paragliding --> stomping grapes --> playing trombone --> playing harp"], "answer": "A", "task_type": "order", "question_id": "Q2170"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B", "B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f87692597b0cd249b0a5ec5b47ce55fc94a9b89d6b1f3012f844980bcd2a14b3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> playing harp --> paragliding --> stomping grapes\n(B) playing harp --> playing trombone --> paragliding --> stomping grapes\n(C) paragliding --> playing harp --> playing trombone --> stomping grapes\n(D) paragliding --> stomping grapes --> playing trombone --> playing harp\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> playing harp --> paragliding --> stomping grapes\n(B) playing harp --> playing trombone --> paragliding --> stomping grapes\n(C) paragliding --> playing harp --> playing trombone --> stomping grapes\n(D) paragliding --> stomping grapes --> playing trombone --> playing harp\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 3, "doc": {"video_name": "needle_33.mp4", "duration": 467.89000000000004, "question": "What is the woman's appearance?\n(A) She is an elderly woman\n(B) She is a young girl\n(C) She is a middle-aged woman\n(D) She is a beautiful woman\n", "candidates": ["She is an elderly woman", "She is a young girl", "She is a middle-aged woman", "She is a beautiful woman"], "answer": "D", "task_type": "needle", "question_id": "Q3"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7ec2a0998e223abb5d2962565abad4f1445c6173853aafe29261b1fb466a1a15", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the woman's appearance?\n(A) She is an elderly woman\n(B) She is a young girl\n(C) She is a middle-aged woman\n(D) She is a beautiful woman\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the woman's appearance?\n(A) She is an elderly woman\n(B) She is a young girl\n(C) She is a middle-aged woman\n(D) She is a beautiful woman\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 7, "doc": {"video_name": "needle_29.mp4", "duration": 461.57, "question": "What are the little girl and her grandmother doing together?\n(A) Watching TV\n(B) Reading a children's book\n(C) Playing a game\n(D) Eating dinner\n", "candidates": ["Watching TV", "Reading a children's book", "Playing a game", "Eating dinner"], "answer": "B", "task_type": "needle", "question_id": "Q7"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "14cf992d6cb43891eac2d8ed21c16292c6869d41ab00d742178259910aa8dd5d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What are the little girl and her grandmother doing together?\n(A) Watching TV\n(B) Reading a children's book\n(C) Playing a game\n(D) Eating dinner\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What are the little girl and her grandmother doing together?\n(A) Watching TV\n(B) Reading a children's book\n(C) Playing a game\n(D) Eating dinner\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 11, "doc": {"video_name": "needle_120.mp4", "duration": 341.5, "question": "Who is the little girl reading a book with?\n(A) Her friend\n(B) Her grandmother\n(C) Her teacher\n(D) Her brother\n", "candidates": ["Her friend", "Her grandmother", "Her teacher", "Her brother"], "answer": "B", "task_type": "needle", "question_id": "Q11"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bb113a2d3634a99e80ea4c01c4ef5ded2f8821f99da1c590496cebadebc700a0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who is the little girl reading a book with?\n(A) Her friend\n(B) Her grandmother\n(C) Her teacher\n(D) Her brother\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Who is the little girl reading a book with?\n(A) Her friend\n(B) Her grandmother\n(C) Her teacher\n(D) Her brother\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 15, "doc": {"video_name": "needle_51.mp4", "duration": 1504.7, "question": "What nationality are the kids having fun in the paddy field?\n(A) American\n(B) Malays\n(C) Chinese\n(D) Indian\n", "candidates": ["American", "Malays", "Chinese", "Indian"], "answer": "B", "task_type": "needle", "question_id": "Q15"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b2fbc1c665dd5e7ef3e9216bf355068f0772d6efd29cbd9354548f551a310a9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What nationality are the kids having fun in the paddy field?\n(A) American\n(B) Malays\n(C) Chinese\n(D) Indian\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What nationality are the kids having fun in the paddy field?\n(A) American\n(B) Malays\n(C) Chinese\n(D) Indian\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 19, "doc": {"video_name": "needle_106.mp4", "duration": 744.44, "question": "What are the young mother and her son doing outdoors in the video?\n(A) They are having a picnic\n(B) They are making a snowman\n(C) They are playing with snow\n(D) They are decorating a Christmas tree\n", "candidates": ["They are having a picnic", "They are making a snowman", "They are playing with snow", "They are decorating a Christmas tree"], "answer": "D", "task_type": "needle", "question_id": "Q19"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3532f81233355d54a77758274bbb34cfd37d6caf58a46b71b5486fbc6cb3ab0a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What are the young mother and her son doing outdoors in the video?\n(A) They are having a picnic\n(B) They are making a snowman\n(C) They are playing with snow\n(D) They are decorating a Christmas tree\n", "task_type": "needle", "pred_answer": "C.", "answer": "D"}, "input": "What are the young mother and her son doing outdoors in the video?\n(A) They are having a picnic\n(B) They are making a snowman\n(C) They are playing with snow\n(D) They are decorating a Christmas tree\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 23, "doc": {"video_name": "needle_33.mp4", "duration": 467.89000000000004, "question": "What is the woman's reaction when the hand comes out of the computer?\n(A) She is surprised\n(B) She is not surprised\n(C) She laughs\n(D) She screams\n", "candidates": ["She is surprised", "She is not surprised", "She laughs", "She screams"], "answer": "A", "task_type": "needle", "question_id": "Q23"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "da0696fbd02e313d6ef39ac742d4b6bcc5a5e406a5b6110a6d5af879ec7f5b3f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the woman's reaction when the hand comes out of the computer?\n(A) She is surprised\n(B) She is not surprised\n(C) She laughs\n(D) She screams\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the woman's reaction when the hand comes out of the computer?\n(A) She is surprised\n(B) She is not surprised\n(C) She laughs\n(D) She screams\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 27, "doc": {"video_name": "needle_18.mp4", "duration": 494.0, "question": "What activity are the people tourists engaged in at the fishing village?\n(A) Swimming\n(B) Fishing\n(C) Walking the embankment\n(D) Sightseeing\n", "candidates": ["Swimming", "Fishing", "Walking the embankment", "Sightseeing"], "answer": "C", "task_type": "needle", "question_id": "Q27"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2817905d132ed6fa17f44dc574cb62a2f47a7ef703dd7de8ebcfaff6ed692a0c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What activity are the people tourists engaged in at the fishing village?\n(A) Swimming\n(B) Fishing\n(C) Walking the embankment\n(D) Sightseeing\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What activity are the people tourists engaged in at the fishing village?\n(A) Swimming\n(B) Fishing\n(C) Walking the embankment\n(D) Sightseeing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 31, "doc": {"video_name": "needle_97.mp4", "duration": 473.59000000000003, "question": "What devices are the couple using while sitting on the couch?\n(A) Tablet and phone\n(B) Phone and TV\n(C) Laptop and phone\n(D) Laptop and TV\n", "candidates": ["Tablet and phone", "Phone and TV", "Laptop and phone", "Laptop and TV"], "answer": "C", "task_type": "needle", "question_id": "Q31"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cfaa5086d5734b18e1155dd2c9245317af04fb8e0c9c40032a1d745094d205c5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What devices are the couple using while sitting on the couch?\n(A) Tablet and phone\n(B) Phone and TV\n(C) Laptop and phone\n(D) Laptop and TV\n", "task_type": "needle", "pred_answer": "A", "answer": "C"}, "input": "What devices are the couple using while sitting on the couch?\n(A) Tablet and phone\n(B) Phone and TV\n(C) Laptop and phone\n(D) Laptop and TV\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 35, "doc": {"video_name": "needle_104.mp4", "duration": 635.6, "question": "What is the condition of the car's windshield as it rides through the mountain country village?\n(A) The windshield is covered in snow\n(B) The windshield is foggy\n(C) The windshield is broken\n(D) The windshield is clean\n", "candidates": ["The windshield is covered in snow", "The windshield is foggy", "The windshield is broken", "The windshield is clean"], "answer": "C", "task_type": "needle", "question_id": "Q35"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bd409ae5452b42152d1a7c0ddb1c586d12045dd467b255b799a0e20dd314ced6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the condition of the car's windshield as it rides through the mountain country village?\n(A) The windshield is covered in snow\n(B) The windshield is foggy\n(C) The windshield is broken\n(D) The windshield is clean\n", "task_type": "needle", "pred_answer": "D", "answer": "C"}, "input": "What is the condition of the car's windshield as it rides through the mountain country village?\n(A) The windshield is covered in snow\n(B) The windshield is foggy\n(C) The windshield is broken\n(D) The windshield is clean\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 39, "doc": {"video_name": "needle_3.mp4", "duration": 496.21999999999997, "question": "Where is the young attractive hispanic medical doctor discussing health issues with the senior patient?\n(A) In a park\n(B) Inside office\n(C) In a hospital ward\n(D) At the patient's home\n", "candidates": ["In a park", "Inside office", "In a hospital ward", "At the patient's home"], "answer": "B", "task_type": "needle", "question_id": "Q39"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "04756bbd8b0c8355309c0feac8fe08e31972dcebba19092a6412eedcf25a75a4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where is the young attractive hispanic medical doctor discussing health issues with the senior patient?\n(A) In a park\n(B) Inside office\n(C) In a hospital ward\n(D) At the patient's home\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Where is the young attractive hispanic medical doctor discussing health issues with the senior patient?\n(A) In a park\n(B) Inside office\n(C) In a hospital ward\n(D) At the patient's home\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 43, "doc": {"video_name": "needle_3.mp4", "duration": 496.21999999999997, "question": "What is the ethnicity of the young medical doctor discussing health issues with the senior patient?\n(A) Caucasian\n(B) Hispanic\n(C) African\n(D) Asian\n", "candidates": ["Caucasian", "Hispanic", "African", "Asian"], "answer": "B", "task_type": "needle", "question_id": "Q43"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cae5ee2bc2d9f795152f74437896475de41ce7e37725963db4672b08488adb45", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the ethnicity of the young medical doctor discussing health issues with the senior patient?\n(A) Caucasian\n(B) Hispanic\n(C) African\n(D) Asian\n", "task_type": "needle", "pred_answer": "A", "answer": "B"}, "input": "What is the ethnicity of the young medical doctor discussing health issues with the senior patient?\n(A) Caucasian\n(B) Hispanic\n(C) African\n(D) Asian\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 47, "doc": {"video_name": "needle_67.mp4", "duration": 475.2, "question": "What can be seen in the background of the power plant block in the video?\n(A) Power poles and chimneys\n(B) Mountains\n(C) Cityscape\n(D) Forest\n", "candidates": ["Power poles and chimneys", "Mountains", "Cityscape", "Forest"], "answer": "A", "task_type": "needle", "question_id": "Q47"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d880de4600d7c7ea046cd7fc01ed6f3fce14c4a67c957569dbfeb7021ce10e7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What can be seen in the background of the power plant block in the video?\n(A) Power poles and chimneys\n(B) Mountains\n(C) Cityscape\n(D) Forest\n", "task_type": "needle", "pred_answer": "A.", "answer": "A"}, "input": "What can be seen in the background of the power plant block in the video?\n(A) Power poles and chimneys\n(B) Mountains\n(C) Cityscape\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 51, "doc": {"video_name": "needle_88.mp4", "duration": 456.0, "question": "What logo is displayed on the screen in the meeting room?\n(A) Microsoft\n(B) Google\n(C) The Goldman Sachs Group\n(D) Apple\n", "candidates": ["Microsoft", "Google", "The Goldman Sachs Group", "Apple"], "answer": "C", "task_type": "needle", "question_id": "Q51"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c940e992d3bf35591c74a6f80abd3dbe4f883a86a0f810c10eae7a57e4e6dcb6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What logo is displayed on the screen in the meeting room?\n(A) Microsoft\n(B) Google\n(C) The Goldman Sachs Group\n(D) Apple\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What logo is displayed on the screen in the meeting room?\n(A) Microsoft\n(B) Google\n(C) The Goldman Sachs Group\n(D) Apple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 55, "doc": {"video_name": "needle_28.mp4", "duration": 461.48, "question": "Who is the little girl sitting on while they read a children's book?\n(A) Her sister\n(B) Her mother\n(C) Her father\n(D) Her grandmother\n", "candidates": ["Her sister", "Her mother", "Her father", "Her grandmother"], "answer": "D", "task_type": "needle", "question_id": "Q55"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f2aa8781c30780b89ccc7760ec49a5414b790e7a20688b179725b73b982b7bb0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Who is the little girl sitting on while they read a children's book?\n(A) Her sister\n(B) Her mother\n(C) Her father\n(D) Her grandmother\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "Who is the little girl sitting on while they read a children's book?\n(A) Her sister\n(B) Her mother\n(C) Her father\n(D) Her grandmother\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 59, "doc": {"video_name": "needle_67.mp4", "duration": 475.2, "question": "What is rising to the sky in the video segment of the power plant block?\n(A) Smoke\n(B) Birds\n(C) Balloons\n(D) Water vapor\n", "candidates": ["Smoke", "Birds", "Balloons", "Water vapor"], "answer": "D", "task_type": "needle", "question_id": "Q59"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fb36bbadc88faaead372cc2d76d3396e4e16e305edc6e1aba687e98da55e96d6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is rising to the sky in the video segment of the power plant block?\n(A) Smoke\n(B) Birds\n(C) Balloons\n(D) Water vapor\n", "task_type": "needle", "pred_answer": "A", "answer": "D"}, "input": "What is rising to the sky in the video segment of the power plant block?\n(A) Smoke\n(B) Birds\n(C) Balloons\n(D) Water vapor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 63, "doc": {"video_name": "needle_26.mp4", "duration": 469.08, "question": "What is attached to the ears of the black cows eating hay at the farm outdoors on a summer sunny day?\n(A) Yellow tags\n(B) Blue tags\n(C) Nothing\n(D) Red tags\n", "candidates": ["Yellow tags", "Blue tags", "Nothing", "Red tags"], "answer": "A", "task_type": "needle", "question_id": "Q63"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1793d499d2e9c87385eb6a0068862fc93a90434193f16986d2e745b3b7442eaf", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is attached to the ears of the black cows eating hay at the farm outdoors on a summer sunny day?\n(A) Yellow tags\n(B) Blue tags\n(C) Nothing\n(D) Red tags\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is attached to the ears of the black cows eating hay at the farm outdoors on a summer sunny day?\n(A) Yellow tags\n(B) Blue tags\n(C) Nothing\n(D) Red tags\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 67, "doc": {"video_name": "needle_68.mp4", "duration": 1367.32, "question": "What season is it when the little Asian girl in a bikini is creating sand piles on the beach at sunset?\n(A) Autumn\n(B) Winter\n(C) Summer\n(D) Spring\n", "candidates": ["Autumn", "Winter", "Summer", "Spring"], "answer": "C", "task_type": "needle", "question_id": "Q67"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6178857e6b92b86851774ea8036a82a1faf6b08a6cfb6a29cf245c373eee38c5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What season is it when the little Asian girl in a bikini is creating sand piles on the beach at sunset?\n(A) Autumn\n(B) Winter\n(C) Summer\n(D) Spring\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What season is it when the little Asian girl in a bikini is creating sand piles on the beach at sunset?\n(A) Autumn\n(B) Winter\n(C) Summer\n(D) Spring\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 71, "doc": {"video_name": "needle_46.mp4", "duration": 461.25, "question": "What event are the students celebrating in the park?\n(A) Admission\n(B) Birthday\n(C) Promotion\n(D) Graduation\n", "candidates": ["Admission", "Birthday", "Promotion", "Graduation"], "answer": "D", "task_type": "needle", "question_id": "Q71"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "02f5096f48c989d0a1fa514df8590ff76236212c6cbf1ef295f9f44b26982a0e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What event are the students celebrating in the park?\n(A) Admission\n(B) Birthday\n(C) Promotion\n(D) Graduation\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What event are the students celebrating in the park?\n(A) Admission\n(B) Birthday\n(C) Promotion\n(D) Graduation\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 75, "doc": {"video_name": "needle_70.mp4", "duration": 464.76, "question": "What is the weather like when the tourists are strolling in the Plaza de Espana in Seville?\n(A) Cloudy\n(B) Snowy\n(C) Rainy\n(D) Sunny\n", "candidates": ["Cloudy", "Snowy", "Rainy", "Sunny"], "answer": "D", "task_type": "needle", "question_id": "Q75"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a27685e2c6aaad97653eb41f4d7141521fb72b86a126b2c50a56d3f06e95ba3a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the weather like when the tourists are strolling in the Plaza de Espana in Seville?\n(A) Cloudy\n(B) Snowy\n(C) Rainy\n(D) Sunny\n", "task_type": "needle", "pred_answer": "A", "answer": "D"}, "input": "What is the weather like when the tourists are strolling in the Plaza de Espana in Seville?\n(A) Cloudy\n(B) Snowy\n(C) Rainy\n(D) Sunny\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 79, "doc": {"video_name": "needle_17.mp4", "duration": 867.89, "question": "What does the Jack Russell dog want to do while being stroked by the owner in sports boots?\n(A) Play fetch\n(B) Run away\n(C) Sleep\n(D) Eat\n", "candidates": ["Play fetch", "Run away", "Sleep", "Eat"], "answer": "B", "task_type": "needle", "question_id": "Q79"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d78d294e2b2b101b5b520fb058fc57e2ac988276f392c24db365af361fb727b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What does the Jack Russell dog want to do while being stroked by the owner in sports boots?\n(A) Play fetch\n(B) Run away\n(C) Sleep\n(D) Eat\n", "task_type": "needle", "pred_answer": "A", "answer": "B"}, "input": "What does the Jack Russell dog want to do while being stroked by the owner in sports boots?\n(A) Play fetch\n(B) Run away\n(C) Sleep\n(D) Eat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 83, "doc": {"video_name": "needle_138.mp4", "duration": 762.02, "question": "What is the mood of the two young women sitting on the bench in the street?\n(A) Sad\n(B) Positive\n(C) Angry\n(D) Indifferent\n", "candidates": ["Sad", "Positive", "Angry", "Indifferent"], "answer": "B", "task_type": "needle", "question_id": "Q83"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ddf4fa3125dca390f6ede4bbe1551ad14fa7070b710bbbf617d9396dd3f0f5f6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the mood of the two young women sitting on the bench in the street?\n(A) Sad\n(B) Positive\n(C) Angry\n(D) Indifferent\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the mood of the two young women sitting on the bench in the street?\n(A) Sad\n(B) Positive\n(C) Angry\n(D) Indifferent\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 87, "doc": {"video_name": "needle_16.mp4", "duration": 467.08, "question": "What type of dog is the owner in sports boots stroking?\n(A) Jack Russell\n(B) German Shepherd\n(C) Golden Retriever\n(D) Bulldog\n", "candidates": ["Jack Russell", "German Shepherd", "Golden Retriever", "Bulldog"], "answer": "A", "task_type": "needle", "question_id": "Q87"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc6cc0941245e9d1280713aad3cb7517ba82613f68960497b00643413070dd3c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of dog is the owner in sports boots stroking?\n(A) Jack Russell\n(B) German Shepherd\n(C) Golden Retriever\n(D) Bulldog\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What type of dog is the owner in sports boots stroking?\n(A) Jack Russell\n(B) German Shepherd\n(C) Golden Retriever\n(D) Bulldog\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 91, "doc": {"video_name": "needle_35.mp4", "duration": 455.36, "question": "What are the dentist and his assistant doing to the patient's teeth in the video segment?\n(A) Cleaning\n(B) Extracting\n(C) Treating\n(D) Whitening\n", "candidates": ["Cleaning", "Extracting", "Treating", "Whitening"], "answer": "C", "task_type": "needle", "question_id": "Q91"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "73744cb96111eac0ebaad831ef90a076290c596bbf8e5e1d7eb647512b2a84c7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the dentist and his assistant doing to the patient's teeth in the video segment?\n(A) Cleaning\n(B) Extracting\n(C) Treating\n(D) Whitening\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What are the dentist and his assistant doing to the patient's teeth in the video segment?\n(A) Cleaning\n(B) Extracting\n(C) Treating\n(D) Whitening\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 95, "doc": {"video_name": "needle_83.mp4", "duration": 464.38, "question": "What is the man's age?\n(A) Mature\n(B) Teenager\n(C) Elderly\n(D) Child\n", "candidates": ["Mature", "Teenager", "Elderly", "Child"], "answer": "A", "task_type": "needle", "question_id": "Q95"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8ca2ee84854326c48c61150e5501e0846335f31c6eecfd0c3db6ee74436ca351", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the man's age?\n(A) Mature\n(B) Teenager\n(C) Elderly\n(D) Child\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the man's age?\n(A) Mature\n(B) Teenager\n(C) Elderly\n(D) Child\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 99, "doc": {"video_name": "needle_61.mp4", "duration": 496.97999999999996, "question": "What is the appearance of the senior businessman by the sea shore?\n(A) Formal with a suit\n(B) Posh looking with glasses\n(C) Sporty with a tracksuit\n(D) Casual with a hat\n", "candidates": ["Formal with a suit", "Posh looking with glasses", "Sporty with a tracksuit", "Casual with a hat"], "answer": "B", "task_type": "needle", "question_id": "Q99"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c3b3ef0fa8dbe8f3adfda75c5772500faa56d98fb96d277c59962b594427285e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the appearance of the senior businessman by the sea shore?\n(A) Formal with a suit\n(B) Posh looking with glasses\n(C) Sporty with a tracksuit\n(D) Casual with a hat\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the appearance of the senior businessman by the sea shore?\n(A) Formal with a suit\n(B) Posh looking with glasses\n(C) Sporty with a tracksuit\n(D) Casual with a hat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 103, "doc": {"video_name": "needle_39.mp4", "duration": 583.8199999999999, "question": "What season is it when the silhouette fishermen are fishing on the beach?\n(A) Winter\n(B) Spring\n(C) Monsoon\n(D) Summer\n", "candidates": ["Winter", "Spring", "Monsoon", "Summer"], "answer": "C", "task_type": "needle", "question_id": "Q103"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "895755e6e6dd91425990cacb597817f5840c6a757ece65d948b14e0a7353f406", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What season is it when the silhouette fishermen are fishing on the beach?\n(A) Winter\n(B) Spring\n(C) Monsoon\n(D) Summer\n", "task_type": "needle", "pred_answer": "D", "answer": "C"}, "input": "What season is it when the silhouette fishermen are fishing on the beach?\n(A) Winter\n(B) Spring\n(C) Monsoon\n(D) Summer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 107, "doc": {"video_name": "needle_102.mp4", "duration": 6028.639999999999, "question": "What is the American toad's strategy to avoid detection in the video?\n(A) Camouflage\n(B) Fighting\n(C) Hiding\n(D) Running away\n", "candidates": ["Camouflage", "Fighting", "Hiding", "Running away"], "answer": "A", "task_type": "needle", "question_id": "Q107"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b0fc6d1149616b2fd56f7f985623f15d32efc80fbdf80e8d2030663f7faca2de", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the American toad's strategy to avoid detection in the video?\n(A) Camouflage\n(B) Fighting\n(C) Hiding\n(D) Running away\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the American toad's strategy to avoid detection in the video?\n(A) Camouflage\n(B) Fighting\n(C) Hiding\n(D) Running away\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 111, "doc": {"video_name": "needle_58.mp4", "duration": 459.98, "question": "What procedure is the female employee of the cosmetology clinic performing on the patient?\n(A) Manicure\n(B) Microblasting\n(C) Haircut\n(D) Facial\n", "candidates": ["Manicure", "Microblasting", "Haircut", "Facial"], "answer": "B", "task_type": "needle", "question_id": "Q111"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "769fc08b7235d275a2cc442631ab453e1670634767c1d3383e3f67f60ca0190e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What procedure is the female employee of the cosmetology clinic performing on the patient?\n(A) Manicure\n(B) Microblasting\n(C) Haircut\n(D) Facial\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What procedure is the female employee of the cosmetology clinic performing on the patient?\n(A) Manicure\n(B) Microblasting\n(C) Haircut\n(D) Facial\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 115, "doc": {"video_name": "needle_82.mp4", "duration": 494.38, "question": "What is the man's age?\n(A) Elderly\n(B) Teenager\n(C) Mature\n(D) Child\n", "candidates": ["Elderly", "Teenager", "Mature", "Child"], "answer": "C", "task_type": "needle", "question_id": "Q115"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d0e5966c9ebc63415524e0c88bb9254f11ca0618447b4bd323fc59eda61fdc4a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the man's age?\n(A) Elderly\n(B) Teenager\n(C) Mature\n(D) Child\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the man's age?\n(A) Elderly\n(B) Teenager\n(C) Mature\n(D) Child\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 119, "doc": {"video_name": "needle_32.mp4", "duration": 467.98, "question": "What is the woman's reaction when the hand comes out of the computer?\n(A) She laughs\n(B) She screams\n(C) She is not surprised\n(D) She is surprised\n", "candidates": ["She laughs", "She screams", "She is not surprised", "She is surprised"], "answer": "D", "task_type": "needle", "question_id": "Q119"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43b63f81ec921cb7662c355864aa67ddb73caff6687190fedf00349a87d34615", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the woman's reaction when the hand comes out of the computer?\n(A) She laughs\n(B) She screams\n(C) She is not surprised\n(D) She is surprised\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "What is the woman's reaction when the hand comes out of the computer?\n(A) She laughs\n(B) She screams\n(C) She is not surprised\n(D) She is surprised\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 123, "doc": {"video_name": "needle_24.mp4", "duration": 462.03999999999996, "question": "What is the prey of the blue fin trevally in the video?\n(A) Small reef fish and anthias\n(B) Seaweed\n(C) Plankton\n(D) Jellyfish\n", "candidates": ["Small reef fish and anthias", "Seaweed", "Plankton", "Jellyfish"], "answer": "A", "task_type": "needle", "question_id": "Q123"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "74f8e50df7f1f444877f67f0f78551f100d8de9497a8d628753d29f5cba8e573", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the prey of the blue fin trevally in the video?\n(A) Small reef fish and anthias\n(B) Seaweed\n(C) Plankton\n(D) Jellyfish\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What is the prey of the blue fin trevally in the video?\n(A) Small reef fish and anthias\n(B) Seaweed\n(C) Plankton\n(D) Jellyfish\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 127, "doc": {"video_name": "needle_23.mp4", "duration": 487.29, "question": "What color is the pumpkin that the chef is using to prepare the soup?\n(A) White\n(B) Green\n(C) Yellow\n(D) Orange\n", "candidates": ["White", "Green", "Yellow", "Orange"], "answer": "D", "task_type": "needle", "question_id": "Q127"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9312db3a6aff10bde880c322abf019a064647d4f6530071c42f14c1a7f4ddc6c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the pumpkin that the chef is using to prepare the soup?\n(A) White\n(B) Green\n(C) Yellow\n(D) Orange\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What color is the pumpkin that the chef is using to prepare the soup?\n(A) White\n(B) Green\n(C) Yellow\n(D) Orange\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 131, "doc": {"video_name": "needle_147.mp4", "duration": 622.0, "question": "What is the senior businessman doing by the sea shore?\n(A) Eating lunch\n(B) Swimming in the sea\n(C) Reading a book\n(D) Having a serious conversation on the cell phone\n", "candidates": ["Eating lunch", "Swimming in the sea", "Reading a book", "Having a serious conversation on the cell phone"], "answer": "D", "task_type": "needle", "question_id": "Q131"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9dbc672789192711b43c5b41c648bd8c7a296a11e54f7aede4c846e938cf66a1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the senior businessman doing by the sea shore?\n(A) Eating lunch\n(B) Swimming in the sea\n(C) Reading a book\n(D) Having a serious conversation on the cell phone\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the senior businessman doing by the sea shore?\n(A) Eating lunch\n(B) Swimming in the sea\n(C) Reading a book\n(D) Having a serious conversation on the cell phone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 135, "doc": {"video_name": "needle_97.mp4", "duration": 473.59000000000003, "question": "What are the couple planning and dreaming about?\n(A) New job\n(B) Vacation\n(C) New home\n(D) Wedding\n", "candidates": ["New job", "Vacation", "New home", "Wedding"], "answer": "C", "task_type": "needle", "question_id": "Q135"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cd5e0ce3cc85e2bef89d66f1de2106425480837bdd6653677bb60116247b39c8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the couple planning and dreaming about?\n(A) New job\n(B) Vacation\n(C) New home\n(D) Wedding\n", "task_type": "needle", "pred_answer": "D", "answer": "C"}, "input": "What are the couple planning and dreaming about?\n(A) New job\n(B) Vacation\n(C) New home\n(D) Wedding\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 139, "doc": {"video_name": "needle_72.mp4", "duration": 471.08, "question": "What is the woman doing in the flower shop?\n(A) Cleaning the shop\n(B) Arranging books\n(C) Watering the plants\n(D) Making a bouquet of fresh flowers\n", "candidates": ["Cleaning the shop", "Arranging books", "Watering the plants", "Making a bouquet of fresh flowers"], "answer": "D", "task_type": "needle", "question_id": "Q139"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d5ace79fff71087aae9d555f01cb0f9120121c4d25826d1e620455bf64a5e273", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the woman doing in the flower shop?\n(A) Cleaning the shop\n(B) Arranging books\n(C) Watering the plants\n(D) Making a bouquet of fresh flowers\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the woman doing in the flower shop?\n(A) Cleaning the shop\n(B) Arranging books\n(C) Watering the plants\n(D) Making a bouquet of fresh flowers\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 143, "doc": {"video_name": "needle_14.mp4", "duration": 468.41999999999996, "question": "What are the young mother and her son doing outdoors in the video?\n(A) They are making a snowman\n(B) They are decorating a Christmas tree\n(C) They are having a picnic\n(D) They are playing with snow\n", "candidates": ["They are making a snowman", "They are decorating a Christmas tree", "They are having a picnic", "They are playing with snow"], "answer": "B", "task_type": "needle", "question_id": "Q143"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a33d602d5db4893c32c28dd6ab02d3288acee3c0b22d8795af0bcb73eb1dc25f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What are the young mother and her son doing outdoors in the video?\n(A) They are making a snowman\n(B) They are decorating a Christmas tree\n(C) They are having a picnic\n(D) They are playing with snow\n", "task_type": "needle", "pred_answer": "D", "answer": "B"}, "input": "What are the young mother and her son doing outdoors in the video?\n(A) They are making a snowman\n(B) They are decorating a Christmas tree\n(C) They are having a picnic\n(D) They are playing with snow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 147, "doc": {"video_name": "needle_123.mp4", "duration": 6960.84, "question": "What event are the students celebrating in the park?\n(A) Promotion\n(B) Birthday\n(C) Graduation\n(D) Admission\n", "candidates": ["Promotion", "Birthday", "Graduation", "Admission"], "answer": "C", "task_type": "needle", "question_id": "Q147"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0efd299ae937e2b6f71c18b24cded8e3966288d77cae15ca9754c10546b66f04", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What event are the students celebrating in the park?\n(A) Promotion\n(B) Birthday\n(C) Graduation\n(D) Admission\n", "task_type": "needle", "pred_answer": "A", "answer": "C"}, "input": "What event are the students celebrating in the park?\n(A) Promotion\n(B) Birthday\n(C) Graduation\n(D) Admission\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 151, "doc": {"video_name": "needle_27.mp4", "duration": 469.20000000000005, "question": "What color are the cows eating hay from the stall at the farm outdoors on a summer sunny day?\n(A) Spotted\n(B) Black\n(C) White\n(D) Brown\n", "candidates": ["Spotted", "Black", "White", "Brown"], "answer": "B", "task_type": "needle", "question_id": "Q151"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "18a738b0ac8bff0367aabb7c336c183f5c94cad60a36cf4ed1b55600da5ff848", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color are the cows eating hay from the stall at the farm outdoors on a summer sunny day?\n(A) Spotted\n(B) Black\n(C) White\n(D) Brown\n", "task_type": "needle", "pred_answer": "A", "answer": "B"}, "input": "What color are the cows eating hay from the stall at the farm outdoors on a summer sunny day?\n(A) Spotted\n(B) Black\n(C) White\n(D) Brown\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 155, "doc": {"video_name": "needle_12.mp4", "duration": 463.15, "question": "What are the volunteers doing to find the casualties of the rocket attack in the simulation drill?\n(A) Using a metal detector\n(B) Using a search dog\n(C) Using a drone\n(D) Digging through the rubble\n", "candidates": ["Using a metal detector", "Using a search dog", "Using a drone", "Digging through the rubble"], "answer": "D", "task_type": "needle", "question_id": "Q155"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "71acf163912e42de81dc35eb4ed2141d73ff61637883d256a9cef4a41835a147", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What are the volunteers doing to find the casualties of the rocket attack in the simulation drill?\n(A) Using a metal detector\n(B) Using a search dog\n(C) Using a drone\n(D) Digging through the rubble\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What are the volunteers doing to find the casualties of the rocket attack in the simulation drill?\n(A) Using a metal detector\n(B) Using a search dog\n(C) Using a drone\n(D) Digging through the rubble\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 159, "doc": {"video_name": "needle_120.mp4", "duration": 341.5, "question": "Who is the little girl sitting on while they read a children's book?\n(A) Her father\n(B) Her grandmother\n(C) Her sister\n(D) Her mother\n", "candidates": ["Her father", "Her grandmother", "Her sister", "Her mother"], "answer": "B", "task_type": "needle", "question_id": "Q159"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f4dfc275655a9cc678a2dcdb7bb1a1afd32fbcfbb5eee1a38b51e7e4b4aa8518", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who is the little girl sitting on while they read a children's book?\n(A) Her father\n(B) Her grandmother\n(C) Her sister\n(D) Her mother\n", "task_type": "needle", "pred_answer": "A", "answer": "B"}, "input": "Who is the little girl sitting on while they read a children's book?\n(A) Her father\n(B) Her grandmother\n(C) Her sister\n(D) Her mother\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 163, "doc": {"video_name": "needle_75.mp4", "duration": 505.0, "question": "What are the two young women doing on the bench in the street?\n(A) Running\n(B) Communicating\n(C) Eating\n(D) Sleeping\n", "candidates": ["Running", "Communicating", "Eating", "Sleeping"], "answer": "B", "task_type": "needle", "question_id": "Q163"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b23b88d53f204368cd353aa807fdda76c4bb27a9706a84162442a54700b332ab", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What are the two young women doing on the bench in the street?\n(A) Running\n(B) Communicating\n(C) Eating\n(D) Sleeping\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What are the two young women doing on the bench in the street?\n(A) Running\n(B) Communicating\n(C) Eating\n(D) Sleeping\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 167, "doc": {"video_name": "needle_139.mp4", "duration": 239.06, "question": "What is the prey of the blue fin trevally in the video?\n(A) Plankton\n(B) Jellyfish\n(C) Small reef fish and anthias\n(D) Seaweed\n", "candidates": ["Plankton", "Jellyfish", "Small reef fish and anthias", "Seaweed"], "answer": "C", "task_type": "needle", "question_id": "Q167"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "84e11c36ed5645c0bac19f9edbbc6218c5c0bf5ff6376979ebbaf9e7aab4e00d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the prey of the blue fin trevally in the video?\n(A) Plankton\n(B) Jellyfish\n(C) Small reef fish and anthias\n(D) Seaweed\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the prey of the blue fin trevally in the video?\n(A) Plankton\n(B) Jellyfish\n(C) Small reef fish and anthias\n(D) Seaweed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 171, "doc": {"video_name": "needle_28.mp4", "duration": 461.48, "question": "What are the little girl and her grandmother doing together?\n(A) Playing a game\n(B) Eating dinner\n(C) Reading a children's book\n(D) Watching TV\n", "candidates": ["Playing a game", "Eating dinner", "Reading a children's book", "Watching TV"], "answer": "C", "task_type": "needle", "question_id": "Q171"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "94032e4e113429aafba7c86e1f4a2a49e0a81686327d23d24d737813632d86c8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the little girl and her grandmother doing together?\n(A) Playing a game\n(B) Eating dinner\n(C) Reading a children's book\n(D) Watching TV\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What are the little girl and her grandmother doing together?\n(A) Playing a game\n(B) Eating dinner\n(C) Reading a children's book\n(D) Watching TV\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 175, "doc": {"video_name": "needle_91.mp4", "duration": 461.59999999999997, "question": "What is happening to the lily of the valley flowers in the video segment?\n(A) Raindrops are falling on them\n(B) They are blooming\n(C) They are wilting\n(D) They are being picked\n", "candidates": ["Raindrops are falling on them", "They are blooming", "They are wilting", "They are being picked"], "answer": "A", "task_type": "needle", "question_id": "Q175"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2cfd04e868f2295b5cb6fefe020045c46f12b103394294c986c4d27a4ffad412", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is happening to the lily of the valley flowers in the video segment?\n(A) Raindrops are falling on them\n(B) They are blooming\n(C) They are wilting\n(D) They are being picked\n", "task_type": "needle", "pred_answer": "A.", "answer": "A"}, "input": "What is happening to the lily of the valley flowers in the video segment?\n(A) Raindrops are falling on them\n(B) They are blooming\n(C) They are wilting\n(D) They are being picked\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 179, "doc": {"video_name": "needle_66.mp4", "duration": 475.20000000000005, "question": "What time of the day is it in the video segment of the power plant block?\n(A) Midday\n(B) Dawn\n(C) Night\n(D) Dusk\n", "candidates": ["Midday", "Dawn", "Night", "Dusk"], "answer": "B", "task_type": "needle", "question_id": "Q179"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "18c463a9de85bba463b394fe6c29102a6cc6afa9b5443c574f6fdbe3390b8125", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What time of the day is it in the video segment of the power plant block?\n(A) Midday\n(B) Dawn\n(C) Night\n(D) Dusk\n", "task_type": "needle", "pred_answer": "D", "answer": "B"}, "input": "What time of the day is it in the video segment of the power plant block?\n(A) Midday\n(B) Dawn\n(C) Night\n(D) Dusk\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 183, "doc": {"video_name": "needle_22.mp4", "duration": 884.64, "question": "What is the chef preparing with the fresh orange pumpkin?\n(A) Pumpkin pie\n(B) Creamy pumpkin soup\n(C) Pumpkin salad\n(D) Roasted pumpkin\n", "candidates": ["Pumpkin pie", "Creamy pumpkin soup", "Pumpkin salad", "Roasted pumpkin"], "answer": "B", "task_type": "needle", "question_id": "Q183"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a60c99e19b38374bd605df0a3a725967b419e4a46daabaeb0048b1194b1a8461", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the chef preparing with the fresh orange pumpkin?\n(A) Pumpkin pie\n(B) Creamy pumpkin soup\n(C) Pumpkin salad\n(D) Roasted pumpkin\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the chef preparing with the fresh orange pumpkin?\n(A) Pumpkin pie\n(B) Creamy pumpkin soup\n(C) Pumpkin salad\n(D) Roasted pumpkin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 187, "doc": {"video_name": "needle_22.mp4", "duration": 884.64, "question": "What is the chef doing with the fresh orange pumpkin on the wooden table?\n(A) Peeling it\n(B) Boiling it\n(C) Roasting it\n(D) Cutting it into slices\n", "candidates": ["Peeling it", "Boiling it", "Roasting it", "Cutting it into slices"], "answer": "D", "task_type": "needle", "question_id": "Q187"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f9c0cd36e041d86b5eed045cec17847b8717bf4f854cfc36ecee5f82edb02f0c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the chef doing with the fresh orange pumpkin on the wooden table?\n(A) Peeling it\n(B) Boiling it\n(C) Roasting it\n(D) Cutting it into slices\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the chef doing with the fresh orange pumpkin on the wooden table?\n(A) Peeling it\n(B) Boiling it\n(C) Roasting it\n(D) Cutting it into slices\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 191, "doc": {"video_name": "needle_21.mp4", "duration": 846.12, "question": "What is the condition of the car's windshield as it rides through the mountain country village?\n(A) The windshield is broken\n(B) The windshield is foggy\n(C) The windshield is covered in snow\n(D) The windshield is clean\n", "candidates": ["The windshield is broken", "The windshield is foggy", "The windshield is covered in snow", "The windshield is clean"], "answer": "A", "task_type": "needle", "question_id": "Q191"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "58800fe2f15b5bc223e2f2cd70838e3925a5f287532af1ebb9b2a61151a97e6c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the condition of the car's windshield as it rides through the mountain country village?\n(A) The windshield is broken\n(B) The windshield is foggy\n(C) The windshield is covered in snow\n(D) The windshield is clean\n", "task_type": "needle", "pred_answer": "D", "answer": "A"}, "input": "What is the condition of the car's windshield as it rides through the mountain country village?\n(A) The windshield is broken\n(B) The windshield is foggy\n(C) The windshield is covered in snow\n(D) The windshield is clean\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 195, "doc": {"video_name": "needle_70.mp4", "duration": 464.76, "question": "What is a notable feature in the Plaza de Espana that the tourists are admiring?\n(A) A fountain\n(B) A tree\n(C) A building\n(D) A statue\n", "candidates": ["A fountain", "A tree", "A building", "A statue"], "answer": "A", "task_type": "needle", "question_id": "Q195"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4adcc3c3e757ff3244bfdd3a9051523313a08bcbdac2e0e72958598b2ae4ac65", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is a notable feature in the Plaza de Espana that the tourists are admiring?\n(A) A fountain\n(B) A tree\n(C) A building\n(D) A statue\n", "task_type": "needle", "pred_answer": "C", "answer": "A"}, "input": "What is a notable feature in the Plaza de Espana that the tourists are admiring?\n(A) A fountain\n(B) A tree\n(C) A building\n(D) A statue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 199, "doc": {"video_name": "needle_66.mp4", "duration": 475.20000000000005, "question": "What can be seen in the background of the power plant block in the video?\n(A) Power poles and chimneys\n(B) Forest\n(C) Mountains\n(D) Cityscape\n", "candidates": ["Power poles and chimneys", "Forest", "Mountains", "Cityscape"], "answer": "A", "task_type": "needle", "question_id": "Q199"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bd429c031fd3a966f9eb6b0c4d1f61d70f8c313b7ffdbe1b417e83e0f758902c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What can be seen in the background of the power plant block in the video?\n(A) Power poles and chimneys\n(B) Forest\n(C) Mountains\n(D) Cityscape\n", "task_type": "needle", "pred_answer": "A.", "answer": "A"}, "input": "What can be seen in the background of the power plant block in the video?\n(A) Power poles and chimneys\n(B) Forest\n(C) Mountains\n(D) Cityscape\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 203, "doc": {"video_name": "needle_43.mp4", "duration": 1516.18, "question": "What is the purpose of the SUV being on the side of the desert highway?\n(A) For a car race\n(B) For a road trip\n(C) For a car show\n(D) For green screen or chroma key\n", "candidates": ["For a car race", "For a road trip", "For a car show", "For green screen or chroma key"], "answer": "D", "task_type": "needle", "question_id": "Q203"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "91a307bc914a3e23e903e7728737cec7de284da63a01ea3f559304ef28f42851", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the purpose of the SUV being on the side of the desert highway?\n(A) For a car race\n(B) For a road trip\n(C) For a car show\n(D) For green screen or chroma key\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the purpose of the SUV being on the side of the desert highway?\n(A) For a car race\n(B) For a road trip\n(C) For a car show\n(D) For green screen or chroma key\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 207, "doc": {"video_name": "needle_11.mp4", "duration": 454.52, "question": "Where is the sound engineer moving levers of a multitrack mixing console?\n(A) On the stage\n(B) In the editing suite\n(C) In the control room\n(D) In the recording studio\n", "candidates": ["On the stage", "In the editing suite", "In the control room", "In the recording studio"], "answer": "C", "task_type": "needle", "question_id": "Q207"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07cc1fe1027059edaa89b66bb8565e1e0fcea77440819ec5f28ca8f4f45e69af", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the sound engineer moving levers of a multitrack mixing console?\n(A) On the stage\n(B) In the editing suite\n(C) In the control room\n(D) In the recording studio\n", "task_type": "needle", "pred_answer": "D", "answer": "C"}, "input": "Where is the sound engineer moving levers of a multitrack mixing console?\n(A) On the stage\n(B) In the editing suite\n(C) In the control room\n(D) In the recording studio\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 211, "doc": {"video_name": "needle_39.mp4", "duration": 583.8199999999999, "question": "What is a popular activity on the beach during the monsoon season?\n(A) Fishing\n(B) Picnicking\n(C) Surfing\n(D) Playing volleyball\n", "candidates": ["Fishing", "Picnicking", "Surfing", "Playing volleyball"], "answer": "A", "task_type": "needle", "question_id": "Q211"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d31465a3529223a034eb1cbcff269482811d3eff904703594a48c46ff3f4bea", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is a popular activity on the beach during the monsoon season?\n(A) Fishing\n(B) Picnicking\n(C) Surfing\n(D) Playing volleyball\n", "task_type": "needle", "pred_answer": "C", "answer": "A"}, "input": "What is a popular activity on the beach during the monsoon season?\n(A) Fishing\n(B) Picnicking\n(C) Surfing\n(D) Playing volleyball\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 215, "doc": {"video_name": "needle_45.mp4", "duration": 500.52, "question": "What is being transferred to the beaker in the laboratory?\n(A) Nothing\n(B) Solid substance\n(C) Gas\n(D) Liquid tester\n", "candidates": ["Nothing", "Solid substance", "Gas", "Liquid tester"], "answer": "D", "task_type": "needle", "question_id": "Q215"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4858b1c03cbd36e3bdf0396d8200072725d14d2fea24b9d3f9f3d924c01c9e2e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is being transferred to the beaker in the laboratory?\n(A) Nothing\n(B) Solid substance\n(C) Gas\n(D) Liquid tester\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "What is being transferred to the beaker in the laboratory?\n(A) Nothing\n(B) Solid substance\n(C) Gas\n(D) Liquid tester\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 219, "doc": {"video_name": "needle_10.mp4", "duration": 454.52, "question": "What is the profession of the person moving levers of a multitrack mixing console in the video?\n(A) Musician\n(B) Cameraman\n(C) Director\n(D) Sound engineer\n", "candidates": ["Musician", "Cameraman", "Director", "Sound engineer"], "answer": "D", "task_type": "needle", "question_id": "Q219"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "171eed331ad21af969d31a301a2998f6b7d46db5e3a470ca74a8cbb3f6a6e8ba", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the profession of the person moving levers of a multitrack mixing console in the video?\n(A) Musician\n(B) Cameraman\n(C) Director\n(D) Sound engineer\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the profession of the person moving levers of a multitrack mixing console in the video?\n(A) Musician\n(B) Cameraman\n(C) Director\n(D) Sound engineer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 223, "doc": {"video_name": "needle_9.mp4", "duration": 461.25, "question": "From where is the female making the video call?\n(A) From her home living room\n(B) From a coffee shop\n(C) From her car\n(D) From her office\n", "candidates": ["From her home living room", "From a coffee shop", "From her car", "From her office"], "answer": "A", "task_type": "needle", "question_id": "Q223"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "209d17b5fb4cd4f5ab2ec503f6680693baf2f2b0189de3a674bcc86c58c4ff63", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "From where is the female making the video call?\n(A) From her home living room\n(B) From a coffee shop\n(C) From her car\n(D) From her office\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "From where is the female making the video call?\n(A) From her home living room\n(B) From a coffee shop\n(C) From her car\n(D) From her office\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 227, "doc": {"video_name": "needle_33.mp4", "duration": 467.89000000000004, "question": "What is the woman doing on the computer?\n(A) Watching a movie\n(B) Playing a game\n(C) Shopping online\n(D) Writing an email\n", "candidates": ["Watching a movie", "Playing a game", "Shopping online", "Writing an email"], "answer": "C", "task_type": "needle", "question_id": "Q227"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "67f201510b54194355d395eef10cccc284f7f95a1ebe9865961a8a79efd4626d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the woman doing on the computer?\n(A) Watching a movie\n(B) Playing a game\n(C) Shopping online\n(D) Writing an email\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the woman doing on the computer?\n(A) Watching a movie\n(B) Playing a game\n(C) Shopping online\n(D) Writing an email\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 231, "doc": {"video_name": "needle_1.mp4", "duration": 506.12, "question": "What is the backdrop of the basketball in the video?\n(A) A clear blue sky\n(B) A city skyline\n(C) A forest\n(D) A bright multi-colored cloudy sky\n", "candidates": ["A clear blue sky", "A city skyline", "A forest", "A bright multi-colored cloudy sky"], "answer": "D", "task_type": "needle", "question_id": "Q231"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ee91e1371d8769fc563f7ba71dfd211b63907dbec7a54ee2a735c2a7ec7eb08", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the backdrop of the basketball in the video?\n(A) A clear blue sky\n(B) A city skyline\n(C) A forest\n(D) A bright multi-colored cloudy sky\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the backdrop of the basketball in the video?\n(A) A clear blue sky\n(B) A city skyline\n(C) A forest\n(D) A bright multi-colored cloudy sky\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 235, "doc": {"video_name": "needle_41.mp4", "duration": 477.07, "question": "What animal is sitting very still at the mouth of the den in the video?\n(A) European toad\n(B) American frog\n(C) American toad\n(D) European frog\n", "candidates": ["European toad", "American frog", "American toad", "European frog"], "answer": "C", "task_type": "needle", "question_id": "Q235"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "853055e7281f224094a8ca992748e42bffd6eb2fb93d464c9159c03ca6877907", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What animal is sitting very still at the mouth of the den in the video?\n(A) European toad\n(B) American frog\n(C) American toad\n(D) European frog\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What animal is sitting very still at the mouth of the den in the video?\n(A) European toad\n(B) American frog\n(C) American toad\n(D) European frog\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 239, "doc": {"video_name": "needle_53.mp4", "duration": 461.01, "question": "What is significant about the remembrance war memorial in Toronto, Canada?\n(A) It is surrounded by thousands of American flags\n(B) There are no flags around it\n(C) It is surrounded by thousands of British flags\n(D) It is surrounded by thousands of Canadian flags\n", "candidates": ["It is surrounded by thousands of American flags", "There are no flags around it", "It is surrounded by thousands of British flags", "It is surrounded by thousands of Canadian flags"], "answer": "D", "task_type": "needle", "question_id": "Q239"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "65624ebaaa03c4af46f592bec399fa3322db765479a9e4a3c423f840cb258d0d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is significant about the remembrance war memorial in Toronto, Canada?\n(A) It is surrounded by thousands of American flags\n(B) There are no flags around it\n(C) It is surrounded by thousands of British flags\n(D) It is surrounded by thousands of Canadian flags\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is significant about the remembrance war memorial in Toronto, Canada?\n(A) It is surrounded by thousands of American flags\n(B) There are no flags around it\n(C) It is surrounded by thousands of British flags\n(D) It is surrounded by thousands of Canadian flags\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 243, "doc": {"video_name": "needle_130.mp4", "duration": 395.09999999999997, "question": "What does the Jack Russell dog want to do while being stroked by the owner in sports boots?\n(A) Eat\n(B) Sleep\n(C) Play fetch\n(D) Run away\n", "candidates": ["Eat", "Sleep", "Play fetch", "Run away"], "answer": "D", "task_type": "needle", "question_id": "Q243"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d2af7cb4fd0005b0076d891e49acf34ec9748109418d831147c97390f31c74b1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What does the Jack Russell dog want to do while being stroked by the owner in sports boots?\n(A) Eat\n(B) Sleep\n(C) Play fetch\n(D) Run away\n", "task_type": "needle", "pred_answer": "C", "answer": "D"}, "input": "What does the Jack Russell dog want to do while being stroked by the owner in sports boots?\n(A) Eat\n(B) Sleep\n(C) Play fetch\n(D) Run away\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 247, "doc": {"video_name": "needle_79.mp4", "duration": 465.05000000000007, "question": "What part of the doctor's face is shown in closeup in the video?\n(A) Nose\n(B) Ear\n(C) Mouth\n(D) Eye\n", "candidates": ["Nose", "Ear", "Mouth", "Eye"], "answer": "D", "task_type": "needle", "question_id": "Q247"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68d51436609c434c0a0fff099edae4bce1db4282de049afa0f730acb1f19d6e0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What part of the doctor's face is shown in closeup in the video?\n(A) Nose\n(B) Ear\n(C) Mouth\n(D) Eye\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What part of the doctor's face is shown in closeup in the video?\n(A) Nose\n(B) Ear\n(C) Mouth\n(D) Eye\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 251, "doc": {"video_name": "needle_142.mp4", "duration": 612.03, "question": "What is significant about the remembrance war memorial in Toronto, Canada?\n(A) It is surrounded by thousands of American flags\n(B) It is surrounded by thousands of Canadian flags\n(C) There are no flags around it\n(D) It is surrounded by thousands of British flags\n", "candidates": ["It is surrounded by thousands of American flags", "It is surrounded by thousands of Canadian flags", "There are no flags around it", "It is surrounded by thousands of British flags"], "answer": "B", "task_type": "needle", "question_id": "Q251"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c0f826e99f734f496bad962cc03bf7ba8d9f00fc8845a032751c45f0e00ca54b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is significant about the remembrance war memorial in Toronto, Canada?\n(A) It is surrounded by thousands of American flags\n(B) It is surrounded by thousands of Canadian flags\n(C) There are no flags around it\n(D) It is surrounded by thousands of British flags\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is significant about the remembrance war memorial in Toronto, Canada?\n(A) It is surrounded by thousands of American flags\n(B) It is surrounded by thousands of Canadian flags\n(C) There are no flags around it\n(D) It is surrounded by thousands of British flags\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 255, "doc": {"video_name": "needle_0.mp4", "duration": 476.12, "question": "What is the backdrop of the basketball in the video?\n(A) A clear blue sky\n(B) A bright multi-colored cloudy sky\n(C) A forest\n(D) A city skyline\n", "candidates": ["A clear blue sky", "A bright multi-colored cloudy sky", "A forest", "A city skyline"], "answer": "B", "task_type": "needle", "question_id": "Q255"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B."]], "filtered_resps": ["B."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a5f12aee7b1a542e972736d31ac0ed3038737b2cbc34e4840270c67b7fd75c11", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the backdrop of the basketball in the video?\n(A) A clear blue sky\n(B) A bright multi-colored cloudy sky\n(C) A forest\n(D) A city skyline\n", "task_type": "needle", "pred_answer": "B.", "answer": "B"}, "input": "What is the backdrop of the basketball in the video?\n(A) A clear blue sky\n(B) A bright multi-colored cloudy sky\n(C) A forest\n(D) A city skyline\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 259, "doc": {"video_name": "needle_124.mp4", "duration": 254.78, "question": "What are the tourists doing at the Plaza de Espana in Seville?\n(A) Admiring the fountain\n(B) Swimming in the fountain\n(C) Having a picnic\n(D) Taking a nap\n", "candidates": ["Admiring the fountain", "Swimming in the fountain", "Having a picnic", "Taking a nap"], "answer": "A", "task_type": "needle", "question_id": "Q259"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fe9960caa4238f082950d92c7f2c03f47bd3be277825aea284e2343fa159f813", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What are the tourists doing at the Plaza de Espana in Seville?\n(A) Admiring the fountain\n(B) Swimming in the fountain\n(C) Having a picnic\n(D) Taking a nap\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "What are the tourists doing at the Plaza de Espana in Seville?\n(A) Admiring the fountain\n(B) Swimming in the fountain\n(C) Having a picnic\n(D) Taking a nap\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 263, "doc": {"video_name": "needle_9.mp4", "duration": 461.25, "question": "Who is the female chatting with in the video call from her home living room?\n(A) Her boss\n(B) Her beautiful friend\n(C) Her mother\n(D) Her brother\n", "candidates": ["Her boss", "Her beautiful friend", "Her mother", "Her brother"], "answer": "B", "task_type": "needle", "question_id": "Q263"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1d3e45d4fd7a598ad9f9bfd2c3d08d74dce5f0e8c4bf37fca609c161feb64a0f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who is the female chatting with in the video call from her home living room?\n(A) Her boss\n(B) Her beautiful friend\n(C) Her mother\n(D) Her brother\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Who is the female chatting with in the video call from her home living room?\n(A) Her boss\n(B) Her beautiful friend\n(C) Her mother\n(D) Her brother\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 267, "doc": {"video_name": "needle_146.mp4", "duration": 399.38, "question": "Who are using the face bow in the video segment?\n(A) The assistant alone\n(B) The dentist and his assistant\n(C) The patient\n(D) The dentist alone\n", "candidates": ["The assistant alone", "The dentist and his assistant", "The patient", "The dentist alone"], "answer": "B", "task_type": "needle", "question_id": "Q267"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f712183741eb45ed7ae44fa77518cfdeca7f287127e525290b342c3b4702f67c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who are using the face bow in the video segment?\n(A) The assistant alone\n(B) The dentist and his assistant\n(C) The patient\n(D) The dentist alone\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Who are using the face bow in the video segment?\n(A) The assistant alone\n(B) The dentist and his assistant\n(C) The patient\n(D) The dentist alone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 271, "doc": {"video_name": "needle_125.mp4", "duration": 7369.86, "question": "What is the result of the forest fire on the dry grass in the video segment?\n(A) The grass is being covered in snow\n(B) The grass is turning green\n(C) The grass is growing taller\n(D) The grass is producing smoke and flame\n", "candidates": ["The grass is being covered in snow", "The grass is turning green", "The grass is growing taller", "The grass is producing smoke and flame"], "answer": "D", "task_type": "needle", "question_id": "Q271"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eb01b728318e2994e70d69c6a15f4e4fa1e57ae6a93f125bd45b2e171a8bbe05", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the result of the forest fire on the dry grass in the video segment?\n(A) The grass is being covered in snow\n(B) The grass is turning green\n(C) The grass is growing taller\n(D) The grass is producing smoke and flame\n", "task_type": "needle", "pred_answer": "D", "answer": "D"}, "input": "What is the result of the forest fire on the dry grass in the video segment?\n(A) The grass is being covered in snow\n(B) The grass is turning green\n(C) The grass is growing taller\n(D) The grass is producing smoke and flame\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 275, "doc": {"video_name": "needle_7.mp4", "duration": 471.23, "question": "Where is the woman in the bikini enjoying the summer sun and tanning during her holidays?\n(A) At the beach\n(B) On a sun lounger\n(C) In a hot tub\n(D) In a swimming pool\n", "candidates": ["At the beach", "On a sun lounger", "In a hot tub", "In a swimming pool"], "answer": "D", "task_type": "needle", "question_id": "Q275"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B."]], "filtered_resps": ["B."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6f7b1d841c7c1c80a20adab9fc2706f29465fe33ddacc17f17e017d63e6e96ce", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where is the woman in the bikini enjoying the summer sun and tanning during her holidays?\n(A) At the beach\n(B) On a sun lounger\n(C) In a hot tub\n(D) In a swimming pool\n", "task_type": "needle", "pred_answer": "B.", "answer": "D"}, "input": "Where is the woman in the bikini enjoying the summer sun and tanning during her holidays?\n(A) At the beach\n(B) On a sun lounger\n(C) In a hot tub\n(D) In a swimming pool\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 279, "doc": {"video_name": "needle_120.mp4", "duration": 341.5, "question": "What are the little girl and her grandmother doing together?\n(A) Reading a children's book\n(B) Eating dinner\n(C) Watching TV\n(D) Playing a game\n", "candidates": ["Reading a children's book", "Eating dinner", "Watching TV", "Playing a game"], "answer": "A", "task_type": "needle", "question_id": "Q279"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "79e3c40f10363df1c62446be8375381855583a86239425b3bdc9ab38c5a0b044", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What are the little girl and her grandmother doing together?\n(A) Reading a children's book\n(B) Eating dinner\n(C) Watching TV\n(D) Playing a game\n", "task_type": "needle", "pred_answer": "B", "answer": "A"}, "input": "What are the little girl and her grandmother doing together?\n(A) Reading a children's book\n(B) Eating dinner\n(C) Watching TV\n(D) Playing a game\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 283, "doc": {"video_name": "needle_50.mp4", "duration": 458.64, "question": "How many cows are tied beside the paddy field where the two Malays kids are having fun?\n(A) Two\n(B) Three\n(C) Four\n(D) One\n", "candidates": ["Two", "Three", "Four", "One"], "answer": "A", "task_type": "needle", "question_id": "Q283"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8900284ce0dadb7fb2a1b4b23b184f2faf56d24591aeec64ed2e5b40f13802c6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many cows are tied beside the paddy field where the two Malays kids are having fun?\n(A) Two\n(B) Three\n(C) Four\n(D) One\n", "task_type": "needle", "pred_answer": "A", "answer": "A"}, "input": "How many cows are tied beside the paddy field where the two Malays kids are having fun?\n(A) Two\n(B) Three\n(C) Four\n(D) One\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 287, "doc": {"video_name": "needle_45.mp4", "duration": 500.52, "question": "What equipment are the scientists using for their research in the laboratory?\n(A) Telescope\n(B) Microscope\n(C) Pipette and beaker\n(D) Computer\n", "candidates": ["Telescope", "Microscope", "Pipette and beaker", "Computer"], "answer": "C", "task_type": "needle", "question_id": "Q287"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d12f7fcf0bf3eb4c6f17812a87b9ad96f44c957e59189896a7330f00c85ba90f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What equipment are the scientists using for their research in the laboratory?\n(A) Telescope\n(B) Microscope\n(C) Pipette and beaker\n(D) Computer\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What equipment are the scientists using for their research in the laboratory?\n(A) Telescope\n(B) Microscope\n(C) Pipette and beaker\n(D) Computer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 291, "doc": {"video_name": "needle_69.mp4", "duration": 461.04, "question": "What is the little Asian girl in a bikini doing on the beach at sunset?\n(A) Building a sandcastle\n(B) Creating sand piles\n(C) Swimming in the sea\n(D) Playing with a beach ball\n", "candidates": ["Building a sandcastle", "Creating sand piles", "Swimming in the sea", "Playing with a beach ball"], "answer": "B", "task_type": "needle", "question_id": "Q291"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f2fb34e40b1c854de144f532aacaf25e1f7f60c46cbe73c2c8560d3305db0f34", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the little Asian girl in a bikini doing on the beach at sunset?\n(A) Building a sandcastle\n(B) Creating sand piles\n(C) Swimming in the sea\n(D) Playing with a beach ball\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the little Asian girl in a bikini doing on the beach at sunset?\n(A) Building a sandcastle\n(B) Creating sand piles\n(C) Swimming in the sea\n(D) Playing with a beach ball\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 295, "doc": {"video_name": "needle_28.mp4", "duration": 461.48, "question": "What is the activity involving the little girl and her grandmother?\n(A) They are gardening\n(B) They are reading a children's book\n(C) They are painting\n(D) They are cooking\n", "candidates": ["They are gardening", "They are reading a children's book", "They are painting", "They are cooking"], "answer": "B", "task_type": "needle", "question_id": "Q295"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68da78033a04192c098e4ac64a718d663884216a013d8ad84450776a57df9836", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the activity involving the little girl and her grandmother?\n(A) They are gardening\n(B) They are reading a children's book\n(C) They are painting\n(D) They are cooking\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the activity involving the little girl and her grandmother?\n(A) They are gardening\n(B) They are reading a children's book\n(C) They are painting\n(D) They are cooking\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 299, "doc": {"video_name": "needle_42.mp4", "duration": 470.05, "question": "What is the setting of the scene where the SUV is parked for green screen or chroma key?\n(A) On a mountain\n(B) In a forest\n(C) In a desert\n(D) In a city\n", "candidates": ["On a mountain", "In a forest", "In a desert", "In a city"], "answer": "C", "task_type": "needle", "question_id": "Q299"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e95c0cafa0961de6504a50a3585a213b18b4dcff0c52c955610295a437a22f8b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the setting of the scene where the SUV is parked for green screen or chroma key?\n(A) On a mountain\n(B) In a forest\n(C) In a desert\n(D) In a city\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "What is the setting of the scene where the SUV is parked for green screen or chroma key?\n(A) On a mountain\n(B) In a forest\n(C) In a desert\n(D) In a city\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 303, "doc": {"video_name": "needle_67.mp4", "duration": 475.2, "question": "What is the view of the power plant block in the video?\n(A) Side view\n(B) Ground level view\n(C) Underground view\n(D) Aerial view\n", "candidates": ["Side view", "Ground level view", "Underground view", "Aerial view"], "answer": "D", "task_type": "needle", "question_id": "Q303"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D."]], "filtered_resps": ["D."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cef0bfd2e52a485b0e8a2e21b80bc4b0af8530a24cb5db09f31e12ddee427d7f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the view of the power plant block in the video?\n(A) Side view\n(B) Ground level view\n(C) Underground view\n(D) Aerial view\n", "task_type": "needle", "pred_answer": "D.", "answer": "D"}, "input": "What is the view of the power plant block in the video?\n(A) Side view\n(B) Ground level view\n(C) Underground view\n(D) Aerial view\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 307, "doc": {"video_name": "needle_44.mp4", "duration": 470.52, "question": "What equipment are the scientists using for their research in the laboratory?\n(A) Computer\n(B) Pipette and beaker\n(C) Microscope\n(D) Telescope\n", "candidates": ["Computer", "Pipette and beaker", "Microscope", "Telescope"], "answer": "B", "task_type": "needle", "question_id": "Q307"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d5a465fe1665a033f8b5c38705b997113ed3053e697e42626f224f26f5ad8970", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What equipment are the scientists using for their research in the laboratory?\n(A) Computer\n(B) Pipette and beaker\n(C) Microscope\n(D) Telescope\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What equipment are the scientists using for their research in the laboratory?\n(A) Computer\n(B) Pipette and beaker\n(C) Microscope\n(D) Telescope\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 311, "doc": {"video_name": "needle_46.mp4", "duration": 461.25, "question": "Where are the students celebrating their graduation?\n(A) At home\n(B) In the school auditorium\n(C) In the park\n(D) In the classroom\n", "candidates": ["At home", "In the school auditorium", "In the park", "In the classroom"], "answer": "C", "task_type": "needle", "question_id": "Q311"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc449c48efe11345a79a4c777582bd5734c60424fe76ea4290d73b0a4171a233", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where are the students celebrating their graduation?\n(A) At home\n(B) In the school auditorium\n(C) In the park\n(D) In the classroom\n", "task_type": "needle", "pred_answer": "B", "answer": "C"}, "input": "Where are the students celebrating their graduation?\n(A) At home\n(B) In the school auditorium\n(C) In the park\n(D) In the classroom\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 315, "doc": {"video_name": "needle_42.mp4", "duration": 470.05, "question": "What is the condition of the highway where the SUV is parked for green screen or chroma key?\n(A) Busy with traffic\n(B) Flooded\n(C) Under construction\n(D) Empty\n", "candidates": ["Busy with traffic", "Flooded", "Under construction", "Empty"], "answer": "D", "task_type": "needle", "question_id": "Q315"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c6673b9d6df65e9cb24d7291cde67515b140e9fe1e3267e371f02794832e9b00", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the condition of the highway where the SUV is parked for green screen or chroma key?\n(A) Busy with traffic\n(B) Flooded\n(C) Under construction\n(D) Empty\n", "task_type": "needle", "pred_answer": "B", "answer": "D"}, "input": "What is the condition of the highway where the SUV is parked for green screen or chroma key?\n(A) Busy with traffic\n(B) Flooded\n(C) Under construction\n(D) Empty\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 319, "doc": {"video_name": "needle_92.mp4", "duration": 458.13, "question": "What is the status of the woman enjoying the summer sunset?\n(A) She is a working woman\n(B) She is a child\n(C) She is a retired woman\n(D) She is a student\n", "candidates": ["She is a working woman", "She is a child", "She is a retired woman", "She is a student"], "answer": "C", "task_type": "needle", "question_id": "Q319"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "762a8b4befe336b18214097b664fab435a761817470dd8d2d3688dfe06884750", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the status of the woman enjoying the summer sunset?\n(A) She is a working woman\n(B) She is a child\n(C) She is a retired woman\n(D) She is a student\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the status of the woman enjoying the summer sunset?\n(A) She is a working woman\n(B) She is a child\n(C) She is a retired woman\n(D) She is a student\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 323, "doc": {"video_name": "needle_6.mp4", "duration": 471.23, "question": "What is the woman wearing while enjoying the summer sun and tanning during her holidays in the swimming pool?\n(A) A sundress\n(B) A one-piece swimsuit\n(C) A bikini\n(D) A sarong\n", "candidates": ["A sundress", "A one-piece swimsuit", "A bikini", "A sarong"], "answer": "C", "task_type": "needle", "question_id": "Q323"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d8cd2b32e20747163c01707a44cb782b3bbfc64e3b7a17ab2009c1cd828fd0c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the woman wearing while enjoying the summer sun and tanning during her holidays in the swimming pool?\n(A) A sundress\n(B) A one-piece swimsuit\n(C) A bikini\n(D) A sarong\n", "task_type": "needle", "pred_answer": "C.", "answer": "C"}, "input": "What is the woman wearing while enjoying the summer sun and tanning during her holidays in the swimming pool?\n(A) A sundress\n(B) A one-piece swimsuit\n(C) A bikini\n(D) A sarong\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 327, "doc": {"video_name": "needle_6.mp4", "duration": 471.23, "question": "What is the woman in the bikini holding while enjoying her summer holidays in the swimming pool?\n(A) A beach ball\n(B) A cocktail\n(C) A book\n(D) A towel\n", "candidates": ["A beach ball", "A cocktail", "A book", "A towel"], "answer": "B", "task_type": "needle", "question_id": "Q327"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1b886b51451a0a141f207c10137d01000a16a32832ec35b2188c18cec2124e4f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the woman in the bikini holding while enjoying her summer holidays in the swimming pool?\n(A) A beach ball\n(B) A cocktail\n(C) A book\n(D) A towel\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What is the woman in the bikini holding while enjoying her summer holidays in the swimming pool?\n(A) A beach ball\n(B) A cocktail\n(C) A book\n(D) A towel\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 331, "doc": {"video_name": "needle_90.mp4", "duration": 461.6, "question": "What color are the leaves of the plant shown in the video segment?\n(A) Yellow\n(B) Green\n(C) Red\n(D) Blue\n", "candidates": ["Yellow", "Green", "Red", "Blue"], "answer": "B", "task_type": "needle", "question_id": "Q331"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "72ff2696a0390a8b21a8c3fcc20ce77d4ffacb9ec796b350f22f31bc5d29e825", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color are the leaves of the plant shown in the video segment?\n(A) Yellow\n(B) Green\n(C) Red\n(D) Blue\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "What color are the leaves of the plant shown in the video segment?\n(A) Yellow\n(B) Green\n(C) Red\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 335, "doc": {"video_name": "needle_5.mp4", "duration": 480.0, "question": "What season is it when the man in the black silhouette is on the lake shore?\n(A) Autumn\n(B) Spring\n(C) Summer\n(D) Winter\n", "candidates": ["Autumn", "Spring", "Summer", "Winter"], "answer": "C", "task_type": "needle", "question_id": "Q335"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "659512305120b8df7ab84d3c98c75cc5a6afec42160dd6cfbc6a4564a155ee4f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What season is it when the man in the black silhouette is on the lake shore?\n(A) Autumn\n(B) Spring\n(C) Summer\n(D) Winter\n", "task_type": "needle", "pred_answer": "A", "answer": "C"}, "input": "What season is it when the man in the black silhouette is on the lake shore?\n(A) Autumn\n(B) Spring\n(C) Summer\n(D) Winter\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 339, "doc": {"video_name": "needle_83.mp4", "duration": 464.38, "question": "What is the background color when the man is laughing and covering his mouth?\n(A) Blue\n(B) Green\n(C) White\n(D) Black\n", "candidates": ["Blue", "Green", "White", "Black"], "answer": "C", "task_type": "needle", "question_id": "Q339"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ede6f5dd7065645544a679b781c531b622e5e2ed9aabacba3aefec75995366b6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the background color when the man is laughing and covering his mouth?\n(A) Blue\n(B) Green\n(C) White\n(D) Black\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is the background color when the man is laughing and covering his mouth?\n(A) Blue\n(B) Green\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 343, "doc": {"video_name": "needle_124.mp4", "duration": 254.78, "question": "Where are the tourists strolling and admiring the fountain?\n(A) Plaza de Espana in Barcelona\n(B) Plaza de Espana in Seville\n(C) Plaza de Espana in Valencia\n(D) Plaza de Espana in Madrid\n", "candidates": ["Plaza de Espana in Barcelona", "Plaza de Espana in Seville", "Plaza de Espana in Valencia", "Plaza de Espana in Madrid"], "answer": "B", "task_type": "needle", "question_id": "Q343"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2100c20fced743f81f9278169f7711d6138652ccd34be5f3889ee9bc20a94101", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where are the tourists strolling and admiring the fountain?\n(A) Plaza de Espana in Barcelona\n(B) Plaza de Espana in Seville\n(C) Plaza de Espana in Valencia\n(D) Plaza de Espana in Madrid\n", "task_type": "needle", "pred_answer": "B", "answer": "B"}, "input": "Where are the tourists strolling and admiring the fountain?\n(A) Plaza de Espana in Barcelona\n(B) Plaza de Espana in Seville\n(C) Plaza de Espana in Valencia\n(D) Plaza de Espana in Madrid\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 347, "doc": {"video_name": "needle_80.mp4", "duration": 463.02, "question": "What is the floor made of where the egg falls and breaks?\n(A) Marble\n(B) Glass\n(C) Wood\n(D) Concrete\n", "candidates": ["Marble", "Glass", "Wood", "Concrete"], "answer": "B", "task_type": "needle", "question_id": "Q347"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1369ea3bd6802e756fa8b4fb85d20592646ca13b4f71c0a54ee996750e1ba3c7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the floor made of where the egg falls and breaks?\n(A) Marble\n(B) Glass\n(C) Wood\n(D) Concrete\n", "task_type": "needle", "pred_answer": "C", "answer": "B"}, "input": "What is the floor made of where the egg falls and breaks?\n(A) Marble\n(B) Glass\n(C) Wood\n(D) Concrete\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 351, "doc": {"video_name": "needle_53.mp4", "duration": 461.01, "question": "What is at the remembrance war memorial in Toronto, Canada?\n(A) A fountain\n(B) A large statue of a horse\n(C) Thousands of Canadian flags\n(D) A large statue of a soldier\n", "candidates": ["A fountain", "A large statue of a horse", "Thousands of Canadian flags", "A large statue of a soldier"], "answer": "C", "task_type": "needle", "question_id": "Q351"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "42bf0b68172f43b6a0cfee87712b34d1100afe814a10132c1bb31fbbf08ad0ac", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is at the remembrance war memorial in Toronto, Canada?\n(A) A fountain\n(B) A large statue of a horse\n(C) Thousands of Canadian flags\n(D) A large statue of a soldier\n", "task_type": "needle", "pred_answer": "C", "answer": "C"}, "input": "What is at the remembrance war memorial in Toronto, Canada?\n(A) A fountain\n(B) A large statue of a horse\n(C) Thousands of Canadian flags\n(D) A large statue of a soldier\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 355, "doc": {"video_name": "surveil_20.mp4", "duration": 485.17, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Shooting\n(C) Shoplifting\n(D) Assault\n", "candidates": ["RoadAccidents", "Shooting", "Shoplifting", "Assault"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q355"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9a2bbe79f3875cd4170d9b969276f6ec6334a468936c8b9f3531495d37364429", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Shooting\n(C) Shoplifting\n(D) Assault\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Shooting\n(C) Shoplifting\n(D) Assault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 359, "doc": {"video_name": "surveil_91.mp4", "duration": 526.55, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shooting\n(C) Normal\n(D) Vandalism\n", "candidates": ["Burglary", "Shooting", "Normal", "Vandalism"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q359"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0247598c342b58bf36083801cfd864366f4f990d0392dd71fb41c7bce21f18ae", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shooting\n(C) Normal\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "A"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shooting\n(C) Normal\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 363, "doc": {"video_name": "surveil_134.mp4", "duration": 231.97, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Shooting\n(C) Shooting\n(D) Robbery\n", "candidates": ["Normal", "Shooting", "Shooting", "Robbery"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q363"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6bd6402d7e7b93d8f9d1dab983698bcdbc34a48005ab436cfdfad542c05a4268", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Shooting\n(C) Shooting\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Shooting\n(C) Shooting\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 367, "doc": {"video_name": "surveil_140.mp4", "duration": 318.39, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Normal\n(C) Stealing\n(D) Arson\n", "candidates": ["Robbery", "Normal", "Stealing", "Arson"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q367"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1dfabf881b852614aeaee067ea5346e3ae0fefa27d167387f5454f19e5ede3be", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Normal\n(C) Stealing\n(D) Arson\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Normal\n(C) Stealing\n(D) Arson\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 371, "doc": {"video_name": "surveil_144.mp4", "duration": 274.67, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Fighting\n(B) Arrest\n(C) Normal\n(D) Stealing\n", "candidates": ["Fighting", "Arrest", "Normal", "Stealing"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q371"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "485ab8dc24cf84bced7e03ddb139fa8ce13522cff6c50512e2a918ed6a3e7d2b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Fighting\n(B) Arrest\n(C) Normal\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Fighting\n(B) Arrest\n(C) Normal\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 375, "doc": {"video_name": "surveil_45.mp4", "duration": 299.83, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Abuse\n(C) Fighting\n(D) Assault\n", "candidates": ["Shoplifting", "Abuse", "Fighting", "Assault"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q375"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9f797c65076c17701cd83d924afbf01bc7534def9cabffa66d73809a274d4a68", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Abuse\n(C) Fighting\n(D) Assault\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Abuse\n(C) Fighting\n(D) Assault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 379, "doc": {"video_name": "surveil_57.mp4", "duration": 1355.21, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arrest\n(B) Abuse\n(C) Burglary\n(D) Vandalism\n", "candidates": ["Arrest", "Abuse", "Burglary", "Vandalism"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q379"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9e9e2c5cfc62b5ca2b65186aa5c1c2a1875d25d051cc098d19510ae5d82bd9b6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arrest\n(B) Abuse\n(C) Burglary\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arrest\n(B) Abuse\n(C) Burglary\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 383, "doc": {"video_name": "surveil_46.mp4", "duration": 539.36, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Explosion\n(C) Assault\n(D) RoadAccidents\n", "candidates": ["Arrest", "Explosion", "Assault", "RoadAccidents"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q383"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "114fec3098a0ff68a29aeb0325dbecb95f4220151bac8fd9237d39ddf1934546", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Explosion\n(C) Assault\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Explosion\n(C) Assault\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 387, "doc": {"video_name": "surveil_173.mp4", "duration": 352.88, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Arrest\n(B) Burglary\n(C) Explosion\n(D) Shooting\n", "candidates": ["Arrest", "Burglary", "Explosion", "Shooting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q387"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "30e58cae73281b0a17dd46e5a801d117279aaca09f3639ae41695027f891dd92", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Arrest\n(B) Burglary\n(C) Explosion\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Arrest\n(B) Burglary\n(C) Explosion\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 391, "doc": {"video_name": "surveil_174.mp4", "duration": 246.88, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Burglary\n(B) Robbery\n(C) Shooting\n(D) Stealing\n", "candidates": ["Burglary", "Robbery", "Shooting", "Stealing"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q391"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dbe5f3ee1952d6b8f13aacc04084ea491d2db9021ab9ae2d8f1182bd6d52ac0c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Burglary\n(B) Robbery\n(C) Shooting\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Burglary\n(B) Robbery\n(C) Shooting\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 395, "doc": {"video_name": "surveil_137.mp4", "duration": 276.23, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Explosion\n(B) RoadAccidents\n(C) Shooting\n(D) Assault\n", "candidates": ["Explosion", "RoadAccidents", "Shooting", "Assault"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q395"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "223c1b06b3f3223c67b68070fe5a847a5a5055e38e102cd989027d560d850d64", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Explosion\n(B) RoadAccidents\n(C) Shooting\n(D) Assault\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Explosion\n(B) RoadAccidents\n(C) Shooting\n(D) Assault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 399, "doc": {"video_name": "surveil_117.mp4", "duration": 387.33, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Arrest\n(B) Arson\n(C) Shoplifting\n(D) Abuse\n", "candidates": ["Arrest", "Arson", "Shoplifting", "Abuse"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q399"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "db63894176f6f94a2a17193c02bc5ec84c10dc6136d972c42487d4c694008c1d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Arrest\n(B) Arson\n(C) Shoplifting\n(D) Abuse\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Arrest\n(B) Arson\n(C) Shoplifting\n(D) Abuse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 403, "doc": {"video_name": "surveil_133.mp4", "duration": 297.95, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arson\n(B) Fighting\n(C) Abuse\n(D) Burglary\n", "candidates": ["Arson", "Fighting", "Abuse", "Burglary"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q403"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0dea4740baf157f73a606a3dd0f99c15b3aac2df23747631f73f17fac72bd46c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arson\n(B) Fighting\n(C) Abuse\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arson\n(B) Fighting\n(C) Abuse\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 407, "doc": {"video_name": "surveil_129.mp4", "duration": 250.19, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Shoplifting\n(C) Vandalism\n(D) Robbery\n", "candidates": ["Shooting", "Shoplifting", "Vandalism", "Robbery"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q407"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "24d518371ec1c6c8a233e5fcc6c064da3fdfeacff5f775f8e1dff5d374b65a81", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Shoplifting\n(C) Vandalism\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Shoplifting\n(C) Vandalism\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 411, "doc": {"video_name": "surveil_109.mp4", "duration": 538.75, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Burglary\n(C) Explosion\n(D) Arrest\n", "candidates": ["Abuse", "Burglary", "Explosion", "Arrest"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q411"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "92cfef7e80176448af0af2286db16a7e6b24f60db4983fab59e8ec448de85216", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Burglary\n(C) Explosion\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Abuse\n(B) Burglary\n(C) Explosion\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 415, "doc": {"video_name": "surveil_14.mp4", "duration": 237.27, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Abuse\n(B) Robbery\n(C) Stealing\n(D) Arrest\n", "candidates": ["Abuse", "Robbery", "Stealing", "Arrest"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q415"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8fb3a320024b80953ee92f2f51cd1a739710e32db16e35a93b2c6e359f09d169", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Abuse\n(B) Robbery\n(C) Stealing\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "D"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Abuse\n(B) Robbery\n(C) Stealing\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 419, "doc": {"video_name": "surveil_71.mp4", "duration": 189.7, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Assault\n(B) Shoplifting\n(C) Normal\n(D) Abuse\n", "candidates": ["Assault", "Shoplifting", "Normal", "Abuse"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q419"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6a53e435d1d4332fd1786858d115260b124c4bee73d41a94d817b8417fe65274", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Assault\n(B) Shoplifting\n(C) Normal\n(D) Abuse\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Assault\n(B) Shoplifting\n(C) Normal\n(D) Abuse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 423, "doc": {"video_name": "surveil_128.mp4", "duration": 208.45, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Fighting\n(C) Arson\n(D) Arrest\n", "candidates": ["Robbery", "Fighting", "Arson", "Arrest"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q423"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "72caeebc4aefe4ddf6366a4d3c463a60fb28e328b149bbee924d63626d80c35e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Fighting\n(C) Arson\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Fighting\n(C) Arson\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 427, "doc": {"video_name": "surveil_15.mp4", "duration": 727.24, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Stealing\n(C) RoadAccidents\n(D) Shooting\n", "candidates": ["Normal", "Stealing", "RoadAccidents", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q427"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "def3d55b50fe3e5c5026188499f0d89a07e594eb74c60b93994b90c27d79d263", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Stealing\n(C) RoadAccidents\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Stealing\n(C) RoadAccidents\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 431, "doc": {"video_name": "surveil_181.mp4", "duration": 225.7, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Explosion\n(C) RoadAccidents\n(D) Burglary\n", "candidates": ["Shoplifting", "Explosion", "RoadAccidents", "Burglary"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q431"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d7cbef3027b7b397da294fd6c6a660f9fcf10e40d06b24d4e5ddc86f8c0b2daf", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Explosion\n(C) RoadAccidents\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Explosion\n(C) RoadAccidents\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 435, "doc": {"video_name": "surveil_190.mp4", "duration": 216.2, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Shoplifting\n(C) Normal\n(D) Burglary\n", "candidates": ["Arrest", "Shoplifting", "Normal", "Burglary"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q435"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a9113b4f102d012d7937fc863eeab42bfb5f6e9613c27e8d99f84990c4aca816", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Shoplifting\n(C) Normal\n(D) Burglary\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arrest\n(B) Shoplifting\n(C) Normal\n(D) Burglary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 439, "doc": {"video_name": "surveil_160.mp4", "duration": 431.48, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Vandalism\n(C) Arson\n(D) RoadAccidents\n", "candidates": ["Stealing", "Vandalism", "Arson", "RoadAccidents"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q439"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c5ce450a66eab5f450927630b5b66f7aee9f4357fd360d0781ee960a806d5244", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Vandalism\n(C) Arson\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Vandalism\n(C) Arson\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 443, "doc": {"video_name": "surveil_183.mp4", "duration": 397.9, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Assault\n(C) Stealing\n(D) Shoplifting\n", "candidates": ["Shooting", "Assault", "Stealing", "Shoplifting"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q443"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f71421ed887171cd251e84a0313650f3b2e9a6d1139a74a6311667dd674b1303", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Assault\n(C) Stealing\n(D) Shoplifting\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Assault\n(C) Stealing\n(D) Shoplifting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 447, "doc": {"video_name": "surveil_59.mp4", "duration": 1200.63, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shooting\n(C) Normal\n(D) RoadAccidents\n", "candidates": ["Burglary", "Shooting", "Normal", "RoadAccidents"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q447"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "767626478ad633890522b14b83a7c15a8fefd1d1ac4cf6fe55d80424bdd74d9a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shooting\n(C) Normal\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shooting\n(C) Normal\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 451, "doc": {"video_name": "surveil_17.mp4", "duration": 554.26, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Normal\n(C) Vandalism\n(D) Arrest\n", "candidates": ["Robbery", "Normal", "Vandalism", "Arrest"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q451"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4998cb8a0f50ce7c32aa4903383db751a97ad0c52b56934be6a00fe468e13448", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Normal\n(C) Vandalism\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Normal\n(C) Vandalism\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 455, "doc": {"video_name": "surveil_53.mp4", "duration": 300.06, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Fighting\n(B) Stealing\n(C) RoadAccidents\n(D) Robbery\n", "candidates": ["Fighting", "Stealing", "RoadAccidents", "Robbery"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q455"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7d88cf21043d6892a2a6e4439a794a020b9c4709588423ea8e6e21bae7a1ac13", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Fighting\n(B) Stealing\n(C) RoadAccidents\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Fighting\n(B) Stealing\n(C) RoadAccidents\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 459, "doc": {"video_name": "surveil_8.mp4", "duration": 223.87, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Burglary\n(B) Fighting\n(C) Robbery\n(D) Assault\n", "candidates": ["Burglary", "Fighting", "Robbery", "Assault"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q459"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a48e19945923adb8070e62ee7b647ceb9fff676347dadda1d0e2080bd09fbc18", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Burglary\n(B) Fighting\n(C) Robbery\n(D) Assault\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Burglary\n(B) Fighting\n(C) Robbery\n(D) Assault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 463, "doc": {"video_name": "surveil_12.mp4", "duration": 192.37, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Normal\n(C) Burglary\n(D) Fighting\n", "candidates": ["Shoplifting", "Normal", "Burglary", "Fighting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q463"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "231b20ae19e29d00c47493f366c540723c2e2e7925bcf1eaa6412781b9b480bc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Normal\n(C) Burglary\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Normal\n(C) Burglary\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 467, "doc": {"video_name": "surveil_182.mp4", "duration": 254.38, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Vandalism\n(B) Arson\n(C) Normal\n(D) Explosion\n", "candidates": ["Vandalism", "Arson", "Normal", "Explosion"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q467"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3a8edb1d8d6a2920980f4f1143fc8b42dead9138f7ff0b9686e9e41630de06cb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Vandalism\n(B) Arson\n(C) Normal\n(D) Explosion\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Vandalism\n(B) Arson\n(C) Normal\n(D) Explosion\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 471, "doc": {"video_name": "surveil_106.mp4", "duration": 393.35, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Burglary\n(C) Fighting\n(D) Arrest\n", "candidates": ["Normal", "Burglary", "Fighting", "Arrest"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q471"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bf38a65adc48ac865f1ce3d0282a845edd4a579ac2c6cd8f0110a846e7ce895c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Burglary\n(C) Fighting\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Burglary\n(C) Fighting\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 475, "doc": {"video_name": "surveil_79.mp4", "duration": 360.62, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Shoplifting\n(C) RoadAccidents\n(D) Shooting\n", "candidates": ["Fighting", "Shoplifting", "RoadAccidents", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q475"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "15c396d19749c629e272ca1d9a834b505f77aadea7d9826f93a38dca179d6017", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Shoplifting\n(C) RoadAccidents\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) Shoplifting\n(C) RoadAccidents\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 479, "doc": {"video_name": "surveil_111.mp4", "duration": 229.6, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Normal\n(C) Explosion\n(D) Fighting\n", "candidates": ["RoadAccidents", "Normal", "Explosion", "Fighting"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q479"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5a595517481f56747be84ba44678dd3069e1d95de470e906f9708f097e2a6231", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Normal\n(C) Explosion\n(D) Fighting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "D"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Normal\n(C) Explosion\n(D) Fighting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 483, "doc": {"video_name": "surveil_153.mp4", "duration": 495.13, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shoplifting\n(C) Shooting\n(D) Stealing\n", "candidates": ["Burglary", "Shoplifting", "Shooting", "Stealing"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q483"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "054b5f40d4d65da74ae0522af04060b1a2564093fc7c3293b827eebaf780682f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shoplifting\n(C) Shooting\n(D) Stealing\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "A"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shoplifting\n(C) Shooting\n(D) Stealing\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 487, "doc": {"video_name": "surveil_177.mp4", "duration": 212.51, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Explosion\n(B) Assault\n(C) RoadAccidents\n(D) Normal\n", "candidates": ["Explosion", "Assault", "RoadAccidents", "Normal"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q487"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "78572831189d5f56cff20458250fd2217a65c8a9ca20a7f41087340dad153e6f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Explosion\n(B) Assault\n(C) RoadAccidents\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Explosion\n(B) Assault\n(C) RoadAccidents\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 491, "doc": {"video_name": "surveil_146.mp4", "duration": 3599.84, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Normal\n(C) Arson\n(D) Shooting\n", "candidates": ["Burglary", "Normal", "Arson", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q491"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "24498fb69e02f0af7bb52fa97ad84ff1e4b374e09a0a62fb341c6b52ceba57f7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Normal\n(C) Arson\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Normal\n(C) Arson\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 495, "doc": {"video_name": "surveil_19.mp4", "duration": 559.82, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Shoplifting\n(C) Explosion\n(D) Abuse\n", "candidates": ["Arson", "Shoplifting", "Explosion", "Abuse"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q495"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f6831df02658eb434759374f4089ab240934c699c87332e1d41e6e8a7b89c457", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Shoplifting\n(C) Explosion\n(D) Abuse\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Shoplifting\n(C) Explosion\n(D) Abuse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 499, "doc": {"video_name": "surveil_25.mp4", "duration": 195.06, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Stealing\n(C) Robbery\n(D) Assault\n", "candidates": ["Burglary", "Stealing", "Robbery", "Assault"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q499"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5f30c31a50047449fc406ac87f69ac9c52ec1e902ed61238668bf3dd484b1cf", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Stealing\n(C) Robbery\n(D) Assault\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Stealing\n(C) Robbery\n(D) Assault\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 503, "doc": {"video_name": "surveil_40.mp4", "duration": 183.31, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Shooting\n(C) RoadAccidents\n(D) Robbery\n", "candidates": ["Stealing", "Shooting", "RoadAccidents", "Robbery"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q503"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "edb6252765a185e0284be3e9b0e3f7924353db2de7a18dfc992d2b1bd8c4521e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Shooting\n(C) RoadAccidents\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "D"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Stealing\n(B) Shooting\n(C) RoadAccidents\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 507, "doc": {"video_name": "surveil_185.mp4", "duration": 907.05, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) RoadAccidents\n(B) Vandalism\n(C) Shoplifting\n(D) Arrest\n", "candidates": ["RoadAccidents", "Vandalism", "Shoplifting", "Arrest"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q507"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9f50fd5ff4e3807fdb65f77797b47e29e8d9d64cde131525d351e888a7ad2446", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) RoadAccidents\n(B) Vandalism\n(C) Shoplifting\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) RoadAccidents\n(B) Vandalism\n(C) Shoplifting\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 511, "doc": {"video_name": "surveil_132.mp4", "duration": 297.57, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Assault\n(C) Stealing\n(D) Shoplifting\n", "candidates": ["RoadAccidents", "Assault", "Stealing", "Shoplifting"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q511"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d85cc3664fad55622123c9adf103365376c4ae9e9e9ec1ae653be1eb089e4072", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Assault\n(C) Stealing\n(D) Shoplifting\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Assault\n(C) Stealing\n(D) Shoplifting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 515, "doc": {"video_name": "surveil_34.mp4", "duration": 353.57, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Fighting\n(B) Arrest\n(C) Shooting\n(D) RoadAccidents\n", "candidates": ["Fighting", "Arrest", "Shooting", "RoadAccidents"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q515"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b39cf97afd72217c6574fc1c16bef067a9c1f2c935b808154d8d74ea89784cf", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Fighting\n(B) Arrest\n(C) Shooting\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "D"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Fighting\n(B) Arrest\n(C) Shooting\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 519, "doc": {"video_name": "surveil_162.mp4", "duration": 236.21, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Robbery\n(C) Shooting\n(D) Normal\n", "candidates": ["Shooting", "Robbery", "Shooting", "Normal"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q519"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8a30c7122c58d706ddf7e03beab611e7097fd54e0dff63731064f78a71c25707", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Robbery\n(C) Shooting\n(D) Normal\n", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "B"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shooting\n(B) Robbery\n(C) Shooting\n(D) Normal\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 523, "doc": {"video_name": "surveil_51.mp4", "duration": 180.0, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Normal\n(B) Fighting\n(C) Assault\n(D) Shoplifting\n", "candidates": ["Normal", "Fighting", "Assault", "Shoplifting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q523"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "590782a9f866b8de4b42ef71d9137a49b839f00a7fca085f28ab19f469c31055", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Normal\n(B) Fighting\n(C) Assault\n(D) Shoplifting\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Normal\n(B) Fighting\n(C) Assault\n(D) Shoplifting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 527, "doc": {"video_name": "surveil_18.mp4", "duration": 306.71, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Vandalism\n(B) Normal\n(C) Shooting\n(D) Abuse\n", "candidates": ["Vandalism", "Normal", "Shooting", "Abuse"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q527"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "94124c624b6ccdc798d59a5077827c2a2b61a6f82b2f06a534a7f75c3d06e950", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Vandalism\n(B) Normal\n(C) Shooting\n(D) Abuse\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Vandalism\n(B) Normal\n(C) Shooting\n(D) Abuse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 531, "doc": {"video_name": "surveil_172.mp4", "duration": 321.85, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Abuse\n(C) Fighting\n(D) Robbery\n", "candidates": ["Arson", "Abuse", "Fighting", "Robbery"], "answer": "D", "task_type": "anomaly_reco", "question_id": "Q531"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "77e49dcdc9b58777d64fc2236f96b3cf04805c3d34820f11214e0c27f0b8761e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Abuse\n(C) Fighting\n(D) Robbery\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Abuse\n(C) Fighting\n(D) Robbery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 535, "doc": {"video_name": "surveil_85.mp4", "duration": 601.84, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Robbery\n(C) Burglary\n(D) Arrest\n", "candidates": ["Normal", "Robbery", "Burglary", "Arrest"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q535"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1e92443e1555a33f5cbf550b66d63396600ae7610a40eb4c22dd6d2518c92cb5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Robbery\n(C) Burglary\n(D) Arrest\n", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "A"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Robbery\n(C) Burglary\n(D) Arrest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 539, "doc": {"video_name": "surveil_197.mp4", "duration": 221.8, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Assault\n(C) Robbery\n(D) Shoplifting\n", "candidates": ["Stealing", "Assault", "Robbery", "Shoplifting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "Q539"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e5839ec9bae8e35fc695ee24580fa90b26b8b1f5b2478a246f9c2c581f1f364b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Assault\n(C) Robbery\n(D) Shoplifting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Assault\n(C) Robbery\n(D) Shoplifting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 543, "doc": {"video_name": "surveil_27.mp4", "duration": 318.46, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) Robbery\n(C) Abuse\n(D) Shooting\n", "candidates": ["Stealing", "Robbery", "Abuse", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "Q543"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "24b34f44324471f668b250c6519a943f34ac20398332e8388f8f7b7696ac3924", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) Robbery\n(C) Abuse\n(D) Shooting\n", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) Robbery\n(C) Abuse\n(D) Shooting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 547, "doc": {"video_name": "surveil_143.mp4", "duration": 341.07, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) RoadAccidents\n(C) Arson\n(D) Vandalism\n", "candidates": ["Burglary", "RoadAccidents", "Arson", "Vandalism"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q547"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "15814218ef38d6d207fedb427fc4f83c7d9f23fdfda7fd7c0ecf20797e27278b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) RoadAccidents\n(C) Arson\n(D) Vandalism\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) RoadAccidents\n(C) Arson\n(D) Vandalism\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 551, "doc": {"video_name": "surveil_159.mp4", "duration": 448.68, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Assault\n(C) Normal\n(D) RoadAccidents\n", "candidates": ["Burglary", "Assault", "Normal", "RoadAccidents"], "answer": "C", "task_type": "anomaly_reco", "question_id": "Q551"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fc2c33d2bfedc9510b6de14a4f76900a8e6b2b0b885c226aec0bee30f4328947", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Assault\n(C) Normal\n(D) RoadAccidents\n", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Burglary\n(B) Assault\n(C) Normal\n(D) RoadAccidents\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 555, "doc": {"video_name": "count_126.mp4", "duration": 572.8599999999999, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'playing trombone' action\n(A) 2\n(B) 1\n(C) 5\n(D) 4\n", "candidates": ["2", "1", "5", "4"], "answer": "B", "task_type": "count", "question_id": "Q555"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "90f377ec9f0d9a1173de2628e1bdd085f5ccce68fe22fb628725783e9b298967", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'playing trombone' action\n(A) 2\n(B) 1\n(C) 5\n(D) 4\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'playing trombone' action\n(A) 2\n(B) 1\n(C) 5\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 559, "doc": {"video_name": "count_115.mp4", "duration": 517.3, "question": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 6\n(B) 0\n(C) 5\n(D) 1\n", "candidates": ["6", "0", "5", "1"], "answer": "D", "task_type": "count", "question_id": "Q559"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "50ccbc3301cc23cfbef121f24839e7e507c4abd3e63a10d206c124226bbb5b07", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 6\n(B) 0\n(C) 5\n(D) 1\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 6\n(B) 0\n(C) 5\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 563, "doc": {"video_name": "count_77.mp4", "duration": 470.02, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cleaning toilet' action\n(A) 1\n(B) 2\n(C) 3\n(D) 0\n", "candidates": ["1", "2", "3", "0"], "answer": "B", "task_type": "count", "question_id": "Q563"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ca0f4517bd178ab5b65b034551c362a46312894381bbbc611ebba42de368c7b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cleaning toilet' action\n(A) 1\n(B) 2\n(C) 3\n(D) 0\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cleaning toilet' action\n(A) 1\n(B) 2\n(C) 3\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 567, "doc": {"video_name": "count_107.mp4", "duration": 500.04999999999995, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 5\n(B) 0\n(C) 6\n(D) 3\n", "candidates": ["5", "0", "6", "3"], "answer": "A", "task_type": "count", "question_id": "Q567"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f33feb79bf56bc131c54cfc9d0f4dd1d2480c85954c0819488bdad66b39c6709", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 5\n(B) 0\n(C) 6\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 5\n(B) 0\n(C) 6\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 571, "doc": {"video_name": "count_70.mp4", "duration": 500.03999999999996, "question": "In this video, how many instances are there of the 'cleaning toilet' action scene in total?\n(A) 5\n(B) 6\n(C) 3\n(D) 0\n", "candidates": ["5", "6", "3", "0"], "answer": "A", "task_type": "count", "question_id": "Q571"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9a36d5fa0614da0d5159802df4b6e4b6414f943b94d286abcfdd18f1922b6a0d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'cleaning toilet' action scene in total?\n(A) 5\n(B) 6\n(C) 3\n(D) 0\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "In this video, how many instances are there of the 'cleaning toilet' action scene in total?\n(A) 5\n(B) 6\n(C) 3\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 575, "doc": {"video_name": "count_114.mp4", "duration": 460.03, "question": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 2\n(B) 3\n(C) 6\n(D) 1\n", "candidates": ["2", "3", "6", "1"], "answer": "D", "task_type": "count", "question_id": "Q575"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "afcab4060e2fc5df23a9ad798f81261b7b37642bb299004beef5a9b1c7d9593e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 2\n(B) 3\n(C) 6\n(D) 1\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 2\n(B) 3\n(C) 6\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 579, "doc": {"video_name": "count_13.mp4", "duration": 490.03999999999996, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'tossing coin' action\n(A) 1\n(B) 6\n(C) 5\n(D) 4\n", "candidates": ["1", "6", "5", "4"], "answer": "D", "task_type": "count", "question_id": "Q579"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e00b1b56b45e30bdb46b59bb231fb4b311980a9983d5ef0679809e3b66a5dff5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'tossing coin' action\n(A) 1\n(B) 6\n(C) 5\n(D) 4\n", "task_type": "count", "pred_answer": "D", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'tossing coin' action\n(A) 1\n(B) 6\n(C) 5\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 583, "doc": {"video_name": "count_195.mp4", "duration": 490.02, "question": "In this video, how many times does the scene of the 'baking cookies' action appear in total?\n(A) 1\n(B) 4\n(C) 5\n(D) 6\n", "candidates": ["1", "4", "5", "6"], "answer": "B", "task_type": "count", "question_id": "Q583"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7e52c8bdb100ecf7152cb76830b7b0bd25517486f575a73b974ae7f0b999cbc0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'baking cookies' action appear in total?\n(A) 1\n(B) 4\n(C) 5\n(D) 6\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many times does the scene of the 'baking cookies' action appear in total?\n(A) 1\n(B) 4\n(C) 5\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 587, "doc": {"video_name": "count_110.mp4", "duration": 470.02, "question": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 2\n(B) 5\n(C) 3\n(D) 6\n", "candidates": ["2", "5", "3", "6"], "answer": "A", "task_type": "count", "question_id": "Q587"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9f1904e77653c634389a2c5446fc102792f4768e44ec19ff3c235e48172093d4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 2\n(B) 5\n(C) 3\n(D) 6\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 2\n(B) 5\n(C) 3\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 591, "doc": {"video_name": "count_158.mp4", "duration": 530.04, "question": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 4\n(B) 5\n(C) 3\n(D) 0\n", "candidates": ["4", "5", "3", "0"], "answer": "B", "task_type": "count", "question_id": "Q591"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "180df59cd63c52699ee76b041ebe373a4d3812a72ac8a5dd64eb63dc3173e672", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 4\n(B) 5\n(C) 3\n(D) 0\n", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 4\n(B) 5\n(C) 3\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 595, "doc": {"video_name": "count_190.mp4", "duration": 470.1, "question": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 1\n(B) 6\n(C) 2\n(D) 0\n", "candidates": ["1", "6", "2", "0"], "answer": "C", "task_type": "count", "question_id": "Q595"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4eaac3aa3c22d4ac2b9de9ec9838cb2165cc5f203f512b01ff9cfaef17e1efdd", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 1\n(B) 6\n(C) 2\n(D) 0\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 1\n(B) 6\n(C) 2\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 599, "doc": {"video_name": "count_172.mp4", "duration": 470.62000000000006, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 4\n(B) 0\n(C) 3\n(D) 6\n", "candidates": ["4", "0", "3", "6"], "answer": "C", "task_type": "count", "question_id": "Q599"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e62532d23e48a6c984b20dcbfc51b121677ac9e9ce9ee1bee983665b9230d47c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 4\n(B) 0\n(C) 3\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 4\n(B) 0\n(C) 3\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 603, "doc": {"video_name": "count_64.mp4", "duration": 484.20000000000005, "question": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 0\n(B) 1\n(C) 2\n(D) 4\n", "candidates": ["0", "1", "2", "4"], "answer": "D", "task_type": "count", "question_id": "Q603"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d1698a837f0ab6ffe93bd7a4e256573b42458fec244dafaecb7dee5083ed492", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 0\n(B) 1\n(C) 2\n(D) 4\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 0\n(B) 1\n(C) 2\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 607, "doc": {"video_name": "count_139.mp4", "duration": 1374.5, "question": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 6\n(B) 0\n(C) 1\n(D) 2\n", "candidates": ["6", "0", "1", "2"], "answer": "D", "task_type": "count", "question_id": "Q607"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bfc9fdb799f6a6f056f6473865598a7b2faa61996a996643e9152d97d1a88c28", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 6\n(B) 0\n(C) 1\n(D) 2\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "In this video, how many times does the scene of the 'making jewelry' action appear in total?\n(A) 6\n(B) 0\n(C) 1\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 611, "doc": {"video_name": "count_146.mp4", "duration": 470.13, "question": "In this video, how many instances are there of the 'javelin throw' action scene in total?\n(A) 1\n(B) 5\n(C) 3\n(D) 2\n", "candidates": ["1", "5", "3", "2"], "answer": "D", "task_type": "count", "question_id": "Q611"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c1bd03c77e9551f95d0c8d288605dd0a52928727f9cedbaf0a74be0d3db29791", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'javelin throw' action scene in total?\n(A) 1\n(B) 5\n(C) 3\n(D) 2\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many instances are there of the 'javelin throw' action scene in total?\n(A) 1\n(B) 5\n(C) 3\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 615, "doc": {"video_name": "count_153.mp4", "duration": 470.06, "question": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 5\n(B) 6\n(C) 2\n(D) 3\n", "candidates": ["5", "6", "2", "3"], "answer": "C", "task_type": "count", "question_id": "Q615"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d2e259e91fb65bb42e2771d3a047b87097ac7dab5965f0b5133dd1367cd94636", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 5\n(B) 6\n(C) 2\n(D) 3\n", "task_type": "count", "pred_answer": "B", "answer": "C"}, "input": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 5\n(B) 6\n(C) 2\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 619, "doc": {"video_name": "count_223.mp4", "duration": 339.04999999999995, "question": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 0\n(B) 3\n(C) 4\n(D) 5\n", "candidates": ["0", "3", "4", "5"], "answer": "D", "task_type": "count", "question_id": "Q619"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "94c9d416422cd2362e3f59bb114937eea6f2f84a94aff2c39aecbcf60af8ead2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 0\n(B) 3\n(C) 4\n(D) 5\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 0\n(B) 3\n(C) 4\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 623, "doc": {"video_name": "count_96.mp4", "duration": 500.03999999999996, "question": "In this video, how many times does the scene of the 'playing harp' action appear in total?\n(A) 3\n(B) 6\n(C) 0\n(D) 5\n", "candidates": ["3", "6", "0", "5"], "answer": "D", "task_type": "count", "question_id": "Q623"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fd67efe571e632ee64bfdb10516c349428ae4bd5811bd050e33a5c986bbf0f0c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'playing harp' action appear in total?\n(A) 3\n(B) 6\n(C) 0\n(D) 5\n", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many times does the scene of the 'playing harp' action appear in total?\n(A) 3\n(B) 6\n(C) 0\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 627, "doc": {"video_name": "count_92.mp4", "duration": 500.03999999999996, "question": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 3\n(B) 2\n(C) 5\n(D) 6\n", "candidates": ["3", "2", "5", "6"], "answer": "C", "task_type": "count", "question_id": "Q627"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "09d2f70137d279940b934a239a96d22e8e2ac34faf9976cff61b919c8249489c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 3\n(B) 2\n(C) 5\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 3\n(B) 2\n(C) 5\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 631, "doc": {"video_name": "count_222.mp4", "duration": 645.04, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 5\n(B) 3\n(C) 0\n(D) 4\n", "candidates": ["5", "3", "0", "4"], "answer": "D", "task_type": "count", "question_id": "Q631"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "378b68ebb9f310688450afc428aee1915e0b65e38b3415e1682a49147f47ad11", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 5\n(B) 3\n(C) 0\n(D) 4\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 5\n(B) 3\n(C) 0\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 635, "doc": {"video_name": "count_58.mp4", "duration": 1395.94, "question": "In this video, how many instances are there of the 'milking cow' action scene in total?\n(A) 2\n(B) 3\n(C) 5\n(D) 4\n", "candidates": ["2", "3", "5", "4"], "answer": "D", "task_type": "count", "question_id": "Q635"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "db3ec6039e5a87fbad0d2e3435a2b170182bcebc155347eca8b71e49c7817b7d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'milking cow' action scene in total?\n(A) 2\n(B) 3\n(C) 5\n(D) 4\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "In this video, how many instances are there of the 'milking cow' action scene in total?\n(A) 2\n(B) 3\n(C) 5\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 639, "doc": {"video_name": "count_55.mp4", "duration": 460.05, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'milking cow' action\n(A) 1\n(B) 3\n(C) 4\n(D) 0\n", "candidates": ["1", "3", "4", "0"], "answer": "A", "task_type": "count", "question_id": "Q639"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bfb6311920f968cdf243272cc22834750e81ac997548ba82572a45f01d8cb5f9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'milking cow' action\n(A) 1\n(B) 3\n(C) 4\n(D) 0\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'milking cow' action\n(A) 1\n(B) 3\n(C) 4\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 643, "doc": {"video_name": "count_52.mp4", "duration": 573.29, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'milking cow' action\n(A) 0\n(B) 1\n(C) 2\n(D) 6\n", "candidates": ["0", "1", "2", "6"], "answer": "C", "task_type": "count", "question_id": "Q643"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b3ef32d9a994bcf86b283ca09bd7592699cd4a4632a0a6cdff93233d727ae1ac", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'milking cow' action\n(A) 0\n(B) 1\n(C) 2\n(D) 6\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'milking cow' action\n(A) 0\n(B) 1\n(C) 2\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 647, "doc": {"video_name": "count_200.mp4", "duration": 799.03, "question": "In this video, how many times does the scene of the 'playing trombone' action appear in total?\n(A) 3\n(B) 1\n(C) 5\n(D) 0\n", "candidates": ["3", "1", "5", "0"], "answer": "B", "task_type": "count", "question_id": "Q647"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "79c7a60ceab514a11dd7cd2176479e4ace5c08654ffa6efd9c47fbeab4243119", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'playing trombone' action appear in total?\n(A) 3\n(B) 1\n(C) 5\n(D) 0\n", "task_type": "count", "pred_answer": "A", "answer": "B"}, "input": "In this video, how many times does the scene of the 'playing trombone' action appear in total?\n(A) 3\n(B) 1\n(C) 5\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 651, "doc": {"video_name": "count_111.mp4", "duration": 460.01, "question": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 6\n(B) 0\n(C) 3\n(D) 1\n", "candidates": ["6", "0", "3", "1"], "answer": "D", "task_type": "count", "question_id": "Q651"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "82359488088b8e2ce1ac46bd12a9b18f100c4d86f277891439ead6e1f2e4bbc6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 6\n(B) 0\n(C) 3\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 6\n(B) 0\n(C) 3\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 655, "doc": {"video_name": "count_88.mp4", "duration": 525.05, "question": "In this video, how many instances are there of the 'jetskiing' action scene in total?\n(A) 0\n(B) 5\n(C) 4\n(D) 1\n", "candidates": ["0", "5", "4", "1"], "answer": "B", "task_type": "count", "question_id": "Q655"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "429295483c56fafed649ba01db3188ec41fd34191ea7c23cbf02db07310c0da7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'jetskiing' action scene in total?\n(A) 0\n(B) 5\n(C) 4\n(D) 1\n", "task_type": "count", "pred_answer": "D", "answer": "B"}, "input": "In this video, how many instances are there of the 'jetskiing' action scene in total?\n(A) 0\n(B) 5\n(C) 4\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 659, "doc": {"video_name": "count_177.mp4", "duration": 1506.07, "question": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 2\n(B) 1\n(C) 0\n(D) 3\n", "candidates": ["2", "1", "0", "3"], "answer": "B", "task_type": "count", "question_id": "Q659"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "563539188bd164245507dbd03707057f0d81c2b8540ce664b7bc1e404222c771", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 2\n(B) 1\n(C) 0\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "B"}, "input": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 2\n(B) 1\n(C) 0\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 663, "doc": {"video_name": "count_83.mp4", "duration": 495.05, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 0\n(B) 4\n(C) 5\n(D) 6\n", "candidates": ["0", "4", "5", "6"], "answer": "C", "task_type": "count", "question_id": "Q663"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e3800159c5cb843e55ecd18bb4c459285f4641f7834fc30cdd2ce5e73350e328", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 0\n(B) 4\n(C) 5\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 0\n(B) 4\n(C) 5\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 667, "doc": {"video_name": "count_80.mp4", "duration": 500.04999999999995, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 5\n(B) 2\n(C) 3\n(D) 4\n", "candidates": ["5", "2", "3", "4"], "answer": "A", "task_type": "count", "question_id": "Q667"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5efc415c74b0aa5c17825f4b4230f8421d69a23694d3823276ff6062da3435b4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 5\n(B) 2\n(C) 3\n(D) 4\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'jetskiing' action\n(A) 5\n(B) 2\n(C) 3\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 671, "doc": {"video_name": "count_191.mp4", "duration": 520.03, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 2\n(B) 1\n(C) 4\n(D) 5\n", "candidates": ["2", "1", "4", "5"], "answer": "C", "task_type": "count", "question_id": "Q671"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df9a835516c2fa75e6d5b7f4d2686be583cb4113a60eb88ef7381625f7d0c5f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 2\n(B) 1\n(C) 4\n(D) 5\n", "task_type": "count", "pred_answer": "B", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 2\n(B) 1\n(C) 4\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 675, "doc": {"video_name": "count_236.mp4", "duration": 660.04, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 2\n(B) 5\n(C) 6\n(D) 3\n", "candidates": ["2", "5", "6", "3"], "answer": "A", "task_type": "count", "question_id": "Q675"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c31a821b453026800e1edc03bd8123d2be0eb488aab008368c15f9f4904cde87", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 2\n(B) 5\n(C) 6\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 2\n(B) 5\n(C) 6\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 679, "doc": {"video_name": "count_47.mp4", "duration": 460.14, "question": "In this video, how many times does the scene of the 'riding mule' action appear in total?\n(A) 2\n(B) 6\n(C) 1\n(D) 5\n", "candidates": ["2", "6", "1", "5"], "answer": "C", "task_type": "count", "question_id": "Q679"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e044163190746b1c02a2cf447dc603fe15c9bc871279a05e9072d03727204548", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'riding mule' action appear in total?\n(A) 2\n(B) 6\n(C) 1\n(D) 5\n", "task_type": "count", "pred_answer": "B", "answer": "C"}, "input": "In this video, how many times does the scene of the 'riding mule' action appear in total?\n(A) 2\n(B) 6\n(C) 1\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 683, "doc": {"video_name": "count_45.mp4", "duration": 500.02, "question": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 4\n(B) 2\n(C) 5\n(D) 1\n", "candidates": ["4", "2", "5", "1"], "answer": "B", "task_type": "count", "question_id": "Q683"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c76c4c52628531aac97e22724b31b4eba0e3e7c58bb907fe3bd671301f87b334", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 4\n(B) 2\n(C) 5\n(D) 1\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 4\n(B) 2\n(C) 5\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 687, "doc": {"video_name": "count_41.mp4", "duration": 500.02, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 5\n(B) 0\n(C) 2\n(D) 1\n", "candidates": ["5", "0", "2", "1"], "answer": "C", "task_type": "count", "question_id": "Q687"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c2eb04372a91f2fd9ed1a1138770c244f58554a934ced29657f5a948d22f8181", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 5\n(B) 0\n(C) 2\n(D) 1\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 5\n(B) 0\n(C) 2\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 691, "doc": {"video_name": "count_147.mp4", "duration": 472.22999999999996, "question": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 6\n(B) 5\n(C) 3\n(D) 0\n", "candidates": ["6", "5", "3", "0"], "answer": "C", "task_type": "count", "question_id": "Q691"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3cb523803aa89e574950314391c3af5b3460c3e3db8b900752462c6cfeb1c38c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 6\n(B) 5\n(C) 3\n(D) 0\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 6\n(B) 5\n(C) 3\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 695, "doc": {"video_name": "count_2.mp4", "duration": 475.75000000000006, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'abseiling' action\n(A) 6\n(B) 3\n(C) 5\n(D) 0\n", "candidates": ["6", "3", "5", "0"], "answer": "B", "task_type": "count", "question_id": "Q695"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a81ab25e033912215013906f8fcb546c22433a61d8d1940add54e7896f432a71", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'abseiling' action\n(A) 6\n(B) 3\n(C) 5\n(D) 0\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'abseiling' action\n(A) 6\n(B) 3\n(C) 5\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 699, "doc": {"video_name": "count_231.mp4", "duration": 491.03, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 2\n(B) 4\n(C) 1\n(D) 6\n", "candidates": ["2", "4", "1", "6"], "answer": "C", "task_type": "count", "question_id": "Q699"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "96234eebfa35899f84d0b0f5b5423e00283be77b4ffa9f54a87ca96e3cdf3080", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 2\n(B) 4\n(C) 1\n(D) 6\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 2\n(B) 4\n(C) 1\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 703, "doc": {"video_name": "count_117.mp4", "duration": 470.01, "question": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 1\n(B) 3\n(C) 4\n(D) 2\n", "candidates": ["1", "3", "4", "2"], "answer": "D", "task_type": "count", "question_id": "Q703"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "27032f759e37142504d14c56c786c0c0c073cb7edeb783926152c40d6bfbca3b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 1\n(B) 3\n(C) 4\n(D) 2\n", "task_type": "count", "pred_answer": "B", "answer": "D"}, "input": "In this video, how many instances are there of the 'stomping grapes' action scene in total?\n(A) 1\n(B) 3\n(C) 4\n(D) 2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 707, "doc": {"video_name": "count_198.mp4", "duration": 490.03000000000003, "question": "In this video, how many times does the scene of the 'baking cookies' action appear in total?\n(A) 4\n(B) 2\n(C) 5\n(D) 1\n", "candidates": ["4", "2", "5", "1"], "answer": "A", "task_type": "count", "question_id": "Q707"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5657bd2b355f1e6e01237bc4c8c3e2050cc5c999ccc6e769126b54525b0d4da5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'baking cookies' action appear in total?\n(A) 4\n(B) 2\n(C) 5\n(D) 1\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "In this video, how many times does the scene of the 'baking cookies' action appear in total?\n(A) 4\n(B) 2\n(C) 5\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 711, "doc": {"video_name": "count_36.mp4", "duration": 505.96, "question": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 3\n(B) 2\n(C) 4\n(D) 6\n", "candidates": ["3", "2", "4", "6"], "answer": "A", "task_type": "count", "question_id": "Q711"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7d82c1586520dcc623bd503a4dd7c4ca4b3d62b2d1d52e2bd755213ef2898acc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 3\n(B) 2\n(C) 4\n(D) 6\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 3\n(B) 2\n(C) 4\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 715, "doc": {"video_name": "count_33.mp4", "duration": 460.0, "question": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 1\n(B) 0\n(C) 5\n(D) 3\n", "candidates": ["1", "0", "5", "3"], "answer": "A", "task_type": "count", "question_id": "Q715"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a6628ceb97bc62a16d825db047308844b81603c6d4d09b256cee15b3efa7add2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 1\n(B) 0\n(C) 5\n(D) 3\n", "task_type": "count", "pred_answer": "D", "answer": "A"}, "input": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 1\n(B) 0\n(C) 5\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 719, "doc": {"video_name": "count_30.mp4", "duration": 1379.97, "question": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 4\n(B) 0\n(C) 2\n(D) 3\n", "candidates": ["4", "0", "2", "3"], "answer": "D", "task_type": "count", "question_id": "Q719"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d7f63f0e49ba1e46731187fa113dfa3a4984a2bcbfcd91253df147463deb20e5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 4\n(B) 0\n(C) 2\n(D) 3\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 4\n(B) 0\n(C) 2\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 723, "doc": {"video_name": "count_179.mp4", "duration": 894.67, "question": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 4\n(B) 6\n(C) 3\n(D) 5\n", "candidates": ["4", "6", "3", "5"], "answer": "A", "task_type": "count", "question_id": "Q723"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "15d2e60f555bcf1a1dbe6a62fb55cc9ca2ae477304bd2db25f97b415ac6d7963", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 4\n(B) 6\n(C) 3\n(D) 5\n", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 4\n(B) 6\n(C) 3\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 727, "doc": {"video_name": "count_168.mp4", "duration": 510.03000000000003, "question": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 4\n(B) 6\n(C) 3\n(D) 5\n", "candidates": ["4", "6", "3", "5"], "answer": "C", "task_type": "count", "question_id": "Q727"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "054edba66756853f48c989bf3fdcf1dd6fb0d51b41167af1398290c95c8e27ba", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 4\n(B) 6\n(C) 3\n(D) 5\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many instances are there of the 'cooking sausages' action scene in total?\n(A) 4\n(B) 6\n(C) 3\n(D) 5\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 731, "doc": {"video_name": "count_238.mp4", "duration": 365.03999999999996, "question": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 5\n(B) 1\n(C) 4\n(D) 3\n", "candidates": ["5", "1", "4", "3"], "answer": "D", "task_type": "count", "question_id": "Q731"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "127d7b989dd0da67b82aefc0adc351a18fb9ac5e66a3fb3e0eec26fe077040e7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 5\n(B) 1\n(C) 4\n(D) 3\n", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 5\n(B) 1\n(C) 4\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 735, "doc": {"video_name": "count_149.mp4", "duration": 499.09999999999997, "question": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 2\n(B) 1\n(C) 5\n(D) 3\n", "candidates": ["2", "1", "5", "3"], "answer": "C", "task_type": "count", "question_id": "Q735"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1b14dc910d02c964ea88250d71b80711db284bb067a306bc736b75bd14dbeba", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 2\n(B) 1\n(C) 5\n(D) 3\n", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 2\n(B) 1\n(C) 5\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 739, "doc": {"video_name": "count_29.mp4", "duration": 488.85, "question": "In this video, how many instances are there of the 'clean and jerk' action scene in total?\n(A) 5\n(B) 1\n(C) 4\n(D) 3\n", "candidates": ["5", "1", "4", "3"], "answer": "B", "task_type": "count", "question_id": "Q739"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f53919710b230ec823bfd6429e92172501166e9def5935f91f2650e1b8a332af", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'clean and jerk' action scene in total?\n(A) 5\n(B) 1\n(C) 4\n(D) 3\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many instances are there of the 'clean and jerk' action scene in total?\n(A) 5\n(B) 1\n(C) 4\n(D) 3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 743, "doc": {"video_name": "count_26.mp4", "duration": 1384.59, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'clean and jerk' action\n(A) 6\n(B) 3\n(C) 5\n(D) 0\n", "candidates": ["6", "3", "5", "0"], "answer": "B", "task_type": "count", "question_id": "Q743"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "086d68b8cc7082939218eba1b2c0a1dc3302ed1c0353bb357bce2b646877a927", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'clean and jerk' action\n(A) 6\n(B) 3\n(C) 5\n(D) 0\n", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'clean and jerk' action\n(A) 6\n(B) 3\n(C) 5\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 747, "doc": {"video_name": "count_163.mp4", "duration": 480.02, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 5\n(B) 2\n(C) 3\n(D) 0\n", "candidates": ["5", "2", "3", "0"], "answer": "C", "task_type": "count", "question_id": "Q747"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "46b32939a72a573a79e873ef729d77c53d5049d2972723762916549a42d0df3c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 5\n(B) 2\n(C) 3\n(D) 0\n", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 5\n(B) 2\n(C) 3\n(D) 0\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 751, "doc": {"video_name": "count_145.mp4", "duration": 460.14, "question": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 1\n(B) 0\n(C) 4\n(D) 6\n", "candidates": ["1", "0", "4", "6"], "answer": "A", "task_type": "count", "question_id": "Q751"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "94ef2bf3dcf138ad44b498a2b6c05bfa54b50040bcb2cca84803d4722ca716c8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 1\n(B) 0\n(C) 4\n(D) 6\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 1\n(B) 0\n(C) 4\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 755, "doc": {"video_name": "count_233.mp4", "duration": 646.04, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 2\n(B) 0\n(C) 4\n(D) 1\n", "candidates": ["2", "0", "4", "1"], "answer": "A", "task_type": "count", "question_id": "Q755"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "529b3e0737a88856c67cd893fcd4e8f4555ae59d7f8d51724848931e9f3a0621", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 2\n(B) 0\n(C) 4\n(D) 1\n", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 2\n(B) 0\n(C) 4\n(D) 1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 759, "doc": {"video_name": "count_144.mp4", "duration": 528.2, "question": "In this video, how many instances are there of the 'javelin throw' action scene in total?\n(A) 5\n(B) 3\n(C) 2\n(D) 4\n", "candidates": ["5", "3", "2", "4"], "answer": "A", "task_type": "count", "question_id": "Q759"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eb4535a875ec1404a978cbd74897f4d0afc39846593d3efe75d53a8683a94d8b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'javelin throw' action scene in total?\n(A) 5\n(B) 3\n(C) 2\n(D) 4\n", "task_type": "count", "pred_answer": "C", "answer": "A"}, "input": "In this video, how many instances are there of the 'javelin throw' action scene in total?\n(A) 5\n(B) 3\n(C) 2\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 763, "doc": {"video_name": "ego_51.mp4", "duration": 480.0333333333334, "question": "Where was the oven turntable plate before I picked it up ?\n(A) The oven turntable was in the oven before I picked it up.\n(B) The oven turntable was in the refrigerator before I picked it up.\n(C) The oven turntable was in the microwave before I picked it up.\n(D) The oven turntable was in the dishwasher before I picked it up.\n", "candidates": ["The oven turntable was in the oven before I picked it up.", "The oven turntable was in the refrigerator before I picked it up.", "The oven turntable was in the microwave before I picked it up.", "The oven turntable was in the dishwasher before I picked it up."], "answer": "C", "task_type": "ego", "question_id": "Q763"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a2a8034724a9288a852904ca87e941f83ce238ba763880dbcf83bd52e8de0241", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the oven turntable plate before I picked it up ?\n(A) The oven turntable was in the oven before I picked it up.\n(B) The oven turntable was in the refrigerator before I picked it up.\n(C) The oven turntable was in the microwave before I picked it up.\n(D) The oven turntable was in the dishwasher before I picked it up.\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "Where was the oven turntable plate before I picked it up ?\n(A) The oven turntable was in the oven before I picked it up.\n(B) The oven turntable was in the refrigerator before I picked it up.\n(C) The oven turntable was in the microwave before I picked it up.\n(D) The oven turntable was in the dishwasher before I picked it up.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 767, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Where was the pair of skating shoes before I picked them up\n(A) in the lost and found department\n(B) at the ice rink\n(C) in a shoe store\n(D) on the road side pavement\n", "candidates": ["in the lost and found department", "at the ice rink", "in a shoe store", "on the road side pavement"], "answer": "D", "task_type": "ego", "question_id": "Q767"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ef265daddaa9c5d755a4e4f48d9e77619ec1351c37ed06f5202191e67ba46802", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the pair of skating shoes before I picked them up\n(A) in the lost and found department\n(B) at the ice rink\n(C) in a shoe store\n(D) on the road side pavement\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the pair of skating shoes before I picked them up\n(A) in the lost and found department\n(B) at the ice rink\n(C) in a shoe store\n(D) on the road side pavement\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 771, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Where was the brown wallet\n(A) next to the brown lamp\n(B) under the brown sofa\n(C) on the brown cabinet top\n(D) in the brown drawer\n", "candidates": ["next to the brown lamp", "under the brown sofa", "on the brown cabinet top", "in the brown drawer"], "answer": "C", "task_type": "ego", "question_id": "Q771"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f026599fe949be2599f9400c682f6feb916bfaa27f2cd3e1b8f7d6310788ea97", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the brown wallet\n(A) next to the brown lamp\n(B) under the brown sofa\n(C) on the brown cabinet top\n(D) in the brown drawer\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was the brown wallet\n(A) next to the brown lamp\n(B) under the brown sofa\n(C) on the brown cabinet top\n(D) in the brown drawer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 775, "doc": {"video_name": "ego_22.mp4", "duration": 480.0, "question": "How many square frames hung on the wall opposite the living room\n(A) 6\n(B) 2\n(C) 4\n(D) 8\n", "candidates": ["6", "2", "4", "8"], "answer": "C", "task_type": "ego", "question_id": "Q775"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "78db97bedcc1c6545c131421d48e9ec1241f9a7fc164479f6e82c8a29b26339c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many square frames hung on the wall opposite the living room\n(A) 6\n(B) 2\n(C) 4\n(D) 8\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "How many square frames hung on the wall opposite the living room\n(A) 6\n(B) 2\n(C) 4\n(D) 8\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 779, "doc": {"video_name": "ego_21.mp4", "duration": 480.0, "question": "Where did I put the jenga box?\n(A) in the car\n(B) in the closet\n(C) on the table\n(D) under the bed\n", "candidates": ["in the car", "in the closet", "on the table", "under the bed"], "answer": "C", "task_type": "ego", "question_id": "Q779"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aeed89c9743eb4e0ae9142ceb61d8e3fc09a52b54c659bb7b1054e596958534d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I put the jenga box?\n(A) in the car\n(B) in the closet\n(C) on the table\n(D) under the bed\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where did I put the jenga box?\n(A) in the car\n(B) in the closet\n(C) on the table\n(D) under the bed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 783, "doc": {"video_name": "ego_51.mp4", "duration": 480.0333333333334, "question": "Did I close the oven door?\n(A) I don't know\n(B) Maybe\n(C) Yes\n(D) No\n", "candidates": ["I don't know", "Maybe", "Yes", "No"], "answer": "D", "task_type": "ego", "question_id": "Q783"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "92756a48d13bcafe151362b422b2418989bf6086f94d05081e4f484b24e52594", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Did I close the oven door?\n(A) I don't know\n(B) Maybe\n(C) Yes\n(D) No\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "Did I close the oven door?\n(A) I don't know\n(B) Maybe\n(C) Yes\n(D) No\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 787, "doc": {"video_name": "ego_66.mp4", "duration": 480.03333333333336, "question": "Where was the insulated drink cup?\n(A) on table\n(B) in the refrigerator\n(C) in the dishwasher\n(D) in the car\n", "candidates": ["on table", "in the refrigerator", "in the dishwasher", "in the car"], "answer": "A", "task_type": "ego", "question_id": "Q787"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3cfbcd1f789805a6cc41b6e786d1e4ffe0e8cbdf63757232ba25c82c59ab1973", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the insulated drink cup?\n(A) on table\n(B) in the refrigerator\n(C) in the dishwasher\n(D) in the car\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where was the insulated drink cup?\n(A) on table\n(B) in the refrigerator\n(C) in the dishwasher\n(D) in the car\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 791, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "Where did I put the orange peels?\n(A) in the laundry hamper in the bedroom\n(B) in the trash bin under the kitchen sink\n(C) in the compost bin in the backyard\n(D) in the recycling bin next to the front door\n", "candidates": ["in the laundry hamper in the bedroom", "in the trash bin under the kitchen sink", "in the compost bin in the backyard", "in the recycling bin next to the front door"], "answer": "B", "task_type": "ego", "question_id": "Q791"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "51f7a538907767fe14523060825922663b00fe5b90a374d7346f03a172f026da", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where did I put the orange peels?\n(A) in the laundry hamper in the bedroom\n(B) in the trash bin under the kitchen sink\n(C) in the compost bin in the backyard\n(D) in the recycling bin next to the front door\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where did I put the orange peels?\n(A) in the laundry hamper in the bedroom\n(B) in the trash bin under the kitchen sink\n(C) in the compost bin in the backyard\n(D) in the recycling bin next to the front door\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 795, "doc": {"video_name": "ego_3.mp4", "duration": 418.0333332874998, "question": "Who did I interact with when I was holding the orange basket?\n(A) Blue jacket boy\n(B) Red hat woman\n(C) Black t-shirt man\n(D) White dress girl\n", "candidates": ["Blue jacket boy", "Red hat woman", "Black t-shirt man", "White dress girl"], "answer": "C", "task_type": "ego", "question_id": "Q795"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1480aa6b702ce69889e885aa95183a135eaf3af95b81b8048cb14ab4f2003cc0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who did I interact with when I was holding the orange basket?\n(A) Blue jacket boy\n(B) Red hat woman\n(C) Black t-shirt man\n(D) White dress girl\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "Who did I interact with when I was holding the orange basket?\n(A) Blue jacket boy\n(B) Red hat woman\n(C) Black t-shirt man\n(D) White dress girl\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 799, "doc": {"video_name": "ego_69.mp4", "duration": 480.0, "question": "Where was the red plastic shopping basket before she picked it up?\n(A) On the floor\n(B) In her hand\n(C) On the shelf\n(D) In the shopping cart\n", "candidates": ["On the floor", "In her hand", "On the shelf", "In the shopping cart"], "answer": "A", "task_type": "ego", "question_id": "Q799"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7092ee49f0de25c738f6f347b7172594076f04a4e5ee65409e8b994fe134b512", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the red plastic shopping basket before she picked it up?\n(A) On the floor\n(B) In her hand\n(C) On the shelf\n(D) In the shopping cart\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where was the red plastic shopping basket before she picked it up?\n(A) On the floor\n(B) In her hand\n(C) On the shelf\n(D) In the shopping cart\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 803, "doc": {"video_name": "ego_32.mp4", "duration": 480.03333333333376, "question": "What did I put in the red tray?\n(A) A wrench\n(B) A hammer\n(C) A pair of pliers\n(D) A screw driver.\n", "candidates": ["A wrench", "A hammer", "A pair of pliers", "A screw driver."], "answer": "D", "task_type": "ego", "question_id": "Q803"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "74fef1f1610ace8059ca5351ebb2105aa992e9f678306536da70856e0851535c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I put in the red tray?\n(A) A wrench\n(B) A hammer\n(C) A pair of pliers\n(D) A screw driver.\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "What did I put in the red tray?\n(A) A wrench\n(B) A hammer\n(C) A pair of pliers\n(D) A screw driver.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 807, "doc": {"video_name": "ego_3.mp4", "duration": 418.0333332874998, "question": "How many ties were kept on the display table?\n(A) Eight\n(B) Four.\n(C) Two\n(D) Six\n", "candidates": ["Eight", "Four.", "Two", "Six"], "answer": "B", "task_type": "ego", "question_id": "Q807"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "40fea811ccee971e5d9c2ebd3ce62568ef7a01dc00a0ccf0a30fdad01347b2d2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many ties were kept on the display table?\n(A) Eight\n(B) Four.\n(C) Two\n(D) Six\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "How many ties were kept on the display table?\n(A) Eight\n(B) Four.\n(C) Two\n(D) Six\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 811, "doc": {"video_name": "ego_65.mp4", "duration": 480.0, "question": "What did I put in the chopping machine?\n(A) wooden piece.\n(B) metal rod\n(C) glass jar\n(D) plastic bottle\n", "candidates": ["wooden piece.", "metal rod", "glass jar", "plastic bottle"], "answer": "A", "task_type": "ego", "question_id": "Q811"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9470d74a32a0e604d165642d4746224d19fcb27773dc3f7b86ed772b98c84be5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I put in the chopping machine?\n(A) wooden piece.\n(B) metal rod\n(C) glass jar\n(D) plastic bottle\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What did I put in the chopping machine?\n(A) wooden piece.\n(B) metal rod\n(C) glass jar\n(D) plastic bottle\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 815, "doc": {"video_name": "ego_83.mp4", "duration": 480.0333333333333, "question": "Did I open the laptop?\n(A) Maybe.\n(B) No.\n(C) I don't know.\n(D) Yes.\n", "candidates": ["Maybe.", "No.", "I don't know.", "Yes."], "answer": "D", "task_type": "ego", "question_id": "Q815"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8ef9ef6d7c6ba1f4081fd269c3d1b60b8b8e7b96d782d1b4191fd9f58faf1a1a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Did I open the laptop?\n(A) Maybe.\n(B) No.\n(C) I don't know.\n(D) Yes.\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Did I open the laptop?\n(A) Maybe.\n(B) No.\n(C) I don't know.\n(D) Yes.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 819, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Did I leave the room door open?\n(A) yes\n(B) maybe\n(C) I don't know\n(D) no\n", "candidates": ["yes", "maybe", "I don't know", "no"], "answer": "A", "task_type": "ego", "question_id": "Q819"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d2206e52170a99b48f81963a42343fd4af6b320fc85d00edc957372845a38857", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Did I leave the room door open?\n(A) yes\n(B) maybe\n(C) I don't know\n(D) no\n", "task_type": "ego", "pred_answer": "D", "answer": "A"}, "input": "Did I leave the room door open?\n(A) yes\n(B) maybe\n(C) I don't know\n(D) no\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 823, "doc": {"video_name": "ego_43.mp4", "duration": 480.0, "question": "Did I leave the door open ?\n(A) Maybe\n(B) NO\n(C) Yes\n(D) I don't know\n", "candidates": ["Maybe", "NO", "Yes", "I don't know"], "answer": "B", "task_type": "ego", "question_id": "Q823"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7d588ea4fa4716b9d6eaa4cfb108d44b680dd729980e85ead819137709cb5860", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did I leave the door open ?\n(A) Maybe\n(B) NO\n(C) Yes\n(D) I don't know\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Did I leave the door open ?\n(A) Maybe\n(B) NO\n(C) Yes\n(D) I don't know\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 827, "doc": {"video_name": "ego_40.mp4", "duration": 480.0666666666667, "question": "Where was the no parking board ?\n(A) On the street.\n(B) Inside a building\n(C) In a park\n(D) In a parking lot\n", "candidates": ["On the street.", "Inside a building", "In a park", "In a parking lot"], "answer": "A", "task_type": "ego", "question_id": "Q827"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "923612f6986b8422131ca14a0f235372f64562475c5553668dfa7b1ea2bd6442", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the no parking board ?\n(A) On the street.\n(B) Inside a building\n(C) In a park\n(D) In a parking lot\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where was the no parking board ?\n(A) On the street.\n(B) Inside a building\n(C) In a park\n(D) In a parking lot\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 831, "doc": {"video_name": "ego_73.mp4", "duration": 480.0, "question": "What did I put in the trash can?\n(A) box\n(B) paper\n(C) banana peel\n(D) plastic bottle\n", "candidates": ["box", "paper", "banana peel", "plastic bottle"], "answer": "A", "task_type": "ego", "question_id": "Q831"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7adbcb788cad4df5ba3fb0f5bb8f30e3edcea8c03e60fcaaf696ddc16c21b6ba", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I put in the trash can?\n(A) box\n(B) paper\n(C) banana peel\n(D) plastic bottle\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "What did I put in the trash can?\n(A) box\n(B) paper\n(C) banana peel\n(D) plastic bottle\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 835, "doc": {"video_name": "ego_55.mp4", "duration": 480.0, "question": "How many white buckets were there in the truck?\n(A) Six.\n(B) Ten\n(C) None\n(D) Three\n", "candidates": ["Six.", "Ten", "None", "Three"], "answer": "A", "task_type": "ego", "question_id": "Q835"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1f8991a5cd4beddf1e9b74cdd8d3ebbc759ea29e6afe90865c1254eaf78415e2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many white buckets were there in the truck?\n(A) Six.\n(B) Ten\n(C) None\n(D) Three\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "How many white buckets were there in the truck?\n(A) Six.\n(B) Ten\n(C) None\n(D) Three\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 839, "doc": {"video_name": "ego_75.mp4", "duration": 480.03333338333334, "question": "Where was the wooden bamboo?\n(A) Nearby tractor.\n(B) Inside the swimming pool.\n(C) In the kitchen.\n(D) On top of the mountain.\n", "candidates": ["Nearby tractor.", "Inside the swimming pool.", "In the kitchen.", "On top of the mountain."], "answer": "A", "task_type": "ego", "question_id": "Q839"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e7a1fa39bdd61c30315082a3bb32dcee424962eb3b5695fb9f013924a324bc76", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the wooden bamboo?\n(A) Nearby tractor.\n(B) Inside the swimming pool.\n(C) In the kitchen.\n(D) On top of the mountain.\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where was the wooden bamboo?\n(A) Nearby tractor.\n(B) Inside the swimming pool.\n(C) In the kitchen.\n(D) On top of the mountain.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 843, "doc": {"video_name": "ego_42.mp4", "duration": 480.0666666666667, "question": "Whom did I talk to at the store?\n(A) elderly person with a cane\n(B) man with a hat\n(C) child holding a balloon\n(D) lady wearing flower pattern top.\n", "candidates": ["elderly person with a cane", "man with a hat", "child holding a balloon", "lady wearing flower pattern top."], "answer": "D", "task_type": "ego", "question_id": "Q843"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f4b5c4aa749ff33a33665661809aa3cb8959b84ae1ad2782a09d631c630bc2e2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Whom did I talk to at the store?\n(A) elderly person with a cane\n(B) man with a hat\n(C) child holding a balloon\n(D) lady wearing flower pattern top.\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Whom did I talk to at the store?\n(A) elderly person with a cane\n(B) man with a hat\n(C) child holding a balloon\n(D) lady wearing flower pattern top.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 847, "doc": {"video_name": "ego_19.mp4", "duration": 480.0, "question": "Where was the L-Shaped Measuring Scale?\n(A) in the toolbox\n(B) on the wooden platform\n(C) hanging on the wall\n(D) underneath the table\n", "candidates": ["in the toolbox", "on the wooden platform", "hanging on the wall", "underneath the table"], "answer": "B", "task_type": "ego", "question_id": "Q847"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "030fa7ca560285fc5fe5b0cea76e01ac7f9bbc292b7be3bcfc51ada5b78f040a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the L-Shaped Measuring Scale?\n(A) in the toolbox\n(B) on the wooden platform\n(C) hanging on the wall\n(D) underneath the table\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the L-Shaped Measuring Scale?\n(A) in the toolbox\n(B) on the wooden platform\n(C) hanging on the wall\n(D) underneath the table\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 851, "doc": {"video_name": "ego_78.mp4", "duration": 480.0, "question": "Where was the hammer?\n(A) Underneath the car\n(B) In the kitchen drawer\n(C) In the toolbox\n(D) On the plywood.\n", "candidates": ["Underneath the car", "In the kitchen drawer", "In the toolbox", "On the plywood."], "answer": "D", "task_type": "ego", "question_id": "Q851"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f65991c333418326457ac15435edd4a9e8fff0682acf719331efe2ed459df3e4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the hammer?\n(A) Underneath the car\n(B) In the kitchen drawer\n(C) In the toolbox\n(D) On the plywood.\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the hammer?\n(A) Underneath the car\n(B) In the kitchen drawer\n(C) In the toolbox\n(D) On the plywood.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 855, "doc": {"video_name": "ego_18.mp4", "duration": 460.7333333333331, "question": "How many screws did I use?\n(A) eight\n(B) four\n(C) six\n(D) two\n", "candidates": ["eight", "four", "six", "two"], "answer": "B", "task_type": "ego", "question_id": "Q855"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a1497f08b4057af6e21755fcf2ff74b58d260e6d23e7d5cf9da5ae716d02f810", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many screws did I use?\n(A) eight\n(B) four\n(C) six\n(D) two\n", "task_type": "ego", "pred_answer": "C", "answer": "B"}, "input": "How many screws did I use?\n(A) eight\n(B) four\n(C) six\n(D) two\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 859, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Where did I keep the wood base panel?\n(A) in the room\n(B) under the bed\n(C) in the kitchen\n(D) in the garage\n", "candidates": ["in the room", "under the bed", "in the kitchen", "in the garage"], "answer": "A", "task_type": "ego", "question_id": "Q859"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "48eea2402e3fd5f71f8c03a974f16bf6a841e5e32c302342c049418e7b1516a4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I keep the wood base panel?\n(A) in the room\n(B) under the bed\n(C) in the kitchen\n(D) in the garage\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I keep the wood base panel?\n(A) in the room\n(B) under the bed\n(C) in the kitchen\n(D) in the garage\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 863, "doc": {"video_name": "ego_41.mp4", "duration": 480.0333333333333, "question": "Did I leave the car ignition on?\n(A) Maybe\n(B) Yes\n(C) I don't know\n(D) No\n", "candidates": ["Maybe", "Yes", "I don't know", "No"], "answer": "B", "task_type": "ego", "question_id": "Q863"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5404d0cf54467c6808fdab494b8e3850de81f66ee567ca675e0a75728df93ddc", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did I leave the car ignition on?\n(A) Maybe\n(B) Yes\n(C) I don't know\n(D) No\n", "task_type": "ego", "pred_answer": "D", "answer": "B"}, "input": "Did I leave the car ignition on?\n(A) Maybe\n(B) Yes\n(C) I don't know\n(D) No\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 867, "doc": {"video_name": "ego_37.mp4", "duration": 480.0, "question": "Where was the red basket?\n(A) In the kitchen.\n(B) On the shelf.\n(C) In the bathroom.\n(D) Near the door.\n", "candidates": ["In the kitchen.", "On the shelf.", "In the bathroom.", "Near the door."], "answer": "D", "task_type": "ego", "question_id": "Q867"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "53d17d6963dc782e8ea226522acf5af06222944ca7980c931f0bb2fb9b451a6e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the red basket?\n(A) In the kitchen.\n(B) On the shelf.\n(C) In the bathroom.\n(D) Near the door.\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "Where was the red basket?\n(A) In the kitchen.\n(B) On the shelf.\n(C) In the bathroom.\n(D) Near the door.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 871, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "In what location did I first see the red pair of slippers\n(A) In my bedroom\n(B) at the entrance of my house\n(C) In a store\n(D) At the park\n", "candidates": ["In my bedroom", "at the entrance of my house", "In a store", "At the park"], "answer": "B", "task_type": "ego", "question_id": "Q871"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "81d062af951dc79c7367f18d5558bcd6471d93e04b93a248e9b430a028057353", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In what location did I first see the red pair of slippers\n(A) In my bedroom\n(B) at the entrance of my house\n(C) In a store\n(D) At the park\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "In what location did I first see the red pair of slippers\n(A) In my bedroom\n(B) at the entrance of my house\n(C) In a store\n(D) At the park\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 875, "doc": {"video_name": "ego_27.mp4", "duration": 480.0, "question": "What did I put in the pocket?\n\n(A) Phone\n(B) Measure Tape\n(C) Pen\n(D) Wallet\n", "candidates": ["Phone", "Measure Tape", "Pen", "Wallet"], "answer": "B", "task_type": "ego", "question_id": "Q875"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "caa645a812c0e78ce61c3ca8f1729748d8006d021b92e206b7bcf4771ea3e984", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I put in the pocket?\n\n(A) Phone\n(B) Measure Tape\n(C) Pen\n(D) Wallet\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What did I put in the pocket?\n\n(A) Phone\n(B) Measure Tape\n(C) Pen\n(D) Wallet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 879, "doc": {"video_name": "ego_26.mp4", "duration": 375.3666666666667, "question": "Where did I put the spanner?\n(A) on the table\n(B) under the bed\n(C) in the car\n(D) in the drawer\n", "candidates": ["on the table", "under the bed", "in the car", "in the drawer"], "answer": "A", "task_type": "ego", "question_id": "Q879"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ce04705bb7945cd8bd5141fdeb4c53acf6ace9e2857968982cc6bcc353be43f7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I put the spanner?\n(A) on the table\n(B) under the bed\n(C) in the car\n(D) in the drawer\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I put the spanner?\n(A) on the table\n(B) under the bed\n(C) in the car\n(D) in the drawer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 883, "doc": {"video_name": "ego_25.mp4", "duration": 480.0, "question": "What word was written on the black bag?\n(A) hiker\n(B) runner\n(C) jogger\n(D) walker\n", "candidates": ["hiker", "runner", "jogger", "walker"], "answer": "D", "task_type": "ego", "question_id": "Q883"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6f94fab3fc86aeb926314d5b3e356243d80e29fc3c3bca3fd0aa50e7a4ee0319", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What word was written on the black bag?\n(A) hiker\n(B) runner\n(C) jogger\n(D) walker\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "What word was written on the black bag?\n(A) hiker\n(B) runner\n(C) jogger\n(D) walker\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 887, "doc": {"video_name": "ego_25.mp4", "duration": 480.0, "question": "How many trash cans did I see on the front porch of the apartment?\n(A) 3\n(B) 4\n(C) 5\n(D) 6\n", "candidates": ["3", "4", "5", "6"], "answer": "D", "task_type": "ego", "question_id": "Q887"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ff0d08e95b558b709d9c29eb9c6bc28280a89ad13a8627ec921cf09159279156", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many trash cans did I see on the front porch of the apartment?\n(A) 3\n(B) 4\n(C) 5\n(D) 6\n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "How many trash cans did I see on the front porch of the apartment?\n(A) 3\n(B) 4\n(C) 5\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 891, "doc": {"video_name": "ego_72.mp4", "duration": 479.9999999999998, "question": "Where was the router?\n(A) on the roof\n(B) under the bed\n(C) on the table.\n(D) in the refrigerator\n", "candidates": ["on the roof", "under the bed", "on the table.", "in the refrigerator"], "answer": "C", "task_type": "ego", "question_id": "Q891"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1635ad0623ca2fa08d8aa4601945bf2891f53ab1b609b7401b607abfb069a73c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the router?\n(A) on the roof\n(B) under the bed\n(C) on the table.\n(D) in the refrigerator\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was the router?\n(A) on the roof\n(B) under the bed\n(C) on the table.\n(D) in the refrigerator\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 895, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "In what location did I first see the orange sticky note\n(A) on the monitor of the desktop \n(B) in the bathroom\n(C) underneath the couch\n(D) on the refrigerator\n", "candidates": ["on the monitor of the desktop ", "in the bathroom", "underneath the couch", "on the refrigerator"], "answer": "A", "task_type": "ego", "question_id": "Q895"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "186fde84325be437c16cf6b47dc177c7c4960b0dd5f4af272b7838309a25a0a1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what location did I first see the orange sticky note\n(A) on the monitor of the desktop \n(B) in the bathroom\n(C) underneath the couch\n(D) on the refrigerator\n", "task_type": "ego", "pred_answer": "D", "answer": "A"}, "input": "In what location did I first see the orange sticky note\n(A) on the monitor of the desktop \n(B) in the bathroom\n(C) underneath the couch\n(D) on the refrigerator\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 899, "doc": {"video_name": "ego_17.mp4", "duration": 480.0333333333333, "question": "Did I cut the wood plank?\n(A) Yes\n(B) Maybe\n(C) I don't know\n(D) No\n", "candidates": ["Yes", "Maybe", "I don't know", "No"], "answer": "A", "task_type": "ego", "question_id": "Q899"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2f582ba096e1aaa219cf865f4d5baa78e21a14237e348ca5e3549c400d7b8428", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Did I cut the wood plank?\n(A) Yes\n(B) Maybe\n(C) I don't know\n(D) No\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Did I cut the wood plank?\n(A) Yes\n(B) Maybe\n(C) I don't know\n(D) No\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 903, "doc": {"video_name": "ego_71.mp4", "duration": 1200.0, "question": "Where was the glass cup before I picked it up?\n(A) on the kitchen counter\n(B) in the bedroom\n(C) in the dishwasher\n(D) in the car\n", "candidates": ["on the kitchen counter", "in the bedroom", "in the dishwasher", "in the car"], "answer": "A", "task_type": "ego", "question_id": "Q903"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5e363d41dae589e28f5793ca28bbcc02f0e0671c28662f14ccca01cd144af6f4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the glass cup before I picked it up?\n(A) on the kitchen counter\n(B) in the bedroom\n(C) in the dishwasher\n(D) in the car\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where was the glass cup before I picked it up?\n(A) on the kitchen counter\n(B) in the bedroom\n(C) in the dishwasher\n(D) in the car\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 907, "doc": {"video_name": "ego_16.mp4", "duration": 480.0, "question": "Where was the water bottle?\n(A) Under the black chair in the dining room\n(B) In the red drawer in the kitchen\n(C) On the white table in the kid bed room\n(D) On the blue table in the living room\n", "candidates": ["Under the black chair in the dining room", "In the red drawer in the kitchen", "On the white table in the kid bed room", "On the blue table in the living room"], "answer": "C", "task_type": "ego", "question_id": "Q907"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "39d832928c48cb07166aca5d907b372bde038a65234b8796916cd4ce2395b3c9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the water bottle?\n(A) Under the black chair in the dining room\n(B) In the red drawer in the kitchen\n(C) On the white table in the kid bed room\n(D) On the blue table in the living room\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was the water bottle?\n(A) Under the black chair in the dining room\n(B) In the red drawer in the kitchen\n(C) On the white table in the kid bed room\n(D) On the blue table in the living room\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 911, "doc": {"video_name": "ego_15.mp4", "duration": 480.0, "question": "Where did I put the big glass cup ?\n(A) In the living room's drawer.\n(B) In the bedroom's closet.\n(C) In the kitchen's shelf.\n(D) In the bathroom's cabinet.\n", "candidates": ["In the living room's drawer.", "In the bedroom's closet.", "In the kitchen's shelf.", "In the bathroom's cabinet."], "answer": "C", "task_type": "ego", "question_id": "Q911"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4743ed1f4f064ab4907b143cd54f91cf6df4dc66b3842fcd213689ae0941ddd5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I put the big glass cup ?\n(A) In the living room's drawer.\n(B) In the bedroom's closet.\n(C) In the kitchen's shelf.\n(D) In the bathroom's cabinet.\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where did I put the big glass cup ?\n(A) In the living room's drawer.\n(B) In the bedroom's closet.\n(C) In the kitchen's shelf.\n(D) In the bathroom's cabinet.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 915, "doc": {"video_name": "ego_69.mp4", "duration": 480.0, "question": "Who did I talk to near the counter?\n(A) Blue t-shirt man\n(B) Red hat woman\n(C) White sneakers boy\n(D) Black jacket girl\n", "candidates": ["Blue t-shirt man", "Red hat woman", "White sneakers boy", "Black jacket girl"], "answer": "A", "task_type": "ego", "question_id": "Q915"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e1919ff8d649698b671a3e01aab453f8a1837914fa2b2b6550edf78931f73d5b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Who did I talk to near the counter?\n(A) Blue t-shirt man\n(B) Red hat woman\n(C) White sneakers boy\n(D) Black jacket girl\n", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "Who did I talk to near the counter?\n(A) Blue t-shirt man\n(B) Red hat woman\n(C) White sneakers boy\n(D) Black jacket girl\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 919, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Where did I put the trashcan?\n(A) In the kitchen\n(B) In the truck\n(C) On the roof\n(D) Under the bed\n", "candidates": ["In the kitchen", "In the truck", "On the roof", "Under the bed"], "answer": "B", "task_type": "ego", "question_id": "Q919"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5848a0de34759f328beb9fa7e49e8390ced3da7b63d20fe863d7a5ed0cc8f6ca", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where did I put the trashcan?\n(A) In the kitchen\n(B) In the truck\n(C) On the roof\n(D) Under the bed\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where did I put the trashcan?\n(A) In the kitchen\n(B) In the truck\n(C) On the roof\n(D) Under the bed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 923, "doc": {"video_name": "ego_43.mp4", "duration": 480.0, "question": "What did I wear on my hands?\n(A) Red hand gloves\n(B) Black hand gloves\n(C) White hand gloves\n(D) Blue hand gloves\n", "candidates": ["Red hand gloves", "Black hand gloves", "White hand gloves", "Blue hand gloves"], "answer": "B", "task_type": "ego", "question_id": "Q923"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a1b8a02ff75936dfc187f0e6500bb45c168b281bc3efd6c4cff78d3538cddbe7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I wear on my hands?\n(A) Red hand gloves\n(B) Black hand gloves\n(C) White hand gloves\n(D) Blue hand gloves\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What did I wear on my hands?\n(A) Red hand gloves\n(B) Black hand gloves\n(C) White hand gloves\n(D) Blue hand gloves\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 927, "doc": {"video_name": "ego_29.mp4", "duration": 428.0666666666666, "question": "How many bolts did I unscrew from the back case of the second radio?\n(A) 6\n(B) 8\n(C) 2\n(D) 4\n", "candidates": ["6", "8", "2", "4"], "answer": "D", "task_type": "ego", "question_id": "Q927"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "909c1d3bfcccc5363c1b5656ab8cc80d8e280f493db3b5bfde776dd496494ee7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many bolts did I unscrew from the back case of the second radio?\n(A) 6\n(B) 8\n(C) 2\n(D) 4\n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "How many bolts did I unscrew from the back case of the second radio?\n(A) 6\n(B) 8\n(C) 2\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 931, "doc": {"video_name": "ego_80.mp4", "duration": 480.03333333333336, "question": "Did the lady plug the coffee machine?\n(A) I don't know\n(B) yes\n(C) no\n(D) maybe\n", "candidates": ["I don't know", "yes", "no", "maybe"], "answer": "B", "task_type": "ego", "question_id": "Q931"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7e3a99e3c50d4179f4fdcbae2c40d92f84ff3e8a26cbd9ca8f17f07113daec4d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did the lady plug the coffee machine?\n(A) I don't know\n(B) yes\n(C) no\n(D) maybe\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Did the lady plug the coffee machine?\n(A) I don't know\n(B) yes\n(C) no\n(D) maybe\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 935, "doc": {"video_name": "ego_47.mp4", "duration": 1200.0333333333333, "question": "How many holes did I last drill into the wood? \n(A) 2\n(B) 1\n(C) 3\n(D) 4\n", "candidates": ["2", "1", "3", "4"], "answer": "D", "task_type": "ego", "question_id": "Q935"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447fc923c2a7f37115a4f56366a9ee334c8062e801f5e0a4032a8c1b3d93db1e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many holes did I last drill into the wood? \n(A) 2\n(B) 1\n(C) 3\n(D) 4\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "How many holes did I last drill into the wood? \n(A) 2\n(B) 1\n(C) 3\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 939, "doc": {"video_name": "ego_23.mp4", "duration": 479.9999999999998, "question": "What did I put in the box?\n(A) pen and paper\n(B) puzzle and coins\n(C) book and keys\n(D) socks and candy\n", "candidates": ["pen and paper", "puzzle and coins", "book and keys", "socks and candy"], "answer": "B", "task_type": "ego", "question_id": "Q939"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "079f29c89e886f28e774acdc1bf3cd2ce89ca8224a6b13620529e5342ff2af73", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I put in the box?\n(A) pen and paper\n(B) puzzle and coins\n(C) book and keys\n(D) socks and candy\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What did I put in the box?\n(A) pen and paper\n(B) puzzle and coins\n(C) book and keys\n(D) socks and candy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 943, "doc": {"video_name": "ego_27.mp4", "duration": 480.0, "question": "Where was the hammer after I used it?\n(A) underneath the couch\n(B) on the kitchen counter\n(C) in the toolbox\n(D) on the wood rack \n", "candidates": ["underneath the couch", "on the kitchen counter", "in the toolbox", "on the wood rack "], "answer": "D", "task_type": "ego", "question_id": "Q943"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9e6c378ab562409a9fb055a5c623862f4260098be4d0350d1ba44e594f02a321", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the hammer after I used it?\n(A) underneath the couch\n(B) on the kitchen counter\n(C) in the toolbox\n(D) on the wood rack \n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the hammer after I used it?\n(A) underneath the couch\n(B) on the kitchen counter\n(C) in the toolbox\n(D) on the wood rack \n\nOnly give the best option.\nBest option: ("}
{"doc_id": 947, "doc": {"video_name": "ego_40.mp4", "duration": 480.0666666666667, "question": "What color of shoes was the man wearing as he crossed the street ?\n(A) Black color\n(B) Blue color\n(C) Yellow color\n(D) Red color\n", "candidates": ["Black color", "Blue color", "Yellow color", "Red color"], "answer": "D", "task_type": "ego", "question_id": "Q947"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07317b5522dc700bf41e1a54a8d2e4b6c2ba190cc0d1fc33a2e7bb5da40c9820", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color of shoes was the man wearing as he crossed the street ?\n(A) Black color\n(B) Blue color\n(C) Yellow color\n(D) Red color\n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "What color of shoes was the man wearing as he crossed the street ?\n(A) Black color\n(B) Blue color\n(C) Yellow color\n(D) Red color\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 951, "doc": {"video_name": "ego_62.mp4", "duration": 480.0, "question": "Where did I put the book?\n(A) on the book shelf\n(B) under the bed\n(C) in the refrigerator\n(D) in the car\n", "candidates": ["on the book shelf", "under the bed", "in the refrigerator", "in the car"], "answer": "A", "task_type": "ego", "question_id": "Q951"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7352552f066fbfa1f50b432776afbb90199d238389c87c2eea1c0661a9935faf", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I put the book?\n(A) on the book shelf\n(B) under the bed\n(C) in the refrigerator\n(D) in the car\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I put the book?\n(A) on the book shelf\n(B) under the bed\n(C) in the refrigerator\n(D) in the car\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 955, "doc": {"video_name": "ego_41.mp4", "duration": 480.0333333333333, "question": "What did I put in the front engine?\n(A) Wooden stick\n(B) Metal spoon\n(C) Glass cup\n(D) Plastic bottle\n", "candidates": ["Wooden stick", "Metal spoon", "Glass cup", "Plastic bottle"], "answer": "D", "task_type": "ego", "question_id": "Q955"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ef8ef5a9adfcaceb48b5bcd22187a1091858d56e737c8d22ec69cdc748e0cc8a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I put in the front engine?\n(A) Wooden stick\n(B) Metal spoon\n(C) Glass cup\n(D) Plastic bottle\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What did I put in the front engine?\n(A) Wooden stick\n(B) Metal spoon\n(C) Glass cup\n(D) Plastic bottle\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 959, "doc": {"video_name": "ego_76.mp4", "duration": 480.03333333333336, "question": "What the green t-shirt man was holding in his hand?\n(A) The red umbrella.\n(B) The black cap.\n(C) The blue backpack.\n(D) The white shoes.\n", "candidates": ["The red umbrella.", "The black cap.", "The blue backpack.", "The white shoes."], "answer": "B", "task_type": "ego", "question_id": "Q959"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85f0778884575ecaaefed15c06fedb89648a67d370f7e39536815363c4e79573", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What the green t-shirt man was holding in his hand?\n(A) The red umbrella.\n(B) The black cap.\n(C) The blue backpack.\n(D) The white shoes.\n", "task_type": "ego", "pred_answer": "A", "answer": "B"}, "input": "What the green t-shirt man was holding in his hand?\n(A) The red umbrella.\n(B) The black cap.\n(C) The blue backpack.\n(D) The white shoes.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 963, "doc": {"video_name": "ego_43.mp4", "duration": 480.0, "question": "What did I put in the dog's neck ?\n(A) I put a muzzle on the dog's neck.\n(B) I put a collar on the dog's neck.\n(C) I put a sweater on the dog's neck.\n(D) I put the dog leash rope around the neck.\n", "candidates": ["I put a muzzle on the dog's neck.", "I put a collar on the dog's neck.", "I put a sweater on the dog's neck.", "I put the dog leash rope around the neck."], "answer": "D", "task_type": "ego", "question_id": "Q963"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e3ff10b70ce7ee5df7b3e4ac2fdf60e0e4ebf9688d376a148836aed60cda8ef3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I put in the dog's neck ?\n(A) I put a muzzle on the dog's neck.\n(B) I put a collar on the dog's neck.\n(C) I put a sweater on the dog's neck.\n(D) I put the dog leash rope around the neck.\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What did I put in the dog's neck ?\n(A) I put a muzzle on the dog's neck.\n(B) I put a collar on the dog's neck.\n(C) I put a sweater on the dog's neck.\n(D) I put the dog leash rope around the neck.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 967, "doc": {"video_name": "ego_15.mp4", "duration": 480.0, "question": "Did I switch off the bedroom light?\n(A) Yes\n(B) I don't remember\n(C) I'm not sure\n(D) No\n", "candidates": ["Yes", "I don't remember", "I'm not sure", "No"], "answer": "A", "task_type": "ego", "question_id": "Q967"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "70aaae3d5b565e7b159898091cf8eefc2633d83538810e68340458beb62d9175", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Did I switch off the bedroom light?\n(A) Yes\n(B) I don't remember\n(C) I'm not sure\n(D) No\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Did I switch off the bedroom light?\n(A) Yes\n(B) I don't remember\n(C) I'm not sure\n(D) No\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 971, "doc": {"video_name": "ego_38.mp4", "duration": 480.0333333333333, "question": "Where did I put the plastic?\n(A) compost bin\n(B) recycling bin\n(C) garbage bin\n(D) black bin\n", "candidates": ["compost bin", "recycling bin", "garbage bin", "black bin"], "answer": "D", "task_type": "ego", "question_id": "Q971"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "58cdd97ae5479d1410513021c0fe051042a1fad9bacd6b1858db758c5cc22d3a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where did I put the plastic?\n(A) compost bin\n(B) recycling bin\n(C) garbage bin\n(D) black bin\n", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "Where did I put the plastic?\n(A) compost bin\n(B) recycling bin\n(C) garbage bin\n(D) black bin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 975, "doc": {"video_name": "ego_44.mp4", "duration": 480.0, "question": "Did I wash my hands?\n(A) No\n(B) Yes\n(C) Maybe\n(D) I don't know\n", "candidates": ["No", "Yes", "Maybe", "I don't know"], "answer": "B", "task_type": "ego", "question_id": "Q975"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "46949c9665b0ef3f1b1ad27f1bd73a478603912f5e76b2ff5ff80fb3d95087c4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did I wash my hands?\n(A) No\n(B) Yes\n(C) Maybe\n(D) I don't know\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Did I wash my hands?\n(A) No\n(B) Yes\n(C) Maybe\n(D) I don't know\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 979, "doc": {"video_name": "ego_73.mp4", "duration": 480.0, "question": "What liquid did I drink?\n(A) milk\n(B) soda\n(C) water\n(D) juice\n", "candidates": ["milk", "soda", "water", "juice"], "answer": "C", "task_type": "ego", "question_id": "Q979"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "546d3d4d2074b491abf2e4bbaf74a074ea6819624d7cd2ce0702ced4aaa74ef8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What liquid did I drink?\n(A) milk\n(B) soda\n(C) water\n(D) juice\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What liquid did I drink?\n(A) milk\n(B) soda\n(C) water\n(D) juice\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 983, "doc": {"video_name": "ego_41.mp4", "duration": 480.0333333333333, "question": "Where was the white can before I picked it up?\n(A) on the kitchen counter\n(B) in the backyard\n(C) inside the toolbox \n(D) underneath the couch\n", "candidates": ["on the kitchen counter", "in the backyard", "inside the toolbox ", "underneath the couch"], "answer": "C", "task_type": "ego", "question_id": "Q983"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4ce8ddbc59f273052ec244d747fe1bd4e737c4e96972dbab839ccaac116e390d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the white can before I picked it up?\n(A) on the kitchen counter\n(B) in the backyard\n(C) inside the toolbox \n(D) underneath the couch\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was the white can before I picked it up?\n(A) on the kitchen counter\n(B) in the backyard\n(C) inside the toolbox \n(D) underneath the couch\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 987, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Where was the empty paint bucket before i kept inside the store room?\n(A) in the garage\n(B) outside house\n(C) in the kitchen\n(D) in the living room\n", "candidates": ["in the garage", "outside house", "in the kitchen", "in the living room"], "answer": "B", "task_type": "ego", "question_id": "Q987"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d6bc31a322f3cf8e150a89c23591e50f0f02afb1ba937a17cd77003cae1fafa", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the empty paint bucket before i kept inside the store room?\n(A) in the garage\n(B) outside house\n(C) in the kitchen\n(D) in the living room\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the empty paint bucket before i kept inside the store room?\n(A) in the garage\n(B) outside house\n(C) in the kitchen\n(D) in the living room\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 991, "doc": {"video_name": "ego_52.mp4", "duration": 480.0, "question": "What text did I read on the Poster?\n(A) Believe in Yourself\n(B) Never Give Up\n(C) Dream Big\n(D) We Can Do It\n", "candidates": ["Believe in Yourself", "Never Give Up", "Dream Big", "We Can Do It"], "answer": "D", "task_type": "ego", "question_id": "Q991"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9c96182ec8eb9416af2eccfdaaaf2370f3b0e4b24f1a27f55f03746043662712", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What text did I read on the Poster?\n(A) Believe in Yourself\n(B) Never Give Up\n(C) Dream Big\n(D) We Can Do It\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What text did I read on the Poster?\n(A) Believe in Yourself\n(B) Never Give Up\n(C) Dream Big\n(D) We Can Do It\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 995, "doc": {"video_name": "ego_73.mp4", "duration": 480.0, "question": "What did I pour in the bottle?\n(A) milk\n(B) juice\n(C) water\n(D) soda\n", "candidates": ["milk", "juice", "water", "soda"], "answer": "C", "task_type": "ego", "question_id": "Q995"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "67d48527d90b6548a11d0ad5dda6c0d29c6bf757ba58a08a723459db974ee8d1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did I pour in the bottle?\n(A) milk\n(B) juice\n(C) water\n(D) soda\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What did I pour in the bottle?\n(A) milk\n(B) juice\n(C) water\n(D) soda\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 999, "doc": {"video_name": "ego_52.mp4", "duration": 480.0, "question": "What picture was drawn in the note book ?\n(A) The tree picture was drawn in the note book.\n(B) The house picture was drawn in the note book.\n(C) The flower picture was drawn in the note book.\n(D) The car picture was drawn in the note book.\n", "candidates": ["The tree picture was drawn in the note book.", "The house picture was drawn in the note book.", "The flower picture was drawn in the note book.", "The car picture was drawn in the note book."], "answer": "C", "task_type": "ego", "question_id": "Q999"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "97997ea99aef5b9f125b4a2b042a4b41d9db2b0210b9558ec111d13b57320b2f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What picture was drawn in the note book ?\n(A) The tree picture was drawn in the note book.\n(B) The house picture was drawn in the note book.\n(C) The flower picture was drawn in the note book.\n(D) The car picture was drawn in the note book.\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What picture was drawn in the note book ?\n(A) The tree picture was drawn in the note book.\n(B) The house picture was drawn in the note book.\n(C) The flower picture was drawn in the note book.\n(D) The car picture was drawn in the note book.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1003, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Where was the watch I wore before I picked it up\n(A) on the white table\n(B) in the drawer\n(C) on the black chair\n(D) in my pocket\n", "candidates": ["on the white table", "in the drawer", "on the black chair", "in my pocket"], "answer": "A", "task_type": "ego", "question_id": "Q1003"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3fad84ac9ee307478b7497cc0ccfde149036164ed317d185d23d815551a421b1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the watch I wore before I picked it up\n(A) on the white table\n(B) in the drawer\n(C) on the black chair\n(D) in my pocket\n", "task_type": "ego", "pred_answer": "D", "answer": "A"}, "input": "Where was the watch I wore before I picked it up\n(A) on the white table\n(B) in the drawer\n(C) on the black chair\n(D) in my pocket\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1007, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "What colour was the pair of trainers shoes\n(A) blue\n(B) black\n(C) white\n(D) red\n", "candidates": ["blue", "black", "white", "red"], "answer": "A", "task_type": "ego", "question_id": "Q1007"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "880a45293ff879c4ce0922e0568b54f26a094432be477780f7af2a2b1222ed2b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What colour was the pair of trainers shoes\n(A) blue\n(B) black\n(C) white\n(D) red\n", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "What colour was the pair of trainers shoes\n(A) blue\n(B) black\n(C) white\n(D) red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1011, "doc": {"video_name": "ego_30.mp4", "duration": 480.0, "question": "What did I put in the drawer?\n(A) Keys\n(B) Phone\n(C) Pen\n(D) Money\n", "candidates": ["Keys", "Phone", "Pen", "Money"], "answer": "D", "task_type": "ego", "question_id": "Q1011"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "361edb5edc577154cd2b496764fdbd850cfadb590a63836a6efbf94416755112", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I put in the drawer?\n(A) Keys\n(B) Phone\n(C) Pen\n(D) Money\n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "What did I put in the drawer?\n(A) Keys\n(B) Phone\n(C) Pen\n(D) Money\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1015, "doc": {"video_name": "ego_70.mp4", "duration": 480.0, "question": "What color was the trolley that i picked  up?\n\n\n(A) Yellow color\n(B) Blue color\n(C) Green color\n(D) Red color\n", "candidates": ["Yellow color", "Blue color", "Green color", "Red color"], "answer": "B", "task_type": "ego", "question_id": "Q1015"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2a096632c3665c7f7e84083a9d3393eeff266f05bd2bf90c1101f68bbe1e4069", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color was the trolley that i picked  up?\n\n\n(A) Yellow color\n(B) Blue color\n(C) Green color\n(D) Red color\n", "task_type": "ego", "pred_answer": "A", "answer": "B"}, "input": "What color was the trolley that i picked  up?\n\n\n(A) Yellow color\n(B) Blue color\n(C) Green color\n(D) Red color\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1019, "doc": {"video_name": "ego_51.mp4", "duration": 480.0333333333334, "question": "What did I put in the pet bowl ?\n(A) I put a toy in the pet bowl.\n(B) I put a blanket in the pet bowl.\n(C) I put water in the pet bowl.\n(D) I put the small piece of meal in the pet bowl.\n", "candidates": ["I put a toy in the pet bowl.", "I put a blanket in the pet bowl.", "I put water in the pet bowl.", "I put the small piece of meal in the pet bowl."], "answer": "D", "task_type": "ego", "question_id": "Q1019"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "195ce2b44bb192267dce1fc28b842a7526524cca8413da22ada94799107fa952", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I put in the pet bowl ?\n(A) I put a toy in the pet bowl.\n(B) I put a blanket in the pet bowl.\n(C) I put water in the pet bowl.\n(D) I put the small piece of meal in the pet bowl.\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "What did I put in the pet bowl ?\n(A) I put a toy in the pet bowl.\n(B) I put a blanket in the pet bowl.\n(C) I put water in the pet bowl.\n(D) I put the small piece of meal in the pet bowl.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1023, "doc": {"video_name": "ego_12.mp4", "duration": 480.0, "question": "Where was the coffee machine ?\n(A) In the refrigerator\n(B) In the bathroom\n(C) In the car\n(D) On the table.\n", "candidates": ["In the refrigerator", "In the bathroom", "In the car", "On the table."], "answer": "D", "task_type": "ego", "question_id": "Q1023"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d809b2eecd1a2f85818e4dce7f52943412c339550d3f84bbe1e137df2dbc4819", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the coffee machine ?\n(A) In the refrigerator\n(B) In the bathroom\n(C) In the car\n(D) On the table.\n", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Where was the coffee machine ?\n(A) In the refrigerator\n(B) In the bathroom\n(C) In the car\n(D) On the table.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1027, "doc": {"video_name": "ego_66.mp4", "duration": 480.03333333333336, "question": "In what room did I see a silver bucket?\n(A) in the bathroom\n(B) in the kitchen\n(C) in the bedroom\n(D) in the living room\n", "candidates": ["in the bathroom", "in the kitchen", "in the bedroom", "in the living room"], "answer": "C", "task_type": "ego", "question_id": "Q1027"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3e38be72528a9de2708d67989d717747b2b5e28a2b3e69ca93899ea88470bb22", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what room did I see a silver bucket?\n(A) in the bathroom\n(B) in the kitchen\n(C) in the bedroom\n(D) in the living room\n", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "In what room did I see a silver bucket?\n(A) in the bathroom\n(B) in the kitchen\n(C) in the bedroom\n(D) in the living room\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1031, "doc": {"video_name": "ego_11.mp4", "duration": 480.0, "question": "Where is the tape measure?\n(A) under the chair\n(B) on the table\n(C) in the toolbox\n(D) in the drawer\n", "candidates": ["under the chair", "on the table", "in the toolbox", "in the drawer"], "answer": "B", "task_type": "ego", "question_id": "Q1031"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "76e95dceb5349ef8d49aa97779365cff7361f0f1d5488d8c983faf40f733e6d9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where is the tape measure?\n(A) under the chair\n(B) on the table\n(C) in the toolbox\n(D) in the drawer\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where is the tape measure?\n(A) under the chair\n(B) on the table\n(C) in the toolbox\n(D) in the drawer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1035, "doc": {"video_name": "ego_9.mp4", "duration": 480.03333333333336, "question": "Did I keep the hammer and white cups on the stairs?\n(A) maybe\n(B) yes\n(C) I don't know\n(D) no\n", "candidates": ["maybe", "yes", "I don't know", "no"], "answer": "B", "task_type": "ego", "question_id": "Q1035"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "04e09df37457ec8b6b7f98c027ffcbb44e033f6ab63400c2b08c9c26ec38ed7d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did I keep the hammer and white cups on the stairs?\n(A) maybe\n(B) yes\n(C) I don't know\n(D) no\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Did I keep the hammer and white cups on the stairs?\n(A) maybe\n(B) yes\n(C) I don't know\n(D) no\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1039, "doc": {"video_name": "ego_81.mp4", "duration": 1200.0333333333333, "question": "Where was the bread before I picked it?\n(A) In the refrigerator\n(B) In the oven\n(C) On the kitchen counter\n(D) At the grocery store\n", "candidates": ["In the refrigerator", "In the oven", "On the kitchen counter", "At the grocery store"], "answer": "A", "task_type": "ego", "question_id": "Q1039"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c2c7abe4e15e308378b95fe2a0fb5423a1ec8377cb88a0d5928f7846c4f30c4d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the bread before I picked it?\n(A) In the refrigerator\n(B) In the oven\n(C) On the kitchen counter\n(D) At the grocery store\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where was the bread before I picked it?\n(A) In the refrigerator\n(B) In the oven\n(C) On the kitchen counter\n(D) At the grocery store\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1043, "doc": {"video_name": "ego_43.mp4", "duration": 480.0, "question": "Where was my purse?\n(A) bathroom\n(B) kitchen premies\n(C) bedroom\n(D) living room\n", "candidates": ["bathroom", "kitchen premies", "bedroom", "living room"], "answer": "B", "task_type": "ego", "question_id": "Q1043"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ba8bcb6ac5c35b0b878af4575151d911b1e730ac639e701a3087ee8da776a83", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was my purse?\n(A) bathroom\n(B) kitchen premies\n(C) bedroom\n(D) living room\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was my purse?\n(A) bathroom\n(B) kitchen premies\n(C) bedroom\n(D) living room\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1047, "doc": {"video_name": "ego_29.mp4", "duration": 428.0666666666666, "question": "How many batteries were on the table?\n(A) 5\n(B) 3\n(C) 2\n(D) 4\n", "candidates": ["5", "3", "2", "4"], "answer": "A", "task_type": "ego", "question_id": "Q1047"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a5232ae3f58420b104e31498f28e2669b883e3160b8fafb9de73d8dd342ccb89", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many batteries were on the table?\n(A) 5\n(B) 3\n(C) 2\n(D) 4\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "How many batteries were on the table?\n(A) 5\n(B) 3\n(C) 2\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1051, "doc": {"video_name": "ego_80.mp4", "duration": 480.03333333333336, "question": "In what room did i sawed the standing mirror?\n(A) Living room\n(B) Kitchen\n(C) Bathroom\n(D) Bed room\n", "candidates": ["Living room", "Kitchen", "Bathroom", "Bed room"], "answer": "D", "task_type": "ego", "question_id": "Q1051"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7cf0f7936745736d0165bbb2bc7be5003da73cd78f20b10e616f6849f56dfb5d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what room did i sawed the standing mirror?\n(A) Living room\n(B) Kitchen\n(C) Bathroom\n(D) Bed room\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "In what room did i sawed the standing mirror?\n(A) Living room\n(B) Kitchen\n(C) Bathroom\n(D) Bed room\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1055, "doc": {"video_name": "ego_76.mp4", "duration": 480.03333333333336, "question": "Who did I talk to at the garage?\n(A) woman with blue tshirt\n(B) woman with red tshirt\n(C) man with yellow tshirt\n(D)  man with green tshirt\n", "candidates": ["woman with blue tshirt", "woman with red tshirt", "man with yellow tshirt", " man with green tshirt"], "answer": "D", "task_type": "ego", "question_id": "Q1055"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b0ed5cae2f3fd42d9eedc81dba2a88e51ec4fc362ce728b824996724de84100", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Who did I talk to at the garage?\n(A) woman with blue tshirt\n(B) woman with red tshirt\n(C) man with yellow tshirt\n(D)  man with green tshirt\n", "task_type": "ego", "pred_answer": "A", "answer": "D"}, "input": "Who did I talk to at the garage?\n(A) woman with blue tshirt\n(B) woman with red tshirt\n(C) man with yellow tshirt\n(D)  man with green tshirt\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1059, "doc": {"video_name": "ego_43.mp4", "duration": 480.0, "question": "What did I take out of the cupboard ?\n(A) cotton hand gloves I took out of the cupboard.\n(B) wool sweater\n(C) plastic hangers\n(D) metal spoon\n", "candidates": ["cotton hand gloves I took out of the cupboard.", "wool sweater", "plastic hangers", "metal spoon"], "answer": "A", "task_type": "ego", "question_id": "Q1059"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "39e093b7cada5b0d9b2baab136f9628011a8668f6d9dc3aa735e87f182190d4d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I take out of the cupboard ?\n(A) cotton hand gloves I took out of the cupboard.\n(B) wool sweater\n(C) plastic hangers\n(D) metal spoon\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What did I take out of the cupboard ?\n(A) cotton hand gloves I took out of the cupboard.\n(B) wool sweater\n(C) plastic hangers\n(D) metal spoon\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1063, "doc": {"video_name": "ego_82.mp4", "duration": 480.0, "question": "where is the butter and bread?\n(A) The butter and bread were in the oven.\n(B) The butter and bread were in the pantry.\n(C) The butter and bread were on the kitchen counter.\n(D) The butter and bread were inside the fridge.\n", "candidates": ["The butter and bread were in the oven.", "The butter and bread were in the pantry.", "The butter and bread were on the kitchen counter.", "The butter and bread were inside the fridge."], "answer": "D", "task_type": "ego", "question_id": "Q1063"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c16645afa2d750d84571819cbb4fd99ef443cb2576a799ecd1f84740c9b05a66", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "where is the butter and bread?\n(A) The butter and bread were in the oven.\n(B) The butter and bread were in the pantry.\n(C) The butter and bread were on the kitchen counter.\n(D) The butter and bread were inside the fridge.\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "where is the butter and bread?\n(A) The butter and bread were in the oven.\n(B) The butter and bread were in the pantry.\n(C) The butter and bread were on the kitchen counter.\n(D) The butter and bread were inside the fridge.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1067, "doc": {"video_name": "ego_73.mp4", "duration": 480.0, "question": "How many tomato packets were there in the fridge?\n(A) two\n(B) one\n(C) four\n(D) three\n", "candidates": ["two", "one", "four", "three"], "answer": "A", "task_type": "ego", "question_id": "Q1067"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4e643d95f8ccea8c880aa830530982d9d6125460bab1b3cca5e942706320dcb8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many tomato packets were there in the fridge?\n(A) two\n(B) one\n(C) four\n(D) three\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "How many tomato packets were there in the fridge?\n(A) two\n(B) one\n(C) four\n(D) three\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1071, "doc": {"video_name": "ego_36.mp4", "duration": 1200.0, "question": "Who did I talk to at the entrance of my house\n(A) The child with the red backpack\n(B) The dog running across the street\n(C) The man in the blue hat\n(D) The woman in the pink top\n", "candidates": ["The child with the red backpack", "The dog running across the street", "The man in the blue hat", "The woman in the pink top"], "answer": "D", "task_type": "ego", "question_id": "Q1071"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ac051c6961ceb50c5ab066e11001ab1ace3436f5ee6474cd9a193d24aebbd3d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Who did I talk to at the entrance of my house\n(A) The child with the red backpack\n(B) The dog running across the street\n(C) The man in the blue hat\n(D) The woman in the pink top\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "Who did I talk to at the entrance of my house\n(A) The child with the red backpack\n(B) The dog running across the street\n(C) The man in the blue hat\n(D) The woman in the pink top\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1075, "doc": {"video_name": "ego_63.mp4", "duration": 447.0, "question": "Where was the cabinet after I removed it?\n(A) in the garage\n(B) in the attic\n(C) on the floor\n(D) in the cupboard\n", "candidates": ["in the garage", "in the attic", "on the floor", "in the cupboard"], "answer": "D", "task_type": "ego", "question_id": "Q1075"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2a3f52ec1b00ac4f712273bdca3c2a265471859516e52ecd9f0f7ff055f5429f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the cabinet after I removed it?\n(A) in the garage\n(B) in the attic\n(C) on the floor\n(D) in the cupboard\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "Where was the cabinet after I removed it?\n(A) in the garage\n(B) in the attic\n(C) on the floor\n(D) in the cupboard\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1079, "doc": {"video_name": "ego_9.mp4", "duration": 480.03333333333336, "question": "Where did I put the gloves and cutter?\n(A) in the kitchen\n(B) in the garage\n(C) on the stairs\n(D) in the bedroom\n", "candidates": ["in the kitchen", "in the garage", "on the stairs", "in the bedroom"], "answer": "C", "task_type": "ego", "question_id": "Q1079"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d112f3274af3932ddb64e4db21a87982b04afba9531f130cd8df46cbb60d254b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I put the gloves and cutter?\n(A) in the kitchen\n(B) in the garage\n(C) on the stairs\n(D) in the bedroom\n", "task_type": "ego", "pred_answer": "B", "answer": "C"}, "input": "Where did I put the gloves and cutter?\n(A) in the kitchen\n(B) in the garage\n(C) on the stairs\n(D) in the bedroom\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1083, "doc": {"video_name": "ego_8.mp4", "duration": 361.0999999541664, "question": "Where was the black heel shoe?\n(A) on the shelf of the shop\n(B) underneath the bed\n(C) in the customer's hand\n(D) in the kitchen drawer\n", "candidates": ["on the shelf of the shop", "underneath the bed", "in the customer's hand", "in the kitchen drawer"], "answer": "A", "task_type": "ego", "question_id": "Q1083"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6830e4fbb0e3128319a9eb4062d023fd29b85401bcad8c2935e2efd22fcbb0e7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the black heel shoe?\n(A) on the shelf of the shop\n(B) underneath the bed\n(C) in the customer's hand\n(D) in the kitchen drawer\n", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "Where was the black heel shoe?\n(A) on the shelf of the shop\n(B) underneath the bed\n(C) in the customer's hand\n(D) in the kitchen drawer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1087, "doc": {"video_name": "ego_7.mp4", "duration": 480.03333333333336, "question": "What word was written on the red pamphlet?\n(A) Sign up extra 20 percent off\n(B) Buy one get one free\n(C) Get 10 percent off\n(D) Limited time offer\n", "candidates": ["Sign up extra 20 percent off", "Buy one get one free", "Get 10 percent off", "Limited time offer"], "answer": "A", "task_type": "ego", "question_id": "Q1087"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc10ba9a1e39182da081740dbbd224650506568ed07e94d947ec344e68647d01", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What word was written on the red pamphlet?\n(A) Sign up extra 20 percent off\n(B) Buy one get one free\n(C) Get 10 percent off\n(D) Limited time offer\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What word was written on the red pamphlet?\n(A) Sign up extra 20 percent off\n(B) Buy one get one free\n(C) Get 10 percent off\n(D) Limited time offer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1091, "doc": {"video_name": "ego_53.mp4", "duration": 480.03333333333353, "question": "What did I put in Dustbin ?\n(A) Basin waste\n(B) Plastic bottle\n(C) Paper towel\n(D) Food scraps\n", "candidates": ["Basin waste", "Plastic bottle", "Paper towel", "Food scraps"], "answer": "A", "task_type": "ego", "question_id": "Q1091"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f4a00dee219b1475c85694db663709948582d24f9dc18ca15b7a109f66dbb5e2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I put in Dustbin ?\n(A) Basin waste\n(B) Plastic bottle\n(C) Paper towel\n(D) Food scraps\n", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What did I put in Dustbin ?\n(A) Basin waste\n(B) Plastic bottle\n(C) Paper towel\n(D) Food scraps\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1095, "doc": {"video_name": "ego_5.mp4", "duration": 480.0, "question": "Did I pick up the paper plate bundle?\n(A) I lost it.\n(B) I don't remember.\n(C) No.\n(D) Yes.\n", "candidates": ["I lost it.", "I don't remember.", "No.", "Yes."], "answer": "D", "task_type": "ego", "question_id": "Q1095"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "92f44c43a66e2299e2c9b6d18692edf96478dd2e4d98a5f7367cbc09ff825248", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Did I pick up the paper plate bundle?\n(A) I lost it.\n(B) I don't remember.\n(C) No.\n(D) Yes.\n", "task_type": "ego", "pred_answer": "C", "answer": "D"}, "input": "Did I pick up the paper plate bundle?\n(A) I lost it.\n(B) I don't remember.\n(C) No.\n(D) Yes.\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1099, "doc": {"video_name": "ego_41.mp4", "duration": 480.0333333333333, "question": "Where did I put the oil trolley?\n(A) In the garage\n(B) Below the shelf\n(C) In the kitchen\n(D) On top of the shelf\n", "candidates": ["In the garage", "Below the shelf", "In the kitchen", "On top of the shelf"], "answer": "B", "task_type": "ego", "question_id": "Q1099"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1c529fe3c24a6e2afd7ca2e3e8f86cdbee22c681419d7f2171f436f70b431f1c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where did I put the oil trolley?\n(A) In the garage\n(B) Below the shelf\n(C) In the kitchen\n(D) On top of the shelf\n", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where did I put the oil trolley?\n(A) In the garage\n(B) Below the shelf\n(C) In the kitchen\n(D) On top of the shelf\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1103, "doc": {"video_name": "ego_52.mp4", "duration": 480.0, "question": "Did I see a Pepsi cans ?\n(A) Maybe\n(B) Yes \n(C) No\n(D) I'm not sure\n", "candidates": ["Maybe", "Yes ", "No", "I'm not sure"], "answer": "B", "task_type": "ego", "question_id": "Q1103"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0caad5731d344036186cc7fec45d2a408ceed2fb3cfab5c4dfd092dac72b6637", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Did I see a Pepsi cans ?\n(A) Maybe\n(B) Yes \n(C) No\n(D) I'm not sure\n", "task_type": "ego", "pred_answer": "C", "answer": "B"}, "input": "Did I see a Pepsi cans ?\n(A) Maybe\n(B) Yes \n(C) No\n(D) I'm not sure\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1107, "doc": {"video_name": "ego_72.mp4", "duration": 479.9999999999998, "question": "Where was the cat?\n(A) on the chair.\n(B) under the table\n(C) in the closet\n(D) outside the window\n", "candidates": ["on the chair.", "under the table", "in the closet", "outside the window"], "answer": "A", "task_type": "ego", "question_id": "Q1107"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ef6ef862873164a3a3e2df2f226c92ff10db81272094189e693ede83db989c7c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the cat?\n(A) on the chair.\n(B) under the table\n(C) in the closet\n(D) outside the window\n", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "Where was the cat?\n(A) on the chair.\n(B) under the table\n(C) in the closet\n(D) outside the window\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1111, "doc": {"video_name": "ego_73.mp4", "duration": 480.0, "question": "What did I pour in the kettle?\n(A) milk\n(B) juice\n(C) water\n(D) coffee\n", "candidates": ["milk", "juice", "water", "coffee"], "answer": "C", "task_type": "ego", "question_id": "Q1111"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "30d49039a2a2301f50964d87cb928f4676a5dc0f540a6fc503a5d2d03f585be2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did I pour in the kettle?\n(A) milk\n(B) juice\n(C) water\n(D) coffee\n", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What did I pour in the kettle?\n(A) milk\n(B) juice\n(C) water\n(D) coffee\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1115, "doc": {"video_name": "movie101_63.mp4", "duration": 748.0, "question": "At the beginning of the video, what kind of vehicle is the man riding?\n(A) Camel cart\n(B) Donkey cart\n(C) Horse carriage\n(D) Ox cart\n", "candidates": ["Camel cart", "Donkey cart", "Horse carriage", "Ox cart"], "answer": "D", "task_type": "plotQA", "question_id": "Q1115"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "972d8dacef56bf0409413751b70d5c8c23f0400f7a23f727918f5f4b68467763", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "At the beginning of the video, what kind of vehicle is the man riding?\n(A) Camel cart\n(B) Donkey cart\n(C) Horse carriage\n(D) Ox cart\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "At the beginning of the video, what kind of vehicle is the man riding?\n(A) Camel cart\n(B) Donkey cart\n(C) Horse carriage\n(D) Ox cart\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1119, "doc": {"video_name": "en_tv_11.mp4", "duration": 465.0, "question": "Why are the woman in blue clothing and the man with a scar on his face surrounded by armed personnel?\n(A) Because they had a fight\n(B) Because they killed someone\n(C) Because they triggered the alarm device\n(D) Because they robbed\n", "candidates": ["Because they had a fight", "Because they killed someone", "Because they triggered the alarm device", "Because they robbed"], "answer": "C", "task_type": "plotQA", "question_id": "Q1119"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "66715feb47f94358a96d3ba81fe356958071d521b0a9bb5579cedb0081259eef", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why are the woman in blue clothing and the man with a scar on his face surrounded by armed personnel?\n(A) Because they had a fight\n(B) Because they killed someone\n(C) Because they triggered the alarm device\n(D) Because they robbed\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Why are the woman in blue clothing and the man with a scar on his face surrounded by armed personnel?\n(A) Because they had a fight\n(B) Because they killed someone\n(C) Because they triggered the alarm device\n(D) Because they robbed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1123, "doc": {"video_name": "movie101_51.mp4", "duration": 796.0, "question": "What is the old man in Japanese clothing doing on the street?\n(A) Meditating\n(B) Preparing food\n(C) Running\n(D) Walking\n", "candidates": ["Meditating", "Preparing food", "Running", "Walking"], "answer": "B", "task_type": "plotQA", "question_id": "Q1123"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "de81b92f57566a5970fbe5b3e3612c14ae4a1caa6deb6912d7d3c9e59fb66dfe", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the old man in Japanese clothing doing on the street?\n(A) Meditating\n(B) Preparing food\n(C) Running\n(D) Walking\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the old man in Japanese clothing doing on the street?\n(A) Meditating\n(B) Preparing food\n(C) Running\n(D) Walking\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1127, "doc": {"video_name": "movie101_79.mp4", "duration": 590.0, "question": "What color is the suit worn by the man taking wedding photos?\n(A) Brown\n(B) Pink\n(C) Black\n(D) White\n", "candidates": ["Brown", "Pink", "Black", "White"], "answer": "B", "task_type": "plotQA", "question_id": "Q1127"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "840b72d5df7d8e238ffb34cdd3208d42715f640f99cdab6cf07651002f75d2b0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the suit worn by the man taking wedding photos?\n(A) Brown\n(B) Pink\n(C) Black\n(D) White\n", "task_type": "plotQA", "pred_answer": "D", "answer": "B"}, "input": "What color is the suit worn by the man taking wedding photos?\n(A) Brown\n(B) Pink\n(C) Black\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1131, "doc": {"video_name": "movie101_30.mp4", "duration": 726.0, "question": "What color is the dress of the little girl taking pictures in the video?\n(A) Blue\n(B) Red\n(C) White\n(D) Blue\n", "candidates": ["Blue", "Red", "White", "Blue"], "answer": "C", "task_type": "plotQA", "question_id": "Q1131"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9050166d9961fa694be6d5133d1d3933f1c05e4a3385c082aa945899a9457900", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the dress of the little girl taking pictures in the video?\n(A) Blue\n(B) Red\n(C) White\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "What color is the dress of the little girl taking pictures in the video?\n(A) Blue\n(B) Red\n(C) White\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1135, "doc": {"video_name": "movie101_73.mp4", "duration": 421.0, "question": "At the end of the video, what is the mood of the man who gets hit?\n(A) Happy\n(B) Sad\n(C) Angry\n(D) Neutral\n", "candidates": ["Happy", "Sad", "Angry", "Neutral"], "answer": "C", "task_type": "plotQA", "question_id": "Q1135"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "81b6d5febb51ce23344aea4da5c0826516b564d1490094e8157105874c5b24f2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "At the end of the video, what is the mood of the man who gets hit?\n(A) Happy\n(B) Sad\n(C) Angry\n(D) Neutral\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "At the end of the video, what is the mood of the man who gets hit?\n(A) Happy\n(B) Sad\n(C) Angry\n(D) Neutral\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1139, "doc": {"video_name": "movie101_1.mp4", "duration": 214.0, "question": "What is the mood of the person in the blue pants after kicking the water?\n(A) Resentful\n(B) Surprised\n(C) Happy\n(D) Depressed\n", "candidates": ["Resentful", "Surprised", "Happy", "Depressed"], "answer": "A", "task_type": "plotQA", "question_id": "Q1139"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "843830197c74800af28b467682e089ce7ce7a99b12361dd05c3b38354a11256b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the mood of the person in the blue pants after kicking the water?\n(A) Resentful\n(B) Surprised\n(C) Happy\n(D) Depressed\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What is the mood of the person in the blue pants after kicking the water?\n(A) Resentful\n(B) Surprised\n(C) Happy\n(D) Depressed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1143, "doc": {"video_name": "en_tv_25.mp4", "duration": 478.0, "question": "What color is the chimney in the video?\n(A) Yellow\n(B) Green\n(C) Alternating orange and white\n(D) White\n", "candidates": ["Yellow", "Green", "Alternating orange and white", "White"], "answer": "C", "task_type": "plotQA", "question_id": "Q1143"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "83b898d887289b57e88ac5842416686f5a8f66be70bcb0a7331a74133338a93d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the chimney in the video?\n(A) Yellow\n(B) Green\n(C) Alternating orange and white\n(D) White\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the chimney in the video?\n(A) Yellow\n(B) Green\n(C) Alternating orange and white\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1147, "doc": {"video_name": "en_tv_11.mp4", "duration": 465.0, "question": "Why does the woman in blue clothing want to enter the house?\n(A) To eat\n(B) To sleep\n(C) To play games\n(D) To operate the computer device in the house\n", "candidates": ["To eat", "To sleep", "To play games", "To operate the computer device in the house"], "answer": "D", "task_type": "plotQA", "question_id": "Q1147"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "21167be361f4a93195c62ae4f4c9bcd3cb642f5ee0385beb5172ae89bfdc0bdf", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why does the woman in blue clothing want to enter the house?\n(A) To eat\n(B) To sleep\n(C) To play games\n(D) To operate the computer device in the house\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Why does the woman in blue clothing want to enter the house?\n(A) To eat\n(B) To sleep\n(C) To play games\n(D) To operate the computer device in the house\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1151, "doc": {"video_name": "tomjerry_10.mp4", "duration": 302.0, "question": "What is the cartoon big mouse taking the cartoon little mouse to do?\n(A) Looking for food\n(B) Listening to music\n(C) Watching movies\n(D) Playing ball\n", "candidates": ["Looking for food", "Listening to music", "Watching movies", "Playing ball"], "answer": "A", "task_type": "plotQA", "question_id": "Q1151"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d335ccaf0ea820b95e1fb183319e0105b4f7cd627a08200b6bf00d6bdadb4da", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the cartoon big mouse taking the cartoon little mouse to do?\n(A) Looking for food\n(B) Listening to music\n(C) Watching movies\n(D) Playing ball\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What is the cartoon big mouse taking the cartoon little mouse to do?\n(A) Looking for food\n(B) Listening to music\n(C) Watching movies\n(D) Playing ball\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1155, "doc": {"video_name": "en_tv_22.mp4", "duration": 441.0, "question": "What material is the wall made of in the scene where it's being broken down?\n(A) Cement\n(B) Wooden Plank\n(C) Bricks\n(D) Plastic Sheet\n", "candidates": ["Cement", "Wooden Plank", "Bricks", "Plastic Sheet"], "answer": "C", "task_type": "plotQA", "question_id": "Q1155"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C. Bricks"]], "filtered_resps": ["C. Bricks"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "428982fe0d9ead5aff976e0be2341bea70d30c40459e64139995bd4871a368f5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What material is the wall made of in the scene where it's being broken down?\n(A) Cement\n(B) Wooden Plank\n(C) Bricks\n(D) Plastic Sheet\n", "task_type": "plotQA", "pred_answer": "C. Bricks", "answer": "C"}, "input": "What material is the wall made of in the scene where it's being broken down?\n(A) Cement\n(B) Wooden Plank\n(C) Bricks\n(D) Plastic Sheet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1159, "doc": {"video_name": "movie101_80.mp4", "duration": 467.0, "question": "In what season does the story in the video take place?\n(A) Summer\n(B) Spring\n(C) Winter\n(D) Autumn\n", "candidates": ["Summer", "Spring", "Winter", "Autumn"], "answer": "C", "task_type": "plotQA", "question_id": "Q1159"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fa8e0c8c3da45fd1879488cd24214419f4a7cf4cf34af180650ccd979ddeb8ae", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what season does the story in the video take place?\n(A) Summer\n(B) Spring\n(C) Winter\n(D) Autumn\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "In what season does the story in the video take place?\n(A) Summer\n(B) Spring\n(C) Winter\n(D) Autumn\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1163, "doc": {"video_name": "en_tv_18.mp4", "duration": 568.0, "question": "What color is the hat of the man talking to the woman?\n(A) Black\n(B) Orange\n(C) Blue\n(D) Green\n", "candidates": ["Black", "Orange", "Blue", "Green"], "answer": "A", "task_type": "plotQA", "question_id": "Q1163"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "263ca57173462a651f97fef1ab525da265bc9621404f0c5dab967f24059b9f85", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the hat of the man talking to the woman?\n(A) Black\n(B) Orange\n(C) Blue\n(D) Green\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the hat of the man talking to the woman?\n(A) Black\n(B) Orange\n(C) Blue\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1167, "doc": {"video_name": "en_tv_26.mp4", "duration": 522.0, "question": "What color is the woman's clothing in the scene where a man gives a woman flowers?\n(A) Green\n(B) White\n(C) Red\n(D) Pink\n", "candidates": ["Green", "White", "Red", "Pink"], "answer": "A", "task_type": "plotQA", "question_id": "Q1167"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d018c9f02c4fe085eb7c6b14805584cf4c54b68b51c3c94a1effcdf4e8880a46", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the woman's clothing in the scene where a man gives a woman flowers?\n(A) Green\n(B) White\n(C) Red\n(D) Pink\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What color is the woman's clothing in the scene where a man gives a woman flowers?\n(A) Green\n(B) White\n(C) Red\n(D) Pink\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1171, "doc": {"video_name": "movie101_28.mp4", "duration": 234.0, "question": "What color is the back of the monster standing on the big monster in the movie?\n(A) Black\n(B) Green\n(C) Blue\n(D) Purple\n", "candidates": ["Black", "Green", "Blue", "Purple"], "answer": "B", "task_type": "plotQA", "question_id": "Q1171"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ddfea222c78a221f4e3acbbe6bde5f49165efa0264cbc9bcf096a46d44448e29", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the back of the monster standing on the big monster in the movie?\n(A) Black\n(B) Green\n(C) Blue\n(D) Purple\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the back of the monster standing on the big monster in the movie?\n(A) Black\n(B) Green\n(C) Blue\n(D) Purple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1175, "doc": {"video_name": "movie101_111.mp4", "duration": 720.0, "question": "Why did two people in white coats push a device onto the stage during the speech?\n(A) There was an unexpected situation on the stage\n(B) They pushed the device onto the stage on their own\n(C) To allow the man to repair the device\n(D) To assist the man in the black suit with his speech\n", "candidates": ["There was an unexpected situation on the stage", "They pushed the device onto the stage on their own", "To allow the man to repair the device", "To assist the man in the black suit with his speech"], "answer": "D", "task_type": "plotQA", "question_id": "Q1175"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8648157aeadc081362e05b8d5c8c817fbe6abe5c377454d6537cf5fbba104a56", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why did two people in white coats push a device onto the stage during the speech?\n(A) There was an unexpected situation on the stage\n(B) They pushed the device onto the stage on their own\n(C) To allow the man to repair the device\n(D) To assist the man in the black suit with his speech\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "Why did two people in white coats push a device onto the stage during the speech?\n(A) There was an unexpected situation on the stage\n(B) They pushed the device onto the stage on their own\n(C) To allow the man to repair the device\n(D) To assist the man in the black suit with his speech\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1179, "doc": {"video_name": "haimian_1.mp4", "duration": 305.0, "question": "What is the cartoon octopus doing?\n(A) Playing games\n(B) Eating a popsicle in a chair\n(C) Sleeping\n(D) Cooking\n", "candidates": ["Playing games", "Eating a popsicle in a chair", "Sleeping", "Cooking"], "answer": "B", "task_type": "plotQA", "question_id": "Q1179"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5271f928f44e44b3b89694c08afe7c6fed4610bbf584b09578880bec5d8fa4c3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the cartoon octopus doing?\n(A) Playing games\n(B) Eating a popsicle in a chair\n(C) Sleeping\n(D) Cooking\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the cartoon octopus doing?\n(A) Playing games\n(B) Eating a popsicle in a chair\n(C) Sleeping\n(D) Cooking\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1183, "doc": {"video_name": "4.mp4", "duration": 603.0, "question": "What color is the man's clothing at the beginning of the video?\n(A) Red\n(B) Black\n(C) Yellow\n(D) White\n", "candidates": ["Red", "Black", "Yellow", "White"], "answer": "B", "task_type": "plotQA", "question_id": "Q1183"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e4f2b62f46ef05b1f194b7af56ca8708bf37d0e53b59a67ce1204cdbc7aaf976", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the man's clothing at the beginning of the video?\n(A) Red\n(B) Black\n(C) Yellow\n(D) White\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the man's clothing at the beginning of the video?\n(A) Red\n(B) Black\n(C) Yellow\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1187, "doc": {"video_name": "movie101_24.mp4", "duration": 311.0, "question": "What color is the package the old man carries in the video?\n(A) Green\n(B) Red\n(C) Blue\n(D) Purple\n", "candidates": ["Green", "Red", "Blue", "Purple"], "answer": "B", "task_type": "plotQA", "question_id": "Q1187"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7898f7cd2b1f1899a0def15a1eebf60385721aeed6b3292b4b2e62149fd78321", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the package the old man carries in the video?\n(A) Green\n(B) Red\n(C) Blue\n(D) Purple\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What color is the package the old man carries in the video?\n(A) Green\n(B) Red\n(C) Blue\n(D) Purple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1191, "doc": {"video_name": "movie101_17.mp4", "duration": 613.0, "question": "What color is the clothes of the bald man sitting in the sedan in the video?\n(A) Green\n(B) White\n(C) Blue\n(D) Red\n", "candidates": ["Green", "White", "Blue", "Red"], "answer": "B", "task_type": "plotQA", "question_id": "Q1191"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "93aeeca51cbbc1cbd00286a3010a269d452af80aaa3e63236249e03c2fc5d046", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the clothes of the bald man sitting in the sedan in the video?\n(A) Green\n(B) White\n(C) Blue\n(D) Red\n", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "What color is the clothes of the bald man sitting in the sedan in the video?\n(A) Green\n(B) White\n(C) Blue\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1195, "doc": {"video_name": "xiaoliyu_1.mp4", "duration": 381.0, "question": "What did the cartoon turtle hit that caused it to get dizzy?\n(A) Shark\n(B) Whale\n(C) Reef\n(D) Cartoon carp\n", "candidates": ["Shark", "Whale", "Reef", "Cartoon carp"], "answer": "D", "task_type": "plotQA", "question_id": "Q1195"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c325bce36e9fdccd5f1eae893770b62613e011e66353bfa5163db8add88b7118", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did the cartoon turtle hit that caused it to get dizzy?\n(A) Shark\n(B) Whale\n(C) Reef\n(D) Cartoon carp\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What did the cartoon turtle hit that caused it to get dizzy?\n(A) Shark\n(B) Whale\n(C) Reef\n(D) Cartoon carp\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1199, "doc": {"video_name": "movie101_15.mp4", "duration": 441.0, "question": "What color is the suit the girl is wearing at the end of the video?\n(A) Black\n(B) White\n(C) Red\n(D) Blue\n", "candidates": ["Black", "White", "Red", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1199"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a9dd2129c091601616ed33fe85459fd65506fc1d39d3bc58ce035cb9c05dc2ed", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the suit the girl is wearing at the end of the video?\n(A) Black\n(B) White\n(C) Red\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the suit the girl is wearing at the end of the video?\n(A) Black\n(B) White\n(C) Red\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1203, "doc": {"video_name": "movie101_86.mp4", "duration": 611.0, "question": "What in the man's hand is knocked off?\n(A) Chopsticks\n(B) Spoon\n(C) Teacup\n(D) Wine glass\n", "candidates": ["Chopsticks", "Spoon", "Teacup", "Wine glass"], "answer": "D", "task_type": "plotQA", "question_id": "Q1203"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c0a4a408674ca2abc6f53acc601cd25552526f5d76f37e58100ce672bf994ed7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What in the man's hand is knocked off?\n(A) Chopsticks\n(B) Spoon\n(C) Teacup\n(D) Wine glass\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "What in the man's hand is knocked off?\n(A) Chopsticks\n(B) Spoon\n(C) Teacup\n(D) Wine glass\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1207, "doc": {"video_name": "en_tv_32.mp4", "duration": 408.0, "question": "What color is the woman's clothing who is taking photos with a mobile phone?\n(A) Green\n(B) Black\n(C) White\n(D) Red\n", "candidates": ["Green", "Black", "White", "Red"], "answer": "B", "task_type": "plotQA", "question_id": "Q1207"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f7bd85d1e637cbc44c9676572e0337266d2306002fb23f1c1a9c6c40a8cbb26d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the woman's clothing who is taking photos with a mobile phone?\n(A) Green\n(B) Black\n(C) White\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the woman's clothing who is taking photos with a mobile phone?\n(A) Green\n(B) Black\n(C) White\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1211, "doc": {"video_name": "en_tv_30.mp4", "duration": 584.0, "question": "What color is the dress of the woman arguing with the man in the room?\n(A) Red\n(B) White\n(C) Blue\n(D) Black\n", "candidates": ["Red", "White", "Blue", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1211"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d1f2b0006d421befae156d97db51b32c5acf4f8dd8c3ea62b706d87b51e372d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the dress of the woman arguing with the man in the room?\n(A) Red\n(B) White\n(C) Blue\n(D) Black\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the dress of the woman arguing with the man in the room?\n(A) Red\n(B) White\n(C) Blue\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1215, "doc": {"video_name": "movie101_11.mp4", "duration": 206.0, "question": "What color is the door in the video scene?\n(A) White\n(B) Blue\n(C) Black\n(D) Red\n", "candidates": ["White", "Blue", "Black", "Red"], "answer": "D", "task_type": "plotQA", "question_id": "Q1215"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5cf2af02151e40d7259426ecab2c5bb4528f63d2db5a50c3f93364345d418d16", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the door in the video scene?\n(A) White\n(B) Blue\n(C) Black\n(D) Red\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What color is the door in the video scene?\n(A) White\n(B) Blue\n(C) Black\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1219, "doc": {"video_name": "movie101_20.mp4", "duration": 476.0, "question": "How many bottles are hanging on the eaves in the video?\n(A) 7\n(B) 6\n(C) 3\n(D) 9\n", "candidates": ["7", "6", "3", "9"], "answer": "A", "task_type": "plotQA", "question_id": "Q1219"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4a2f597b34a5f76957219fc78d1fa6148f8f759168d0ee7e1c4fef492c6ae6c1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many bottles are hanging on the eaves in the video?\n(A) 7\n(B) 6\n(C) 3\n(D) 9\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "How many bottles are hanging on the eaves in the video?\n(A) 7\n(B) 6\n(C) 3\n(D) 9\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1223, "doc": {"video_name": "movie101_40.mp4", "duration": 450.0, "question": "What is the expression of the person riding the horse when directly exposed to sunlight?\n(A) Neutral\n(B) Grimacing\n(C) Sad\n(D) Excited\n", "candidates": ["Neutral", "Grimacing", "Sad", "Excited"], "answer": "B", "task_type": "plotQA", "question_id": "Q1223"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "77b43d23dbebb977f75c91f9f8c3df0b048f774fb04d1877f5c5d5fce0cb2016", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the expression of the person riding the horse when directly exposed to sunlight?\n(A) Neutral\n(B) Grimacing\n(C) Sad\n(D) Excited\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the expression of the person riding the horse when directly exposed to sunlight?\n(A) Neutral\n(B) Grimacing\n(C) Sad\n(D) Excited\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1227, "doc": {"video_name": "en_tv_28.mp4", "duration": 575.0, "question": "In the video, what instrument is the woman playing?\n(A) Piano\n(B) Guqin\n(C) Cello\n(D) Violin\n", "candidates": ["Piano", "Guqin", "Cello", "Violin"], "answer": "C", "task_type": "plotQA", "question_id": "Q1227"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b645d69f55569cb7c6bf6843876c4424b6aaa20594ceea8957a49554df3032b0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the video, what instrument is the woman playing?\n(A) Piano\n(B) Guqin\n(C) Cello\n(D) Violin\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "In the video, what instrument is the woman playing?\n(A) Piano\n(B) Guqin\n(C) Cello\n(D) Violin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1231, "doc": {"video_name": "movie101_108.mp4", "duration": 330.0, "question": "Why do the girls' parents want them to take a bath?\n(A) They got food on themselves\n(B) The weather is too hot\n(C) They got oil on their clothes\n(D) Because they got wet in the rain\n", "candidates": ["They got food on themselves", "The weather is too hot", "They got oil on their clothes", "Because they got wet in the rain"], "answer": "D", "task_type": "plotQA", "question_id": "Q1231"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "582b711b333425e758144e8278290de0289ac2778fd2341f4c735de02e3b377f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why do the girls' parents want them to take a bath?\n(A) They got food on themselves\n(B) The weather is too hot\n(C) They got oil on their clothes\n(D) Because they got wet in the rain\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "Why do the girls' parents want them to take a bath?\n(A) They got food on themselves\n(B) The weather is too hot\n(C) They got oil on their clothes\n(D) Because they got wet in the rain\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1235, "doc": {"video_name": "movie101_107.mp4", "duration": 471.0, "question": "Why does the woman get off the truck at the beginning of the video?\n(A) Someone stopped the truck\n(B) There's an obstacle ahead\n(C) The truck ran out of fuel\n(D) To unload goods for the marketplace\n", "candidates": ["Someone stopped the truck", "There's an obstacle ahead", "The truck ran out of fuel", "To unload goods for the marketplace"], "answer": "D", "task_type": "plotQA", "question_id": "Q1235"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b68a4a73a7ccc54de1112e937e5c4dbd1dd61136546963e2b0c2aa471b252bb5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why does the woman get off the truck at the beginning of the video?\n(A) Someone stopped the truck\n(B) There's an obstacle ahead\n(C) The truck ran out of fuel\n(D) To unload goods for the marketplace\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Why does the woman get off the truck at the beginning of the video?\n(A) Someone stopped the truck\n(B) There's an obstacle ahead\n(C) The truck ran out of fuel\n(D) To unload goods for the marketplace\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1239, "doc": {"video_name": "movie101_36.mp4", "duration": 415.0, "question": "What color is the scarf worn by the girl playing in the snow in a plaid shirt in the video?\n(A) White\n(B) Red\n(C) Blue\n(D) Green\n", "candidates": ["White", "Red", "Blue", "Green"], "answer": "B", "task_type": "plotQA", "question_id": "Q1239"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ccc1f5f9f7804fffeeb0040535f99e74876b9fb35160d98cacda776b18a7b81b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the scarf worn by the girl playing in the snow in a plaid shirt in the video?\n(A) White\n(B) Red\n(C) Blue\n(D) Green\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the scarf worn by the girl playing in the snow in a plaid shirt in the video?\n(A) White\n(B) Red\n(C) Blue\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1243, "doc": {"video_name": "movie101_19.mp4", "duration": 477.0, "question": "What kind of animal are the people in the video leading?\n(A) Dog\n(B) Pig\n(C) Sheep\n(D) Tiger\n", "candidates": ["Dog", "Pig", "Sheep", "Tiger"], "answer": "A", "task_type": "plotQA", "question_id": "Q1243"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "64b479eed967f87554e5ec14dded48427a8744f59a53def49772e8da6fb3c94d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What kind of animal are the people in the video leading?\n(A) Dog\n(B) Pig\n(C) Sheep\n(D) Tiger\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What kind of animal are the people in the video leading?\n(A) Dog\n(B) Pig\n(C) Sheep\n(D) Tiger\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1247, "doc": {"video_name": "movie101_20.mp4", "duration": 476.0, "question": "What color is the woman's dress in the video?\n(A) Yellow\n(B) White\n(C) Black\n(D) Blue\n", "candidates": ["Yellow", "White", "Black", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1247"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "785fe13f86ee78e7799b831d56dc32278a08113dbe9eee9e3a53b15fc20ab0e2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the woman's dress in the video?\n(A) Yellow\n(B) White\n(C) Black\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What color is the woman's dress in the video?\n(A) Yellow\n(B) White\n(C) Black\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1251, "doc": {"video_name": "xiaoliyu_2.mp4", "duration": 364.0, "question": "Who took the purple glowing shell?\n(A) Carp\n(B) Lobster\n(C) Turtle\n(D) Frog\n", "candidates": ["Carp", "Lobster", "Turtle", "Frog"], "answer": "B", "task_type": "plotQA", "question_id": "Q1251"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "50a5f0cc6482369f5a0f42f595fc1ca85a59cbb229c38f7d7977511b162dfd6b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who took the purple glowing shell?\n(A) Carp\n(B) Lobster\n(C) Turtle\n(D) Frog\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Who took the purple glowing shell?\n(A) Carp\n(B) Lobster\n(C) Turtle\n(D) Frog\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1255, "doc": {"video_name": "xiaoliyu_2.mp4", "duration": 364.0, "question": "What does the lobster use to hang the frog upside down?\n(A) Hand\n(B) Seaweed\n(C) Rope\n(D) Antennae\n", "candidates": ["Hand", "Seaweed", "Rope", "Antennae"], "answer": "D", "task_type": "plotQA", "question_id": "Q1255"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "86d90a3ce4253aab21883ff46f073e6e476f429bcc436ab628247b656f60c9f8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What does the lobster use to hang the frog upside down?\n(A) Hand\n(B) Seaweed\n(C) Rope\n(D) Antennae\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What does the lobster use to hang the frog upside down?\n(A) Hand\n(B) Seaweed\n(C) Rope\n(D) Antennae\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1259, "doc": {"video_name": "en_tv_12.mp4", "duration": 587.0, "question": "Who ran into the smoke-filled room and carried the girl out?\n(A) Firefighter\n(B) Police\n(C) Passerby\n(D) Teacher\n", "candidates": ["Firefighter", "Police", "Passerby", "Teacher"], "answer": "A", "task_type": "plotQA", "question_id": "Q1259"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d2502aeffc1fbf08be45015e54a67d17294ffc1206a67c9f722cf6e52f1d47bd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Who ran into the smoke-filled room and carried the girl out?\n(A) Firefighter\n(B) Police\n(C) Passerby\n(D) Teacher\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "Who ran into the smoke-filled room and carried the girl out?\n(A) Firefighter\n(B) Police\n(C) Passerby\n(D) Teacher\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1263, "doc": {"video_name": "movie101_22.mp4", "duration": 335.0, "question": "What is the shape of the building in the video?\n(A) An elephant\n(B) A person\n(C) A large Buddha\n(D) A cow\n", "candidates": ["An elephant", "A person", "A large Buddha", "A cow"], "answer": "C", "task_type": "plotQA", "question_id": "Q1263"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "67c933621345290d7c60ae7dc9579aaa91c381acbb9214cbc8568b0b8f9f2b39", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the shape of the building in the video?\n(A) An elephant\n(B) A person\n(C) A large Buddha\n(D) A cow\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What is the shape of the building in the video?\n(A) An elephant\n(B) A person\n(C) A large Buddha\n(D) A cow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1267, "doc": {"video_name": "movie101_26.mp4", "duration": 763.0, "question": "What color are the glasses the man in the video is wearing?\n(A) Red\n(B) Black\n(C) Green\n(D) Blue\n", "candidates": ["Red", "Black", "Green", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1267"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "219d72ed9ccc72e0ade73a5092305a448ff01fb68fe4fd3f7d79f1a4d4773801", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color are the glasses the man in the video is wearing?\n(A) Red\n(B) Black\n(C) Green\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color are the glasses the man in the video is wearing?\n(A) Red\n(B) Black\n(C) Green\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1271, "doc": {"video_name": "en_tv_29.mp4", "duration": 451.0, "question": "What color is the sweater of the old man talking to the woman at the beginning of the video?\n(A) Black\n(B) Purple\n(C) Green\n(D) White\n", "candidates": ["Black", "Purple", "Green", "White"], "answer": "B", "task_type": "plotQA", "question_id": "Q1271"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "469c97589f1454d0777962ec95bb829c73593d03f7da281330335f21ba4ef3e9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the sweater of the old man talking to the woman at the beginning of the video?\n(A) Black\n(B) Purple\n(C) Green\n(D) White\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the sweater of the old man talking to the woman at the beginning of the video?\n(A) Black\n(B) Purple\n(C) Green\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1275, "doc": {"video_name": "en_tv_5.mp4", "duration": 491.0, "question": "What happened to the woman in the sewer after she attacked the police?\n(A) Hit by an object\n(B) She was washed away by the water\n(C) Knocked out by someone\n(D) Taken away by someone\n", "candidates": ["Hit by an object", "She was washed away by the water", "Knocked out by someone", "Taken away by someone"], "answer": "B", "task_type": "plotQA", "question_id": "Q1275"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3b05da6119f8aafc841c23fb0ffcd16ba8344bff847eb7ccead48e8a0048bcb2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What happened to the woman in the sewer after she attacked the police?\n(A) Hit by an object\n(B) She was washed away by the water\n(C) Knocked out by someone\n(D) Taken away by someone\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What happened to the woman in the sewer after she attacked the police?\n(A) Hit by an object\n(B) She was washed away by the water\n(C) Knocked out by someone\n(D) Taken away by someone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1279, "doc": {"video_name": "movie101_68.mp4", "duration": 445.0, "question": "How many people are in the snowy mountains at the beginning of the film?\n(A) 3\n(B) 2\n(C) 1\n(D) 4\n", "candidates": ["3", "2", "1", "4"], "answer": "D", "task_type": "plotQA", "question_id": "Q1279"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2238bddca46177c8063c6c177a200c6e0aa3093fbcc6ff88060c06e7b084b208", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many people are in the snowy mountains at the beginning of the film?\n(A) 3\n(B) 2\n(C) 1\n(D) 4\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "How many people are in the snowy mountains at the beginning of the film?\n(A) 3\n(B) 2\n(C) 1\n(D) 4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1283, "doc": {"video_name": "tomjerry_4.mp4", "duration": 311.0, "question": "What are the cartoon cat and mouse doing in front of the pink curtain?\n(A) Drinking water\n(B) Fighting\n(C) Clapping hands\n(D) Eating snacks\n", "candidates": ["Drinking water", "Fighting", "Clapping hands", "Eating snacks"], "answer": "C", "task_type": "plotQA", "question_id": "Q1283"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e58919fc0d6710944671c4564cb8dacf6a731b32a860b088fa17d30391b1b98b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What are the cartoon cat and mouse doing in front of the pink curtain?\n(A) Drinking water\n(B) Fighting\n(C) Clapping hands\n(D) Eating snacks\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What are the cartoon cat and mouse doing in front of the pink curtain?\n(A) Drinking water\n(B) Fighting\n(C) Clapping hands\n(D) Eating snacks\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1287, "doc": {"video_name": "haimian_4.mp4", "duration": 323.0, "question": "What is the mood of the cartoon octopus in the room?\n(A) Lonely\n(B) Sad\n(C) Happy\n(D) Angry\n", "candidates": ["Lonely", "Sad", "Happy", "Angry"], "answer": "A", "task_type": "plotQA", "question_id": "Q1287"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c702548d97aa34a8be29a02e750091b3b814b77f4b39a9812cd4b5a84fa87e07", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the mood of the cartoon octopus in the room?\n(A) Lonely\n(B) Sad\n(C) Happy\n(D) Angry\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What is the mood of the cartoon octopus in the room?\n(A) Lonely\n(B) Sad\n(C) Happy\n(D) Angry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1291, "doc": {"video_name": "haimian_10.mp4", "duration": 361.0, "question": "What shape did the cartoon starfish trim the cartoon sponge into?\n(A) Square\n(B) Diamond\n(C) Round\n(D) Cylindrical\n", "candidates": ["Square", "Diamond", "Round", "Cylindrical"], "answer": "D", "task_type": "plotQA", "question_id": "Q1291"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d165da22715e0f1c910c85706fb56f49ca2893b1e9da0e9ebd7145b6aa81fbfb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What shape did the cartoon starfish trim the cartoon sponge into?\n(A) Square\n(B) Diamond\n(C) Round\n(D) Cylindrical\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "What shape did the cartoon starfish trim the cartoon sponge into?\n(A) Square\n(B) Diamond\n(C) Round\n(D) Cylindrical\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1295, "doc": {"video_name": "en_tv_14.mp4", "duration": 459.0, "question": "What color is the bag on the table when the man and woman are chatting in the room?\n(A) Blue\n(B) Yellow\n(C) White\n(D) Red\n", "candidates": ["Blue", "Yellow", "White", "Red"], "answer": "A", "task_type": "plotQA", "question_id": "Q1295"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d4c685fbd3e8c4c5eff557a8076776171a2f14f3b857f318815eee865fc3bc5d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the bag on the table when the man and woman are chatting in the room?\n(A) Blue\n(B) Yellow\n(C) White\n(D) Red\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the bag on the table when the man and woman are chatting in the room?\n(A) Blue\n(B) Yellow\n(C) White\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1299, "doc": {"video_name": "xiaoliyu_3.mp4", "duration": 405.0, "question": "What did the cartoon snake place on the ground?\n(A) Peach\n(B) Orange\n(C) Apple\n(D) An object with two horns in yellow and blue\n", "candidates": ["Peach", "Orange", "Apple", "An object with two horns in yellow and blue"], "answer": "D", "task_type": "plotQA", "question_id": "Q1299"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9bfb7dad5ff7591cbf8fbb97185aebff2bb7ef0cfff6e7341ddc5a06edf1c09e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did the cartoon snake place on the ground?\n(A) Peach\n(B) Orange\n(C) Apple\n(D) An object with two horns in yellow and blue\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What did the cartoon snake place on the ground?\n(A) Peach\n(B) Orange\n(C) Apple\n(D) An object with two horns in yellow and blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1303, "doc": {"video_name": "movie101_82.mp4", "duration": 467.0, "question": "What color is the woman's dress in the video?\n(A) Yellow\n(B) White\n(C) Red\n(D) Black\n", "candidates": ["Yellow", "White", "Red", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1303"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b27ccbd063bdd09c3267b0a1528e1a6b8968b07b790413887450002f2ece18bd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the woman's dress in the video?\n(A) Yellow\n(B) White\n(C) Red\n(D) Black\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What color is the woman's dress in the video?\n(A) Yellow\n(B) White\n(C) Red\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1307, "doc": {"video_name": "movie101_19.mp4", "duration": 477.0, "question": "What color is the flower field at the end of the video?\n(A) Red\n(B) Green\n(C) Yellow\n(D) Blue\n", "candidates": ["Red", "Green", "Yellow", "Blue"], "answer": "C", "task_type": "plotQA", "question_id": "Q1307"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9dd6b541f1d8ae522ee9e7339b7c85216b41bef75027bc22a54898cd49aadfb5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the flower field at the end of the video?\n(A) Red\n(B) Green\n(C) Yellow\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the flower field at the end of the video?\n(A) Red\n(B) Green\n(C) Yellow\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1311, "doc": {"video_name": "movie101_79.mp4", "duration": 590.0, "question": "What is the man eating in the clothing store?\n(A) Burger\n(B) Pizza\n(C) Mothballs\n(D) Candy\n", "candidates": ["Burger", "Pizza", "Mothballs", "Candy"], "answer": "C", "task_type": "plotQA", "question_id": "Q1311"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8102ca27e1b52899db422560ac26c48eb4fd78e3b431afb6f0dd2206464cf430", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the man eating in the clothing store?\n(A) Burger\n(B) Pizza\n(C) Mothballs\n(D) Candy\n", "task_type": "plotQA", "pred_answer": "C.", "answer": "C"}, "input": "What is the man eating in the clothing store?\n(A) Burger\n(B) Pizza\n(C) Mothballs\n(D) Candy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1315, "doc": {"video_name": "movie101_30.mp4", "duration": 726.0, "question": "How many knives are stuck in the clown's back in the video?\n(A) Three\n(B) One\n(C) Two\n(D) Four\n", "candidates": ["Three", "One", "Two", "Four"], "answer": "C", "task_type": "plotQA", "question_id": "Q1315"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3907679c2184aa65b6a250632b2e07469874c7d9efae593941c82337ddc9fd88", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many knives are stuck in the clown's back in the video?\n(A) Three\n(B) One\n(C) Two\n(D) Four\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "How many knives are stuck in the clown's back in the video?\n(A) Three\n(B) One\n(C) Two\n(D) Four\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1319, "doc": {"video_name": "movie101_12.mp4", "duration": 346.0, "question": "What color is the incense burner in the video?\n(A) White\n(B) Black\n(C) Yellow\n(D) Blue\n", "candidates": ["White", "Black", "Yellow", "Blue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1319"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "33685f70c05d7a0059b8fd5eacbe0aabc3daf261b973e95571af7960ab077787", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the incense burner in the video?\n(A) White\n(B) Black\n(C) Yellow\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What color is the incense burner in the video?\n(A) White\n(B) Black\n(C) Yellow\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1323, "doc": {"video_name": "en_tv_10.mp4", "duration": 545.0, "question": "What is the woman in the plaid shirt watching inside the house?\n(A) Book\n(B) Surveillance video\n(C) Movie\n(D) Newspaper\n", "candidates": ["Book", "Surveillance video", "Movie", "Newspaper"], "answer": "B", "task_type": "plotQA", "question_id": "Q1323"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "42486b8c209004d5b77c7dd02e07ab769e88fb6b11fbe260316c337facd8340e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the woman in the plaid shirt watching inside the house?\n(A) Book\n(B) Surveillance video\n(C) Movie\n(D) Newspaper\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the woman in the plaid shirt watching inside the house?\n(A) Book\n(B) Surveillance video\n(C) Movie\n(D) Newspaper\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1327, "doc": {"video_name": "movie101_6.mp4", "duration": 693.0, "question": "What color is the car the man drives in the video?\n(A) Green\n(B) Black\n(C) White\n(D) Yellow\n", "candidates": ["Green", "Black", "White", "Yellow"], "answer": "B", "task_type": "plotQA", "question_id": "Q1327"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "595273ef9f680938b8beb7037d407fb7933158d6c1814982b73c9367a64e5f73", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the car the man drives in the video?\n(A) Green\n(B) Black\n(C) White\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What color is the car the man drives in the video?\n(A) Green\n(B) Black\n(C) White\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1331, "doc": {"video_name": "movie101_6.mp4", "duration": 693.0, "question": "Does the scene where several people are having a barbecue together take place during the day or at night?\n(A) Morning\n(B) Dawn\n(C) Night\n(D) Evening\n", "candidates": ["Morning", "Dawn", "Night", "Evening"], "answer": "C", "task_type": "plotQA", "question_id": "Q1331"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a8ca7e97352aca30a3e07ad7dc5c40642411619b3d20336fcc30af640c158972", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Does the scene where several people are having a barbecue together take place during the day or at night?\n(A) Morning\n(B) Dawn\n(C) Night\n(D) Evening\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Does the scene where several people are having a barbecue together take place during the day or at night?\n(A) Morning\n(B) Dawn\n(C) Night\n(D) Evening\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1335, "doc": {"video_name": "movie101_6.mp4", "duration": 693.0, "question": "In the scene where the protagonist woman is shopping for clothes in a mall, does she wear glasses?\n(A) Wears glasses\n(B) Wears a hat\n(C) No\n(D) Wears gloves\n", "candidates": ["Wears glasses", "Wears a hat", "No", "Wears gloves"], "answer": "C", "task_type": "plotQA", "question_id": "Q1335"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ff7c702a55a705b5d4e2717360fa2eb50b09f1d1f0aca5a42526111eaded802", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the scene where the protagonist woman is shopping for clothes in a mall, does she wear glasses?\n(A) Wears glasses\n(B) Wears a hat\n(C) No\n(D) Wears gloves\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "In the scene where the protagonist woman is shopping for clothes in a mall, does she wear glasses?\n(A) Wears glasses\n(B) Wears a hat\n(C) No\n(D) Wears gloves\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1339, "doc": {"video_name": "movie101_55.mp4", "duration": 315.0, "question": "Why is the person in the gray suit jacket standing at the door of the ward?\n(A) Drinking water\n(B) Eating\n(C) Waiting for someone\n(D) To visit the injured child\n", "candidates": ["Drinking water", "Eating", "Waiting for someone", "To visit the injured child"], "answer": "D", "task_type": "plotQA", "question_id": "Q1339"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5bc38ae24b5f0591950db590538027faf964b41be2f639dd4de2ac81935b0b2f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why is the person in the gray suit jacket standing at the door of the ward?\n(A) Drinking water\n(B) Eating\n(C) Waiting for someone\n(D) To visit the injured child\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Why is the person in the gray suit jacket standing at the door of the ward?\n(A) Drinking water\n(B) Eating\n(C) Waiting for someone\n(D) To visit the injured child\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1343, "doc": {"video_name": "movie101_76.mp4", "duration": 688.0, "question": "What does the man in the video use to pull the woman to walk?\n(A) Rope\n(B) Scarf\n(C) Backpack\n(D) Tree branch\n", "candidates": ["Rope", "Scarf", "Backpack", "Tree branch"], "answer": "B", "task_type": "plotQA", "question_id": "Q1343"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c49d487bec2b6f252cec3fa34048e000c57c5686355a603b44f7dcd5dee1738a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What does the man in the video use to pull the woman to walk?\n(A) Rope\n(B) Scarf\n(C) Backpack\n(D) Tree branch\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What does the man in the video use to pull the woman to walk?\n(A) Rope\n(B) Scarf\n(C) Backpack\n(D) Tree branch\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1347, "doc": {"video_name": "movie101_76.mp4", "duration": 688.0, "question": "After the train stops, what animal appears?\n(A) Dog\n(B) Cat\n(C) Horse\n(D) Deer\n", "candidates": ["Dog", "Cat", "Horse", "Deer"], "answer": "D", "task_type": "plotQA", "question_id": "Q1347"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ec84790702e616ca495b760b415ba32b150f3e6292ece836cb69edbde46d2bc8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "After the train stops, what animal appears?\n(A) Dog\n(B) Cat\n(C) Horse\n(D) Deer\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "After the train stops, what animal appears?\n(A) Dog\n(B) Cat\n(C) Horse\n(D) Deer\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1351, "doc": {"video_name": "movie101_57.mp4", "duration": 753.0, "question": "What did the old man in the video receive as a gift from the police?\n(A) Television\n(B) Sunglasses\n(C) Food\n(D) Money\n", "candidates": ["Television", "Sunglasses", "Food", "Money"], "answer": "B", "task_type": "plotQA", "question_id": "Q1351"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ea97c0f74ac4295c6279768481f3221e8f2c09c10a90c70dd8b7f8164722faf3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did the old man in the video receive as a gift from the police?\n(A) Television\n(B) Sunglasses\n(C) Food\n(D) Money\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What did the old man in the video receive as a gift from the police?\n(A) Television\n(B) Sunglasses\n(C) Food\n(D) Money\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1355, "doc": {"video_name": "en_tv_10.mp4", "duration": 545.0, "question": "At the beginning of the video, what do the two men not find on the street?\n(A) Cell phone\n(B) Vehicle\n(C) Corpse\n(D) Watch\n", "candidates": ["Cell phone", "Vehicle", "Corpse", "Watch"], "answer": "D", "task_type": "plotQA", "question_id": "Q1355"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f0a8b0bccd90fe9dfb3cfbe1e30450a6451127feea63e94b0ce7dcb60df08f9b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "At the beginning of the video, what do the two men not find on the street?\n(A) Cell phone\n(B) Vehicle\n(C) Corpse\n(D) Watch\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "At the beginning of the video, what do the two men not find on the street?\n(A) Cell phone\n(B) Vehicle\n(C) Corpse\n(D) Watch\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1359, "doc": {"video_name": "movie101_104.mp4", "duration": 514.0, "question": "What is the girl with glasses' mood after receiving the letter for the second time?\n(A) Neutral\n(B) Excited\n(C) Disappointed\n(D) Happy\n", "candidates": ["Neutral", "Excited", "Disappointed", "Happy"], "answer": "D", "task_type": "plotQA", "question_id": "Q1359"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fa6d50e60ca263ebb90ee1a6092e4e5f16672b045e831ac324f7059d3b34e5ba", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the girl with glasses' mood after receiving the letter for the second time?\n(A) Neutral\n(B) Excited\n(C) Disappointed\n(D) Happy\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What is the girl with glasses' mood after receiving the letter for the second time?\n(A) Neutral\n(B) Excited\n(C) Disappointed\n(D) Happy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1363, "doc": {"video_name": "movie101_31.mp4", "duration": 403.0, "question": "How many bullets are there on the table in the movie clip?\n(A) 2\n(B) 8\n(C) 6\n(D) 10\n", "candidates": ["2", "8", "6", "10"], "answer": "D", "task_type": "plotQA", "question_id": "Q1363"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "55d38d2fcada02fcdebb934a9c861d2dd49a88526313ebe88b9e1742560dd747", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many bullets are there on the table in the movie clip?\n(A) 2\n(B) 8\n(C) 6\n(D) 10\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "How many bullets are there on the table in the movie clip?\n(A) 2\n(B) 8\n(C) 6\n(D) 10\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1367, "doc": {"video_name": "xiaoliyu_1.mp4", "duration": 381.0, "question": "What did the cartoon catfish and cartoon lobster do to the cartoon turtle?\n(A) Kicked the cartoon turtle, knocking it into the rocks\n(B) Threw it\n(C) Buried it\n(D) Knocked it out\n", "candidates": ["Kicked the cartoon turtle, knocking it into the rocks", "Threw it", "Buried it", "Knocked it out"], "answer": "A", "task_type": "plotQA", "question_id": "Q1367"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3b05acbeed78cbe24de72e1871742e61c19f695a4ccd7ffd581755ee495c47ea", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did the cartoon catfish and cartoon lobster do to the cartoon turtle?\n(A) Kicked the cartoon turtle, knocking it into the rocks\n(B) Threw it\n(C) Buried it\n(D) Knocked it out\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What did the cartoon catfish and cartoon lobster do to the cartoon turtle?\n(A) Kicked the cartoon turtle, knocking it into the rocks\n(B) Threw it\n(C) Buried it\n(D) Knocked it out\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1371, "doc": {"video_name": "movie101_82.mp4", "duration": 467.0, "question": "What does the man in the prison swallow in the video?\n(A) Razor blade\n(B) Needle\n(C) Key\n(D) Stone\n", "candidates": ["Razor blade", "Needle", "Key", "Stone"], "answer": "A", "task_type": "plotQA", "question_id": "Q1371"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e93faa02046fc437415ff3d042f5d44f52b03ab0f743bc2462d5d942bbe0af8f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What does the man in the prison swallow in the video?\n(A) Razor blade\n(B) Needle\n(C) Key\n(D) Stone\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What does the man in the prison swallow in the video?\n(A) Razor blade\n(B) Needle\n(C) Key\n(D) Stone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1375, "doc": {"video_name": "movie101_57.mp4", "duration": 753.0, "question": "What is the occupation of the old man in the video?\n(A) Scrap collector\n(B) Teacher\n(C) Police officer\n(D) Doctor\n", "candidates": ["Scrap collector", "Teacher", "Police officer", "Doctor"], "answer": "A", "task_type": "plotQA", "question_id": "Q1375"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "49f783ff02cecb799e390cf009b16a62a8dd4dbc253963dd2e31e04f245df35b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the occupation of the old man in the video?\n(A) Scrap collector\n(B) Teacher\n(C) Police officer\n(D) Doctor\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What is the occupation of the old man in the video?\n(A) Scrap collector\n(B) Teacher\n(C) Police officer\n(D) Doctor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1379, "doc": {"video_name": "movie101_16.mp4", "duration": 740.0, "question": "What color is the sweater worn by the man at the beginning of the video?\n(A) Red\n(B) Blue\n(C) Yellow\n(D) White\n", "candidates": ["Red", "Blue", "Yellow", "White"], "answer": "A", "task_type": "plotQA", "question_id": "Q1379"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "19064b7e1c3aa8e7489cc77b43380d1a8863c9d688d54859966da8748fb0f304", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the sweater worn by the man at the beginning of the video?\n(A) Red\n(B) Blue\n(C) Yellow\n(D) White\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the sweater worn by the man at the beginning of the video?\n(A) Red\n(B) Blue\n(C) Yellow\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1383, "doc": {"video_name": "movie101_0.mp4", "duration": 789.0, "question": "What is the mood of the man in the red suit when he is talking with the woman in the tunnel?\n(A) Sad\n(B) Neutral\n(C) Happy\n(D) Disappointed\n", "candidates": ["Sad", "Neutral", "Happy", "Disappointed"], "answer": "C", "task_type": "plotQA", "question_id": "Q1383"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05415265475a9b3a7309c265ae97203e8fee24857850551278fd851a9ade4a6b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the mood of the man in the red suit when he is talking with the woman in the tunnel?\n(A) Sad\n(B) Neutral\n(C) Happy\n(D) Disappointed\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "What is the mood of the man in the red suit when he is talking with the woman in the tunnel?\n(A) Sad\n(B) Neutral\n(C) Happy\n(D) Disappointed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1387, "doc": {"video_name": "en_tv_32.mp4", "duration": 408.0, "question": "What is on the dining table at the beginning of the video?\n(A) Flowers\n(B) Fruit\n(C) Computer\n(D) Dumplings\n", "candidates": ["Flowers", "Fruit", "Computer", "Dumplings"], "answer": "B", "task_type": "plotQA", "question_id": "Q1387"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eb0b94b7d810ec4622256938b7f1bc689cd0b1bb668fc4744e9cdf004ed6ec3e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is on the dining table at the beginning of the video?\n(A) Flowers\n(B) Fruit\n(C) Computer\n(D) Dumplings\n", "task_type": "plotQA", "pred_answer": "D", "answer": "B"}, "input": "What is on the dining table at the beginning of the video?\n(A) Flowers\n(B) Fruit\n(C) Computer\n(D) Dumplings\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1391, "doc": {"video_name": "movie101_33.mp4", "duration": 240.0, "question": "How many colors of smoke grenades are there in the movie?\n(A) One\n(B) Three\n(C) Four\n(D) Two\n", "candidates": ["One", "Three", "Four", "Two"], "answer": "B", "task_type": "plotQA", "question_id": "Q1391"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d77360e23d35e80495613accf793f00423e1cf73d36964c87d99d94ac444c4d2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many colors of smoke grenades are there in the movie?\n(A) One\n(B) Three\n(C) Four\n(D) Two\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "How many colors of smoke grenades are there in the movie?\n(A) One\n(B) Three\n(C) Four\n(D) Two\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1395, "doc": {"video_name": "tomjerry_8.mp4", "duration": 308.0, "question": "What did the cartoon mouse grab when it fell from the building?\n(A) A rope\n(B) A curtain\n(C) A power line\n(D) A hook\n", "candidates": ["A rope", "A curtain", "A power line", "A hook"], "answer": "D", "task_type": "plotQA", "question_id": "Q1395"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d408aad045daa8bca0b3e662fe1057e33b0c8ec7122fc3923f2c7add09fb8cf", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did the cartoon mouse grab when it fell from the building?\n(A) A rope\n(B) A curtain\n(C) A power line\n(D) A hook\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What did the cartoon mouse grab when it fell from the building?\n(A) A rope\n(B) A curtain\n(C) A power line\n(D) A hook\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1399, "doc": {"video_name": "movie101_23.mp4", "duration": 473.0, "question": "What shape is the building in the video?\n(A) Piano\n(B) Pineapple\n(C) Guitar\n(D) Apple\n", "candidates": ["Piano", "Pineapple", "Guitar", "Apple"], "answer": "C", "task_type": "plotQA", "question_id": "Q1399"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b1ea08889d41e705965c92cb826c143b8f9363e787a9b54b7105694249b5a990", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What shape is the building in the video?\n(A) Piano\n(B) Pineapple\n(C) Guitar\n(D) Apple\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What shape is the building in the video?\n(A) Piano\n(B) Pineapple\n(C) Guitar\n(D) Apple\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1403, "doc": {"video_name": "movie101_69.mp4", "duration": 385.0, "question": "In the video, what did the man in black dig the goods out of?\n(A) Cabinet\n(B) Water\n(C) Soil\n(D) Ice\n", "candidates": ["Cabinet", "Water", "Soil", "Ice"], "answer": "D", "task_type": "plotQA", "question_id": "Q1403"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5653e4b456a7bd23bda9babbca68eb9e6b0b579c403dee41d3a8031b9835fa7c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, what did the man in black dig the goods out of?\n(A) Cabinet\n(B) Water\n(C) Soil\n(D) Ice\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "In the video, what did the man in black dig the goods out of?\n(A) Cabinet\n(B) Water\n(C) Soil\n(D) Ice\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1407, "doc": {"video_name": "movie101_3.mp4", "duration": 535.0, "question": "Why didn't the naked man at the beginning of the video manage to escape?\n(A) Fell down\n(B) Caught\n(C) Injured\n(D) He was bound by chains\n", "candidates": ["Fell down", "Caught", "Injured", "He was bound by chains"], "answer": "D", "task_type": "plotQA", "question_id": "Q1407"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9d6d8e65a0c080e3ece0d0b92dc6debad4a317665004d48464b74efe149c6d92", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why didn't the naked man at the beginning of the video manage to escape?\n(A) Fell down\n(B) Caught\n(C) Injured\n(D) He was bound by chains\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Why didn't the naked man at the beginning of the video manage to escape?\n(A) Fell down\n(B) Caught\n(C) Injured\n(D) He was bound by chains\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1411, "doc": {"video_name": "haimian_9.mp4", "duration": 444.0, "question": "What did the cartoon sponge and the cartoon starfish buy in the store?\n(A) Ice Cream\n(B) Coke\n(C) Pizza\n(D) Burger\n", "candidates": ["Ice Cream", "Coke", "Pizza", "Burger"], "answer": "A", "task_type": "plotQA", "question_id": "Q1411"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e8546c4736fa11d93733815e74a8cdd9bf606e4e15779fa494f29db7fb8f5848", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did the cartoon sponge and the cartoon starfish buy in the store?\n(A) Ice Cream\n(B) Coke\n(C) Pizza\n(D) Burger\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What did the cartoon sponge and the cartoon starfish buy in the store?\n(A) Ice Cream\n(B) Coke\n(C) Pizza\n(D) Burger\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1415, "doc": {"video_name": "movie101_107.mp4", "duration": 471.0, "question": "What does the man in the grey vest do at the marketplace?\n(A) Buys raw meat\n(B) Argues with the woman selling fish\n(C) Buys vegetables\n(D) Helps others carry goods\n", "candidates": ["Buys raw meat", "Argues with the woman selling fish", "Buys vegetables", "Helps others carry goods"], "answer": "B", "task_type": "plotQA", "question_id": "Q1415"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "21c2166c43be58777559baeab669bb8237e998f5ee28625e608932e9d3ecc46d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What does the man in the grey vest do at the marketplace?\n(A) Buys raw meat\n(B) Argues with the woman selling fish\n(C) Buys vegetables\n(D) Helps others carry goods\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What does the man in the grey vest do at the marketplace?\n(A) Buys raw meat\n(B) Argues with the woman selling fish\n(C) Buys vegetables\n(D) Helps others carry goods\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1419, "doc": {"video_name": "movie101_81.mp4", "duration": 252.0, "question": "What is the woman doing at the end of the video?\n(A) Eating\n(B) Playing\n(C) Drinking\n(D) Sleeping\n", "candidates": ["Eating", "Playing", "Drinking", "Sleeping"], "answer": "C", "task_type": "plotQA", "question_id": "Q1419"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85193194de3ffbbf285e04a6e73eddb7a203fc123448af35dceb369000816f69", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the woman doing at the end of the video?\n(A) Eating\n(B) Playing\n(C) Drinking\n(D) Sleeping\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What is the woman doing at the end of the video?\n(A) Eating\n(B) Playing\n(C) Drinking\n(D) Sleeping\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1423, "doc": {"video_name": "movie101_76.mp4", "duration": 688.0, "question": "At the beginning of the video, what does the man in green give to the woman?\n(A) Train ticket\n(B) Bread\n(C) Water cup\n(D) Key\n", "candidates": ["Train ticket", "Bread", "Water cup", "Key"], "answer": "A", "task_type": "plotQA", "question_id": "Q1423"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98a11eb6b03960fb92654f04ef77aa332842674b9ee4bb893876a3ec47ba9909", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "At the beginning of the video, what does the man in green give to the woman?\n(A) Train ticket\n(B) Bread\n(C) Water cup\n(D) Key\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "At the beginning of the video, what does the man in green give to the woman?\n(A) Train ticket\n(B) Bread\n(C) Water cup\n(D) Key\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1427, "doc": {"video_name": "xiaoliyu_8.mp4", "duration": 347.0, "question": "Where does the cartoon fish hide to escape the attack of the brown cartoon fish?\n(A) In the seaweed\n(B) In the coral pile\n(C) Under the rock\n(D) Undersea crevice\n", "candidates": ["In the seaweed", "In the coral pile", "Under the rock", "Undersea crevice"], "answer": "D", "task_type": "plotQA", "question_id": "Q1427"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a0b3b942bf8efffcc86993d1722aac7dee311bf8ea1d22572ad74c6b93c260ac", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where does the cartoon fish hide to escape the attack of the brown cartoon fish?\n(A) In the seaweed\n(B) In the coral pile\n(C) Under the rock\n(D) Undersea crevice\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Where does the cartoon fish hide to escape the attack of the brown cartoon fish?\n(A) In the seaweed\n(B) In the coral pile\n(C) Under the rock\n(D) Undersea crevice\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1431, "doc": {"video_name": "xiaoliyu_8.mp4", "duration": 347.0, "question": "Why does the brown cartoon fish attack the cartoon snake and cartoon lobster?\n(A) Because it's angry\n(B) No reason\n(C) Because the cartoon snake attacked them from behind\n(D) Because it's fun\n", "candidates": ["Because it's angry", "No reason", "Because the cartoon snake attacked them from behind", "Because it's fun"], "answer": "C", "task_type": "plotQA", "question_id": "Q1431"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c16de3fc101a856eb87b06b6bbc1c88769175c7f7df4e83bd39baecfee97196", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why does the brown cartoon fish attack the cartoon snake and cartoon lobster?\n(A) Because it's angry\n(B) No reason\n(C) Because the cartoon snake attacked them from behind\n(D) Because it's fun\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Why does the brown cartoon fish attack the cartoon snake and cartoon lobster?\n(A) Because it's angry\n(B) No reason\n(C) Because the cartoon snake attacked them from behind\n(D) Because it's fun\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1435, "doc": {"video_name": "movie101_103.mp4", "duration": 294.0, "question": "How does the woman in the white coat leave after eating?\n(A) Someone else drives her away\n(B) Rides her own bike\n(C) Takes the subway\n(D) Takes public bike\n", "candidates": ["Someone else drives her away", "Rides her own bike", "Takes the subway", "Takes public bike"], "answer": "A", "task_type": "plotQA", "question_id": "Q1435"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "39f55459adf4d753aa30eab662fba868338285ce54fed0dd63418f227bb5e1dd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How does the woman in the white coat leave after eating?\n(A) Someone else drives her away\n(B) Rides her own bike\n(C) Takes the subway\n(D) Takes public bike\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "How does the woman in the white coat leave after eating?\n(A) Someone else drives her away\n(B) Rides her own bike\n(C) Takes the subway\n(D) Takes public bike\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1439, "doc": {"video_name": "movie101_103.mp4", "duration": 294.0, "question": "At the beginning of the video, what are the two men doing in the office?\n(A) Each looking at their own phones\n(B) Eating\n(C) Working\n(D) Drinking\n", "candidates": ["Each looking at their own phones", "Eating", "Working", "Drinking"], "answer": "A", "task_type": "plotQA", "question_id": "Q1439"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2507524371bbae5d1ffe225b650a8cb5551b1598cacbe7e5572ac64bfc8f1d4e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "At the beginning of the video, what are the two men doing in the office?\n(A) Each looking at their own phones\n(B) Eating\n(C) Working\n(D) Drinking\n", "task_type": "plotQA", "pred_answer": "A.", "answer": "A"}, "input": "At the beginning of the video, what are the two men doing in the office?\n(A) Each looking at their own phones\n(B) Eating\n(C) Working\n(D) Drinking\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1443, "doc": {"video_name": "xiaoliyu_5.mp4", "duration": 415.0, "question": "What are the cartoon shrimp and cartoon seal disguised as?\n(A) Cartoon fish and cartoon snake\n(B) Cartoon hippo and cartoon dolphin\n(C) Cartoon snake and cartoon shark\n(D) Disguised as a cartoon lobster and a cartoon catfish\n", "candidates": ["Cartoon fish and cartoon snake", "Cartoon hippo and cartoon dolphin", "Cartoon snake and cartoon shark", "Disguised as a cartoon lobster and a cartoon catfish"], "answer": "D", "task_type": "plotQA", "question_id": "Q1443"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "791a8ef88ac05b2773c9c7ca943be9ada833ab1b45e510ce1cd409a1d1ef250b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What are the cartoon shrimp and cartoon seal disguised as?\n(A) Cartoon fish and cartoon snake\n(B) Cartoon hippo and cartoon dolphin\n(C) Cartoon snake and cartoon shark\n(D) Disguised as a cartoon lobster and a cartoon catfish\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "What are the cartoon shrimp and cartoon seal disguised as?\n(A) Cartoon fish and cartoon snake\n(B) Cartoon hippo and cartoon dolphin\n(C) Cartoon snake and cartoon shark\n(D) Disguised as a cartoon lobster and a cartoon catfish\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1447, "doc": {"video_name": "movie101_109.mp4", "duration": 670.0, "question": "Why are the people in military uniforms heading deep into the prairie?\n(A) Rest\n(B) To capture someone\n(C) Hide\n(D) Eat\n", "candidates": ["Rest", "To capture someone", "Hide", "Eat"], "answer": "B", "task_type": "plotQA", "question_id": "Q1447"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5d6828ebd8698f3dfd3cc8fa27e1f80adbea4e86a42a0ba48093017a57d7b76", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why are the people in military uniforms heading deep into the prairie?\n(A) Rest\n(B) To capture someone\n(C) Hide\n(D) Eat\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Why are the people in military uniforms heading deep into the prairie?\n(A) Rest\n(B) To capture someone\n(C) Hide\n(D) Eat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1451, "doc": {"video_name": "tomjerry_11.mp4", "duration": 322.0, "question": "What did the cartoon cat throw at the two cartoon mice?\n(A) A shoe\n(B) A black ball\n(C) A pair of gloves\n(D) A flat pan\n", "candidates": ["A shoe", "A black ball", "A pair of gloves", "A flat pan"], "answer": "B", "task_type": "plotQA", "question_id": "Q1451"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "52ab2c081da3f06dfd677e586e0bcb8c3e43b51ac20248668df6d6ea5f27a5dd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did the cartoon cat throw at the two cartoon mice?\n(A) A shoe\n(B) A black ball\n(C) A pair of gloves\n(D) A flat pan\n", "task_type": "plotQA", "pred_answer": "D", "answer": "B"}, "input": "What did the cartoon cat throw at the two cartoon mice?\n(A) A shoe\n(B) A black ball\n(C) A pair of gloves\n(D) A flat pan\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1455, "doc": {"video_name": "en_tv_29.mp4", "duration": 451.0, "question": "What animal appears in the video?\n(A) Dog\n(B) Snake\n(C) Dinosaur\n(D) Cat\n", "candidates": ["Dog", "Snake", "Dinosaur", "Cat"], "answer": "C", "task_type": "plotQA", "question_id": "Q1455"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "963440cca67bfb8c679bf7995db8044e6e18e8f66a8136e78e93bbb6302cc70b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What animal appears in the video?\n(A) Dog\n(B) Snake\n(C) Dinosaur\n(D) Cat\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What animal appears in the video?\n(A) Dog\n(B) Snake\n(C) Dinosaur\n(D) Cat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1459, "doc": {"video_name": "movie101_54.mp4", "duration": 580.0, "question": "In the video, what is the person in black clothes discussing with the person sitting in the car?\n(A) An important deal\n(B) An important arrest operation\n(C) An important match\n(D) A sudden natural disaster\n", "candidates": ["An important deal", "An important arrest operation", "An important match", "A sudden natural disaster"], "answer": "B", "task_type": "plotQA", "question_id": "Q1459"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7b354002d3ad7071713cb26cb7eb7ebf48f6dc69122f07bbb9d50a4333c17b1c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the video, what is the person in black clothes discussing with the person sitting in the car?\n(A) An important deal\n(B) An important arrest operation\n(C) An important match\n(D) A sudden natural disaster\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "In the video, what is the person in black clothes discussing with the person sitting in the car?\n(A) An important deal\n(B) An important arrest operation\n(C) An important match\n(D) A sudden natural disaster\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1463, "doc": {"video_name": "movie101_13.mp4", "duration": 456.0, "question": "What color is the top that the little girl who picks up the box in the video wearing?\n(A) Purple\n(B) Black\n(C) Red\n(D) White\n", "candidates": ["Purple", "Black", "Red", "White"], "answer": "A", "task_type": "plotQA", "question_id": "Q1463"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e0d402884bd2f69d17444a75e88c9c6999f4f4d46157929393d4c4aa8d2dbcb1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the top that the little girl who picks up the box in the video wearing?\n(A) Purple\n(B) Black\n(C) Red\n(D) White\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What color is the top that the little girl who picks up the box in the video wearing?\n(A) Purple\n(B) Black\n(C) Red\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1467, "doc": {"video_name": "movie101_109.mp4", "duration": 670.0, "question": "How do the people in military uniforms deal with the abandoned vehicle after inspecting it at the beginning of the video?\n(A) Dismantle\n(B) Throw away\n(C) Ignite\n(D) Sell\n", "candidates": ["Dismantle", "Throw away", "Ignite", "Sell"], "answer": "C", "task_type": "plotQA", "question_id": "Q1467"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "723afaa925ad876d78cee816b117826eb31bd3720081d22077141e1ff403145c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How do the people in military uniforms deal with the abandoned vehicle after inspecting it at the beginning of the video?\n(A) Dismantle\n(B) Throw away\n(C) Ignite\n(D) Sell\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "How do the people in military uniforms deal with the abandoned vehicle after inspecting it at the beginning of the video?\n(A) Dismantle\n(B) Throw away\n(C) Ignite\n(D) Sell\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1471, "doc": {"video_name": "en_tv_39.mp4", "duration": 476.0, "question": "What color is the man's clothing who sent the woman to the hospital?\n(A) Black\n(B) Green\n(C) Red\n(D) Grey\n", "candidates": ["Black", "Green", "Red", "Grey"], "answer": "B", "task_type": "plotQA", "question_id": "Q1471"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "909db980b423f212bade8a05b1740e5e4ab1913fee4af159932024aa1a2a059a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the man's clothing who sent the woman to the hospital?\n(A) Black\n(B) Green\n(C) Red\n(D) Grey\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What color is the man's clothing who sent the woman to the hospital?\n(A) Black\n(B) Green\n(C) Red\n(D) Grey\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1475, "doc": {"video_name": "movie101_24.mp4", "duration": 311.0, "question": "What color is the little goldfish that appears in the video?\n(A) Purple\n(B) Green\n(C) Blue\n(D) Red\n", "candidates": ["Purple", "Green", "Blue", "Red"], "answer": "D", "task_type": "plotQA", "question_id": "Q1475"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "105a7851614ad788810a85fb7433863e0068a039a2d98e56dee8f605cff9186b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the little goldfish that appears in the video?\n(A) Purple\n(B) Green\n(C) Blue\n(D) Red\n", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "What color is the little goldfish that appears in the video?\n(A) Purple\n(B) Green\n(C) Blue\n(D) Red\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1479, "doc": {"video_name": "en_tv_20.mp4", "duration": 426.0, "question": "What color is the coat of the man with a black hat in the amusement park?\n(A) Yellow\n(B) White\n(C) Green\n(D) Blue\n", "candidates": ["Yellow", "White", "Green", "Blue"], "answer": "C", "task_type": "plotQA", "question_id": "Q1479"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "82bc18dfa13073cfd52a5dda022d793e8e5240fcb2d7e0a3e4881aa2519fb29f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the coat of the man with a black hat in the amusement park?\n(A) Yellow\n(B) White\n(C) Green\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What color is the coat of the man with a black hat in the amusement park?\n(A) Yellow\n(B) White\n(C) Green\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1483, "doc": {"video_name": "movie101_7.mp4", "duration": 279.0, "question": "What color is the cover of the books on the bookshelf at the end of the video?\n(A) Green\n(B) Yellow\n(C) Red\n(D) White\n", "candidates": ["Green", "Yellow", "Red", "White"], "answer": "D", "task_type": "plotQA", "question_id": "Q1483"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9dbac3520fbedab024466cfe15076e03625810c442ae1f36c7c3e5f139c02399", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the cover of the books on the bookshelf at the end of the video?\n(A) Green\n(B) Yellow\n(C) Red\n(D) White\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What color is the cover of the books on the bookshelf at the end of the video?\n(A) Green\n(B) Yellow\n(C) Red\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1487, "doc": {"video_name": "movie101_112.mp4", "duration": 322.0, "question": "How did the person lying in the bed in the camp get there?\n(A) He was carried there\n(B) He ran there\n(C) He walked there\n(D) He was brought to the camp by horse\n", "candidates": ["He was carried there", "He ran there", "He walked there", "He was brought to the camp by horse"], "answer": "D", "task_type": "plotQA", "question_id": "Q1487"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "286baa47b1bba94db69af3390182c503d12fc3a0205cb346f9dd574d101ff378", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How did the person lying in the bed in the camp get there?\n(A) He was carried there\n(B) He ran there\n(C) He walked there\n(D) He was brought to the camp by horse\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "How did the person lying in the bed in the camp get there?\n(A) He was carried there\n(B) He ran there\n(C) He walked there\n(D) He was brought to the camp by horse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1491, "doc": {"video_name": "movie101_87.mp4", "duration": 599.0, "question": "What is the man's mood at the end of the video?\n(A) Joyful\n(B) Crying\n(C) Angry\n(D) Neutral\n", "candidates": ["Joyful", "Crying", "Angry", "Neutral"], "answer": "B", "task_type": "plotQA", "question_id": "Q1491"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d3fb83bc75eacef39d2cb6416d090840ca8438cc707fbc4ad4226d752c86831", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the man's mood at the end of the video?\n(A) Joyful\n(B) Crying\n(C) Angry\n(D) Neutral\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the man's mood at the end of the video?\n(A) Joyful\n(B) Crying\n(C) Angry\n(D) Neutral\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1495, "doc": {"video_name": "movie101_54.mp4", "duration": 580.0, "question": "What did the person in the black leather jacket encounter while escaping on a motorcycle?\n(A) Hit a pedestrian\n(B) Hit by a car\n(C) Motorcycle malfunction\n(D) Scammed\n", "candidates": ["Hit a pedestrian", "Hit by a car", "Motorcycle malfunction", "Scammed"], "answer": "B", "task_type": "plotQA", "question_id": "Q1495"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f9ccc3cdfd2b6a4925087a49e47dbb4192c06e28fc502fb240d73d62a2b697cb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did the person in the black leather jacket encounter while escaping on a motorcycle?\n(A) Hit a pedestrian\n(B) Hit by a car\n(C) Motorcycle malfunction\n(D) Scammed\n", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "What did the person in the black leather jacket encounter while escaping on a motorcycle?\n(A) Hit a pedestrian\n(B) Hit by a car\n(C) Motorcycle malfunction\n(D) Scammed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1499, "doc": {"video_name": "movie101_39.mp4", "duration": 473.0, "question": "What color is the dress of the woman with the duster in the video?\n(A) White\n(B) Black\n(C) Green\n(D) Yellow\n", "candidates": ["White", "Black", "Green", "Yellow"], "answer": "A", "task_type": "plotQA", "question_id": "Q1499"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "79119e0610d06a46fb44cd1f2db208dfec6196bd5e3eccc75ded19c6439ee91a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the dress of the woman with the duster in the video?\n(A) White\n(B) Black\n(C) Green\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the dress of the woman with the duster in the video?\n(A) White\n(B) Black\n(C) Green\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1503, "doc": {"video_name": "movie101_34.mp4", "duration": 748.0, "question": "How many candles are lit in the video?\n(A) One\n(B) Three\n(C) Four\n(D) Two\n", "candidates": ["One", "Three", "Four", "Two"], "answer": "C", "task_type": "plotQA", "question_id": "Q1503"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c39714439e3828f34dc254171243a72103127e34a68b5d93430911e85cbd436c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How many candles are lit in the video?\n(A) One\n(B) Three\n(C) Four\n(D) Two\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "How many candles are lit in the video?\n(A) One\n(B) Three\n(C) Four\n(D) Two\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1507, "doc": {"video_name": "en_tv_31.mp4", "duration": 577.0, "question": "What does the drone operated by the two men hit?\n(A) Photo\n(B) Cabinet\n(C) Vase\n(D) Table\n", "candidates": ["Photo", "Cabinet", "Vase", "Table"], "answer": "A", "task_type": "plotQA", "question_id": "Q1507"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "22d5e0e317ed18f429292e7654535a2383c614efc368ccadac961a065b04270a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What does the drone operated by the two men hit?\n(A) Photo\n(B) Cabinet\n(C) Vase\n(D) Table\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What does the drone operated by the two men hit?\n(A) Photo\n(B) Cabinet\n(C) Vase\n(D) Table\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1511, "doc": {"video_name": "xiaoliyu_5.mp4", "duration": 415.0, "question": "What is the reaction of the cartoon crab group when they see the cartoon turtle?\n(A) Happy\n(B) Crying\n(C) Surprised\n(D) Run away\n", "candidates": ["Happy", "Crying", "Surprised", "Run away"], "answer": "D", "task_type": "plotQA", "question_id": "Q1511"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "548fff96f0e6c9f51d66c2bb2dd8120b8082f87635e770f11f6cfe4b8e4f3b11", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the reaction of the cartoon crab group when they see the cartoon turtle?\n(A) Happy\n(B) Crying\n(C) Surprised\n(D) Run away\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What is the reaction of the cartoon crab group when they see the cartoon turtle?\n(A) Happy\n(B) Crying\n(C) Surprised\n(D) Run away\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1515, "doc": {"video_name": "movie101_101.mp4", "duration": 318.0, "question": "Why did the person in white uniform intercept the vehicle?\n(A) Because a sandstorm occurred\n(B) Because an earthquake occurred\n(C) Because a tsunami occurred\n(D) Because someone hijacked the speaker on stage\n", "candidates": ["Because a sandstorm occurred", "Because an earthquake occurred", "Because a tsunami occurred", "Because someone hijacked the speaker on stage"], "answer": "D", "task_type": "plotQA", "question_id": "Q1515"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1f322c11c82dc5754fe9e2b0313790ac4aa9e8511e50da53dab73ea333d53a55", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why did the person in white uniform intercept the vehicle?\n(A) Because a sandstorm occurred\n(B) Because an earthquake occurred\n(C) Because a tsunami occurred\n(D) Because someone hijacked the speaker on stage\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Why did the person in white uniform intercept the vehicle?\n(A) Because a sandstorm occurred\n(B) Because an earthquake occurred\n(C) Because a tsunami occurred\n(D) Because someone hijacked the speaker on stage\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1519, "doc": {"video_name": "movie101_101.mp4", "duration": 318.0, "question": "Why did people in the venue run away in terror?\n(A) A sandstorm occurred in the venue\n(B) An explosion occurred in the venue\n(C) An earthquake occurred in the venue\n(D) A tsunami occurred in the venue\n", "candidates": ["A sandstorm occurred in the venue", "An explosion occurred in the venue", "An earthquake occurred in the venue", "A tsunami occurred in the venue"], "answer": "B", "task_type": "plotQA", "question_id": "Q1519"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fbf7296e66e0cba831a59062540ebc96b153ac6b15615c5b57bce8446a4acd65", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why did people in the venue run away in terror?\n(A) A sandstorm occurred in the venue\n(B) An explosion occurred in the venue\n(C) An earthquake occurred in the venue\n(D) A tsunami occurred in the venue\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Why did people in the venue run away in terror?\n(A) A sandstorm occurred in the venue\n(B) An explosion occurred in the venue\n(C) An earthquake occurred in the venue\n(D) A tsunami occurred in the venue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1523, "doc": {"video_name": "haimian_7.mp4", "duration": 426.0, "question": "Why is the Cartoon Octopus angry?\n(A) Because the Cartoon Shark hit it\n(B) Because the Cartoon Sponge hit it\n(C) Because the Cartoon Sponge got all the burgers on it\n(D) Because it was ignored by the Cartoon Sponge\n", "candidates": ["Because the Cartoon Shark hit it", "Because the Cartoon Sponge hit it", "Because the Cartoon Sponge got all the burgers on it", "Because it was ignored by the Cartoon Sponge"], "answer": "C", "task_type": "plotQA", "question_id": "Q1523"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "efe39375e71dc72078b72fc70d3169d0db837280187b5cf32a3aff711dfb7dba", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why is the Cartoon Octopus angry?\n(A) Because the Cartoon Shark hit it\n(B) Because the Cartoon Sponge hit it\n(C) Because the Cartoon Sponge got all the burgers on it\n(D) Because it was ignored by the Cartoon Sponge\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "Why is the Cartoon Octopus angry?\n(A) Because the Cartoon Shark hit it\n(B) Because the Cartoon Sponge hit it\n(C) Because the Cartoon Sponge got all the burgers on it\n(D) Because it was ignored by the Cartoon Sponge\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1527, "doc": {"video_name": "haimian_7.mp4", "duration": 426.0, "question": "What do the other cartoon animals do to the Cartoon Shark and Cartoon Sponge?\n(A) Eat with them\n(B) Dance with them\n(C) Play with them\n(D) Throw them out\n", "candidates": ["Eat with them", "Dance with them", "Play with them", "Throw them out"], "answer": "D", "task_type": "plotQA", "question_id": "Q1527"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6ee60ee7586d89be275fa20a311136c5a47fbd92301ac030b8e4d4e60fe4370e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What do the other cartoon animals do to the Cartoon Shark and Cartoon Sponge?\n(A) Eat with them\n(B) Dance with them\n(C) Play with them\n(D) Throw them out\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What do the other cartoon animals do to the Cartoon Shark and Cartoon Sponge?\n(A) Eat with them\n(B) Dance with them\n(C) Play with them\n(D) Throw them out\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1531, "doc": {"video_name": "movie101_10.mp4", "duration": 716.0, "question": "What color is the background wall that appears in the hospital scene?\n(A) Red\n(B) Yellow\n(C) Green\n(D) Blue\n", "candidates": ["Red", "Yellow", "Green", "Blue"], "answer": "C", "task_type": "plotQA", "question_id": "Q1531"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "50e48f4df0aa49cddcc5f3b4631e7e85f0e41d604c06f46cb6e07b9f4da44399", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the background wall that appears in the hospital scene?\n(A) Red\n(B) Yellow\n(C) Green\n(D) Blue\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the background wall that appears in the hospital scene?\n(A) Red\n(B) Yellow\n(C) Green\n(D) Blue\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1535, "doc": {"video_name": "movie101_4.mp4", "duration": 560.0, "question": "Why didn't the person in the red jacket stab the person in camouflage clothing at the end of the video?\n(A) Got injured\n(B) Ran away\n(C) Dropped the weapon\n(D) Stopped by a person on a motorcycle\n", "candidates": ["Got injured", "Ran away", "Dropped the weapon", "Stopped by a person on a motorcycle"], "answer": "D", "task_type": "plotQA", "question_id": "Q1535"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c117aa342278a252ac1de341ca38efa3aa3805e6d9333524505beff4623c2e70", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why didn't the person in the red jacket stab the person in camouflage clothing at the end of the video?\n(A) Got injured\n(B) Ran away\n(C) Dropped the weapon\n(D) Stopped by a person on a motorcycle\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "Why didn't the person in the red jacket stab the person in camouflage clothing at the end of the video?\n(A) Got injured\n(B) Ran away\n(C) Dropped the weapon\n(D) Stopped by a person on a motorcycle\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1539, "doc": {"video_name": "movie101_34.mp4", "duration": 748.0, "question": "What color is the hoodie that the boy in the video is wearing?\n(A) Green\n(B) Red\n(C) White\n(D) Black\n", "candidates": ["Green", "Red", "White", "Black"], "answer": "A", "task_type": "plotQA", "question_id": "Q1539"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9686e6e436bc78d87ccdb1ac076f4f42edd076c4cc3f9ab2d4f91db6d2562c0b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the hoodie that the boy in the video is wearing?\n(A) Green\n(B) Red\n(C) White\n(D) Black\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What color is the hoodie that the boy in the video is wearing?\n(A) Green\n(B) Red\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1543, "doc": {"video_name": "movie101_54.mp4", "duration": 580.0, "question": "How did the person in the black leather jacket get to the hospital after the car accident?\n(A) An ambulance took him to the hospital\n(B) He walked to the hospital himself\n(C) A red taxi took him to the hospital\n(D) A police car took him to the hospital\n", "candidates": ["An ambulance took him to the hospital", "He walked to the hospital himself", "A red taxi took him to the hospital", "A police car took him to the hospital"], "answer": "C", "task_type": "plotQA", "question_id": "Q1543"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9e3835dc2e281850ed7aa4f62893ca39fc209ef20c8987122992ae806738e7ea", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "How did the person in the black leather jacket get to the hospital after the car accident?\n(A) An ambulance took him to the hospital\n(B) He walked to the hospital himself\n(C) A red taxi took him to the hospital\n(D) A police car took him to the hospital\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "How did the person in the black leather jacket get to the hospital after the car accident?\n(A) An ambulance took him to the hospital\n(B) He walked to the hospital himself\n(C) A red taxi took him to the hospital\n(D) A police car took him to the hospital\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1547, "doc": {"video_name": "en_tv_13.mp4", "duration": 585.0, "question": "What does the man in the checkered shirt get up to bring when people are eating?\n(A) Bowl\n(B) Cup\n(C) Plate\n(D) Spoon\n", "candidates": ["Bowl", "Cup", "Plate", "Spoon"], "answer": "B", "task_type": "plotQA", "question_id": "Q1547"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7f590addffc3d8d34606a8ee8df21768260ec3c0cbd158f3b06a65d4ac123731", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What does the man in the checkered shirt get up to bring when people are eating?\n(A) Bowl\n(B) Cup\n(C) Plate\n(D) Spoon\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What does the man in the checkered shirt get up to bring when people are eating?\n(A) Bowl\n(B) Cup\n(C) Plate\n(D) Spoon\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1551, "doc": {"video_name": "movie101_88.mp4", "duration": 348.0, "question": "What is the man doing when they arrive at the seaside?\n(A) Dancing\n(B) Playing cotton candy\n(C) Singing\n(D) Acting\n", "candidates": ["Dancing", "Playing cotton candy", "Singing", "Acting"], "answer": "B", "task_type": "plotQA", "question_id": "Q1551"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "80740d85a66946d016ab9ad52d0aec9cf2e2d465898c793495254ddbf9b1f07b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the man doing when they arrive at the seaside?\n(A) Dancing\n(B) Playing cotton candy\n(C) Singing\n(D) Acting\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the man doing when they arrive at the seaside?\n(A) Dancing\n(B) Playing cotton candy\n(C) Singing\n(D) Acting\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1555, "doc": {"video_name": "movie101_69.mp4", "duration": 385.0, "question": "Why did the man in the floral shirt run away?\n(A) Being hunted\n(B) Rushing for time\n(C) Dine and dash\n(D) Chasing the bus\n", "candidates": ["Being hunted", "Rushing for time", "Dine and dash", "Chasing the bus"], "answer": "A", "task_type": "plotQA", "question_id": "Q1555"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "58bc5852b84e6f8e7b488b416ba233627cdb0e4875c7dc763d48fcd1cb081258", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why did the man in the floral shirt run away?\n(A) Being hunted\n(B) Rushing for time\n(C) Dine and dash\n(D) Chasing the bus\n", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "Why did the man in the floral shirt run away?\n(A) Being hunted\n(B) Rushing for time\n(C) Dine and dash\n(D) Chasing the bus\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1559, "doc": {"video_name": "movie101_70.mp4", "duration": 361.0, "question": "What animal shape is the movable camera in the film?\n(A) Dog\n(B) Cat\n(C) Rabbit\n(D) Spider\n", "candidates": ["Dog", "Cat", "Rabbit", "Spider"], "answer": "D", "task_type": "plotQA", "question_id": "Q1559"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5b637e7a2eb08ae31e6a25a9ffbe1cd451395f2ebcf2025c0f142af90b9fb56", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What animal shape is the movable camera in the film?\n(A) Dog\n(B) Cat\n(C) Rabbit\n(D) Spider\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What animal shape is the movable camera in the film?\n(A) Dog\n(B) Cat\n(C) Rabbit\n(D) Spider\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1563, "doc": {"video_name": "8.mp4", "duration": 507.29, "question": "What color is the top worn by the pink-haired cartoon character that appears at the end of the video?\n(A) Red\n(B) Green\n(C) Blue\n(D) Orange\n", "candidates": ["Red", "Green", "Blue", "Orange"], "answer": "A", "task_type": "plotQA", "question_id": "Q1563"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a15cfa0665bdaa34f8ad30a265f3c759bc0f72cb71bea3a4961a66ba7c52aa9d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What color is the top worn by the pink-haired cartoon character that appears at the end of the video?\n(A) Red\n(B) Green\n(C) Blue\n(D) Orange\n", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "What color is the top worn by the pink-haired cartoon character that appears at the end of the video?\n(A) Red\n(B) Green\n(C) Blue\n(D) Orange\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1567, "doc": {"video_name": "en_tv_32.mp4", "duration": 408.0, "question": "What is the emotion of the woman in purple when she sees the damaged photo?\n(A) Crying\n(B) Angry\n(C) Neutral\n(D) Joy\n", "candidates": ["Crying", "Angry", "Neutral", "Joy"], "answer": "B", "task_type": "plotQA", "question_id": "Q1567"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "842c22a5c74bbb5fcfa119eeab600897ca1c1a67cb6bb76fa9d5c48f616be35d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the emotion of the woman in purple when she sees the damaged photo?\n(A) Crying\n(B) Angry\n(C) Neutral\n(D) Joy\n", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "What is the emotion of the woman in purple when she sees the damaged photo?\n(A) Crying\n(B) Angry\n(C) Neutral\n(D) Joy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1571, "doc": {"video_name": "tomjerry_8.mp4", "duration": 308.0, "question": "What did the cartoon cat use to catch the cartoon mouse in the end?\n(A) A rope\n(B) A hook\n(C) A basin\n(D) A bucket\n", "candidates": ["A rope", "A hook", "A basin", "A bucket"], "answer": "B", "task_type": "plotQA", "question_id": "Q1571"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "26264e8f65745ef44cc5a7d11af8a7cfc3d366f478e71fe300f5e66050ece871", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did the cartoon cat use to catch the cartoon mouse in the end?\n(A) A rope\n(B) A hook\n(C) A basin\n(D) A bucket\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What did the cartoon cat use to catch the cartoon mouse in the end?\n(A) A rope\n(B) A hook\n(C) A basin\n(D) A bucket\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1575, "doc": {"video_name": "movie101_89.mp4", "duration": 677.0, "question": "How many people are in the frame at the end of the video?\n(A) 9\n(B) 8\n(C) 7\n(D) 6\n", "candidates": ["9", "8", "7", "6"], "answer": "D", "task_type": "plotQA", "question_id": "Q1575"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "752787046a6fdfcd4af58ccaef114852fb2c47cd896db1594ba7784f5ad7f2dc", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many people are in the frame at the end of the video?\n(A) 9\n(B) 8\n(C) 7\n(D) 6\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "How many people are in the frame at the end of the video?\n(A) 9\n(B) 8\n(C) 7\n(D) 6\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1579, "doc": {"video_name": "en_tv_38.mp4", "duration": 410.0, "question": "What does the woman in the video take and leave the ward with?\n(A) Water\n(B) Mobile phone\n(C) Computer\n(D) Soup\n", "candidates": ["Water", "Mobile phone", "Computer", "Soup"], "answer": "D", "task_type": "plotQA", "question_id": "Q1579"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f507fef23c5461886e63f96c4ce6c3b1c81fc526b2e7e4ad7aea4286221d7021", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What does the woman in the video take and leave the ward with?\n(A) Water\n(B) Mobile phone\n(C) Computer\n(D) Soup\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "What does the woman in the video take and leave the ward with?\n(A) Water\n(B) Mobile phone\n(C) Computer\n(D) Soup\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1583, "doc": {"video_name": "movie101_40.mp4", "duration": 450.0, "question": "What color is the hair of the person under the umbrella in the video?\n(A) Black\n(B) Blue\n(C) White\n(D) Green\n", "candidates": ["Black", "Blue", "White", "Green"], "answer": "C", "task_type": "plotQA", "question_id": "Q1583"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "97ae6890a981f156596fa1792b55a70502ffb65b816d4be466f62be05462bf04", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the hair of the person under the umbrella in the video?\n(A) Black\n(B) Blue\n(C) White\n(D) Green\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the hair of the person under the umbrella in the video?\n(A) Black\n(B) Blue\n(C) White\n(D) Green\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1587, "doc": {"video_name": "1.mp4", "duration": 401.23, "question": "What is the man's mood after his unsuccessful attempt to hit the wall?\n(A) Surprised\n(B) Neutral\n(C) Happy\n(D) Disappointed\n", "candidates": ["Surprised", "Neutral", "Happy", "Disappointed"], "answer": "D", "task_type": "plotQA", "question_id": "Q1587"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "837ab75d4c21807649088ef9fa59e2c5a38d2a5327a18acc4cb4d599f1977652", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the man's mood after his unsuccessful attempt to hit the wall?\n(A) Surprised\n(B) Neutral\n(C) Happy\n(D) Disappointed\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What is the man's mood after his unsuccessful attempt to hit the wall?\n(A) Surprised\n(B) Neutral\n(C) Happy\n(D) Disappointed\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1591, "doc": {"video_name": "movie101_7.mp4", "duration": 279.0, "question": "What else was on the bookshelf at the end of the video besides books?\n(A) Cellphone\n(B) Water Cup\n(C) Remote Control\n(D) A pot of green plant\n", "candidates": ["Cellphone", "Water Cup", "Remote Control", "A pot of green plant"], "answer": "D", "task_type": "plotQA", "question_id": "Q1591"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87604f89c376d3af906c9975c921d6941b2e403798096dd3cbb63baa7f95bad8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What else was on the bookshelf at the end of the video besides books?\n(A) Cellphone\n(B) Water Cup\n(C) Remote Control\n(D) A pot of green plant\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What else was on the bookshelf at the end of the video besides books?\n(A) Cellphone\n(B) Water Cup\n(C) Remote Control\n(D) A pot of green plant\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1595, "doc": {"video_name": "tomjerry_3.mp4", "duration": 303.0, "question": "What was the cartoon mouse tied up with by the cartoon cat?\n(A) Black thread\n(B) Blue thread\n(C) White thread\n(D) Red thread\n", "candidates": ["Black thread", "Blue thread", "White thread", "Red thread"], "answer": "A", "task_type": "plotQA", "question_id": "Q1595"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5dc2f098b5a1fa4399ab1fbfb533497e19a89dba5ab9ebc82e25f038e7a90b3b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What was the cartoon mouse tied up with by the cartoon cat?\n(A) Black thread\n(B) Blue thread\n(C) White thread\n(D) Red thread\n", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "What was the cartoon mouse tied up with by the cartoon cat?\n(A) Black thread\n(B) Blue thread\n(C) White thread\n(D) Red thread\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1599, "doc": {"video_name": "xiaoliyu_4.mp4", "duration": 399.0, "question": "What did the cartoon turtle encounter in the clouds?\n(A) Cartoon carp\n(B) Cartoon car\n(C) Cartoon seahorse\n(D) Cartoon little angel\n", "candidates": ["Cartoon carp", "Cartoon car", "Cartoon seahorse", "Cartoon little angel"], "answer": "D", "task_type": "plotQA", "question_id": "Q1599"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e384a97ef85dac6c75d6dc13a6afa64869fe212f95685cdfc8a309eee8676082", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did the cartoon turtle encounter in the clouds?\n(A) Cartoon carp\n(B) Cartoon car\n(C) Cartoon seahorse\n(D) Cartoon little angel\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What did the cartoon turtle encounter in the clouds?\n(A) Cartoon carp\n(B) Cartoon car\n(C) Cartoon seahorse\n(D) Cartoon little angel\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1603, "doc": {"video_name": "movie101_105.mp4", "duration": 744.0, "question": "What happened after the two children got off the car at the beginning of the video?\n(A) Reading\n(B) Their purchases were stolen\n(C) Eating snacks\n(D) Drinking water\n", "candidates": ["Reading", "Their purchases were stolen", "Eating snacks", "Drinking water"], "answer": "B", "task_type": "plotQA", "question_id": "Q1603"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4a7e8d28bf863bc9e31432652ee1ff610a90abfcc89c2240163f73dba676bad2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What happened after the two children got off the car at the beginning of the video?\n(A) Reading\n(B) Their purchases were stolen\n(C) Eating snacks\n(D) Drinking water\n", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What happened after the two children got off the car at the beginning of the video?\n(A) Reading\n(B) Their purchases were stolen\n(C) Eating snacks\n(D) Drinking water\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1607, "doc": {"video_name": "tomjerry_1.mp4", "duration": 311.0, "question": "What action does the cartoon mouse take after hitting the brown cartoon cat?\n(A) Drinking water\n(B) Playing ball\n(C) Sticks to the cartoon cat's face\n(D) Eating snacks\n", "candidates": ["Drinking water", "Playing ball", "Sticks to the cartoon cat's face", "Eating snacks"], "answer": "C", "task_type": "plotQA", "question_id": "Q1607"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1c2d26fb1128640cb6f52e7675fe7224f4173b8a408650e432f716947eb80bd5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What action does the cartoon mouse take after hitting the brown cartoon cat?\n(A) Drinking water\n(B) Playing ball\n(C) Sticks to the cartoon cat's face\n(D) Eating snacks\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What action does the cartoon mouse take after hitting the brown cartoon cat?\n(A) Drinking water\n(B) Playing ball\n(C) Sticks to the cartoon cat's face\n(D) Eating snacks\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1611, "doc": {"video_name": "movie101_60.mp4", "duration": 402.0, "question": "After the man and the child took a bath, what did they eat?\n(A) Fried dough stick\n(B) Chinese pancake\n(C) Tofu pudding\n(D) Steamed bun\n", "candidates": ["Fried dough stick", "Chinese pancake", "Tofu pudding", "Steamed bun"], "answer": "D", "task_type": "plotQA", "question_id": "Q1611"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f10e6246db59a6018b23237d2d28c897c98184428ae8607e6c01fc1ff450a8b5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "After the man and the child took a bath, what did they eat?\n(A) Fried dough stick\n(B) Chinese pancake\n(C) Tofu pudding\n(D) Steamed bun\n", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "After the man and the child took a bath, what did they eat?\n(A) Fried dough stick\n(B) Chinese pancake\n(C) Tofu pudding\n(D) Steamed bun\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1615, "doc": {"video_name": "movie101_109.mp4", "duration": 670.0, "question": "What does the man do after being beaten?\n(A) Faint\n(B) Run away\n(C) Treat wounds\n(D) Digs out white animal fur from the ground\n", "candidates": ["Faint", "Run away", "Treat wounds", "Digs out white animal fur from the ground"], "answer": "D", "task_type": "plotQA", "question_id": "Q1615"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f5a14cd5e5521f7a9c63a6830e58a8d36deee0487130fefedc39a80681259a90", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What does the man do after being beaten?\n(A) Faint\n(B) Run away\n(C) Treat wounds\n(D) Digs out white animal fur from the ground\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What does the man do after being beaten?\n(A) Faint\n(B) Run away\n(C) Treat wounds\n(D) Digs out white animal fur from the ground\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1619, "doc": {"video_name": "en_tv_12.mp4", "duration": 587.0, "question": "Why did the old man stop doing push-ups?\n(A) He was preparing to do something else\n(B) A man interrupted him\n(C) A woman in purple interrupted him\n(D) He felt physically exhausted\n", "candidates": ["He was preparing to do something else", "A man interrupted him", "A woman in purple interrupted him", "He felt physically exhausted"], "answer": "C", "task_type": "plotQA", "question_id": "Q1619"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2acc40f172be2724d73ec974fe5934b309391eab04f2035822bf959d71e828b0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Why did the old man stop doing push-ups?\n(A) He was preparing to do something else\n(B) A man interrupted him\n(C) A woman in purple interrupted him\n(D) He felt physically exhausted\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "Why did the old man stop doing push-ups?\n(A) He was preparing to do something else\n(B) A man interrupted him\n(C) A woman in purple interrupted him\n(D) He felt physically exhausted\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1623, "doc": {"video_name": "movie101_84.mp4", "duration": 387.0, "question": "What color is the man's scarf at the end of the video?\n(A) Green\n(B) Purple\n(C) Black\n(D) Yellow\n", "candidates": ["Green", "Purple", "Black", "Yellow"], "answer": "C", "task_type": "plotQA", "question_id": "Q1623"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2468410d460ca022ec22b948c46079fbbf9e0e68616a674ca9c3bc10cf2795a1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the man's scarf at the end of the video?\n(A) Green\n(B) Purple\n(C) Black\n(D) Yellow\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the man's scarf at the end of the video?\n(A) Green\n(B) Purple\n(C) Black\n(D) Yellow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1627, "doc": {"video_name": "en_tv_27.mp4", "duration": 486.0, "question": "What turns into a human shape in the video?\n(A) Cabinet\n(B) Table\n(C) Mud\n(D) Water\n", "candidates": ["Cabinet", "Table", "Mud", "Water"], "answer": "D", "task_type": "plotQA", "question_id": "Q1627"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "69ff0b48b96573472de841b0f2351022df2fcb6f4ff903c88d47be9c7e833eae", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What turns into a human shape in the video?\n(A) Cabinet\n(B) Table\n(C) Mud\n(D) Water\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "What turns into a human shape in the video?\n(A) Cabinet\n(B) Table\n(C) Mud\n(D) Water\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1631, "doc": {"video_name": "en_tv_16.mp4", "duration": 552.0, "question": "What color is the exit light when the woman walks into the room in the video?\n(A) Yellow\n(B) Green\n(C) Orange\n(D) White\n", "candidates": ["Yellow", "Green", "Orange", "White"], "answer": "C", "task_type": "plotQA", "question_id": "Q1631"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0195ef6e11ec33e59b46b50c4408c1159dc918cad78576f1d06722781c574a22", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the exit light when the woman walks into the room in the video?\n(A) Yellow\n(B) Green\n(C) Orange\n(D) White\n", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "What color is the exit light when the woman walks into the room in the video?\n(A) Yellow\n(B) Green\n(C) Orange\n(D) White\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1635, "doc": {"video_name": "en_tv_2.mp4", "duration": 509.0, "question": "Why does the man in the black leather jacket enter the closed room where the woman in the red coat is trapped?\n(A) To get something\n(B) To eat\n(C) To look for something\n(D) To bring food for the woman\n", "candidates": ["To get something", "To eat", "To look for something", "To bring food for the woman"], "answer": "D", "task_type": "plotQA", "question_id": "Q1635"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a586322147d61a69451f7dcd0c193a2f516957c639591464e88d19ee3c4efc3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why does the man in the black leather jacket enter the closed room where the woman in the red coat is trapped?\n(A) To get something\n(B) To eat\n(C) To look for something\n(D) To bring food for the woman\n", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "Why does the man in the black leather jacket enter the closed room where the woman in the red coat is trapped?\n(A) To get something\n(B) To eat\n(C) To look for something\n(D) To bring food for the woman\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1639, "doc": {"video_name": "movie101_11.mp4", "duration": 206.0, "question": "What color is the bird that flies out in the video?\n(A) Red\n(B) Blue\n(C) Green\n(D) Black\n", "candidates": ["Red", "Blue", "Green", "Black"], "answer": "D", "task_type": "plotQA", "question_id": "Q1639"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "11484962882644aa2c5c6e0c293be56c4581f4785564ff3bf73b8508b514ecd0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the bird that flies out in the video?\n(A) Red\n(B) Blue\n(C) Green\n(D) Black\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What color is the bird that flies out in the video?\n(A) Red\n(B) Blue\n(C) Green\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1643, "doc": {"video_name": "movie101_84.mp4", "duration": 387.0, "question": "What color is the woman's dress in the video?\n(A) Green\n(B) Yellow\n(C) White\n(D) Black\n", "candidates": ["Green", "Yellow", "White", "Black"], "answer": "C", "task_type": "plotQA", "question_id": "Q1643"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bf57e48b67e90f3f95d782ff57f12aed067114b23b8145c8290794a69c998ed0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the woman's dress in the video?\n(A) Green\n(B) Yellow\n(C) White\n(D) Black\n", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the woman's dress in the video?\n(A) Green\n(B) Yellow\n(C) White\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1647, "doc": {"video_name": "haimian_4.mp4", "duration": 323.0, "question": "What is the purpose of the two small nets?\n(A) To receive signals\n(B) To greet other people\n(C) To filter out small animals\n(D) To catch cartoon jellyfish\n", "candidates": ["To receive signals", "To greet other people", "To filter out small animals", "To catch cartoon jellyfish"], "answer": "D", "task_type": "plotQA", "question_id": "Q1647"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3650346e2079d603e9888846f065899084d61ffd0627d383e06a738f6c76ebc4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the purpose of the two small nets?\n(A) To receive signals\n(B) To greet other people\n(C) To filter out small animals\n(D) To catch cartoon jellyfish\n", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What is the purpose of the two small nets?\n(A) To receive signals\n(B) To greet other people\n(C) To filter out small animals\n(D) To catch cartoon jellyfish\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1651, "doc": {"video_name": "movie101_39.mp4", "duration": 473.0, "question": "What color is the dress of the woman looking in the mirror in the video?\n(A) Green\n(B) Purple\n(C) Yellow\n(D) Black\n", "candidates": ["Green", "Purple", "Yellow", "Black"], "answer": "B", "task_type": "plotQA", "question_id": "Q1651"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "64f3d17a2c72f341c42f91dea9472a5edffa15d1fc3c504a81df57acca8c47bb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What color is the dress of the woman looking in the mirror in the video?\n(A) Green\n(B) Purple\n(C) Yellow\n(D) Black\n", "task_type": "plotQA", "pred_answer": "C", "answer": "B"}, "input": "What color is the dress of the woman looking in the mirror in the video?\n(A) Green\n(B) Purple\n(C) Yellow\n(D) Black\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1655, "doc": {"video_name": "en_tv_28.mp4", "duration": 575.0, "question": "What type of film is this?\n(A) History\n(B) Romance\n(C) Action\n(D) Comedy\n", "candidates": ["History", "Romance", "Action", "Comedy"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1655"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2f544fa7eebca576dcd491f41c879c6d0a0719d2576416d9902911c61a4a983a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) History\n(B) Romance\n(C) Action\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "C"}, "input": "What type of film is this?\n(A) History\n(B) Romance\n(C) Action\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1659, "doc": {"video_name": "movie101_52.mp4", "duration": 338.0, "question": "What season is it in the video?\n(A) Summer\n(B) Spring\n(C) Autumn\n(D) Winter\n", "candidates": ["Summer", "Spring", "Autumn", "Winter"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1659"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f3263eae14f9048bbb3b246f6438b039524e8dddda45bde3fe3733ca12d2a35d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What season is it in the video?\n(A) Summer\n(B) Spring\n(C) Autumn\n(D) Winter\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What season is it in the video?\n(A) Summer\n(B) Spring\n(C) Autumn\n(D) Winter\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1663, "doc": {"video_name": "en_tv_1.mp4", "duration": 514.0, "question": "Who is the main character shown in the video?\n(A) A man in a red coat\n(B) A woman in a green coat\n(C) A woman in a blue coat\n(D) A woman in a red coat\n", "candidates": ["A man in a red coat", "A woman in a green coat", "A woman in a blue coat", "A woman in a red coat"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1663"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D."]], "filtered_resps": ["D."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b76eeb4c02ff67d3433ecb035571918cdc9eacdab2969153b758e156b831e6e5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Who is the main character shown in the video?\n(A) A man in a red coat\n(B) A woman in a green coat\n(C) A woman in a blue coat\n(D) A woman in a red coat\n", "task_type": "topic_reasoning", "pred_answer": "D.", "answer": "D"}, "input": "Who is the main character shown in the video?\n(A) A man in a red coat\n(B) A woman in a green coat\n(C) A woman in a blue coat\n(D) A woman in a red coat\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1667, "doc": {"video_name": "movie101_11.mp4", "duration": 206.0, "question": "What type of movie is the scene in the video?\n(A) Historical drama\n(B) Horror\n(C) Action\n(D) Comedy\n", "candidates": ["Historical drama", "Horror", "Action", "Comedy"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1667"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "08f11f572497f4b7b89786cc7612ed8a4debf3f2ae1fa93f5a254a5035949cde", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of movie is the scene in the video?\n(A) Historical drama\n(B) Horror\n(C) Action\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What type of movie is the scene in the video?\n(A) Historical drama\n(B) Horror\n(C) Action\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1671, "doc": {"video_name": "ego_21.mp4", "duration": 480.02, "question": "What is the main activity of the first person perspective character in this video?\n(A) Mopping the floor\n(B) Washing clothes\n(C) Wiping windows\n(D) Sweeping the floor\n", "candidates": ["Mopping the floor", "Washing clothes", "Wiping windows", "Sweeping the floor"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1671"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f98c09124d7f80ff2d9b451f502c04b5a0c200df483798d480da78a5af9fbc91", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the main activity of the first person perspective character in this video?\n(A) Mopping the floor\n(B) Washing clothes\n(C) Wiping windows\n(D) Sweeping the floor\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "D"}, "input": "What is the main activity of the first person perspective character in this video?\n(A) Mopping the floor\n(B) Washing clothes\n(C) Wiping windows\n(D) Sweeping the floor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1675, "doc": {"video_name": "en_tv_22.mp4", "duration": 441.0, "question": "What genre of movie is the clip in the video from?\n(A) Horror\n(B) Action\n(C) War\n(D) Documentary\n", "candidates": ["Horror", "Action", "War", "Documentary"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1675"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "48eec1c179d9fe0712eef0aa4b75f2214313d1fd2098fc557f02ce91aad5d88c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What genre of movie is the clip in the video from?\n(A) Horror\n(B) Action\n(C) War\n(D) Documentary\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What genre of movie is the clip in the video from?\n(A) Horror\n(B) Action\n(C) War\n(D) Documentary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1679, "doc": {"video_name": "tomjerry_8.mp4", "duration": 308.0, "question": "What type of video is this?\n(A) Mystery\n(B) Romance\n(C) Science fiction\n(D) Cartoon animation\n", "candidates": ["Mystery", "Romance", "Science fiction", "Cartoon animation"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1679"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e563a8bbf2039a52f15edc8d3331cece698a25cc8d0a9b5575cfc8fe45990c0d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Mystery\n(B) Romance\n(C) Science fiction\n(D) Cartoon animation\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of video is this?\n(A) Mystery\n(B) Romance\n(C) Science fiction\n(D) Cartoon animation\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1683, "doc": {"video_name": "tomjerry_7.mp4", "duration": 306.0, "question": "Who are the characters in the video?\n(A) Two cartoon cats\n(B) Two cartoon cats and two cartoon mice\n(C) Two cartoon cats and one cartoon mouse\n(D) One cartoon cat and one cartoon mouse\n", "candidates": ["Two cartoon cats", "Two cartoon cats and two cartoon mice", "Two cartoon cats and one cartoon mouse", "One cartoon cat and one cartoon mouse"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1683"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9bb0f9fa1506bab51e267916c45e61b23e031ca11e7a4e4aa26f909f2e7d3929", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who are the characters in the video?\n(A) Two cartoon cats\n(B) Two cartoon cats and two cartoon mice\n(C) Two cartoon cats and one cartoon mouse\n(D) One cartoon cat and one cartoon mouse\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "Who are the characters in the video?\n(A) Two cartoon cats\n(B) Two cartoon cats and two cartoon mice\n(C) Two cartoon cats and one cartoon mouse\n(D) One cartoon cat and one cartoon mouse\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1687, "doc": {"video_name": "movie101_30.mp4", "duration": 726.0, "question": "What is the genre of this movie clip?\n(A) Comedy\n(B) Horror\n(C) Action\n(D) War\n", "candidates": ["Comedy", "Horror", "Action", "War"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1687"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "250271e980f4405de1bcafba6b16e0633c9d3a5f5ef6eeb9671d361c64fa5fe6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the genre of this movie clip?\n(A) Comedy\n(B) Horror\n(C) Action\n(D) War\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the genre of this movie clip?\n(A) Comedy\n(B) Horror\n(C) Action\n(D) War\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1691, "doc": {"video_name": "movie101_106.mp4", "duration": 746.0, "question": "What is the style of the characters' clothing in the video?\n(A) Ancient royal style\n(B) Western style\n(C) Ethnic style\n(D) Exotic style\n", "candidates": ["Ancient royal style", "Western style", "Ethnic style", "Exotic style"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1691"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A."]], "filtered_resps": ["A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "15bc928a4c8ae85e8f41a66a8cc060892ab738c06473091c70a139bfca7f886c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the style of the characters' clothing in the video?\n(A) Ancient royal style\n(B) Western style\n(C) Ethnic style\n(D) Exotic style\n", "task_type": "topic_reasoning", "pred_answer": "A.", "answer": "A"}, "input": "What is the style of the characters' clothing in the video?\n(A) Ancient royal style\n(B) Western style\n(C) Ethnic style\n(D) Exotic style\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1695, "doc": {"video_name": "movie101_73.mp4", "duration": 421.0, "question": "What is the main story of the film?\n(A) Blind date\n(B) Fighting\n(C) Dancing\n(D) Eating\n", "candidates": ["Blind date", "Fighting", "Dancing", "Eating"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1695"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6611be70eb0c22d8d80566b7dac88f0f414278b15f480a5e147248ea5774dec3", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the main story of the film?\n(A) Blind date\n(B) Fighting\n(C) Dancing\n(D) Eating\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the main story of the film?\n(A) Blind date\n(B) Fighting\n(C) Dancing\n(D) Eating\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1699, "doc": {"video_name": "movie101_12.mp4", "duration": 346.0, "question": "What is the weather in the scene of the video?\n(A) Foggy\n(B) Snowy\n(C) Rainy\n(D) Sunny\n", "candidates": ["Foggy", "Snowy", "Rainy", "Sunny"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1699"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd6f87542c6af392fb795f51bf43b87b16b238777830d83d99a1a9f704ce3f37", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the weather in the scene of the video?\n(A) Foggy\n(B) Snowy\n(C) Rainy\n(D) Sunny\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "D"}, "input": "What is the weather in the scene of the video?\n(A) Foggy\n(B) Snowy\n(C) Rainy\n(D) Sunny\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1703, "doc": {"video_name": "ego_2.mp4", "duration": 480.03, "question": "What is the weather like in the video?\n(A) Sunny\n(B) Snowing\n(C) Windy\n(D) Drizzling\n", "candidates": ["Sunny", "Snowing", "Windy", "Drizzling"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1703"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f1fb37bbcee8a06cef47b2411abd2c380aab2883f9b7c8227926113f7e355a09", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the weather like in the video?\n(A) Sunny\n(B) Snowing\n(C) Windy\n(D) Drizzling\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the weather like in the video?\n(A) Sunny\n(B) Snowing\n(C) Windy\n(D) Drizzling\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1707, "doc": {"video_name": "movie101_33.mp4", "duration": 240.0, "question": "What is the genre of this movie clip?\n(A) Horror\n(B) Comedy\n(C) War\n(D) Action\n", "candidates": ["Horror", "Comedy", "War", "Action"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1707"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ad4f69dc8cd71b296c0d8d171ed4d31b23296e5edc2a5624e0568c285dd95c96", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the genre of this movie clip?\n(A) Horror\n(B) Comedy\n(C) War\n(D) Action\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the genre of this movie clip?\n(A) Horror\n(B) Comedy\n(C) War\n(D) Action\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1711, "doc": {"video_name": "movie101_61.mp4", "duration": 627.0, "question": "What type of film is this?\n(A) Romance\n(B) Thriller\n(C) Mystery\n(D) Action\n", "candidates": ["Romance", "Thriller", "Mystery", "Action"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1711"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dad2492a429a367821ec347a54dd30afe9e5fcbf7ac2d50ad6abd84844a2438a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Romance\n(B) Thriller\n(C) Mystery\n(D) Action\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What type of film is this?\n(A) Romance\n(B) Thriller\n(C) Mystery\n(D) Action\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1715, "doc": {"video_name": "AWA-16.mp4", "duration": 450.0, "question": "What is the setting of the video?\n(A) Ocean\n(B) Desert\n(C) City\n(D) Grassland\n", "candidates": ["Ocean", "Desert", "City", "Grassland"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1715"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fa8c41da37cc4524f14d754dbdf24f3877aa336e776f43493a789a60eb7f3751", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the setting of the video?\n(A) Ocean\n(B) Desert\n(C) City\n(D) Grassland\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the setting of the video?\n(A) Ocean\n(B) Desert\n(C) City\n(D) Grassland\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1719, "doc": {"video_name": "ego_2.mp4", "duration": 480.03, "question": "Where is the person in the video?\n(A) Car dealership\n(B) Fruit store\n(C) Clothing store\n(D) Mobile phone store\n", "candidates": ["Car dealership", "Fruit store", "Clothing store", "Mobile phone store"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1719"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a52abd474328da99fe621b133f9e3af2baa29264604c83213569c6fb449b65e0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the person in the video?\n(A) Car dealership\n(B) Fruit store\n(C) Clothing store\n(D) Mobile phone store\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "Where is the person in the video?\n(A) Car dealership\n(B) Fruit store\n(C) Clothing store\n(D) Mobile phone store\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1723, "doc": {"video_name": "AWB-10.mp4", "duration": 450.0, "question": "What is the main content related to in the video?\n(A) Plants\n(B) Food\n(C) Humans\n(D) Animals\n", "candidates": ["Plants", "Food", "Humans", "Animals"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1723"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2edc7c6b35ea03d227dc991ffb7b3c05d174e604ac7046aba28971ab9734a6df", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the main content related to in the video?\n(A) Plants\n(B) Food\n(C) Humans\n(D) Animals\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the main content related to in the video?\n(A) Plants\n(B) Food\n(C) Humans\n(D) Animals\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1727, "doc": {"video_name": "movie101_82.mp4", "duration": 467.0, "question": "What story does the entire video tell?\n(A) Drama performance\n(B) Chase event\n(C) Wedding scene\n(D) Criminal investigation\n", "candidates": ["Drama performance", "Chase event", "Wedding scene", "Criminal investigation"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1727"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "79f2ceab3bbbe87c28ec77bfb1f6ff3eea6f232bd0c1906f3a38a812768936b8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What story does the entire video tell?\n(A) Drama performance\n(B) Chase event\n(C) Wedding scene\n(D) Criminal investigation\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What story does the entire video tell?\n(A) Drama performance\n(B) Chase event\n(C) Wedding scene\n(D) Criminal investigation\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1731, "doc": {"video_name": "en_tv_33.mp4", "duration": 532.0, "question": "Where does the story in the video take place?\n(A) Countryside\n(B) Desert\n(C) City\n(D) Seaside\n", "candidates": ["Countryside", "Desert", "City", "Seaside"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1731"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "227d70f194003ef26668c0818a3e84b86b13aad23dee525d616b784e95e444c8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where does the story in the video take place?\n(A) Countryside\n(B) Desert\n(C) City\n(D) Seaside\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "Where does the story in the video take place?\n(A) Countryside\n(B) Desert\n(C) City\n(D) Seaside\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1735, "doc": {"video_name": "movie101_22.mp4", "duration": 335.0, "question": "What is the weather in the scene of the video?\n(A) Sunny\n(B) Snowy\n(C) Foggy\n(D) Rainy\n", "candidates": ["Sunny", "Snowy", "Foggy", "Rainy"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1735"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d97ac9b0ee88fddff038c83a0caecde5f79247409eda3bfcde39601cd85ab6e9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the weather in the scene of the video?\n(A) Sunny\n(B) Snowy\n(C) Foggy\n(D) Rainy\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the weather in the scene of the video?\n(A) Sunny\n(B) Snowy\n(C) Foggy\n(D) Rainy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1739, "doc": {"video_name": "222.mp4", "duration": 480.0, "question": "What type of video is this?\n(A) It's a video related to animals\n(B) It's a video related to nature\n(C) It's a video related to food\n(D) It's a video related to traditional culture\n", "candidates": ["It's a video related to animals", "It's a video related to nature", "It's a video related to food", "It's a video related to traditional culture"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1739"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "da5db02a4da827673bc3729fd463f7c59440177512dec12f2b95448037e4f7d5", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) It's a video related to animals\n(B) It's a video related to nature\n(C) It's a video related to food\n(D) It's a video related to traditional culture\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) It's a video related to animals\n(B) It's a video related to nature\n(C) It's a video related to food\n(D) It's a video related to traditional culture\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1743, "doc": {"video_name": "xiaoliyu_3.mp4", "duration": 405.0, "question": "What type of video is this?\n(A) Thriller\n(B) Cartoon animation\n(C) Mystery\n(D) Science fiction\n", "candidates": ["Thriller", "Cartoon animation", "Mystery", "Science fiction"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1743"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "724678124d0cb6c501c02636adb1e0f17decd004f43f174f432b67eb927fef61", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Thriller\n(B) Cartoon animation\n(C) Mystery\n(D) Science fiction\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Thriller\n(B) Cartoon animation\n(C) Mystery\n(D) Science fiction\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1747, "doc": {"video_name": "movie101_89.mp4", "duration": 677.0, "question": "What type of film is this?\n(A) Comedy\n(B) Mystery\n(C) Action\n(D) Documentary\n", "candidates": ["Comedy", "Mystery", "Action", "Documentary"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1747"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5eabf661f2a8a0d057a7ae8c25cc029631274099f9580e6a40617ece4353aaaf", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Comedy\n(B) Mystery\n(C) Action\n(D) Documentary\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of film is this?\n(A) Comedy\n(B) Mystery\n(C) Action\n(D) Documentary\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1751, "doc": {"video_name": "movie101_20.mp4", "duration": 476.0, "question": "What type of movie is the scene in the video from?\n(A) Science fiction\n(B) Comedy\n(C) War film\n(D) Horror\n", "candidates": ["Science fiction", "Comedy", "War film", "Horror"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1751"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0605d61d31b4a40c17aa6a9521b893011a117d91b76139d0b39246e7bd19a8b3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of movie is the scene in the video from?\n(A) Science fiction\n(B) Comedy\n(C) War film\n(D) Horror\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of movie is the scene in the video from?\n(A) Science fiction\n(B) Comedy\n(C) War film\n(D) Horror\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1755, "doc": {"video_name": "ego_17.mp4", "duration": 480.03, "question": "What is the main activity of the first person character in this first person video?\n(A) Drilling holes in glass\n(B) Punching holes in wood\n(C) Drilling holes in diamonds\n(D) Punching holes in the wall\n", "candidates": ["Drilling holes in glass", "Punching holes in wood", "Drilling holes in diamonds", "Punching holes in the wall"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1755"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "28dafd622009dc5f3be080dcdce1194d7acf2cd72218dbceed10c850b51b93d3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main activity of the first person character in this first person video?\n(A) Drilling holes in glass\n(B) Punching holes in wood\n(C) Drilling holes in diamonds\n(D) Punching holes in the wall\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the main activity of the first person character in this first person video?\n(A) Drilling holes in glass\n(B) Punching holes in wood\n(C) Drilling holes in diamonds\n(D) Punching holes in the wall\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1759, "doc": {"video_name": "haimian_1.mp4", "duration": 305.0, "question": "What is the protagonist of the video?\n(A) Shark\n(B) Cartoon Sponge\n(C) Starfish\n(D) Octopus\n", "candidates": ["Shark", "Cartoon Sponge", "Starfish", "Octopus"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1759"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "67c951d3a514046ae6c4146799c2b492ae8d5d700191086cf4286b54b69dad83", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the protagonist of the video?\n(A) Shark\n(B) Cartoon Sponge\n(C) Starfish\n(D) Octopus\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the protagonist of the video?\n(A) Shark\n(B) Cartoon Sponge\n(C) Starfish\n(D) Octopus\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1763, "doc": {"video_name": "movie101_85.mp4", "duration": 560.0, "question": "What type of film is this?\n(A) Mystery\n(B) Animation\n(C) Action\n(D) Comedy\n", "candidates": ["Mystery", "Animation", "Action", "Comedy"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1763"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f562a94ef2bfb9555dd5094805d2802c1509daa3252d700cb7523cce594f6627", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Mystery\n(B) Animation\n(C) Action\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of film is this?\n(A) Mystery\n(B) Animation\n(C) Action\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1767, "doc": {"video_name": "movie101_26.mp4", "duration": 763.0, "question": "What is the genre of this movie clip?\n(A) War\n(B) Documentary Drama\n(C) Horror\n(D) Comedy\n", "candidates": ["War", "Documentary Drama", "Horror", "Comedy"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1767"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9838f2679d4af0615ff1d2363ad359a5cf7bd5626751b7379eb9a575d85e5d2c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the genre of this movie clip?\n(A) War\n(B) Documentary Drama\n(C) Horror\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the genre of this movie clip?\n(A) War\n(B) Documentary Drama\n(C) Horror\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1771, "doc": {"video_name": "movie101_111.mp4", "duration": 720.0, "question": "What type of video is this?\n(A) Comedy\n(B) Science fiction\n(C) Action\n(D) Mystery\n", "candidates": ["Comedy", "Science fiction", "Action", "Mystery"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1771"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "44e3fe6a71c43ba2544348ac5cccc4fa517d35dcc4010bd7e0b7fba9a1f76e58", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Comedy\n(B) Science fiction\n(C) Action\n(D) Mystery\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of video is this?\n(A) Comedy\n(B) Science fiction\n(C) Action\n(D) Mystery\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1775, "doc": {"video_name": "AWA-15.mp4", "duration": 450.0, "question": "What is the main background of the video?\n(A) Ocean\n(B) Lake\n(C) Grassland\n(D) Desert\n", "candidates": ["Ocean", "Lake", "Grassland", "Desert"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1775"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "18272c30452605cfb1be4eaf7bffd6103aa8dd1de09395fdb1f7fe0b361ee5da", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the main background of the video?\n(A) Ocean\n(B) Lake\n(C) Grassland\n(D) Desert\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the main background of the video?\n(A) Ocean\n(B) Lake\n(C) Grassland\n(D) Desert\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1779, "doc": {"video_name": "xiaoliyu_6.mp4", "duration": 400.0, "question": "What is the living environment of the giant octopus?\n(A) Bright\n(B) Dark and lightless\n(C) Spacious\n(D) Sunny\n", "candidates": ["Bright", "Dark and lightless", "Spacious", "Sunny"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1779"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aeddc7464b9d4455a4771ae23426d2f9ae5e4e7e883c97e0462ea8e3a02ed83d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the living environment of the giant octopus?\n(A) Bright\n(B) Dark and lightless\n(C) Spacious\n(D) Sunny\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the living environment of the giant octopus?\n(A) Bright\n(B) Dark and lightless\n(C) Spacious\n(D) Sunny\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1783, "doc": {"video_name": "movie101_28.mp4", "duration": 234.0, "question": "In what setting does the scene in the video take place?\n(A) Island\n(B) Snowy Mountain\n(C) Forest\n(D) City\n", "candidates": ["Island", "Snowy Mountain", "Forest", "City"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1783"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43d6f7571380fc303a355e80de5a8873dea1aff28aa09321497aa620f8a6a09d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what setting does the scene in the video take place?\n(A) Island\n(B) Snowy Mountain\n(C) Forest\n(D) City\n", "task_type": "topic_reasoning", "pred_answer": "C.", "answer": "C"}, "input": "In what setting does the scene in the video take place?\n(A) Island\n(B) Snowy Mountain\n(C) Forest\n(D) City\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1787, "doc": {"video_name": "237.mp4", "duration": 480.0, "question": "What is this video related to?\n(A) This video is related to holidays\n(B) This video is related to nature\n(C) This video is related to traditional culture\n(D) This video is related to food\n", "candidates": ["This video is related to holidays", "This video is related to nature", "This video is related to traditional culture", "This video is related to food"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1787"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4407fbb41653c8c62b53b0b560b33642b7150f3cba71d35a893ad094991462bb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is this video related to?\n(A) This video is related to holidays\n(B) This video is related to nature\n(C) This video is related to traditional culture\n(D) This video is related to food\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is this video related to?\n(A) This video is related to holidays\n(B) This video is related to nature\n(C) This video is related to traditional culture\n(D) This video is related to food\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1791, "doc": {"video_name": "tomjerry_9.mp4", "duration": 328.0, "question": "When does the video take place?\n(A) Evening\n(B) Morning\n(C) Night\n(D) Noon\n", "candidates": ["Evening", "Morning", "Night", "Noon"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1791"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2c61f47f1c600ede4295e179ce8538bbb06175d69f357ab057d13e36bd866db5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "When does the video take place?\n(A) Evening\n(B) Morning\n(C) Night\n(D) Noon\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "When does the video take place?\n(A) Evening\n(B) Morning\n(C) Night\n(D) Noon\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1795, "doc": {"video_name": "204.mp4", "duration": 480.0, "question": "What type of video is this?\n(A) This is a video documenting characters\n(B) This is a video documenting daily life\n(C) This is a video documenting animals\n(D) This is a video documenting food\n", "candidates": ["This is a video documenting characters", "This is a video documenting daily life", "This is a video documenting animals", "This is a video documenting food"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1795"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "00f550175f83812596473416c4644e7d71974c39f4b39876bed2bdea308e48d3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) This is a video documenting characters\n(B) This is a video documenting daily life\n(C) This is a video documenting animals\n(D) This is a video documenting food\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of video is this?\n(A) This is a video documenting characters\n(B) This is a video documenting daily life\n(C) This is a video documenting animals\n(D) This is a video documenting food\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1799, "doc": {"video_name": "tomjerry_1.mp4", "duration": 311.0, "question": "Where is the main setting of the video?\n(A) Desert\n(B) Grassland\n(C) Outside the house\n(D) Inside the house\n", "candidates": ["Desert", "Grassland", "Outside the house", "Inside the house"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1799"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d3cc9942c154d8007311b55653e1ef9d8c57aa07d6f68f34c12e492f243eff6e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the main setting of the video?\n(A) Desert\n(B) Grassland\n(C) Outside the house\n(D) Inside the house\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "Where is the main setting of the video?\n(A) Desert\n(B) Grassland\n(C) Outside the house\n(D) Inside the house\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1803, "doc": {"video_name": "en_tv_12.mp4", "duration": 587.0, "question": "What does the video primarily describe?\n(A) People's daily life\n(B) A global natural disaster\n(C) A group of people performing and partying at a concert\n(D) A vivid outdoor adventure\n", "candidates": ["People's daily life", "A global natural disaster", "A group of people performing and partying at a concert", "A vivid outdoor adventure"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1803"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2aa154cf28a00315398a16f1971bf8c8b2c075e153f89daca3b689e4130efb16", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What does the video primarily describe?\n(A) People's daily life\n(B) A global natural disaster\n(C) A group of people performing and partying at a concert\n(D) A vivid outdoor adventure\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "A"}, "input": "What does the video primarily describe?\n(A) People's daily life\n(B) A global natural disaster\n(C) A group of people performing and partying at a concert\n(D) A vivid outdoor adventure\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1807, "doc": {"video_name": "movie101_78.mp4", "duration": 607.0, "question": "In what setting does the event in the video take place?\n(A) Modern Countryside\n(B) Ancient Folklore\n(C) Modern City\n(D) Ancient Palace\n", "candidates": ["Modern Countryside", "Ancient Folklore", "Modern City", "Ancient Palace"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1807"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "75125d3ccdd74aaa95956f4196d3a8368ad461b868714d5830b465e06c618caa", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In what setting does the event in the video take place?\n(A) Modern Countryside\n(B) Ancient Folklore\n(C) Modern City\n(D) Ancient Palace\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "In what setting does the event in the video take place?\n(A) Modern Countryside\n(B) Ancient Folklore\n(C) Modern City\n(D) Ancient Palace\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1811, "doc": {"video_name": "215.mp4", "duration": 480.0, "question": "Where is the main environment in the video?\n(A) River\n(B) Forest\n(C) Desert\n(D) Ocean\n", "candidates": ["River", "Forest", "Desert", "Ocean"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1811"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6d58d3f30559a22ae7c661aa66303b5784bcbb33805f3cf7cfca53d68daf5738", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the main environment in the video?\n(A) River\n(B) Forest\n(C) Desert\n(D) Ocean\n", "task_type": "topic_reasoning", "pred_answer": "C.", "answer": "C"}, "input": "Where is the main environment in the video?\n(A) River\n(B) Forest\n(C) Desert\n(D) Ocean\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1815, "doc": {"video_name": "movie101_21.mp4", "duration": 289.0, "question": "What is the environment in the scene of the video?\n(A) Island\n(B) City\n(C) Grassland\n(D) Forest\n", "candidates": ["Island", "City", "Grassland", "Forest"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1815"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6204c2d0af88582bdc2c9af5e65ad9e3123f8818ce6fde6f9609f98e21cf8118", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the environment in the scene of the video?\n(A) Island\n(B) City\n(C) Grassland\n(D) Forest\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "B"}, "input": "What is the environment in the scene of the video?\n(A) Island\n(B) City\n(C) Grassland\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1819, "doc": {"video_name": "4.mp4", "duration": 603.0, "question": "What environment does the video start in?\n(A) Grassland\n(B) In the water\n(C) Desert\n(D) On a tree\n", "candidates": ["Grassland", "In the water", "Desert", "On a tree"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1819"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5fe52c5d2e71dc25295fc8e61c90902df08f0601928e8ef9741981c100a02d59", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What environment does the video start in?\n(A) Grassland\n(B) In the water\n(C) Desert\n(D) On a tree\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What environment does the video start in?\n(A) Grassland\n(B) In the water\n(C) Desert\n(D) On a tree\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1823, "doc": {"video_name": "ego_28.mp4", "duration": 480.0, "question": "What is the first-person character mainly doing in this video?\n(A) Installing stairs\n(B) Installing doors and windows\n(C) Installing air conditioning\n(D) Installing stove\n", "candidates": ["Installing stairs", "Installing doors and windows", "Installing air conditioning", "Installing stove"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1823"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4e8c84f9387d42681eb6f578b96d4652076151c5e6b32cb602fbd0ffddbb22fc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the first-person character mainly doing in this video?\n(A) Installing stairs\n(B) Installing doors and windows\n(C) Installing air conditioning\n(D) Installing stove\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the first-person character mainly doing in this video?\n(A) Installing stairs\n(B) Installing doors and windows\n(C) Installing air conditioning\n(D) Installing stove\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1827, "doc": {"video_name": "movie101_57.mp4", "duration": 753.0, "question": "What type of movie is this?\n(A) Comedy\n(B) Mystery\n(C) Thriller\n(D) Romance\n", "candidates": ["Comedy", "Mystery", "Thriller", "Romance"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1827"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "96b36028c01584d9d57c141753b677158f7b882a26f9583bee09ae173ff3f8f7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of movie is this?\n(A) Comedy\n(B) Mystery\n(C) Thriller\n(D) Romance\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "A"}, "input": "What type of movie is this?\n(A) Comedy\n(B) Mystery\n(C) Thriller\n(D) Romance\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1831, "doc": {"video_name": "206.mp4", "duration": 480.0, "question": "What is the main scene shown in the video?\n(A) Desert\n(B) Forest\n(C) City\n(D) Ocean\n", "candidates": ["Desert", "Forest", "City", "Ocean"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1831"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f4b911f93e8643f5d1019a8baec2ba832a672bc5aad8c2ae112108282fb70b19", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main scene shown in the video?\n(A) Desert\n(B) Forest\n(C) City\n(D) Ocean\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the main scene shown in the video?\n(A) Desert\n(B) Forest\n(C) City\n(D) Ocean\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1835, "doc": {"video_name": "AWA-4.mp4", "duration": 450.0, "question": "What is the main subject of the video?\n(A) Poultry\n(B) Various types of dinosaurs\n(C) Fish\n(D) Birds\n", "candidates": ["Poultry", "Various types of dinosaurs", "Fish", "Birds"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1835"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "56d716aadd30674bd05682c0266cfa63e8ab4fee1914f294ab7107de5c40f407", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main subject of the video?\n(A) Poultry\n(B) Various types of dinosaurs\n(C) Fish\n(D) Birds\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the main subject of the video?\n(A) Poultry\n(B) Various types of dinosaurs\n(C) Fish\n(D) Birds\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1839, "doc": {"video_name": "en_tv_33.mp4", "duration": 532.0, "question": "What genre does this film belong to?\n(A) Thriller\n(B) Sci-fi\n(C) Romance\n(D) Comedy\n", "candidates": ["Thriller", "Sci-fi", "Romance", "Comedy"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1839"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a4a879e56bf4f90437eb7e3bb91f1eaf3d2f3ec9f4408baae3829b0575a7930d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What genre does this film belong to?\n(A) Thriller\n(B) Sci-fi\n(C) Romance\n(D) Comedy\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "B"}, "input": "What genre does this film belong to?\n(A) Thriller\n(B) Sci-fi\n(C) Romance\n(D) Comedy\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1843, "doc": {"video_name": "221.mp4", "duration": 480.0, "question": "Who is the protagonist in the video?\n(A) An elderly\n(B) A woman\n(C) A man\n(D) A child\n", "candidates": ["An elderly", "A woman", "A man", "A child"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1843"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1bd76f6f1b4301e10ebb8e9f80500056a6447f7d8732772b7308d6bb603dd9e8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who is the protagonist in the video?\n(A) An elderly\n(B) A woman\n(C) A man\n(D) A child\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "Who is the protagonist in the video?\n(A) An elderly\n(B) A woman\n(C) A man\n(D) A child\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1847, "doc": {"video_name": "AWC-8.mp4", "duration": 450.0, "question": "What type of video is this?\n(A) Traditional Festivals\n(B) Natural Science Popularization\n(C) History and Culture\n(D) Food and Flavor\n", "candidates": ["Traditional Festivals", "Natural Science Popularization", "History and Culture", "Food and Flavor"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1847"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d0cd00d890f5c1e57d0ae961685d855c0ecb7fe728f95f5e295ad03c2c56f544", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Traditional Festivals\n(B) Natural Science Popularization\n(C) History and Culture\n(D) Food and Flavor\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Traditional Festivals\n(B) Natural Science Popularization\n(C) History and Culture\n(D) Food and Flavor\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1851, "doc": {"video_name": "movie101_88.mp4", "duration": 348.0, "question": "What is the main setting shown in the video?\n(A) Desert\n(B) Ocean\n(C) City\n(D) Forest\n", "candidates": ["Desert", "Ocean", "City", "Forest"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1851"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9c679a3da2fb6a4a31fa831246bf3d96c4e24b584f79685872f69f4b8f8ae006", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main setting shown in the video?\n(A) Desert\n(B) Ocean\n(C) City\n(D) Forest\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "B"}, "input": "What is the main setting shown in the video?\n(A) Desert\n(B) Ocean\n(C) City\n(D) Forest\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1855, "doc": {"video_name": "ego_14.mp4", "duration": 480.07, "question": "Where is the person in the video?\n(A) Basement\n(B) Bathroom\n(C) Bedroom\n(D) Kitchen\n", "candidates": ["Basement", "Bathroom", "Bedroom", "Kitchen"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1855"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a89f254093d790080de5e02507d6c01d9803c685761ac378988c9cf66ef011b9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the person in the video?\n(A) Basement\n(B) Bathroom\n(C) Bedroom\n(D) Kitchen\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "Where is the person in the video?\n(A) Basement\n(B) Bathroom\n(C) Bedroom\n(D) Kitchen\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1859, "doc": {"video_name": "ego_10.mp4", "duration": 420.57, "question": "What is the first-person character doing in this first-person video?\n(A) Textile making\n(B) Glass making\n(C) Woodworking\n(D) Ceramics making\n", "candidates": ["Textile making", "Glass making", "Woodworking", "Ceramics making"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1859"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2eed3cc33f8807301585610766a201af8004a579e3831668d5461bd15301664f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the first-person character doing in this first-person video?\n(A) Textile making\n(B) Glass making\n(C) Woodworking\n(D) Ceramics making\n", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What is the first-person character doing in this first-person video?\n(A) Textile making\n(B) Glass making\n(C) Woodworking\n(D) Ceramics making\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1863, "doc": {"video_name": "movie101_85.mp4", "duration": 560.0, "question": "What is the main setting shown in the video?\n(A) City\n(B) Desert\n(C) Forest\n(D) Ocean\n", "candidates": ["City", "Desert", "Forest", "Ocean"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1863"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c04045e123f9c076337c90cf17e24d4fb7288071a9f12ce64b4b1e339e6873ca", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the main setting shown in the video?\n(A) City\n(B) Desert\n(C) Forest\n(D) Ocean\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the main setting shown in the video?\n(A) City\n(B) Desert\n(C) Forest\n(D) Ocean\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1867, "doc": {"video_name": "AWD-5.mp4", "duration": 450.09, "question": "What type of video is this?\n(A) Traditional Festivals\n(B) Natural Science\n(C) Food Flavor\n(D) History and Culture\n", "candidates": ["Traditional Festivals", "Natural Science", "Food Flavor", "History and Culture"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1867"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "079722ca59bff732390fb01d2ccfb51200ed8437f301eaa3cb727bf19e68b9d8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Traditional Festivals\n(B) Natural Science\n(C) Food Flavor\n(D) History and Culture\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Traditional Festivals\n(B) Natural Science\n(C) Food Flavor\n(D) History and Culture\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1871, "doc": {"video_name": "AWB-15.mp4", "duration": 450.0, "question": "What is the main environment in the video?\n(A) Gobi\n(B) Forest\n(C) Grassland\n(D) Desert\n", "candidates": ["Gobi", "Forest", "Grassland", "Desert"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1871"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C."]], "filtered_resps": ["C."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5bce6596d908c5c1ec9f1b506f03ebd5427719c668713a5a0b995511470e510", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the main environment in the video?\n(A) Gobi\n(B) Forest\n(C) Grassland\n(D) Desert\n", "task_type": "topic_reasoning", "pred_answer": "C.", "answer": "C"}, "input": "What is the main environment in the video?\n(A) Gobi\n(B) Forest\n(C) Grassland\n(D) Desert\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1875, "doc": {"video_name": "game_29.mp4", "duration": 275.11, "question": "Where is the game protagonist setting up the armor equipment?\n(A) In an open field outside\n(B) In a small underground bunker\n(C) Inside a large indoor building\n(D) On the top of a high mountain\n", "candidates": ["In an open field outside", "In a small underground bunker", "Inside a large indoor building", "On the top of a high mountain"], "answer": "C", "task_type": "topic_reasoning", "question_id": "Q1875"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B) In a small underground bunker"]], "filtered_resps": ["B) In a small underground bunker"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "72b7d81215442b82576b99b0aff7e5f937aeb0948fb2ff11547ca502d1ad816c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where is the game protagonist setting up the armor equipment?\n(A) In an open field outside\n(B) In a small underground bunker\n(C) Inside a large indoor building\n(D) On the top of a high mountain\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "C"}, "input": "Where is the game protagonist setting up the armor equipment?\n(A) In an open field outside\n(B) In a small underground bunker\n(C) Inside a large indoor building\n(D) On the top of a high mountain\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1879, "doc": {"video_name": "game_4.mp4", "duration": 187.62, "question": "What color is the object being built in the video?\n(A) green\n(B) blue\n(C) white\n(D) brown\n", "candidates": ["green", "blue", "white", "brown"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1879"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b23e79ba87110eef94bf35803a7f43f805c67be1add58863a13197e8b6efd357", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color is the object being built in the video?\n(A) green\n(B) blue\n(C) white\n(D) brown\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What color is the object being built in the video?\n(A) green\n(B) blue\n(C) white\n(D) brown\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1883, "doc": {"video_name": "game_18.mp4", "duration": 236.31, "question": "What animal appears in this gameplay video?\n(A) Wolf\n(B) Horse\n(C) Sheep\n(D) Wild boar\n", "candidates": ["Wolf", "Horse", "Sheep", "Wild boar"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1883"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "144c8c0af2502fbe51a95013bf1732eed77a63d27250c2fc04e09a96283d00ec", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What animal appears in this gameplay video?\n(A) Wolf\n(B) Horse\n(C) Sheep\n(D) Wild boar\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What animal appears in this gameplay video?\n(A) Wolf\n(B) Horse\n(C) Sheep\n(D) Wild boar\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1887, "doc": {"video_name": "game_17.mp4", "duration": 182.65, "question": "Where is the setting of this gameplay video?\n(A) Jungle\n(B) Mountains\n(C) Forest\n(D) Desert\n", "candidates": ["Jungle", "Mountains", "Forest", "Desert"], "answer": "D", "task_type": "topic_reasoning", "question_id": "Q1887"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d2b683151445d8f85554e3379d936cd4a517e211e59282579eaa983e22ebd628", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where is the setting of this gameplay video?\n(A) Jungle\n(B) Mountains\n(C) Forest\n(D) Desert\n", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "Where is the setting of this gameplay video?\n(A) Jungle\n(B) Mountains\n(C) Forest\n(D) Desert\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1891, "doc": {"video_name": "game_2.mp4", "duration": 290.83, "question": "What did the man in the video put on the constructed device?\n(A) cups\n(B) explosive boxes\n(C) bags\n(D) guns\n", "candidates": ["cups", "explosive boxes", "bags", "guns"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1891"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bec6dafdd6bbf279bc28f4fa2878bb6435d70f05408637030e12feccc6d02246", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did the man in the video put on the constructed device?\n(A) cups\n(B) explosive boxes\n(C) bags\n(D) guns\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What did the man in the video put on the constructed device?\n(A) cups\n(B) explosive boxes\n(C) bags\n(D) guns\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1895, "doc": {"video_name": "game_33.mp4", "duration": 244.93, "question": "What is the game player doing?\n(A) Building a windmill\n(B) Constructing a tank\n(C) Performing a flight\n(D) Digging a hole\n", "candidates": ["Building a windmill", "Constructing a tank", "Performing a flight", "Digging a hole"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1895"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a66772771c35d9dd1fd4fbe1837fc093ecc5d910568c32c6e76930959acc1e03", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the game player doing?\n(A) Building a windmill\n(B) Constructing a tank\n(C) Performing a flight\n(D) Digging a hole\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the game player doing?\n(A) Building a windmill\n(B) Constructing a tank\n(C) Performing a flight\n(D) Digging a hole\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1899, "doc": {"video_name": "game_35.mp4", "duration": 202.64, "question": "What is the protagonist mainly constructing in the game?\n(A) Christmas Doorbell\n(B) Digging basements\n(C) Building houses\n(D) Tanks\n", "candidates": ["Christmas Doorbell", "Digging basements", "Building houses", "Tanks"], "answer": "A", "task_type": "topic_reasoning", "question_id": "Q1899"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8a1e4c84676acdcaa66dce1bef976fa1b659cb70dae4413cbdca74dfa1641ec5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the protagonist mainly constructing in the game?\n(A) Christmas Doorbell\n(B) Digging basements\n(C) Building houses\n(D) Tanks\n", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the protagonist mainly constructing in the game?\n(A) Christmas Doorbell\n(B) Digging basements\n(C) Building houses\n(D) Tanks\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1903, "doc": {"video_name": "game_36.mp4", "duration": 264.5, "question": "What scene is demonstrated in the video?\n(A) How to build a bunker in the game\n(B) How to build a bee farm in the game\n(C) Educational content about the social behavior of bees\n(D) How to get along with bees\n", "candidates": ["How to build a bunker in the game", "How to build a bee farm in the game", "Educational content about the social behavior of bees", "How to get along with bees"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1903"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1352da782c894e8888dd837cf6b4a47dc2caa8e436695ceacfd29c618b3278b2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What scene is demonstrated in the video?\n(A) How to build a bunker in the game\n(B) How to build a bee farm in the game\n(C) Educational content about the social behavior of bees\n(D) How to get along with bees\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What scene is demonstrated in the video?\n(A) How to build a bunker in the game\n(B) How to build a bee farm in the game\n(C) Educational content about the social behavior of bees\n(D) How to get along with bees\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1907, "doc": {"video_name": "game_31.mp4", "duration": 309.43, "question": "What does this appear to be a video of?\n(A) A game match\n(B) Game tutorial video\n(C) Behind-the-scenes of game development\n(D) Game developer interview\n", "candidates": ["A game match", "Game tutorial video", "Behind-the-scenes of game development", "Game developer interview"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1907"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2c5cb280f547d88f2ac08997d5dff0c982af9ae78243322f7c8b55152a2b988e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What does this appear to be a video of?\n(A) A game match\n(B) Game tutorial video\n(C) Behind-the-scenes of game development\n(D) Game developer interview\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What does this appear to be a video of?\n(A) A game match\n(B) Game tutorial video\n(C) Behind-the-scenes of game development\n(D) Game developer interview\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1911, "doc": {"video_name": "game_28.mp4", "duration": 194.1, "question": "What is the protagonist of the game doing?\n(A) Building a house\n(B) Building a blinking device\n(C) Digging a tunnel\n(D) Building a scarecrow\n", "candidates": ["Building a house", "Building a blinking device", "Digging a tunnel", "Building a scarecrow"], "answer": "B", "task_type": "topic_reasoning", "question_id": "Q1911"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ba88186eb0191f8884c1a19fb02ffd6cd897c334874ca0cefccffa9f1f4dc314", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the protagonist of the game doing?\n(A) Building a house\n(B) Building a blinking device\n(C) Digging a tunnel\n(D) Building a scarecrow\n", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the protagonist of the game doing?\n(A) Building a house\n(B) Building a blinking device\n(C) Digging a tunnel\n(D) Building a scarecrow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1915, "doc": {"video_name": "order_126.mp4", "duration": 665.3399999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)Woman tapes her hands with white tape; (2)Woman starts boxing in the ring with a guy; (3)Woman does sit ups on a towel on the beach; (4)Pictures of woman in her bikini are shown.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "candidates": ["2->1->3->4", "3->2->1->4", "4->3->2->1", "1->2->3->4"], "answer": "D", "task_type": "order", "question_id": "Q1915"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3de4ffb574be4fb720cd8655aa4e63700ff34e91a409d0bb7bac7fc9051088fe", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Woman tapes her hands with white tape; (2)Woman starts boxing in the ring with a guy; (3)Woman does sit ups on a towel on the beach; (4)Pictures of woman in her bikini are shown.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Woman tapes her hands with white tape; (2)Woman starts boxing in the ring with a guy; (3)Woman does sit ups on a towel on the beach; (4)Pictures of woman in her bikini are shown.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 4->3->2->1\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1919, "doc": {"video_name": "order_108.mp4", "duration": 667.7199999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)Water is added to the cup and more limes are squeezed in by hand; (2)The small bowls of salt are arranged and limes are sliced in halves; (3)The cup is stirred with more water and a set of cups filled with the refreshment are seen; (4)The limes are juiced into a cup using a hand held press.\n(A) 2->4->1->3\n(B) 4->2->1->3\n(C) 2->1->4->3\n(D) 1->2->4->3\n", "candidates": ["2->4->1->3", "4->2->1->3", "2->1->4->3", "1->2->4->3"], "answer": "A", "task_type": "order", "question_id": "Q1919"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "980eec936ab66ea106c3e537411a3f71c02e803d184361c598f2ca4dabd164e2", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Water is added to the cup and more limes are squeezed in by hand; (2)The small bowls of salt are arranged and limes are sliced in halves; (3)The cup is stirred with more water and a set of cups filled with the refreshment are seen; (4)The limes are juiced into a cup using a hand held press.\n(A) 2->4->1->3\n(B) 4->2->1->3\n(C) 2->1->4->3\n(D) 1->2->4->3\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Water is added to the cup and more limes are squeezed in by hand; (2)The small bowls of salt are arranged and limes are sliced in halves; (3)The cup is stirred with more water and a set of cups filled with the refreshment are seen; (4)The limes are juiced into a cup using a hand held press.\n(A) 2->4->1->3\n(B) 4->2->1->3\n(C) 2->1->4->3\n(D) 1->2->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1923, "doc": {"video_name": "order_295.mp4", "duration": 519.0899999999999, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) paragliding --> javelin throw --> playing harp --> carving pumpkin\n(B) javelin throw --> playing harp --> paragliding --> carving pumpkin\n(C) javelin throw --> playing harp --> carving pumpkin --> paragliding\n(D) playing harp --> javelin throw --> paragliding --> carving pumpkin\n", "candidates": ["paragliding --> javelin throw --> playing harp --> carving pumpkin", "javelin throw --> playing harp --> paragliding --> carving pumpkin", "javelin throw --> playing harp --> carving pumpkin --> paragliding", "playing harp --> javelin throw --> paragliding --> carving pumpkin"], "answer": "D", "task_type": "order", "question_id": "Q1923"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "00edaa309d7f565f2e54da66d79cd568887cfd566a1e6cae372053dde21d3400", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) paragliding --> javelin throw --> playing harp --> carving pumpkin\n(B) javelin throw --> playing harp --> paragliding --> carving pumpkin\n(C) javelin throw --> playing harp --> carving pumpkin --> paragliding\n(D) playing harp --> javelin throw --> paragliding --> carving pumpkin\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) paragliding --> javelin throw --> playing harp --> carving pumpkin\n(B) javelin throw --> playing harp --> paragliding --> carving pumpkin\n(C) javelin throw --> playing harp --> carving pumpkin --> paragliding\n(D) playing harp --> javelin throw --> paragliding --> carving pumpkin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1927, "doc": {"video_name": "order_203.mp4", "duration": 490.01, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> milking cow --> stomping grapes --> riding mule\n(B) stomping grapes --> riding mule --> playing trombone --> milking cow\n(C) playing trombone --> stomping grapes --> milking cow --> riding mule\n(D) riding mule --> milking cow --> stomping grapes --> playing trombone\n", "candidates": ["playing trombone --> milking cow --> stomping grapes --> riding mule", "stomping grapes --> riding mule --> playing trombone --> milking cow", "playing trombone --> stomping grapes --> milking cow --> riding mule", "riding mule --> milking cow --> stomping grapes --> playing trombone"], "answer": "D", "task_type": "order", "question_id": "Q1927"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4036036ec697ce2adedbfde043f08f553204a7edaac09738048e560922e73167", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> milking cow --> stomping grapes --> riding mule\n(B) stomping grapes --> riding mule --> playing trombone --> milking cow\n(C) playing trombone --> stomping grapes --> milking cow --> riding mule\n(D) riding mule --> milking cow --> stomping grapes --> playing trombone\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> milking cow --> stomping grapes --> riding mule\n(B) stomping grapes --> riding mule --> playing trombone --> milking cow\n(C) playing trombone --> stomping grapes --> milking cow --> riding mule\n(D) riding mule --> milking cow --> stomping grapes --> playing trombone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1931, "doc": {"video_name": "order_75.mp4", "duration": 688.89, "question": "Arrange the following events from the video in the correct chronological order: (1)Woman begins ripping the wrapping paper with her hands; (2)Woman sets the box on top of the wrapping paper and begins wrapping the box; (3)Woman lifts up a box and sets it on the table; (4)Woman grabs a pair of scissors and tape.\n(A) 3->4->1->2\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "candidates": ["3->4->1->2", "4->3->2->1", "1->2->3->4", "2->1->3->4"], "answer": "A", "task_type": "order", "question_id": "Q1931"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d6fb07e2498f7c3d9f316e987acb9f3406c583cb6676fc77f240f34d79fc556d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Woman begins ripping the wrapping paper with her hands; (2)Woman sets the box on top of the wrapping paper and begins wrapping the box; (3)Woman lifts up a box and sets it on the table; (4)Woman grabs a pair of scissors and tape.\n(A) 3->4->1->2\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Woman begins ripping the wrapping paper with her hands; (2)Woman sets the box on top of the wrapping paper and begins wrapping the box; (3)Woman lifts up a box and sets it on the table; (4)Woman grabs a pair of scissors and tape.\n(A) 3->4->1->2\n(B) 4->3->2->1\n(C) 1->2->3->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1935, "doc": {"video_name": "order_73.mp4", "duration": 714.15, "question": "Arrange the following events from the video in the correct chronological order: (1)A shirtless man lifts a ball onto one shoulder; (2)A series of tug of war matches are shown; (3)A third man flips a heavy tire; (4)Individuals are shown exercising with weights, kegs, or tires in a parking lot.\n(A) 1->2->3->4\n(B) 3->1->2->4\n(C) 2->1->3->4\n(D) 1->3->4->2\n", "candidates": ["1->2->3->4", "3->1->2->4", "2->1->3->4", "1->3->4->2"], "answer": "D", "task_type": "order", "question_id": "Q1935"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ca6b6e3322090005f69023dd473d4f3c7828115a1b867e6a3f657b641221fdf0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A shirtless man lifts a ball onto one shoulder; (2)A series of tug of war matches are shown; (3)A third man flips a heavy tire; (4)Individuals are shown exercising with weights, kegs, or tires in a parking lot.\n(A) 1->2->3->4\n(B) 3->1->2->4\n(C) 2->1->3->4\n(D) 1->3->4->2\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A shirtless man lifts a ball onto one shoulder; (2)A series of tug of war matches are shown; (3)A third man flips a heavy tire; (4)Individuals are shown exercising with weights, kegs, or tires in a parking lot.\n(A) 1->2->3->4\n(B) 3->1->2->4\n(C) 2->1->3->4\n(D) 1->3->4->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1939, "doc": {"video_name": "order_71.mp4", "duration": 665.25, "question": "Arrange the following events from the video in the correct chronological order: (1) Woman measures and cuts the wallpaper; (2) Woman grabs wallpaper paste and materials; (3) Woman hangs the wallpaper and flattens it; (4) Woman pastes the wallpaper with a brush and soaks it.\n(A) 4->3->2->1\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 3->2->1->4\n", "candidates": ["4->3->2->1", "2->1->4->3", "1->2->3->4", "3->2->1->4"], "answer": "B", "task_type": "order", "question_id": "Q1939"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ea0a6130701c5212cbd0d7e056b9a0e56e9cf57b733a8e0b925668e11e5735e9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) Woman measures and cuts the wallpaper; (2) Woman grabs wallpaper paste and materials; (3) Woman hangs the wallpaper and flattens it; (4) Woman pastes the wallpaper with a brush and soaks it.\n(A) 4->3->2->1\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 3->2->1->4\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1) Woman measures and cuts the wallpaper; (2) Woman grabs wallpaper paste and materials; (3) Woman hangs the wallpaper and flattens it; (4) Woman pastes the wallpaper with a brush and soaks it.\n(A) 4->3->2->1\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 3->2->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1943, "doc": {"video_name": "order_18.mp4", "duration": 679.45, "question": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 1->2->3->4\n(B) 3->4->1->2\n(C) 2->1->4->3\n(D) 4->3->2->1\n", "candidates": ["1->2->3->4", "3->4->1->2", "2->1->4->3", "4->3->2->1"], "answer": "C", "task_type": "order", "question_id": "Q1943"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a1527bb2d989a7002620058e3826f9f54e8bb838b75e7adfd0afe0d1557c6c66", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 1->2->3->4\n(B) 3->4->1->2\n(C) 2->1->4->3\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 1->2->3->4\n(B) 3->4->1->2\n(C) 2->1->4->3\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1947, "doc": {"video_name": "order_16.mp4", "duration": 681.2, "question": "Arrange the following events from the video in the correct chronological order: (1)People crash into the bottom of a bridge; (2)People are sitting on a raft going down a river; (3)People are walking across the water and down a trail; (4)People are carrying their raft and get into a van.\n(A) 1->2->3->4\n(B) 1->3->2->4\n(C) 2->1->3->4\n(D) 2->3->1->4\n", "candidates": ["1->2->3->4", "1->3->2->4", "2->1->3->4", "2->3->1->4"], "answer": "C", "task_type": "order", "question_id": "Q1947"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b8c5764fcb86ef513509f7fcb06638cba02ee742625f6aee9f19dbe3d97cf65", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)People crash into the bottom of a bridge; (2)People are sitting on a raft going down a river; (3)People are walking across the water and down a trail; (4)People are carrying their raft and get into a van.\n(A) 1->2->3->4\n(B) 1->3->2->4\n(C) 2->1->3->4\n(D) 2->3->1->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)People crash into the bottom of a bridge; (2)People are sitting on a raft going down a river; (3)People are walking across the water and down a trail; (4)People are carrying their raft and get into a van.\n(A) 1->2->3->4\n(B) 1->3->2->4\n(C) 2->1->3->4\n(D) 2->3->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1951, "doc": {"video_name": "order_276.mp4", "duration": 7200.02, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> clean and jerk --> zumba --> cleaning toilet\n(B) cleaning toilet --> zumba --> clean and jerk --> riding mule\n(C) cleaning toilet --> zumba --> riding mule --> clean and jerk\n(D) riding mule --> cleaning toilet --> zumba --> clean and jerk\n", "candidates": ["riding mule --> clean and jerk --> zumba --> cleaning toilet", "cleaning toilet --> zumba --> clean and jerk --> riding mule", "cleaning toilet --> zumba --> riding mule --> clean and jerk", "riding mule --> cleaning toilet --> zumba --> clean and jerk"], "answer": "A", "task_type": "order", "question_id": "Q1951"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e5301a000c2da0f98713c9adc74222b63db653621ec02ecb99426f4f67dfb344", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> clean and jerk --> zumba --> cleaning toilet\n(B) cleaning toilet --> zumba --> clean and jerk --> riding mule\n(C) cleaning toilet --> zumba --> riding mule --> clean and jerk\n(D) riding mule --> cleaning toilet --> zumba --> clean and jerk\n", "task_type": "order", "pred_answer": "D", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> clean and jerk --> zumba --> cleaning toilet\n(B) cleaning toilet --> zumba --> clean and jerk --> riding mule\n(C) cleaning toilet --> zumba --> riding mule --> clean and jerk\n(D) riding mule --> cleaning toilet --> zumba --> clean and jerk\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1955, "doc": {"video_name": "order_121.mp4", "duration": 673.38, "question": "Arrange the following events from the video in the correct chronological order: (1)The interviewer plays with the dogs; (2)A man has dogs on a city street near a car; (3)We see a title screen over the UK flag; (4)We see a banner across the bottom of the screen and the man kneeling playing with his dogs.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 4->3->2->1\n", "candidates": ["2->1->3->4", "1->2->3->4", "3->2->1->4", "4->3->2->1"], "answer": "C", "task_type": "order", "question_id": "Q1955"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d7649ad211c35f41f751468e33341e566816842dae8b4e89f88ea2ab9dea2146", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The interviewer plays with the dogs; (2)A man has dogs on a city street near a car; (3)We see a title screen over the UK flag; (4)We see a banner across the bottom of the screen and the man kneeling playing with his dogs.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "A", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The interviewer plays with the dogs; (2)A man has dogs on a city street near a car; (3)We see a title screen over the UK flag; (4)We see a banner across the bottom of the screen and the man kneeling playing with his dogs.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1959, "doc": {"video_name": "order_283.mp4", "duration": 515.05, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) stomping grapes --> tossing coin --> zumba --> cleaning toilet\n(B) zumba --> tossing coin --> stomping grapes --> cleaning toilet\n(C) tossing coin --> cleaning toilet --> stomping grapes --> zumba\n(D) cleaning toilet --> stomping grapes --> zumba --> tossing coin\n", "candidates": ["stomping grapes --> tossing coin --> zumba --> cleaning toilet", "zumba --> tossing coin --> stomping grapes --> cleaning toilet", "tossing coin --> cleaning toilet --> stomping grapes --> zumba", "cleaning toilet --> stomping grapes --> zumba --> tossing coin"], "answer": "D", "task_type": "order", "question_id": "Q1959"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4647849a4e4316c912ec39bd8073420283d62ee77814afb533cb851ea73e8bad", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) stomping grapes --> tossing coin --> zumba --> cleaning toilet\n(B) zumba --> tossing coin --> stomping grapes --> cleaning toilet\n(C) tossing coin --> cleaning toilet --> stomping grapes --> zumba\n(D) cleaning toilet --> stomping grapes --> zumba --> tossing coin\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) stomping grapes --> tossing coin --> zumba --> cleaning toilet\n(B) zumba --> tossing coin --> stomping grapes --> cleaning toilet\n(C) tossing coin --> cleaning toilet --> stomping grapes --> zumba\n(D) cleaning toilet --> stomping grapes --> zumba --> tossing coin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1963, "doc": {"video_name": "order_328.mp4", "duration": 593.3299999999999, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) playing trombone --> shredding paper --> jetskiing --> carving pumpkin\n(B) playing trombone --> carving pumpkin --> shredding paper --> jetskiing\n(C) shredding paper --> playing trombone --> carving pumpkin --> jetskiing\n(D) playing trombone --> carving pumpkin --> jetskiing --> shredding paper\n", "candidates": ["playing trombone --> shredding paper --> jetskiing --> carving pumpkin", "playing trombone --> carving pumpkin --> shredding paper --> jetskiing", "shredding paper --> playing trombone --> carving pumpkin --> jetskiing", "playing trombone --> carving pumpkin --> jetskiing --> shredding paper"], "answer": "D", "task_type": "order", "question_id": "Q1963"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "298325f6c367a13926e8577235c09ac95d0f09d05fcc5913ae22be895fa9948b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) playing trombone --> shredding paper --> jetskiing --> carving pumpkin\n(B) playing trombone --> carving pumpkin --> shredding paper --> jetskiing\n(C) shredding paper --> playing trombone --> carving pumpkin --> jetskiing\n(D) playing trombone --> carving pumpkin --> jetskiing --> shredding paper\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) playing trombone --> shredding paper --> jetskiing --> carving pumpkin\n(B) playing trombone --> carving pumpkin --> shredding paper --> jetskiing\n(C) shredding paper --> playing trombone --> carving pumpkin --> jetskiing\n(D) playing trombone --> carving pumpkin --> jetskiing --> shredding paper\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1967, "doc": {"video_name": "order_257.mp4", "duration": 1394.62, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> playing harp --> carving pumpkin --> pole vault\n(B) zumba --> pole vault --> carving pumpkin --> playing harp\n(C) pole vault --> zumba --> carving pumpkin --> playing harp\n(D) zumba --> pole vault --> playing harp --> carving pumpkin\n", "candidates": ["zumba --> playing harp --> carving pumpkin --> pole vault", "zumba --> pole vault --> carving pumpkin --> playing harp", "pole vault --> zumba --> carving pumpkin --> playing harp", "zumba --> pole vault --> playing harp --> carving pumpkin"], "answer": "A", "task_type": "order", "question_id": "Q1967"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fcef4832299fdebba0369d7eebdd639bf075f8b19bd884b99822b92fb86e34b0", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> playing harp --> carving pumpkin --> pole vault\n(B) zumba --> pole vault --> carving pumpkin --> playing harp\n(C) pole vault --> zumba --> carving pumpkin --> playing harp\n(D) zumba --> pole vault --> playing harp --> carving pumpkin\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> playing harp --> carving pumpkin --> pole vault\n(B) zumba --> pole vault --> carving pumpkin --> playing harp\n(C) pole vault --> zumba --> carving pumpkin --> playing harp\n(D) zumba --> pole vault --> playing harp --> carving pumpkin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1971, "doc": {"video_name": "order_176.mp4", "duration": 942.72, "question": "Arrange the following events from the video in the correct chronological order: (1)The woman hangs the washed clothes on a line; (2)The woman fills a metal bucket with water; (3)The woman washes and scrubs clothes by hand; (4)The woman places a small wooden stool near a larger bucket.\n(A) 2->4->3->1\n(B) 3->2->4->1\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "candidates": ["2->4->3->1", "3->2->4->1", "4->3->2->1", "1->2->3->4"], "answer": "A", "task_type": "order", "question_id": "Q1971"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "10f306e4244f66658ad63261c01bc01f61c0c99dbfeaef2038ac4a6424263dc6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The woman hangs the washed clothes on a line; (2)The woman fills a metal bucket with water; (3)The woman washes and scrubs clothes by hand; (4)The woman places a small wooden stool near a larger bucket.\n(A) 2->4->3->1\n(B) 3->2->4->1\n(C) 4->3->2->1\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The woman hangs the washed clothes on a line; (2)The woman fills a metal bucket with water; (3)The woman washes and scrubs clothes by hand; (4)The woman places a small wooden stool near a larger bucket.\n(A) 2->4->3->1\n(B) 3->2->4->1\n(C) 4->3->2->1\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1975, "doc": {"video_name": "order_158.mp4", "duration": 686.27, "question": "Arrange the following events from the video in the correct chronological order: (1) Woman measures and cuts the wallpaper; (2) Woman grabs wallpaper paste and materials; (3) Woman hangs the wallpaper and flattens it; (4) Woman pastes the wallpaper with a brush and soaks it.\n(A) 1->2->3->4\n(B) 2->1->4->3\n(C) 4->3->2->1\n(D) 3->2->1->4\n", "candidates": ["1->2->3->4", "2->1->4->3", "4->3->2->1", "3->2->1->4"], "answer": "B", "task_type": "order", "question_id": "Q1975"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "660c5fb461580a46fda75bb0777db654a01a869ed5e1be6624064b4390f2e22d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) Woman measures and cuts the wallpaper; (2) Woman grabs wallpaper paste and materials; (3) Woman hangs the wallpaper and flattens it; (4) Woman pastes the wallpaper with a brush and soaks it.\n(A) 1->2->3->4\n(B) 2->1->4->3\n(C) 4->3->2->1\n(D) 3->2->1->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1) Woman measures and cuts the wallpaper; (2) Woman grabs wallpaper paste and materials; (3) Woman hangs the wallpaper and flattens it; (4) Woman pastes the wallpaper with a brush and soaks it.\n(A) 1->2->3->4\n(B) 2->1->4->3\n(C) 4->3->2->1\n(D) 3->2->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1979, "doc": {"video_name": "order_334.mp4", "duration": 1365.29, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) stomping grapes --> carving pumpkin --> milking cow --> shredding paper\n(B) shredding paper --> stomping grapes --> milking cow --> carving pumpkin\n(C) shredding paper --> milking cow --> stomping grapes --> carving pumpkin\n(D) milking cow --> shredding paper --> stomping grapes --> carving pumpkin\n", "candidates": ["stomping grapes --> carving pumpkin --> milking cow --> shredding paper", "shredding paper --> stomping grapes --> milking cow --> carving pumpkin", "shredding paper --> milking cow --> stomping grapes --> carving pumpkin", "milking cow --> shredding paper --> stomping grapes --> carving pumpkin"], "answer": "A", "task_type": "order", "question_id": "Q1979"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "92ffab57587a8d89d9ae3d958f750080f159143ba1ba2176410c39440791c633", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) stomping grapes --> carving pumpkin --> milking cow --> shredding paper\n(B) shredding paper --> stomping grapes --> milking cow --> carving pumpkin\n(C) shredding paper --> milking cow --> stomping grapes --> carving pumpkin\n(D) milking cow --> shredding paper --> stomping grapes --> carving pumpkin\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) stomping grapes --> carving pumpkin --> milking cow --> shredding paper\n(B) shredding paper --> stomping grapes --> milking cow --> carving pumpkin\n(C) shredding paper --> milking cow --> stomping grapes --> carving pumpkin\n(D) milking cow --> shredding paper --> stomping grapes --> carving pumpkin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1983, "doc": {"video_name": "order_69.mp4", "duration": 665.38, "question": "Arrange the following events from the video in the correct chronological order: (1) Woman measures and cuts the wallpaper; (2) Woman grabs wallpaper paste and materials; (3) Woman hangs the wallpaper and flattens it; (4) Woman pastes the wallpaper with a brush and soaks it.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 2->1->4->3\n(D) 3->2->1->4\n", "candidates": ["1->2->3->4", "4->3->2->1", "2->1->4->3", "3->2->1->4"], "answer": "C", "task_type": "order", "question_id": "Q1983"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0c38fc7fbcb0a41153be3a190b248d81b76c2258a869566bba9787f397a12422", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) Woman measures and cuts the wallpaper; (2) Woman grabs wallpaper paste and materials; (3) Woman hangs the wallpaper and flattens it; (4) Woman pastes the wallpaper with a brush and soaks it.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 2->1->4->3\n(D) 3->2->1->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1) Woman measures and cuts the wallpaper; (2) Woman grabs wallpaper paste and materials; (3) Woman hangs the wallpaper and flattens it; (4) Woman pastes the wallpaper with a brush and soaks it.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 2->1->4->3\n(D) 3->2->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1987, "doc": {"video_name": "order_341.mp4", "duration": 519.3399999999999, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> milking cow --> pole vault --> stomping grapes\n(B) stomping grapes --> pole vault --> riding mule --> milking cow\n(C) riding mule --> stomping grapes --> pole vault --> milking cow\n(D) pole vault --> riding mule --> stomping grapes --> milking cow\n", "candidates": ["riding mule --> milking cow --> pole vault --> stomping grapes", "stomping grapes --> pole vault --> riding mule --> milking cow", "riding mule --> stomping grapes --> pole vault --> milking cow", "pole vault --> riding mule --> stomping grapes --> milking cow"], "answer": "A", "task_type": "order", "question_id": "Q1987"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "12536d2127e266be5051745d40856d974260968d28a8230990a2d57818de96b9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> milking cow --> pole vault --> stomping grapes\n(B) stomping grapes --> pole vault --> riding mule --> milking cow\n(C) riding mule --> stomping grapes --> pole vault --> milking cow\n(D) pole vault --> riding mule --> stomping grapes --> milking cow\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> milking cow --> pole vault --> stomping grapes\n(B) stomping grapes --> pole vault --> riding mule --> milking cow\n(C) riding mule --> stomping grapes --> pole vault --> milking cow\n(D) pole vault --> riding mule --> stomping grapes --> milking cow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1991, "doc": {"video_name": "order_65.mp4", "duration": 1071.78, "question": "Arrange the following events from the video in the correct chronological order: (1)The action of the lures is shown underwater as several different fish go after the lures; (2)Several men show off the different lures they are using for ice fishing; (3)The video ends with the closing credits and Graphics shown on the screen; (4)An introduction comes onto the screen for a video about fishing lures.\n(A) 1->2->3->4\n(B) 4->2->1->3\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "candidates": ["1->2->3->4", "4->2->1->3", "3->2->1->4", "2->1->3->4"], "answer": "B", "task_type": "order", "question_id": "Q1991"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d3d1679a02b3c04a2bb086a64a3073f719c25497fa2fa09cdc542f1675beaf1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The action of the lures is shown underwater as several different fish go after the lures; (2)Several men show off the different lures they are using for ice fishing; (3)The video ends with the closing credits and Graphics shown on the screen; (4)An introduction comes onto the screen for a video about fishing lures.\n(A) 1->2->3->4\n(B) 4->2->1->3\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The action of the lures is shown underwater as several different fish go after the lures; (2)Several men show off the different lures they are using for ice fishing; (3)The video ends with the closing credits and Graphics shown on the screen; (4)An introduction comes onto the screen for a video about fishing lures.\n(A) 1->2->3->4\n(B) 4->2->1->3\n(C) 3->2->1->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1995, "doc": {"video_name": "order_63.mp4", "duration": 674.62, "question": "Arrange the following events from the video in the correct chronological order: (1)The action of the lures is shown underwater as several different fish go after the lures; (2)Several men show off the different lures they are using for ice fishing; (3)The video ends with the closing credits and Graphics shown on the screen; (4)An introduction comes onto the screen for a video about fishing lures.\n(A) 3->2->1->4\n(B) 4->2->1->3\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "candidates": ["3->2->1->4", "4->2->1->3", "1->2->3->4", "2->1->3->4"], "answer": "B", "task_type": "order", "question_id": "Q1995"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "df2a43a1961210220815f97ad4f400e9470c6e4a4d9511feb836229b58d93da0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The action of the lures is shown underwater as several different fish go after the lures; (2)Several men show off the different lures they are using for ice fishing; (3)The video ends with the closing credits and Graphics shown on the screen; (4)An introduction comes onto the screen for a video about fishing lures.\n(A) 3->2->1->4\n(B) 4->2->1->3\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The action of the lures is shown underwater as several different fish go after the lures; (2)Several men show off the different lures they are using for ice fishing; (3)The video ends with the closing credits and Graphics shown on the screen; (4)An introduction comes onto the screen for a video about fishing lures.\n(A) 3->2->1->4\n(B) 4->2->1->3\n(C) 1->2->3->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 1999, "doc": {"video_name": "order_139.mp4", "duration": 694.26, "question": "Arrange the following events from the video in the correct chronological order: (1)The batter is poured into bowls and dye is added; (2)Ingredients are shown on a counter; (3)The cake is frosted with blue frosting and sprinkles are added; (4)The pans are greased and the different colored batter is poured into them.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 3->4->1->2\n(D) 2->1->4->3\n", "candidates": ["1->2->3->4", "4->3->2->1", "3->4->1->2", "2->1->4->3"], "answer": "D", "task_type": "order", "question_id": "Q1999"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8512922bfae11f5c2b2f7dc49f6caba02db9b1e3247d3210f8645446b82cdfd3", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The batter is poured into bowls and dye is added; (2)Ingredients are shown on a counter; (3)The cake is frosted with blue frosting and sprinkles are added; (4)The pans are greased and the different colored batter is poured into them.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 3->4->1->2\n(D) 2->1->4->3\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The batter is poured into bowls and dye is added; (2)Ingredients are shown on a counter; (3)The cake is frosted with blue frosting and sprinkles are added; (4)The pans are greased and the different colored batter is poured into them.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 3->4->1->2\n(D) 2->1->4->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2003, "doc": {"video_name": "order_330.mp4", "duration": 522.04, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) playing trombone --> milking cow --> cleaning toilet --> jetskiing\n(B) playing trombone --> cleaning toilet --> milking cow --> jetskiing\n(C) cleaning toilet --> playing trombone --> milking cow --> jetskiing\n(D) playing trombone --> jetskiing --> milking cow --> cleaning toilet\n", "candidates": ["playing trombone --> milking cow --> cleaning toilet --> jetskiing", "playing trombone --> cleaning toilet --> milking cow --> jetskiing", "cleaning toilet --> playing trombone --> milking cow --> jetskiing", "playing trombone --> jetskiing --> milking cow --> cleaning toilet"], "answer": "A", "task_type": "order", "question_id": "Q2003"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "49b8d1883d67d3163c6718cd0b4bf486394b4bf9ffb7e0ee7d6929b9f213d025", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) playing trombone --> milking cow --> cleaning toilet --> jetskiing\n(B) playing trombone --> cleaning toilet --> milking cow --> jetskiing\n(C) cleaning toilet --> playing trombone --> milking cow --> jetskiing\n(D) playing trombone --> jetskiing --> milking cow --> cleaning toilet\n", "task_type": "order", "pred_answer": "B", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) playing trombone --> milking cow --> cleaning toilet --> jetskiing\n(B) playing trombone --> cleaning toilet --> milking cow --> jetskiing\n(C) cleaning toilet --> playing trombone --> milking cow --> jetskiing\n(D) playing trombone --> jetskiing --> milking cow --> cleaning toilet\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2007, "doc": {"video_name": "order_209.mp4", "duration": 6071.650000000001, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) pole vault --> abseiling --> clean and jerk --> making jewelry\n(B) clean and jerk --> abseiling --> pole vault --> making jewelry\n(C) making jewelry --> abseiling --> clean and jerk --> pole vault\n(D) pole vault --> clean and jerk --> abseiling --> making jewelry\n", "candidates": ["pole vault --> abseiling --> clean and jerk --> making jewelry", "clean and jerk --> abseiling --> pole vault --> making jewelry", "making jewelry --> abseiling --> clean and jerk --> pole vault", "pole vault --> clean and jerk --> abseiling --> making jewelry"], "answer": "D", "task_type": "order", "question_id": "Q2007"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3cbe4f88310ff41c4f4cd9eca3db28949723ac88e5557bb04caa12f23f4fb307", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) pole vault --> abseiling --> clean and jerk --> making jewelry\n(B) clean and jerk --> abseiling --> pole vault --> making jewelry\n(C) making jewelry --> abseiling --> clean and jerk --> pole vault\n(D) pole vault --> clean and jerk --> abseiling --> making jewelry\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) pole vault --> abseiling --> clean and jerk --> making jewelry\n(B) clean and jerk --> abseiling --> pole vault --> making jewelry\n(C) making jewelry --> abseiling --> clean and jerk --> pole vault\n(D) pole vault --> clean and jerk --> abseiling --> making jewelry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2011, "doc": {"video_name": "order_322.mp4", "duration": 6126.740000000001, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> playing harp --> tossing coin --> cleaning toilet\n(B) jetskiing --> tossing coin --> cleaning toilet --> playing harp\n(C) cleaning toilet --> tossing coin --> jetskiing --> playing harp\n(D) cleaning toilet --> jetskiing --> tossing coin --> playing harp\n", "candidates": ["jetskiing --> playing harp --> tossing coin --> cleaning toilet", "jetskiing --> tossing coin --> cleaning toilet --> playing harp", "cleaning toilet --> tossing coin --> jetskiing --> playing harp", "cleaning toilet --> jetskiing --> tossing coin --> playing harp"], "answer": "D", "task_type": "order", "question_id": "Q2011"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8c020280f190552c2fbce893485b10325e649b0ec861df43879f8420b84d1725", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> playing harp --> tossing coin --> cleaning toilet\n(B) jetskiing --> tossing coin --> cleaning toilet --> playing harp\n(C) cleaning toilet --> tossing coin --> jetskiing --> playing harp\n(D) cleaning toilet --> jetskiing --> tossing coin --> playing harp\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> playing harp --> tossing coin --> cleaning toilet\n(B) jetskiing --> tossing coin --> cleaning toilet --> playing harp\n(C) cleaning toilet --> tossing coin --> jetskiing --> playing harp\n(D) cleaning toilet --> jetskiing --> tossing coin --> playing harp\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2015, "doc": {"video_name": "order_304.mp4", "duration": 485.98, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) milking cow --> paragliding --> water sliding --> riding mule\n(B) water sliding --> paragliding --> riding mule --> milking cow\n(C) water sliding --> milking cow --> riding mule --> paragliding\n(D) paragliding --> milking cow --> riding mule --> water sliding\n", "candidates": ["milking cow --> paragliding --> water sliding --> riding mule", "water sliding --> paragliding --> riding mule --> milking cow", "water sliding --> milking cow --> riding mule --> paragliding", "paragliding --> milking cow --> riding mule --> water sliding"], "answer": "C", "task_type": "order", "question_id": "Q2015"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f4d65daad9574696d1d59a22c3045dc0a9e3a1e8349ce7e02b3c73d4babcfb26", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) milking cow --> paragliding --> water sliding --> riding mule\n(B) water sliding --> paragliding --> riding mule --> milking cow\n(C) water sliding --> milking cow --> riding mule --> paragliding\n(D) paragliding --> milking cow --> riding mule --> water sliding\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) milking cow --> paragliding --> water sliding --> riding mule\n(B) water sliding --> paragliding --> riding mule --> milking cow\n(C) water sliding --> milking cow --> riding mule --> paragliding\n(D) paragliding --> milking cow --> riding mule --> water sliding\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2019, "doc": {"video_name": "order_99.mp4", "duration": 1578.18, "question": "Arrange the following events from the video in the correct chronological order: (1)The trainer and class step in a circle and up on the platform; (2)The trainer leads an aerobic class with people in a gym; (3)The trainer and class walk over then in reverse over the platform; (4)The trainer and class step up sideways on the platform.\n(A) 4->3->2->1\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 3->4->1->2\n", "candidates": ["4->3->2->1", "2->1->4->3", "1->2->3->4", "3->4->1->2"], "answer": "B", "task_type": "order", "question_id": "Q2019"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc6bbac330cbc5b35fad471be4731e3a93e7caa7e4b22b56373d78e29d6d5a1b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The trainer and class step in a circle and up on the platform; (2)The trainer leads an aerobic class with people in a gym; (3)The trainer and class walk over then in reverse over the platform; (4)The trainer and class step up sideways on the platform.\n(A) 4->3->2->1\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 3->4->1->2\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The trainer and class step in a circle and up on the platform; (2)The trainer leads an aerobic class with people in a gym; (3)The trainer and class walk over then in reverse over the platform; (4)The trainer and class step up sideways on the platform.\n(A) 4->3->2->1\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 3->4->1->2\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2023, "doc": {"video_name": "order_205.mp4", "duration": 490.03999999999996, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) paragliding --> zumba --> playing trombone --> making jewelry\n(B) zumba --> making jewelry --> playing trombone --> paragliding\n(C) paragliding --> zumba --> making jewelry --> playing trombone\n(D) zumba --> playing trombone --> paragliding --> making jewelry\n", "candidates": ["paragliding --> zumba --> playing trombone --> making jewelry", "zumba --> making jewelry --> playing trombone --> paragliding", "paragliding --> zumba --> making jewelry --> playing trombone", "zumba --> playing trombone --> paragliding --> making jewelry"], "answer": "D", "task_type": "order", "question_id": "Q2023"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "14e1db9531a782d86cf112951647b4a14f83a856c7775199c77aaad19d1db4d2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) paragliding --> zumba --> playing trombone --> making jewelry\n(B) zumba --> making jewelry --> playing trombone --> paragliding\n(C) paragliding --> zumba --> making jewelry --> playing trombone\n(D) zumba --> playing trombone --> paragliding --> making jewelry\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) paragliding --> zumba --> playing trombone --> making jewelry\n(B) zumba --> making jewelry --> playing trombone --> paragliding\n(C) paragliding --> zumba --> making jewelry --> playing trombone\n(D) zumba --> playing trombone --> paragliding --> making jewelry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2027, "doc": {"video_name": "order_230.mp4", "duration": 486.96999999999997, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cleaning toilet --> tossing coin --> playing trombone --> shredding paper\n(B) cleaning toilet --> playing trombone --> tossing coin --> shredding paper\n(C) tossing coin --> cleaning toilet --> playing trombone --> shredding paper\n(D) cleaning toilet --> tossing coin --> shredding paper --> playing trombone\n", "candidates": ["cleaning toilet --> tossing coin --> playing trombone --> shredding paper", "cleaning toilet --> playing trombone --> tossing coin --> shredding paper", "tossing coin --> cleaning toilet --> playing trombone --> shredding paper", "cleaning toilet --> tossing coin --> shredding paper --> playing trombone"], "answer": "C", "task_type": "order", "question_id": "Q2027"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "187f34553cc84aa0833be2e0606397b3d90548a9924a9f78275fda5f7df3b436", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cleaning toilet --> tossing coin --> playing trombone --> shredding paper\n(B) cleaning toilet --> playing trombone --> tossing coin --> shredding paper\n(C) tossing coin --> cleaning toilet --> playing trombone --> shredding paper\n(D) cleaning toilet --> tossing coin --> shredding paper --> playing trombone\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cleaning toilet --> tossing coin --> playing trombone --> shredding paper\n(B) cleaning toilet --> playing trombone --> tossing coin --> shredding paper\n(C) tossing coin --> cleaning toilet --> playing trombone --> shredding paper\n(D) cleaning toilet --> tossing coin --> shredding paper --> playing trombone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2031, "doc": {"video_name": "order_289.mp4", "duration": 6936.370000000001, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> pole vault --> clean and jerk --> stomping grapes\n(B) clean and jerk --> cooking sausages --> pole vault --> stomping grapes\n(C) clean and jerk --> stomping grapes --> cooking sausages --> pole vault\n(D) pole vault --> stomping grapes --> cooking sausages --> clean and jerk\n", "candidates": ["cooking sausages --> pole vault --> clean and jerk --> stomping grapes", "clean and jerk --> cooking sausages --> pole vault --> stomping grapes", "clean and jerk --> stomping grapes --> cooking sausages --> pole vault", "pole vault --> stomping grapes --> cooking sausages --> clean and jerk"], "answer": "D", "task_type": "order", "question_id": "Q2031"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5e9c3c75a42e9c27023a40326b18207f848c7f623403ea47875691be85129c37", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> pole vault --> clean and jerk --> stomping grapes\n(B) clean and jerk --> cooking sausages --> pole vault --> stomping grapes\n(C) clean and jerk --> stomping grapes --> cooking sausages --> pole vault\n(D) pole vault --> stomping grapes --> cooking sausages --> clean and jerk\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> pole vault --> clean and jerk --> stomping grapes\n(B) clean and jerk --> cooking sausages --> pole vault --> stomping grapes\n(C) clean and jerk --> stomping grapes --> cooking sausages --> pole vault\n(D) pole vault --> stomping grapes --> cooking sausages --> clean and jerk\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2035, "doc": {"video_name": "order_296.mp4", "duration": 518.9, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> clean and jerk --> tossing coin --> abseiling\n(B) abseiling --> riding mule --> clean and jerk --> tossing coin\n(C) riding mule --> abseiling --> tossing coin --> clean and jerk\n(D) abseiling --> tossing coin --> clean and jerk --> riding mule\n", "candidates": ["riding mule --> clean and jerk --> tossing coin --> abseiling", "abseiling --> riding mule --> clean and jerk --> tossing coin", "riding mule --> abseiling --> tossing coin --> clean and jerk", "abseiling --> tossing coin --> clean and jerk --> riding mule"], "answer": "B", "task_type": "order", "question_id": "Q2035"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f8200a649440da09d6723edd8d2a2d24f2eaaf94853d3131f36bbe9a45c19e7a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> clean and jerk --> tossing coin --> abseiling\n(B) abseiling --> riding mule --> clean and jerk --> tossing coin\n(C) riding mule --> abseiling --> tossing coin --> clean and jerk\n(D) abseiling --> tossing coin --> clean and jerk --> riding mule\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> clean and jerk --> tossing coin --> abseiling\n(B) abseiling --> riding mule --> clean and jerk --> tossing coin\n(C) riding mule --> abseiling --> tossing coin --> clean and jerk\n(D) abseiling --> tossing coin --> clean and jerk --> riding mule\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2039, "doc": {"video_name": "order_58.mp4", "duration": 673.02, "question": "Arrange the following events from the video in the correct chronological order: (1)The boy begins hopping on the squares, starting from his driveway; (2)The girl joins him near the sidewalk and walks along his side as he hops across the squares; (3)He hops till he reaches the end of the sidewalk which marks the end of the hopscotch squares; (4)After he's done hopping he smiles and begins walking back.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "3->2->1->4", "1->2->3->4", "2->1->3->4"], "answer": "C", "task_type": "order", "question_id": "Q2039"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d65a8055cc5ac99135726f9410cc001fcc3a3d366b9226c180a4f95a148faf1", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The boy begins hopping on the squares, starting from his driveway; (2)The girl joins him near the sidewalk and walks along his side as he hops across the squares; (3)He hops till he reaches the end of the sidewalk which marks the end of the hopscotch squares; (4)After he's done hopping he smiles and begins walking back.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The boy begins hopping on the squares, starting from his driveway; (2)The girl joins him near the sidewalk and walks along his side as he hops across the squares; (3)He hops till he reaches the end of the sidewalk which marks the end of the hopscotch squares; (4)After he's done hopping he smiles and begins walking back.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2043, "doc": {"video_name": "order_55.mp4", "duration": 665.54, "question": "Arrange the following events from the video in the correct chronological order: (1)A man picks up the baby from the pool; (2)A person carries two bags out of a house; (3)A baby falls into the swimming pool; (4)A dog walks out of a house.\n(A) 1->3->2->4\n(B) 2->4->3->1\n(C) 4->2->1->3\n(D) 3->1->2->4\n", "candidates": ["1->3->2->4", "2->4->3->1", "4->2->1->3", "3->1->2->4"], "answer": "B", "task_type": "order", "question_id": "Q2043"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "45c7ba403afe25399042c3e471a076905e4544ae011082f181bf4ec03caa7607", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A man picks up the baby from the pool; (2)A person carries two bags out of a house; (3)A baby falls into the swimming pool; (4)A dog walks out of a house.\n(A) 1->3->2->4\n(B) 2->4->3->1\n(C) 4->2->1->3\n(D) 3->1->2->4\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A man picks up the baby from the pool; (2)A person carries two bags out of a house; (3)A baby falls into the swimming pool; (4)A dog walks out of a house.\n(A) 1->3->2->4\n(B) 2->4->3->1\n(C) 4->2->1->3\n(D) 3->1->2->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2047, "doc": {"video_name": "order_52.mp4", "duration": 695.78, "question": "Arrange the following events from the video in the correct chronological order: (1)A guy sits and talks inside; (2)A man surfs on a body of water; (3)A lady spins on skates; (4)The credits of the video are shown.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "1->2->3->4", "3->2->1->4", "2->1->3->4"], "answer": "B", "task_type": "order", "question_id": "Q2047"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ce8a71d6daa4eda456e76d3ca10e53b05530bc90c75c98f4720dc700a6e7ffb2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A guy sits and talks inside; (2)A man surfs on a body of water; (3)A lady spins on skates; (4)The credits of the video are shown.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A guy sits and talks inside; (2)A man surfs on a body of water; (3)A lady spins on skates; (4)The credits of the video are shown.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2051, "doc": {"video_name": "order_259.mp4", "duration": 388.07, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) paragliding --> milking cow --> clean and jerk --> stomping grapes\n(B) stomping grapes --> clean and jerk --> milking cow --> paragliding\n(C) clean and jerk --> paragliding --> stomping grapes --> milking cow\n(D) paragliding --> clean and jerk --> stomping grapes --> milking cow\n", "candidates": ["paragliding --> milking cow --> clean and jerk --> stomping grapes", "stomping grapes --> clean and jerk --> milking cow --> paragliding", "clean and jerk --> paragliding --> stomping grapes --> milking cow", "paragliding --> clean and jerk --> stomping grapes --> milking cow"], "answer": "C", "task_type": "order", "question_id": "Q2051"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "323ac2644a518a23bfa6d9954e53f4735bdbe2aeeb5b4d8949b002c29136b6e7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) paragliding --> milking cow --> clean and jerk --> stomping grapes\n(B) stomping grapes --> clean and jerk --> milking cow --> paragliding\n(C) clean and jerk --> paragliding --> stomping grapes --> milking cow\n(D) paragliding --> clean and jerk --> stomping grapes --> milking cow\n", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) paragliding --> milking cow --> clean and jerk --> stomping grapes\n(B) stomping grapes --> clean and jerk --> milking cow --> paragliding\n(C) clean and jerk --> paragliding --> stomping grapes --> milking cow\n(D) paragliding --> clean and jerk --> stomping grapes --> milking cow\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2055, "doc": {"video_name": "order_329.mp4", "duration": 488.55999999999995, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) tossing coin --> jetskiing --> zumba --> cooking sausages\n(B) tossing coin --> zumba --> jetskiing --> cooking sausages\n(C) tossing coin --> jetskiing --> cooking sausages --> zumba\n(D) jetskiing --> cooking sausages --> zumba --> tossing coin\n", "candidates": ["tossing coin --> jetskiing --> zumba --> cooking sausages", "tossing coin --> zumba --> jetskiing --> cooking sausages", "tossing coin --> jetskiing --> cooking sausages --> zumba", "jetskiing --> cooking sausages --> zumba --> tossing coin"], "answer": "A", "task_type": "order", "question_id": "Q2055"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "82f44bdc0f43289833d6d24db07396272f8c0ea99a39772e54441600309b3978", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) tossing coin --> jetskiing --> zumba --> cooking sausages\n(B) tossing coin --> zumba --> jetskiing --> cooking sausages\n(C) tossing coin --> jetskiing --> cooking sausages --> zumba\n(D) jetskiing --> cooking sausages --> zumba --> tossing coin\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) tossing coin --> jetskiing --> zumba --> cooking sausages\n(B) tossing coin --> zumba --> jetskiing --> cooking sausages\n(C) tossing coin --> jetskiing --> cooking sausages --> zumba\n(D) jetskiing --> cooking sausages --> zumba --> tossing coin\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2059, "doc": {"video_name": "order_0.mp4", "duration": 665.74, "question": "Arrange the following events from the video in the correct chronological order: (1)A young and a kid are doing balance in a balance rope; (2)People are walking in a bridge to see a competition of men doing tricks on top of a balance rope; (3)A man is jumping and doing tricks in a balance rope above a cold river; (4)The boy is in a competition in snowy path doing tricks on a balance rope with people behind a fence watching him.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "3->2->1->4", "1->2->3->4", "2->1->3->4"], "answer": "C", "task_type": "order", "question_id": "Q2059"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "09b962575a5778a31c3a0222fa3fce97a14d88df2c6318354e050e4fa42ddfcc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A young and a kid are doing balance in a balance rope; (2)People are walking in a bridge to see a competition of men doing tricks on top of a balance rope; (3)A man is jumping and doing tricks in a balance rope above a cold river; (4)The boy is in a competition in snowy path doing tricks on a balance rope with people behind a fence watching him.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A young and a kid are doing balance in a balance rope; (2)People are walking in a bridge to see a competition of men doing tricks on top of a balance rope; (3)A man is jumping and doing tricks in a balance rope above a cold river; (4)The boy is in a competition in snowy path doing tricks on a balance rope with people behind a fence watching him.\n(A) 4->3->2->1\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2063, "doc": {"video_name": "order_318.mp4", "duration": 520.03, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> cooking sausages --> javelin throw --> carving pumpkin\n(B) riding mule --> javelin throw --> cooking sausages --> carving pumpkin\n(C) javelin throw --> riding mule --> carving pumpkin --> cooking sausages\n(D) cooking sausages --> javelin throw --> carving pumpkin --> riding mule\n", "candidates": ["riding mule --> cooking sausages --> javelin throw --> carving pumpkin", "riding mule --> javelin throw --> cooking sausages --> carving pumpkin", "javelin throw --> riding mule --> carving pumpkin --> cooking sausages", "cooking sausages --> javelin throw --> carving pumpkin --> riding mule"], "answer": "D", "task_type": "order", "question_id": "Q2063"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1b5778ac8755a92039888aeb5322079d90ffad30c0c8b70bc0addc6a5abe304f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> cooking sausages --> javelin throw --> carving pumpkin\n(B) riding mule --> javelin throw --> cooking sausages --> carving pumpkin\n(C) javelin throw --> riding mule --> carving pumpkin --> cooking sausages\n(D) cooking sausages --> javelin throw --> carving pumpkin --> riding mule\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> cooking sausages --> javelin throw --> carving pumpkin\n(B) riding mule --> javelin throw --> cooking sausages --> carving pumpkin\n(C) javelin throw --> riding mule --> carving pumpkin --> cooking sausages\n(D) cooking sausages --> javelin throw --> carving pumpkin --> riding mule\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2067, "doc": {"video_name": "order_100.mp4", "duration": 672.54, "question": "Arrange the following events from the video in the correct chronological order: (1)The trainer and class step in a circle and up on the platform; (2)The trainer leads an aerobic class with people in a gym; (3)The trainer and class walk over then in reverse over the platform; (4)The trainer and class step up sideways on the platform.\n(A) 3->4->1->2\n(B) 1->2->3->4\n(C) 2->1->4->3\n(D) 4->3->2->1\n", "candidates": ["3->4->1->2", "1->2->3->4", "2->1->4->3", "4->3->2->1"], "answer": "C", "task_type": "order", "question_id": "Q2067"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b7331086dd7b344e222eeb7b6521ef710c1db40c8015c8f8c3a8afec0a0e0d67", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The trainer and class step in a circle and up on the platform; (2)The trainer leads an aerobic class with people in a gym; (3)The trainer and class walk over then in reverse over the platform; (4)The trainer and class step up sideways on the platform.\n(A) 3->4->1->2\n(B) 1->2->3->4\n(C) 2->1->4->3\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The trainer and class step in a circle and up on the platform; (2)The trainer leads an aerobic class with people in a gym; (3)The trainer and class walk over then in reverse over the platform; (4)The trainer and class step up sideways on the platform.\n(A) 3->4->1->2\n(B) 1->2->3->4\n(C) 2->1->4->3\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2071, "doc": {"video_name": "order_81.mp4", "duration": 796.8399999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)The instructor finishes the class; (2)A woman's indoor aerobic class is in process; (3)The logo 'Zumba Toning' appears on screen; (4)The camera briefly scans to the mirrored wall and then back to the class.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 3->2->4->1\n(D) 2->1->3->4\n", "candidates": ["1->2->3->4", "4->3->2->1", "3->2->4->1", "2->1->3->4"], "answer": "C", "task_type": "order", "question_id": "Q2071"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3cfbdd4a8b231accd59b352abfc02ef850e010468d72300775118bb5a0559d4e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The instructor finishes the class; (2)A woman's indoor aerobic class is in process; (3)The logo 'Zumba Toning' appears on screen; (4)The camera briefly scans to the mirrored wall and then back to the class.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 3->2->4->1\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The instructor finishes the class; (2)A woman's indoor aerobic class is in process; (3)The logo 'Zumba Toning' appears on screen; (4)The camera briefly scans to the mirrored wall and then back to the class.\n(A) 1->2->3->4\n(B) 4->3->2->1\n(C) 3->2->4->1\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2075, "doc": {"video_name": "order_229.mp4", "duration": 605.46, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) zumba --> milking cow --> javelin throw --> water sliding\n(B) zumba --> water sliding --> javelin throw --> milking cow\n(C) javelin throw --> water sliding --> zumba --> milking cow\n(D) milking cow --> water sliding --> javelin throw --> zumba\n", "candidates": ["zumba --> milking cow --> javelin throw --> water sliding", "zumba --> water sliding --> javelin throw --> milking cow", "javelin throw --> water sliding --> zumba --> milking cow", "milking cow --> water sliding --> javelin throw --> zumba"], "answer": "C", "task_type": "order", "question_id": "Q2075"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "79ddf4427b97d779726d42a04bcb06d297b7441f69cd3925ce15a3b513ea4b5a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) zumba --> milking cow --> javelin throw --> water sliding\n(B) zumba --> water sliding --> javelin throw --> milking cow\n(C) javelin throw --> water sliding --> zumba --> milking cow\n(D) milking cow --> water sliding --> javelin throw --> zumba\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) zumba --> milking cow --> javelin throw --> water sliding\n(B) zumba --> water sliding --> javelin throw --> milking cow\n(C) javelin throw --> water sliding --> zumba --> milking cow\n(D) milking cow --> water sliding --> javelin throw --> zumba\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2079, "doc": {"video_name": "order_49.mp4", "duration": 682.37, "question": "Arrange the following events from the video in the correct chronological order: (1)The woman starts working on her nails using bottles from a box next to her; (2)The words \"Love Food & Money with Angie Greenup\" appears on screen; (3)Her twitter handle and subscribe screen are shown while she holds her dogs; (4)The woman speaks to the camera from her living room while her dogs play fight behind her.\n(A) 2->1->4->3\n(B) 4->2->1->3\n(C) 1->2->4->3\n(D) 2->4->1->3\n", "candidates": ["2->1->4->3", "4->2->1->3", "1->2->4->3", "2->4->1->3"], "answer": "D", "task_type": "order", "question_id": "Q2079"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e754dd2a1600cd8bb5e0fa7c0b02ecc6516ddad4b3c33fc5e3a479626522bf87", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The woman starts working on her nails using bottles from a box next to her; (2)The words \"Love Food & Money with Angie Greenup\" appears on screen; (3)Her twitter handle and subscribe screen are shown while she holds her dogs; (4)The woman speaks to the camera from her living room while her dogs play fight behind her.\n(A) 2->1->4->3\n(B) 4->2->1->3\n(C) 1->2->4->3\n(D) 2->4->1->3\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The woman starts working on her nails using bottles from a box next to her; (2)The words \"Love Food & Money with Angie Greenup\" appears on screen; (3)Her twitter handle and subscribe screen are shown while she holds her dogs; (4)The woman speaks to the camera from her living room while her dogs play fight behind her.\n(A) 2->1->4->3\n(B) 4->2->1->3\n(C) 1->2->4->3\n(D) 2->4->1->3\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2083, "doc": {"video_name": "order_324.mp4", "duration": 490.02, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cooking sausages --> cleaning toilet --> jetskiing --> stomping grapes\n(B) cooking sausages --> jetskiing --> stomping grapes --> cleaning toilet\n(C) cleaning toilet --> jetskiing --> stomping grapes --> cooking sausages\n(D) jetskiing --> stomping grapes --> cleaning toilet --> cooking sausages\n", "candidates": ["cooking sausages --> cleaning toilet --> jetskiing --> stomping grapes", "cooking sausages --> jetskiing --> stomping grapes --> cleaning toilet", "cleaning toilet --> jetskiing --> stomping grapes --> cooking sausages", "jetskiing --> stomping grapes --> cleaning toilet --> cooking sausages"], "answer": "B", "task_type": "order", "question_id": "Q2083"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4379ece4c8bf8440ce400aa6fb77171de4d2024a1c695160da8741feb50b0b7f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cooking sausages --> cleaning toilet --> jetskiing --> stomping grapes\n(B) cooking sausages --> jetskiing --> stomping grapes --> cleaning toilet\n(C) cleaning toilet --> jetskiing --> stomping grapes --> cooking sausages\n(D) jetskiing --> stomping grapes --> cleaning toilet --> cooking sausages\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) cooking sausages --> cleaning toilet --> jetskiing --> stomping grapes\n(B) cooking sausages --> jetskiing --> stomping grapes --> cleaning toilet\n(C) cleaning toilet --> jetskiing --> stomping grapes --> cooking sausages\n(D) jetskiing --> stomping grapes --> cleaning toilet --> cooking sausages\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2087, "doc": {"video_name": "order_243.mp4", "duration": 414.05999999999995, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) pole vault --> zumba --> cleaning toilet --> shredding paper\n(B) pole vault --> shredding paper --> cleaning toilet --> zumba\n(C) zumba --> cleaning toilet --> shredding paper --> pole vault\n(D) cleaning toilet --> shredding paper --> pole vault --> zumba\n", "candidates": ["pole vault --> zumba --> cleaning toilet --> shredding paper", "pole vault --> shredding paper --> cleaning toilet --> zumba", "zumba --> cleaning toilet --> shredding paper --> pole vault", "cleaning toilet --> shredding paper --> pole vault --> zumba"], "answer": "B", "task_type": "order", "question_id": "Q2087"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c9abbe562b6ae0d76cdc18c9169de49246ffa0d8e9de7cb071f1dfbfec35e07d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) pole vault --> zumba --> cleaning toilet --> shredding paper\n(B) pole vault --> shredding paper --> cleaning toilet --> zumba\n(C) zumba --> cleaning toilet --> shredding paper --> pole vault\n(D) cleaning toilet --> shredding paper --> pole vault --> zumba\n", "task_type": "order", "pred_answer": "A", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) pole vault --> zumba --> cleaning toilet --> shredding paper\n(B) pole vault --> shredding paper --> cleaning toilet --> zumba\n(C) zumba --> cleaning toilet --> shredding paper --> pole vault\n(D) cleaning toilet --> shredding paper --> pole vault --> zumba\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2091, "doc": {"video_name": "order_331.mp4", "duration": 375.04999999999995, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> baking cookies --> playing harp --> cooking sausages\n(B) playing harp --> zumba --> baking cookies --> cooking sausages\n(C) playing harp --> cooking sausages --> zumba --> baking cookies\n(D) zumba --> baking cookies --> cooking sausages --> playing harp\n", "candidates": ["zumba --> baking cookies --> playing harp --> cooking sausages", "playing harp --> zumba --> baking cookies --> cooking sausages", "playing harp --> cooking sausages --> zumba --> baking cookies", "zumba --> baking cookies --> cooking sausages --> playing harp"], "answer": "D", "task_type": "order", "question_id": "Q2091"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a84cf2cf0a8f2226797cfee6ba8be2e4b1998f139984ee86642a57425deee589", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> baking cookies --> playing harp --> cooking sausages\n(B) playing harp --> zumba --> baking cookies --> cooking sausages\n(C) playing harp --> cooking sausages --> zumba --> baking cookies\n(D) zumba --> baking cookies --> cooking sausages --> playing harp\n", "task_type": "order", "pred_answer": "A", "answer": "D"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> baking cookies --> playing harp --> cooking sausages\n(B) playing harp --> zumba --> baking cookies --> cooking sausages\n(C) playing harp --> cooking sausages --> zumba --> baking cookies\n(D) zumba --> baking cookies --> cooking sausages --> playing harp\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2095, "doc": {"video_name": "order_313.mp4", "duration": 490.03, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) playing trombone --> cooking sausages --> making jewelry --> shredding paper\n(B) cooking sausages --> making jewelry --> shredding paper --> playing trombone\n(C) making jewelry --> shredding paper --> playing trombone --> cooking sausages\n(D) playing trombone --> shredding paper --> cooking sausages --> making jewelry\n", "candidates": ["playing trombone --> cooking sausages --> making jewelry --> shredding paper", "cooking sausages --> making jewelry --> shredding paper --> playing trombone", "making jewelry --> shredding paper --> playing trombone --> cooking sausages", "playing trombone --> shredding paper --> cooking sausages --> making jewelry"], "answer": "D", "task_type": "order", "question_id": "Q2095"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1e4457558d4ef8c60e3ab284878fc8ac880c33a9c6fd337df3b880c7d531f7e6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) playing trombone --> cooking sausages --> making jewelry --> shredding paper\n(B) cooking sausages --> making jewelry --> shredding paper --> playing trombone\n(C) making jewelry --> shredding paper --> playing trombone --> cooking sausages\n(D) playing trombone --> shredding paper --> cooking sausages --> making jewelry\n", "task_type": "order", "pred_answer": "A", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) playing trombone --> cooking sausages --> making jewelry --> shredding paper\n(B) cooking sausages --> making jewelry --> shredding paper --> playing trombone\n(C) making jewelry --> shredding paper --> playing trombone --> cooking sausages\n(D) playing trombone --> shredding paper --> cooking sausages --> making jewelry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2099, "doc": {"video_name": "order_7.mp4", "duration": 681.8, "question": "Arrange the following events from the video in the correct chronological order: (1)A guy approaches a weight on a stage; (2)A man massages a guy's shoulders; (3)A guy lifts a weight on a stage and releases it; (4)A guy kisses the weight plates.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 1->3->2->4\n(D) 2->3->1->4\n", "candidates": ["2->1->3->4", "1->2->3->4", "1->3->2->4", "2->3->1->4"], "answer": "B", "task_type": "order", "question_id": "Q2099"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "88b9a905886e0449f0b784db36559de3dc2c6e12729585a88a503b1f456f938e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A guy approaches a weight on a stage; (2)A man massages a guy's shoulders; (3)A guy lifts a weight on a stage and releases it; (4)A guy kisses the weight plates.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 1->3->2->4\n(D) 2->3->1->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A guy approaches a weight on a stage; (2)A man massages a guy's shoulders; (3)A guy lifts a weight on a stage and releases it; (4)A guy kisses the weight plates.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 1->3->2->4\n(D) 2->3->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2103, "doc": {"video_name": "order_224.mp4", "duration": 482.23999999999995, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) playing harp --> javelin throw --> cleaning toilet --> baking cookies\n(B) baking cookies --> javelin throw --> cleaning toilet --> playing harp\n(C) javelin throw --> playing harp --> baking cookies --> cleaning toilet\n(D) cleaning toilet --> baking cookies --> javelin throw --> playing harp\n", "candidates": ["playing harp --> javelin throw --> cleaning toilet --> baking cookies", "baking cookies --> javelin throw --> cleaning toilet --> playing harp", "javelin throw --> playing harp --> baking cookies --> cleaning toilet", "cleaning toilet --> baking cookies --> javelin throw --> playing harp"], "answer": "A", "task_type": "order", "question_id": "Q2103"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "38d1fb24a6358eab0ac5acafe8dd1da39a3254774cdddde4e51d670bae70b4e7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) playing harp --> javelin throw --> cleaning toilet --> baking cookies\n(B) baking cookies --> javelin throw --> cleaning toilet --> playing harp\n(C) javelin throw --> playing harp --> baking cookies --> cleaning toilet\n(D) cleaning toilet --> baking cookies --> javelin throw --> playing harp\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) playing harp --> javelin throw --> cleaning toilet --> baking cookies\n(B) baking cookies --> javelin throw --> cleaning toilet --> playing harp\n(C) javelin throw --> playing harp --> baking cookies --> cleaning toilet\n(D) cleaning toilet --> baking cookies --> javelin throw --> playing harp\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2107, "doc": {"video_name": "order_206.mp4", "duration": 489.13, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> javelin throw --> zumba --> playing harp\n(B) javelin throw --> playing trombone --> zumba --> playing harp\n(C) playing trombone --> playing harp --> zumba --> javelin throw\n(D) zumba --> playing trombone --> playing harp --> javelin throw\n", "candidates": ["playing trombone --> javelin throw --> zumba --> playing harp", "javelin throw --> playing trombone --> zumba --> playing harp", "playing trombone --> playing harp --> zumba --> javelin throw", "zumba --> playing trombone --> playing harp --> javelin throw"], "answer": "A", "task_type": "order", "question_id": "Q2107"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aaf119ac27b11d2a555c9cc8d6f360ce3294132bde1938837ecdfcbd1e04da0f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> javelin throw --> zumba --> playing harp\n(B) javelin throw --> playing trombone --> zumba --> playing harp\n(C) playing trombone --> playing harp --> zumba --> javelin throw\n(D) zumba --> playing trombone --> playing harp --> javelin throw\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) playing trombone --> javelin throw --> zumba --> playing harp\n(B) javelin throw --> playing trombone --> zumba --> playing harp\n(C) playing trombone --> playing harp --> zumba --> javelin throw\n(D) zumba --> playing trombone --> playing harp --> javelin throw\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2111, "doc": {"video_name": "order_117.mp4", "duration": 669.22, "question": "Arrange the following events from the video in the correct chronological order: (1)A white car drives by in the background; (2)A black car drives by in the background; (3)Two people walk by in the background; (4)The ball is kicked into the camera.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 4->3->2->1\n", "candidates": ["2->1->3->4", "1->2->3->4", "3->2->1->4", "4->3->2->1"], "answer": "B", "task_type": "order", "question_id": "Q2111"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d7d9ee70ebde790f4875105d8503b5259012266a4f503363b6d463e42a3093d4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A white car drives by in the background; (2)A black car drives by in the background; (3)Two people walk by in the background; (4)The ball is kicked into the camera.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A white car drives by in the background; (2)A black car drives by in the background; (3)Two people walk by in the background; (4)The ball is kicked into the camera.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2115, "doc": {"video_name": "order_124.mp4", "duration": 702.7, "question": "Arrange the following events from the video in the correct chronological order: (1)The woman hangs the washed clothes on a line; (2)The woman fills a metal bucket with water; (3)The woman washes and scrubs clothes by hand; (4)The woman places a small wooden stool near a larger bucket.\n(A) 1->2->3->4\n(B) 3->2->4->1\n(C) 4->3->2->1\n(D) 2->4->3->1\n", "candidates": ["1->2->3->4", "3->2->4->1", "4->3->2->1", "2->4->3->1"], "answer": "D", "task_type": "order", "question_id": "Q2115"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f055b5da001aca7e1c9d735590e16d92a2507c067dd0d422c351c5b210e42cca", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The woman hangs the washed clothes on a line; (2)The woman fills a metal bucket with water; (3)The woman washes and scrubs clothes by hand; (4)The woman places a small wooden stool near a larger bucket.\n(A) 1->2->3->4\n(B) 3->2->4->1\n(C) 4->3->2->1\n(D) 2->4->3->1\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The woman hangs the washed clothes on a line; (2)The woman fills a metal bucket with water; (3)The woman washes and scrubs clothes by hand; (4)The woman places a small wooden stool near a larger bucket.\n(A) 1->2->3->4\n(B) 3->2->4->1\n(C) 4->3->2->1\n(D) 2->4->3->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2119, "doc": {"video_name": "order_38.mp4", "duration": 673.61, "question": "Arrange the following events from the video in the correct chronological order: (1)The word BIKE is overlaid on a mountain scene; (2)Oregon daily emerald logo and title card pops up; (3)The instructions follow with a man in a white ensemble and purple hat; (4)REPAIR is then overlaid under BIKE, Becoming BIKE REPAIR.\n(A) 1->2->4->3\n(B) 2->1->4->3\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "candidates": ["1->2->4->3", "2->1->4->3", "2->1->3->4", "1->2->3->4"], "answer": "B", "task_type": "order", "question_id": "Q2119"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "51a857f5afca684cc99501e85c0be697197d53e898a65afcfeee3b20ed6f9912", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The word BIKE is overlaid on a mountain scene; (2)Oregon daily emerald logo and title card pops up; (3)The instructions follow with a man in a white ensemble and purple hat; (4)REPAIR is then overlaid under BIKE, Becoming BIKE REPAIR.\n(A) 1->2->4->3\n(B) 2->1->4->3\n(C) 2->1->3->4\n(D) 1->2->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The word BIKE is overlaid on a mountain scene; (2)Oregon daily emerald logo and title card pops up; (3)The instructions follow with a man in a white ensemble and purple hat; (4)REPAIR is then overlaid under BIKE, Becoming BIKE REPAIR.\n(A) 1->2->4->3\n(B) 2->1->4->3\n(C) 2->1->3->4\n(D) 1->2->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2123, "doc": {"video_name": "order_293.mp4", "duration": 487.58, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) clean and jerk --> stomping grapes --> zumba --> water sliding\n(B) water sliding --> stomping grapes --> clean and jerk --> zumba\n(C) water sliding --> stomping grapes --> zumba --> clean and jerk\n(D) clean and jerk --> water sliding --> zumba --> stomping grapes\n", "candidates": ["clean and jerk --> stomping grapes --> zumba --> water sliding", "water sliding --> stomping grapes --> clean and jerk --> zumba", "water sliding --> stomping grapes --> zumba --> clean and jerk", "clean and jerk --> water sliding --> zumba --> stomping grapes"], "answer": "A", "task_type": "order", "question_id": "Q2123"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0a3387a6f551f97496a5f7e9f0ea3a6cacca5dfc7bf080a642777c4ad96434a6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) clean and jerk --> stomping grapes --> zumba --> water sliding\n(B) water sliding --> stomping grapes --> clean and jerk --> zumba\n(C) water sliding --> stomping grapes --> zumba --> clean and jerk\n(D) clean and jerk --> water sliding --> zumba --> stomping grapes\n", "task_type": "order", "pred_answer": "A", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) clean and jerk --> stomping grapes --> zumba --> water sliding\n(B) water sliding --> stomping grapes --> clean and jerk --> zumba\n(C) water sliding --> stomping grapes --> zumba --> clean and jerk\n(D) clean and jerk --> water sliding --> zumba --> stomping grapes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2127, "doc": {"video_name": "order_31.mp4", "duration": 665.1, "question": "Arrange the following events from the video in the correct chronological order: (1)A man on the street with a poster sign tries to get customers; (2)A university swim team is doing a fund raiser washing cars; (3)The students thank people in the video and to come support them; (4)A black screen appears with a website address.\n(A) 2->1->3->4\n(B) 1->3->2->4\n(C) 1->2->3->4\n(D) 2->3->1->4\n", "candidates": ["2->1->3->4", "1->3->2->4", "1->2->3->4", "2->3->1->4"], "answer": "A", "task_type": "order", "question_id": "Q2127"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c01d973ee454fb5b0649dfb5bdfa33239c48f99c3bb54d68b2ed1e71c365b11f", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A man on the street with a poster sign tries to get customers; (2)A university swim team is doing a fund raiser washing cars; (3)The students thank people in the video and to come support them; (4)A black screen appears with a website address.\n(A) 2->1->3->4\n(B) 1->3->2->4\n(C) 1->2->3->4\n(D) 2->3->1->4\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A man on the street with a poster sign tries to get customers; (2)A university swim team is doing a fund raiser washing cars; (3)The students thank people in the video and to come support them; (4)A black screen appears with a website address.\n(A) 2->1->3->4\n(B) 1->3->2->4\n(C) 1->2->3->4\n(D) 2->3->1->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2131, "doc": {"video_name": "order_338.mp4", "duration": 520.03, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) shredding paper --> making jewelry --> cooking sausages --> carving pumpkin\n(B) carving pumpkin --> making jewelry --> cooking sausages --> shredding paper\n(C) cooking sausages --> carving pumpkin --> making jewelry --> shredding paper\n(D) shredding paper --> carving pumpkin --> cooking sausages --> making jewelry\n", "candidates": ["shredding paper --> making jewelry --> cooking sausages --> carving pumpkin", "carving pumpkin --> making jewelry --> cooking sausages --> shredding paper", "cooking sausages --> carving pumpkin --> making jewelry --> shredding paper", "shredding paper --> carving pumpkin --> cooking sausages --> making jewelry"], "answer": "B", "task_type": "order", "question_id": "Q2131"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6fe978e8fa543964a2217990f3b021a96bb5b63a5c55965908e7b16daaff750c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) shredding paper --> making jewelry --> cooking sausages --> carving pumpkin\n(B) carving pumpkin --> making jewelry --> cooking sausages --> shredding paper\n(C) cooking sausages --> carving pumpkin --> making jewelry --> shredding paper\n(D) shredding paper --> carving pumpkin --> cooking sausages --> making jewelry\n", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) shredding paper --> making jewelry --> cooking sausages --> carving pumpkin\n(B) carving pumpkin --> making jewelry --> cooking sausages --> shredding paper\n(C) cooking sausages --> carving pumpkin --> making jewelry --> shredding paper\n(D) shredding paper --> carving pumpkin --> cooking sausages --> making jewelry\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2135, "doc": {"video_name": "order_4.mp4", "duration": 708.4100000000001, "question": "Arrange the following events from the video in the correct chronological order: (1)The group begins to dance in unison; (2)Some of the group are on their feet; (3)A group gathers to the center of a gym floor; (4)Some are in wheel chairs.\n(A) 1->2->3->4\n(B) 2->3->1->4\n(C) 4->3->2->1\n(D) 3->1->2->4\n", "candidates": ["1->2->3->4", "2->3->1->4", "4->3->2->1", "3->1->2->4"], "answer": "D", "task_type": "order", "question_id": "Q2135"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3eb177932bfd7f847103309ed5d42521d9ab7755a4263a42bf35c2cdec52130f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The group begins to dance in unison; (2)Some of the group are on their feet; (3)A group gathers to the center of a gym floor; (4)Some are in wheel chairs.\n(A) 1->2->3->4\n(B) 2->3->1->4\n(C) 4->3->2->1\n(D) 3->1->2->4\n", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The group begins to dance in unison; (2)Some of the group are on their feet; (3)A group gathers to the center of a gym floor; (4)Some are in wheel chairs.\n(A) 1->2->3->4\n(B) 2->3->1->4\n(C) 4->3->2->1\n(D) 3->1->2->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2139, "doc": {"video_name": "order_256.mp4", "duration": 490.03999999999996, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) shredding paper --> clean and jerk --> playing trombone --> javelin throw\n(B) javelin throw --> shredding paper --> playing trombone --> clean and jerk\n(C) clean and jerk --> javelin throw --> playing trombone --> shredding paper\n(D) shredding paper --> javelin throw --> clean and jerk --> playing trombone\n", "candidates": ["shredding paper --> clean and jerk --> playing trombone --> javelin throw", "javelin throw --> shredding paper --> playing trombone --> clean and jerk", "clean and jerk --> javelin throw --> playing trombone --> shredding paper", "shredding paper --> javelin throw --> clean and jerk --> playing trombone"], "answer": "B", "task_type": "order", "question_id": "Q2139"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b7bab4cc73a543eb193ced9b1cef950fc8c73a320210838d90e88cf66054e03f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) shredding paper --> clean and jerk --> playing trombone --> javelin throw\n(B) javelin throw --> shredding paper --> playing trombone --> clean and jerk\n(C) clean and jerk --> javelin throw --> playing trombone --> shredding paper\n(D) shredding paper --> javelin throw --> clean and jerk --> playing trombone\n", "task_type": "order", "pred_answer": "A", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) shredding paper --> clean and jerk --> playing trombone --> javelin throw\n(B) javelin throw --> shredding paper --> playing trombone --> clean and jerk\n(C) clean and jerk --> javelin throw --> playing trombone --> shredding paper\n(D) shredding paper --> javelin throw --> clean and jerk --> playing trombone\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2143, "doc": {"video_name": "order_175.mp4", "duration": 591.4, "question": "Arrange the following events from the video in the correct chronological order: (1)The interviewer plays with the dogs; (2)A man has dogs on a city street near a car; (3)We see a title screen over the UK flag; (4)We see a banner across the bottom of the screen and the man kneeling playing with his dogs.\n(A) 3->2->1->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 2->1->3->4\n", "candidates": ["3->2->1->4", "1->2->3->4", "4->3->2->1", "2->1->3->4"], "answer": "A", "task_type": "order", "question_id": "Q2143"}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4560238a7b4f4e0189d3b992ed7a745cfd3db197426fe0f81c505c580ede5f31", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The interviewer plays with the dogs; (2)A man has dogs on a city street near a car; (3)We see a title screen over the UK flag; (4)We see a banner across the bottom of the screen and the man kneeling playing with his dogs.\n(A) 3->2->1->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The interviewer plays with the dogs; (2)A man has dogs on a city street near a car; (3)We see a title screen over the UK flag; (4)We see a banner across the bottom of the screen and the man kneeling playing with his dogs.\n(A) 3->2->1->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2147, "doc": {"video_name": "order_5.mp4", "duration": 678.4100000000001, "question": "Arrange the following events from the video in the correct chronological order: (1)The group begins to dance in unison; (2)Some of the group are on their feet; (3)A group gathers to the center of a gym floor; (4)Some are in wheel chairs.\n(A) 1->2->3->4\n(B) 3->1->2->4\n(C) 2->3->1->4\n(D) 4->3->2->1\n", "candidates": ["1->2->3->4", "3->1->2->4", "2->3->1->4", "4->3->2->1"], "answer": "B", "task_type": "order", "question_id": "Q2147"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "82f3edc2b7db866dea6233750c2e8f3805b68569839ccb7919a8f0d1f1ecf508", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The group begins to dance in unison; (2)Some of the group are on their feet; (3)A group gathers to the center of a gym floor; (4)Some are in wheel chairs.\n(A) 1->2->3->4\n(B) 3->1->2->4\n(C) 2->3->1->4\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "D", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The group begins to dance in unison; (2)Some of the group are on their feet; (3)A group gathers to the center of a gym floor; (4)Some are in wheel chairs.\n(A) 1->2->3->4\n(B) 3->1->2->4\n(C) 2->3->1->4\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2151, "doc": {"video_name": "order_149.mp4", "duration": 694.78, "question": "Arrange the following events from the video in the correct chronological order: (1)The bowler from the blue team hits an overhand ball to the batter; (2)The bowler raises his hand to claim that the batter has not made a run; (3)The bowler causes the batter to get out by hitting the stumps behind him, the entire team cheers; (4)The batter walks out and another batter from his team comes on the field.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "1->2->3->4", "3->2->1->4", "2->1->3->4"], "answer": "B", "task_type": "order", "question_id": "Q2151"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "793702c447bee02d88e84d4cc74175ba559c7c5d8c62d0f0d5e011a3a02402af", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The bowler from the blue team hits an overhand ball to the batter; (2)The bowler raises his hand to claim that the batter has not made a run; (3)The bowler causes the batter to get out by hitting the stumps behind him, the entire team cheers; (4)The batter walks out and another batter from his team comes on the field.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The bowler from the blue team hits an overhand ball to the batter; (2)The bowler raises his hand to claim that the batter has not made a run; (3)The bowler causes the batter to get out by hitting the stumps behind him, the entire team cheers; (4)The batter walks out and another batter from his team comes on the field.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2155, "doc": {"video_name": "order_29.mp4", "duration": 679.6700000000001, "question": "Arrange the following events from the video in the correct chronological order: (1) Cheese is sprinkled on the spaghetti; (2) A plate of spaghetti is shown; (3) Vegetables are added to the pot; (4) All of the contents get mixed and cooked.\n(A) 4->3->2->1\n(B) 2->3->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "2->3->1->4", "1->2->3->4", "2->1->3->4"], "answer": "D", "task_type": "order", "question_id": "Q2155"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1144e01f185d3f96fb86ef18ce9981f9ebd5c241f6814e6daeec44b78797811c", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) Cheese is sprinkled on the spaghetti; (2) A plate of spaghetti is shown; (3) Vegetables are added to the pot; (4) All of the contents get mixed and cooked.\n(A) 4->3->2->1\n(B) 2->3->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1) Cheese is sprinkled on the spaghetti; (2) A plate of spaghetti is shown; (3) Vegetables are added to the pot; (4) All of the contents get mixed and cooked.\n(A) 4->3->2->1\n(B) 2->3->1->4\n(C) 1->2->3->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2159, "doc": {"video_name": "order_26.mp4", "duration": 688.0699999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)Two people are paddling down rapids on a river in canoes; (2)One of them stops at a bank where there is a person in a blue canoe; (3)People are seen in a group large red tube rapids ride; (4)They pass by a building and then fall into the water.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "candidates": ["4->3->2->1", "1->2->3->4", "3->2->1->4", "2->1->3->4"], "answer": "B", "task_type": "order", "question_id": "Q2159"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "01c6c684b3cd4ba91736e2f332db6915c98fc055fab10a4a06bd22ff4c06ab93", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Two people are paddling down rapids on a river in canoes; (2)One of them stops at a bank where there is a person in a blue canoe; (3)People are seen in a group large red tube rapids ride; (4)They pass by a building and then fall into the water.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Two people are paddling down rapids on a river in canoes; (2)One of them stops at a bank where there is a person in a blue canoe; (3)People are seen in a group large red tube rapids ride; (4)They pass by a building and then fall into the water.\n(A) 4->3->2->1\n(B) 1->2->3->4\n(C) 3->2->1->4\n(D) 2->1->3->4\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2163, "doc": {"video_name": "order_22.mp4", "duration": 667.29, "question": "Arrange the following events from the video in the correct chronological order: (1)Mestre Calango performs by the water on the pier; (2)View of a large body of water with a city around it; (3)Mestre Calango takes his shirt and shoes off and performs on the beach; (4)Credits overlay a black screen.\n(A) 2->3->1->4\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 4->3->2->1\n", "candidates": ["2->3->1->4", "1->2->3->4", "2->1->3->4", "4->3->2->1"], "answer": "C", "task_type": "order", "question_id": "Q2163"}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68c1520080dea0be8cca78c1ab57b9efde3c8bb953545cde71f1f21209f09fb3", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Mestre Calango performs by the water on the pier; (2)View of a large body of water with a city around it; (3)Mestre Calango takes his shirt and shoes off and performs on the beach; (4)Credits overlay a black screen.\n(A) 2->3->1->4\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 4->3->2->1\n", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Mestre Calango performs by the water on the pier; (2)View of a large body of water with a city around it; (3)Mestre Calango takes his shirt and shoes off and performs on the beach; (4)Credits overlay a black screen.\n(A) 2->3->1->4\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 4->3->2->1\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2167, "doc": {"video_name": "order_233.mp4", "duration": 490.03, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> cleaning toilet --> stomping grapes --> riding mule\n(B) riding mule --> cleaning toilet --> javelin throw --> stomping grapes\n(C) riding mule --> stomping grapes --> javelin throw --> cleaning toilet\n(D) javelin throw --> riding mule --> cleaning toilet --> stomping grapes\n", "candidates": ["javelin throw --> cleaning toilet --> stomping grapes --> riding mule", "riding mule --> cleaning toilet --> javelin throw --> stomping grapes", "riding mule --> stomping grapes --> javelin throw --> cleaning toilet", "javelin throw --> riding mule --> cleaning toilet --> stomping grapes"], "answer": "D", "task_type": "order", "question_id": "Q2167"}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "808fe26b3410dedceffb47f0f3db9ac7a56c734cd8352d6e0799bc50c0c7fe26", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> cleaning toilet --> stomping grapes --> riding mule\n(B) riding mule --> cleaning toilet --> javelin throw --> stomping grapes\n(C) riding mule --> stomping grapes --> javelin throw --> cleaning toilet\n(D) javelin throw --> riding mule --> cleaning toilet --> stomping grapes\n", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> cleaning toilet --> stomping grapes --> riding mule\n(B) riding mule --> cleaning toilet --> javelin throw --> stomping grapes\n(C) riding mule --> stomping grapes --> javelin throw --> cleaning toilet\n(D) javelin throw --> riding mule --> cleaning toilet --> stomping grapes\n\nOnly give the best option.\nBest option: ("}
{"doc_id": 2171, "doc": {"video_name": "order_303.mp4", "duration": 513.05, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) javelin throw --> zumba --> riding mule --> paragliding\n(B) riding mule --> javelin throw --> paragliding --> zumba\n(C) paragliding --> javelin throw --> riding mule --> zumba\n(D) riding mule --> zumba --> javelin throw --> paragliding\n", "candidates": ["javelin throw --> zumba --> riding mule --> paragliding", "riding mule --> javelin throw --> paragliding --> zumba", "paragliding --> javelin throw --> riding mule --> zumba", "riding mule --> zumba --> javelin throw --> paragliding"], "answer": "B", "task_type": "order", "question_id": "Q2171"}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B", "B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6961e1efca0b49d19646e1c8752482bce8cbdbd98377ea2e5faee6953c5e8d1f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) javelin throw --> zumba --> riding mule --> paragliding\n(B) riding mule --> javelin throw --> paragliding --> zumba\n(C) paragliding --> javelin throw --> riding mule --> zumba\n(D) riding mule --> zumba --> javelin throw --> paragliding\n", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) javelin throw --> zumba --> riding mule --> paragliding\n(B) riding mule --> javelin throw --> paragliding --> zumba\n(C) paragliding --> javelin throw --> riding mule --> zumba\n(D) riding mule --> zumba --> javelin throw --> paragliding\n\nOnly give the best option.\nBest option: ("}
